
\chapter{The logic of types. III. The Curry-Howard correspondence\label{chap:5-Curry-Howard}}

\global\long\def\gunderline#1{\mathunderline{greenunder}{#1}}%
\global\long\def\bef{\forwardcompose}%
\global\long\def\bbnum#1{\custombb{#1}}%
Fully parametric functions (introduced in Section~\ref{sec:Fully-parametric-functions})
perform operations so general that their code works in the same way
for all types. An example of a fully parametric function is:
\begin{lstlisting}
def before[A, B, C](f: A => B, g: B => C): A => C = { x => g(f(x)) }
\end{lstlisting}

We have seen in Section~\ref{subsec:Deriving-a-function-s-code}
that for certain functions of this kind one can derive the code unambiguously
from the type signature. There exists a mathematical theory (called
the \textbf{Curry-Howard correspondence}) that gives precise conditions
for the possibility of deriving a function\textsf{'}s code from its type. There
is also a systematic derivation algorithm that either produces the
function\textsf{'}s code or proves that the given type signature cannot be
implemented. This chapter describes the main results and applications
of that theory to functional programming.

\section{Values computed by fully parametric functions}

\subsection{Motivation and outlook\label{subsec:Motivation-and-outlook}}

Consider the following sketch of a fully parametric function\textsf{'}s Scala
code:
\begin{lstlisting}
def f[A, B, ...]: ... = {
  val x: Either[A, B] = ... // Some expression here.
  ... 
}
\end{lstlisting}
If this program compiles without type errors, it means that the types
match and, in particular, that the function \lstinline!f! is able
to compute a value \lstinline!x! of type \lstinline!Either[A, B]!.

It is sometimes \emph{impossible} to compute a value of a certain
type in fully parametric code. For example, the fully parametric function
\lstinline!fmap! shown in Example~\ref{subsec:Disjunctive-Example-option-1}
cannot compute a value of type \lstinline!A!:
\begin{lstlisting}
def fmap[A, B](f: A => B): Option[A] => Option[B] = {
  val x: A = ???            // Cannot compute x here!
  ...
}
\end{lstlisting}
The reason is that no fully parametric code can compute values of
type \lstinline!A! \textsf{``}from scratch\textsf{''}, that is, without using any
previously given value of type \lstinline!A! and without applying
a previously given function that returns a value of type \lstinline!A!.
In \lstinline!fmap!, no values of type \lstinline!A! are given as
arguments; the given function \lstinline!f: A => B! returns values
of type \lstinline!B! and not \lstinline!A!. The code of \lstinline!fmap!
must perform pattern matching on an argument of type \lstinline!Option[A]!:
\begin{lstlisting}
def fmap[A, B](f: A => B)(pa: Option[A]): Option[B] = pa match {
  case None       => 
    val x: A = ??? // Cannot compute x here!
    ...
  case Some(a)    =>
    val x: A = a   // Can compute x in this scope.
    ...
}
\end{lstlisting}
Since the case \lstinline!None! has no values of type \lstinline!A!,
we are unable to compute a value \lstinline!x! in that scope (as
long as \lstinline!fmap! remains a fully parametric function). 

\textsf{``}Being able\textsf{''} to compute \lstinline!x: A! means that, if needed,
the code should be able to \emph{return} \lstinline!x! as a result
value. This requires computing \lstinline!x! in all cases, not just
within one part (\lstinline!case ...!) of a pattern-matching expression.
In other words, we should be able to implement the following type
signature:
\begin{lstlisting}
def bad[A, B](f: A => B)(pa: Option[A]): A = ???
\end{lstlisting}

So, the question \textsf{``}can we compute a value of type \lstinline!A!
within a fully parametric function with arguments of type \lstinline!B!
and \lstinline!C!\textsf{''} is equivalent to the question \textsf{``}can be implement
a fully parametric function of type \lstinline!(B, C) => A!\textsf{''}. From
now on, we will focus on the latter kind of questions.

Here are some other examples where \emph{no} fully parametric code
can implement a given type signature: 
\begin{lstlisting}
def bad2[A, B](f: A => B): A = ???

def bad3[A, B, C](p: A => Either[B, C]): Either[A => B, A => C] = ???
\end{lstlisting}

The problem with \lstinline!bad2! is that no data of type \lstinline!A!
is given, while the given function \lstinline!f! returns values of
type \lstinline!B!, not \lstinline!A!. 

The problem with \lstinline!bad3! is that it needs to hard-code the
decision of whether to return the \lstinline!Left! or the \lstinline!Right!
part of \lstinline!Either!. That decision cannot depend on the function
\lstinline!p! because one cannot pattern-match on a function, and
because \lstinline!bad3! does not receive any data of type \lstinline!A!
and so cannot call \lstinline!p!. Suppose \lstinline!bad3! is hard-coded
to always return a \lstinline!Left(f)! with some \lstinline!f: A => B!.
It is then necessary to compute \lstinline!f! from \lstinline!p!,
but that is impossible: the given function \lstinline!p! may return
either \lstinline!Left(b)! or \lstinline!Right(c)! for different
values of its argument (of type \lstinline!A!). This data is insufficient
to create a function of type \lstinline!A => B!. Similarly, \lstinline!bad3!
is not able to return \lstinline!Right(f)! with some \lstinline!f: A => C!.

In all these examples, we see that the impossibility of implementing
a type signature means that the information given in a function\textsf{'}s
arguments is in some way insufficient for computing the result value.

The type signature inverse to that of \lstinline!bad3! is:
\begin{lstlisting}
def good3[A, B, C](q: Either[A => B, A => C]): A => Either[B, C] = ???
\end{lstlisting}
This type signature \emph{can} be implemented, but only in one way:
\begin{lstlisting}
def good3[A, B, C](q: Either[A => B, A => C]): A => Either[B, C] = q match {
  case Left(k)    => { a => Left(k(a))  }
  case Right(k)   => { a => Right(k(a)) }
}
\end{lstlisting}

So, when working with fully parametric code and looking at some type
signature of a function, we may ask the question \textemdash{} is
that type signature implementable, and if so, can we derive the code
by just \textsf{``}following the types\textsf{''}?

It is remarkable that this question makes sense at all. When working
with non-FP languages, the notion of fully parametric functions is
usually not relevant, and implementations cannot be derived from types.
But in functional programming, fully parametric functions are used
often. It is then important for the programmer to know whether a given
fully parametric type signature can be implemented, and if so, to
be able to derive the code.

Can we prove rigorously that the functions \lstinline!bad!, \lstinline!bad2!,
\lstinline!bad3! cannot be implemented by any fully parametric code?
Or, perhaps, we were mistaken and a clever trick \emph{could} produce
some code for those type signatures? 

So far, we only saw informal arguments about whether values of certain
types can be computed. To make those arguments rigorous, we need to
translate statements such as \textsf{``}a fully parametric function \lstinline!before!
can compute a value of type \lstinline!C => A!\textsf{''} into mathematical
formulas with rules for proving them true or false.

The first step towards a rigorous mathematical formulation is to choose
a precise notation. In Section~\ref{subsec:Disjunctions-and-conjunctions},
we denoted by ${\cal CH}(A)$ the proposition \textsf{``}we ${\cal C}\!$an
${\cal H}\!$ave a value of type $A$ within a fully parametric function\textsf{''}.
When writing the code of that function, we may use the function\textsf{'}s
arguments, which might have types, say, $X$, $Y$, ..., $Z$. So,
we are interested in proving statements like this:
\begin{align}
 & \text{a fully parametric expression can compute a value of type }A\nonumber \\
 & \text{using previously given values of types }X,Y,...,Z\quad.\label{eq:ch-CH-proposition-def}
\end{align}
Here $X$, $Y$, ..., $Z$, $A$ may be either type parameters or
more complicated type expressions, such as $B\rightarrow C$ or $(C\rightarrow D)\rightarrow E$,
built from some type parameters.

If values of types $X$, $Y$, ..., $Z$ are given, it means we \textsf{``}already
have\textsf{''} values of those types, i.e., the propositions ${\cal CH}(X)$,
${\cal CH}(Y)$, ..., ${\cal CH}(Z)$ will be already true. So, proposition~(\ref{eq:ch-CH-proposition-def})
is equivalent to \textsf{``}${\cal CH}(A)$ is true assuming ${\cal CH}(X)$,
${\cal CH}(Y)$, ..., ${\cal CH}(Z)$ are true\textsf{''}. In mathematical
logic, a statement of this form is called a \textbf{sequent} and\index{sequent (in logic)}
is denoted using the symbol $\vdash$ (called the \textsf{``}\textbf{turnstile}\textsf{''}):\index{0@$\vdash$ (turnstile) symbol}\index{turnstile (vdash) symbol@turnstile ($\vdash$) symbol}
\begin{align}
{\color{greenunder}\text{sequent}:}\quad & {\cal CH}(X),{\cal CH}(Y),...,{\cal CH}(Z)\vdash{\cal CH}(A)\quad.\label{eq:ch-example-sequent}
\end{align}
The assumptions ${\cal CH}(X)$, ${\cal CH}(Y)$, ..., ${\cal CH}(Z)$
are called \textbf{premises}\index{sequent (in logic)!premises} and
the proposition ${\cal CH}(A)$ is called the \textbf{goal}\index{sequent (in logic)!goal}
of the sequent.

Sequents provide a notation for the questions about types of fully
parametric functions. Since our goal is to answer such questions rigorously,
we will need to be able to \emph{prove} sequents of the form~(\ref{eq:ch-example-sequent}).
The following sequents correspond to the type signatures we just saw:
\begin{align*}
{\color{greenunder}\text{\texttt{fmap} for \texttt{Option}}:}\quad & {\cal CH}(\text{\texttt{A => B}})\vdash{\cal CH}(\text{\texttt{Option[A] => Option[B]}})\\
{\color{greenunder}\text{the function \texttt{before}}:}\quad & {\cal CH}(\text{\texttt{A => B}}),{\cal CH}(\text{\texttt{B => C}})\vdash{\cal CH}(\text{\texttt{A => C}})\\
{\color{greenunder}\text{the function }\text{\texttt{bad}}:}\quad & {\cal CH}(\text{\texttt{A => B}}),{\cal CH}(\text{\texttt{Option[A]}})\vdash{\cal CH}(\text{\texttt{A}})\\
{\color{greenunder}\text{the function }\text{\texttt{bad2}}:}\quad & {\cal CH}(\text{\texttt{A => B}}),{\cal CH}(\text{\texttt{B => C}})\vdash{\cal CH}(\text{\texttt{A}})
\end{align*}
So far, we only saw informal arguments towards proving the first two
sequents and disproving the last two. We will now develop tools for
proving sequents rigorously. 

In formal logic, sequents are proved\index{proof (in logic)} by starting
with certain axioms and following certain derivation rules. Different
choices of axioms and derivation rules will give different \emph{logics}.
We will need to discover the correct logic for reasoning about sequents
with ${\cal CH}$-propositions. To discover that logic\textsf{'}s complete
set of axioms and derivation rules, we need to examine systematically
what types and code constructions are possible in a fully parametric
function. The resulting logic is known under the name \textsf{``}constructive
logic\textsf{''}. That logic\textsf{'}s axioms and derivation rules directly correspond
to programming language constructions allowed by fully parametric
code. For that reason, constructive logic gives correct answers about
implementable and non-implementable type signatures of fully parametric
functions.

We will then be able to borrow the results and methods already available
in the mathematical literature on formal logic. The main result is
an algorithm (called LJT) for finding a proof for a given sequent
in the constructive logic. If a proof is found, the algorithm also
provides the code of a function that has the type signature corresponding
to the sequent. If a proof is not found, it means that the given type
signature cannot be implemented by fully parametric code.

\subsection{Type notation and ${\cal CH}$-propositions for standard type constructions\label{subsec:Type-notation-and-standard-type-constructions}}

Here and in the following sections, we will be reasoning about sequents
of the form:
\[
{\cal CH}(X),{\cal CH}(Y),...,{\cal CH}(Z)\vdash{\cal CH}(A)
\]
that represent type signatures of fully parametric functions. It will
be convenient to shorten the notation and to denote the set of all
premises by the symbol $\Gamma$. We will then write just $\Gamma\vdash{\cal CH}(A)$
instead of ${\cal CH}(X),{\cal CH}(Y),...,{\cal CH}(Z)\vdash{\cal CH}(A)$. 

In Section~\ref{subsec:Disjunctions-and-conjunctions} we saw examples
of reasoning about ${\cal CH}$-propositions for case classes and
for disjunctive types. We will now extend this reasoning systematically
to all type constructions that fully parametric programs could use.
The result will be that we rewrite ${\cal CH}$-propositions with
arbitrary type expressions, such as ${\cal CH}($\lstinline!Either[(A, A), Option[B] => Either[(A, B), C]]!$)$,
in terms of ${\cal CH}$-propositions for simple type parameters:
${\cal CH}(A)$, ${\cal CH}(B)$, etc. A special type notation\index{type notation}
explained in this section will help us write type expressions more
concisely. (See Appendix~\ref{chap:Appendix-Notations} on page~\pageref{chap:Appendix-Notations}
for a summary of the type notation.)

There exist \textbf{six} \textbf{standard type constructions}\index{six type constructions}
supported by all functional languages: primitive types, including
the \lstinline!Unit! type and the void type (\lstinline!Nothing!),
tuples (also called \index{product types}\textbf{product types}),
disjunctive types (also called \index{co-product types}\textbf{co-product
types}), function types, parameterized types, and recursive types.
We will now derive the rules for writing ${\cal CH}$-propositions
for each of these type constructions except recursive types.

\paragraph{1a) Rule for the \texttt{Unit} type}

The \lstinline!Unit! type has only a single value \lstinline!()!,
an \textsf{``}empty tuple\textsf{''}. This value can be \emph{always} computed since
it does not need any previous data:
\begin{lstlisting}
def f[...]: ... = {
  ...
  val x: Unit = () // We can always compute a `Unit` value.
  ...
\end{lstlisting}
So, the proposition ${\cal CH}($\lstinline!Unit!$)$ is always true.
In the type notation, the \lstinline!Unit! type is denoted by $\bbnum 1$.
We may write the rule as $\mathcal{CH}(\bbnum 1)=True$. 

Named unit types\index{unit type!named} also have a single value
that is always possible to compute. For example:
\begin{lstlisting}
final case class N1()
\end{lstlisting}
defines a named unit type. We can compute a value of type \lstinline!N1!
without using any other given values:
\begin{lstlisting}
val x: N1 = N1()
\end{lstlisting}
So, the proposition ${\cal CH}($\lstinline!N1!$)$ is always true.
In the type notation, named unit types are also denoted by $\bbnum 1$,
just as the \lstinline!Unit! type itself.

\paragraph{1b) Rule for the void type}

The Scala type \lstinline!Nothing! has no values, so the proposition
${\cal CH}($\lstinline!Nothing!$)$ is always false. The type \lstinline!Nothing!
is denoted by $\bbnum 0$ in the type notation. So, the rule is $\mathcal{CH}(\bbnum 0)=False$.

\paragraph{1c) Rule for primitive types}

For a specific primitive (or library-defined) type such as \lstinline!Int!
or \lstinline!String!, the corresponding ${\cal CH}$-proposition
is \emph{always true} because we may use a constant value, e.g.:
\begin{lstlisting}
def f[...]: ... {
   ...
   val x: String = "abc" // We can always compute a `String` value.
   ...
}
\end{lstlisting}
So, the rule for primitive types is the same as that for the \lstinline!Unit!
type. For example, $\mathcal{CH}(\text{String})=True$.

\paragraph{2) Rule for tuple types}

To compute a value of a tuple type \lstinline!(A, B)! requires computing
a value of type \lstinline!A! \emph{and} a value of type \lstinline!B!.
This is expressed by the logical formula ${\cal CH}($\lstinline!(A, B)!$)={\cal CH}(A)\wedge{\cal CH}(B)$.
A similar formula holds for case classes, as Eq.~(\ref{eq:curry-howard-example-case-class})
shows. In the type notation, the tuple \lstinline!(A, B)! is written
as $A\times B$. Tuples and case classes with more than two parts
are denoted by $A\times B\times...\times C$. For example, the Scala
definition:
\begin{lstlisting}
case class Person(firstName: String, lastName: String, age: Int)
\end{lstlisting}
is written in the type notation as $\text{String}\times\text{String}\times\text{Int}$.
So, the rule for tuples is:
\[
{\cal CH}\left(A\times B\times...\times C\right)={\cal CH}(A)\wedge{\cal CH}(B)\wedge...\wedge{\cal CH}(C)\quad.
\]


\paragraph{3) Rule for disjunctive types}

A disjunctive type may consist of several cases. Having a value of
a disjunctive type means to have a value of (at least) one of those
cases. An example of translating this relationship into a formula
was shown by Eq.~(\ref{eq:curry-howard-example-disjunction}). For
the standard disjunctive type \lstinline!Either[A, B]!, we have the
logical formula ${\cal CH}($\lstinline!Either[A, B]!$)={\cal CH}(A)\vee{\cal CH}(B)$.
In the type notation, the Scala type \lstinline!Either[A, B]! is
written as $A+B$. As another example, the Scala definition:
\begin{lstlisting}
sealed trait RootsOfQ
final case class NoRoots()                      extends RootsOfQ
final case class OneRoot(x: Double)             extends RootsOfQ
final case class TwoRoots(x: Double, y: Double) extends RootsOfQ
\end{lstlisting}
is translated to the type notation as:
\[
\text{RootsOfQ}\triangleq\bbnum 1+\text{Double}+\text{Double}\times\text{Double}\quad.
\]
The type notation is significantly shorter because it omits all case
class names and part names from the type definitions. Using the type
notation, the rule for disjunctive types is written as:
\[
{\cal CH}\left(A+B+...+C\right)={\cal CH}(A)\vee{\cal CH}(B)\vee...\vee{\cal CH}(C)\quad.
\]


\paragraph{4) Rule for function types}

Consider now a function type such as \lstinline!A => B!. This type
is written in the type notation as $A\rightarrow B$. To compute a
value of that type, we need to write code like this:
\begin{lstlisting}
val f: A => B = { (a: A) =>
  ??? // Compute a value of type B in this scope.
}
\end{lstlisting}
The inner scope of this function needs to compute a value of type
$B$, and the given value \lstinline!a: A! may be used for that.
So, ${\cal CH}(A\rightarrow B)$ is true if and only if we are able
to compute a value of type $B$ when we are given a value of type
$A$. To translate this statement into the language of logical propositions,
we need to use the logical\index{logical implication} \textbf{implication},
${\cal CH}(A)\Rightarrow{\cal CH}(B)$, which means that ${\cal CH}(B)$
can be proved if we already have a proof of ${\cal CH}(A)$. So, the
rule for function types is:
\[
{\cal CH}(A\rightarrow B)={\cal CH}(A)\Rightarrow{\cal CH}(B)\quad.
\]


\paragraph{5) Rule for parameterized types}

Here is an example of a function with type parameters:
\begin{lstlisting}
def f[A, B]: A => (A => B) => B = { x => g => g(x) }
\end{lstlisting}
Being able to define the body of such a function is the same as being
able to compute a value of type \lstinline!A => (A => B) => B! for
\emph{all} possible Scala types \lstinline!A! and \lstinline!B!.
In the notation of logic, this is written as:
\[
{\cal CH}\left(\forall(A,B).\,A\rightarrow(A\rightarrow B)\rightarrow B\right)\quad,
\]
and is equivalent to:
\[
\forall(A,B).\,{\cal CH}\left(A\rightarrow(A\rightarrow B)\rightarrow B\right)\quad.
\]
The symbol $\forall$ means \textsf{``}for all\textsf{''} and is called the \index{universal quantifier (forall)@universal quantifier ($\forall$)}\textbf{universal
quantifier} in logic. We read $\forall A.\,{\cal CH}(B)$ as the proposition
\textsf{``}for all types $A$, we can compute a value of type $B$\textsf{''}.

So, the rule for parameterized types with the type notation $\forall A.\,F^{A}$
is:
\[
{\cal CH}(\forall A.\,F^{A})=\forall A.\,{\cal CH}(F^{A})\quad.
\]

The type notation for the type signature of \lstinline!f! is written
in one of the following ways:
\[
f^{A,B}:A\rightarrow\left(A\rightarrow B\right)\rightarrow B\quad,\quad\text{or equivalently}:\quad f:\forall(A,B).\,A\rightarrow\left(A\rightarrow B\right)\rightarrow B\quad.
\]
The type quantifier (\textsf{``}for all $A$ and $B$\textsf{''}) indicates that $f$
can be used with all types $A$ and $B$.

In Scala, longer type expressions can be named, and those names (called
\textbf{type aliases}\index{type alias}) can be used to make code
shorter. Type aliases may also contain type parameters. Defining and
using a type alias for the type of the function \lstinline!f! looks
like this:
\begin{lstlisting}
type F[A, B] = A => (A => B) => B
def f[A, B]: F[A, B] = { x => g => g(x) }
\end{lstlisting}
This is written in the type notation by placing all type parameters
into superscripts:
\begin{align*}
F^{A,B} & \triangleq A\rightarrow\left(A\rightarrow B\right)\rightarrow B\quad,\\
f^{A,B}:F^{A,B} & \triangleq x^{:A}\rightarrow g^{:A\rightarrow B}\rightarrow g(x)\quad,
\end{align*}
or equivalently (although somewhat less readably) as:
\[
f:\big(\forall(A,B).\,F^{A,B}\big)\triangleq\forall(A,B).\,x^{:A}\rightarrow g^{:A\rightarrow B}\rightarrow g(x)\quad.
\]

In Scala 3, the function \lstinline!f! can be written as a value
(\lstinline!val!) via this syntax:
\begin{lstlisting}
val f: [A, B] => A => (A => B) => B = {   // Valid only in Scala 3.
  [A, B] => (x: A) => (g: A => B) => g(x)
}
\end{lstlisting}
This syntax closely corresponds to the code notation $\forall(A,B).\,x^{:A}\rightarrow g^{:A\rightarrow B}\rightarrow g(x)$.

Case classes and disjunctive types use \emph{names} for the types
and their parts. However, those names only add convenience for programmers
and do not affect the computational properties of types. The type
notation is designed to support nameless type expressions.

The rules just shown will allow us to express ${\cal CH}$-propositions
for complicated types via ${\cal CH}$-propositions for type parameters.
Then any type signature can be rewritten as a sequent that contains
${\cal CH}$-propositions only for the individual type parameters. 

In this way, we see a correspondence between a fully parametric type
signature and a logical sequent that expresses the statement \textsf{``}the
type signature can be implemented\textsf{''}. This is the first part of the
Curry-Howard correspondence.

Table~\ref{tab:ch-correspondence-type-notation-CH-propositions}
summarizes the type notation and shows how to translate it into logic
formulas with ${\cal CH}$-propositions. Apart from recursive types
(which we do not consider in this chapter), Table~\ref{tab:ch-correspondence-type-notation-CH-propositions}
lists all type constructions that may be used in the code of a fully
parametric function.

The precedence\index{type notation!operator precedence} of operators
in the type notation is chosen to have fewer parentheses in the type
expressions that are frequently used. The rules of precedence are:
\begin{itemize}
\item The type product operator ($\times$) groups stronger than the disjunctive
operator ($+$), so that type expressions such as $A+B\times C$ have
the same operator precedence as in standard arithmetic. That is, $A+B\times C$
means $A+\left(B\times C\right)$. This convention makes type expressions
easier to read.
\item The function type arrow ($\rightarrow$) groups weaker than the operators
$+$ and $\times$, so that often-used types such as $A\rightarrow\bbnum 1+B$
(representing \lstinline!A => Option[B]!) or $A\times B\rightarrow C$
(representing \lstinline!((A, B)) => C!) can be written without any
parentheses. Type expressions such as $\left(A\rightarrow B\right)\times C$
will require parentheses but are needed less often.
\item The type quantifiers group weaker than all other operators, so we
can write types such as $\forall A.\,A\rightarrow A\rightarrow A$
without parentheses. This is helpful because type quantifiers are
most often placed at the top level of a type expression. When that
is not the case, parentheses are necessary, e.g., in the type expression
$\left(\forall A.\,A\rightarrow A\rightarrow A\right)\rightarrow\bbnum 1+\bbnum 1$.
\end{itemize}
\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\small{}Scala syntax} & \textbf{\small{}Type notation} & \textbf{\small{}${\cal CH}$-proposition}\tabularnewline
\hline 
\hline 
\lstinline![A]! & $A$ & ${\cal CH}(A)$\tabularnewline
\hline 
\lstinline!(A, B)! & $A\times B$ & ${\cal CH}(A)$ $\wedge$ ${\cal CH}(B)$\tabularnewline
\hline 
\lstinline!Either[A, B]! & $A+B$ & ${\cal CH}(A)$ $\vee$ ${\cal CH}(B)$\tabularnewline
\hline 
\lstinline!A => B! & $A\rightarrow B$ & ${\cal CH}(A)$ $\Rightarrow$ ${\cal CH}(B)$\tabularnewline
\hline 
\lstinline!Unit! & $\bbnum 1$ & ${\cal CH}(\bbnum 1)=True$\tabularnewline
\hline 
{\small{}}\lstinline!Int!{\small{}, }\lstinline!String!{\small{},
...} & {\small{}$\text{Int}$, $\text{String}$, ...} & ${\cal CH}(\text{Int})=True$\tabularnewline
\hline 
\lstinline!Nothing! & $\bbnum 0$ & ${\cal CH}(\bbnum 0)=False$\tabularnewline
\hline 
\lstinline!def f[A]: F[A]! & $f^{A}:F^{A}$ & $\forall A.\,{\cal CH}(F^{A})$\tabularnewline
\hline 
\lstinline!val f: [A] => F[A]!{\small{} (Scala 3)} & $f:\forall A.\,F^{A}$ & $\forall A.\,{\cal CH}(F^{A})$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{The correspondence\index{type notation} between types and ${\cal CH}$-propositions.\label{tab:ch-correspondence-type-notation-CH-propositions}}
\end{table}


\subsection{Examples: Type notation\index{examples (with code)}}

From now on, we will prefer to write types in the type notation rather
than in the Scala syntax. The type notation allows us to write nameless
type expressions and makes the structure of complicated types clearer
than in the Scala syntax. Names of types and parts of types are, of
course, helpful for reminding programmers of the meaning of data in
a program. However, writing names for every part of every type does
not help reasoning about the properties of types. Once the programmer
has finished deriving the necessary types and verifying their properties,
the type notation can be straightforwardly translated into Scala code.
Let us get some experience doing that.

\subsubsection{Example \label{subsec:Example-ch-dupl-function}\ref{subsec:Example-ch-dupl-function}}

Define a function \lstinline!delta! taking an argument \lstinline!x!
and returning the pair \lstinline!(x, x)!. Derive the most general
type for this function. Write the type signature of \lstinline!delta!
in the type notation, and translate it into a ${\cal CH}$-proposition.
Simplify the ${\cal CH}$-proposition if possible.

\subparagraph{Solution}

Begin by writing the code of the function:
\begin{lstlisting}
def delta(x: ...) = (x, x)
\end{lstlisting}
To derive the most general type for \lstinline!delta!, first assume
\lstinline!x: A!, where \lstinline!A! is a type parameter; then
the tuple \lstinline!(x, x)! has type \lstinline!(A, A)!. We do
not see any constraints on the type parameter \lstinline!A!. So,
\lstinline!A! represents an arbitrary type and needs to be added
to the type signature of \lstinline!delta!:
\begin{lstlisting}
def delta[A](x: A): (A, A) = (x, x)
\end{lstlisting}
We find that the most general type of \lstinline!delta! is \lstinline!A => (A, A)!.
We also note that \lstinline!delta! seems to be the only way of implementing
a fully parametric function with type signature \lstinline!A => (A, A)!.

We will use the letter $\Delta$ for the function \lstinline!delta!.
In the type notation, the type signature of $\Delta$ is:
\[
\Delta^{A}:A\rightarrow A\times A\quad.
\]
So, the proposition ${\cal CH}(\Delta)$ (meaning \textsf{``}the function
$\Delta$ can be implemented\textsf{''}) is:
\[
{\cal CH}(\Delta)=\forall A.{\cal \,CH}\left(A\rightarrow A\times A\right)\quad.
\]
In the type expression $A\rightarrow A\times A$, the product symbol
($\times$) binds stronger than the function arrow ($\rightarrow$),
so the parentheses in $A\rightarrow\left(A\times A\right)$ may be
omitted.

Using the rules for transforming ${\cal CH}$-propositions, we rewrite:
\begin{align*}
 & {\cal CH}(A\rightarrow A\times A)\\
{\color{greenunder}\text{rule for function types}:}\quad & ={\cal CH}(A)\Rightarrow{\cal CH}(A\times A)\\
{\color{greenunder}\text{rule for tuple types}:}\quad & ={\cal CH}(A)\Rightarrow\left({\cal CH}(A)\wedge{\cal CH}(A)\right)\quad.
\end{align*}
Thus the proposition ${\cal CH}(\Delta)$ is equivalent to:
\[
{\cal CH}(\Delta)=\forall A.\,{\cal CH}(A)\Rightarrow({\cal CH}(A)\wedge{\cal CH}(A))\quad.
\]

It is intuitively clear that the proposition ${\cal CH}(\Delta)$
is true: it just says that if ${\cal CH}(A)$ is true then ${\cal CH}(A)$
and ${\cal CH}(A)$ is true. The point of writing ${\cal CH}(\Delta)$
in a mathematical notation is to prepare for proving this proposition
rigorously rather than based on intuition.

\subsubsection{Example \label{subsec:Example-ch-notation-function-1}\ref{subsec:Example-ch-notation-function-1}}

The standard types \lstinline!Either[A, B]! and \lstinline!Option[A]!
are written in the type notation as:
\[
\text{Either}^{A,B}\triangleq A+B\quad,\quad\quad\text{Opt}^{A}\triangleq\bbnum 1+A\quad.
\]
The type \lstinline!Either[A, B]! is written as $A+B$ by definition
of the disjunctive type notation ($+$). The type \lstinline!Option[A]!
has two disjoint cases, \lstinline!None! and \lstinline!Some[A]!.
The case class \lstinline!None! is a \textsf{``}\index{unit type!named}named
\lstinline!Unit!\textsf{''} and is denoted by $\bbnum 1$. The case class
\lstinline!Some[A]! contains a single value of type $A$. So, the
type notation for \lstinline!Option[A]! is $\bbnum 1+A$. We will
also sometimes write $\text{Opt}^{A}$ to denote \lstinline!Option[A]!.

\subsubsection{Example \label{subsec:Example-ch-notation-function-1-a}\ref{subsec:Example-ch-notation-function-1-a}}

The Scala definition of the disjunctive type \lstinline!UserAction!:
\begin{lstlisting}
sealed trait UserAction
final case class SetName(first: String, last: String) extends UserAction
final case class SetEmail(email: String)              extends UserAction
final case class SetUserId(id: Long)                  extends UserAction
\end{lstlisting}
is written in the type notation as:
\begin{equation}
\text{UserAction}\triangleq\text{String}\times\text{String}+\text{String}+\text{Long}\quad.\label{eq:ch-example-case-class-type-notation}
\end{equation}
The type operation $\times$ groups stronger than $+$, as in arithmetic.
To derive the type notation~(\ref{eq:ch-example-case-class-type-notation}),
we first drop all names from case classes and get three nameless tuples
\lstinline!(String, String)!, \lstinline!(String)!, and \lstinline!(Long)!.
Each of these tuples is then converted into a product using the operator
$\times$, and all products are \textsf{``}summed\textsf{''} in the type notation
using the operator $+$.

\subsubsection{Example \label{subsec:Example-ch-notation-function-2}\ref{subsec:Example-ch-notation-function-2}}

The parameterized disjunctive type \lstinline!Either3! is a generalization
of \lstinline!Either!:
\begin{lstlisting}
sealed trait Either3[A, B, C]
final case class Left[A, B, C](x: A)   extends Either3[A, B, C]
final case class Middle[A, B, C](x: B) extends Either3[A, B, C]
final case class Right[A, B, C](x: C)  extends Either3[A, B, C]
\end{lstlisting}
This disjunctive type is written in the type notation as $\text{Either3}^{A,B,C}\triangleq A+B+C$.

\subsubsection{Example \label{subsec:Example-ch-notation-function-3}\ref{subsec:Example-ch-notation-function-3}}

Define a Scala type constructor \lstinline!F[A]! corresponding to
the type notation:
\[
F^{A}\triangleq\bbnum 1+\text{Int}\times A\times A+\text{Int}\times\left(\text{Int}\rightarrow A\right)\quad.
\]


\subparagraph{Solution}

The formula for $F^{A}$ defines a disjunctive type \lstinline!F[A]!
with three parts. To implement \lstinline!F[A]! in Scala, we need
to choose names for each of the disjoint parts, which will become
case classes. For the purposes of this example, let us choose names
\lstinline!F1!, \lstinline!F2!, and \lstinline!F3!. Each of these
case classes needs to have the same type parameter \lstinline!A!.
So, we begin writing the code as:
\begin{lstlisting}
sealed trait F[A]
final case class F1[A](...) extends F[A]
final case class F2[A](...) extends F[A]
final case class F3[A](...) extends F[A]
\end{lstlisting}
Each of these case classes represents one part of the disjunctive
type: \lstinline!F1! represents $\bbnum 1$, \lstinline!F2! represents
$\text{Int}\times A\times A$, and \lstinline!F3! represents $\text{Int}\times\left(\text{Int}\rightarrow A\right)$.
It remains to choose names and define the case classes:
\begin{lstlisting}
sealed trait F[A]
final case class F1[A]()                     extends F[A]  // Named unit type.
final case class F2[A](n: Int, x1: A, x2: A) extends F[A]
final case class F3[A](n: Int, f: Int => A)  extends F[A]
\end{lstlisting}
The names \lstinline!n!, \lstinline!x1!, \lstinline!x2!, and \lstinline!f!
are chosen arbitrarily.

\subsubsection{Example \label{subsec:Example-ch-notation-function-4}\ref{subsec:Example-ch-notation-function-4}}

Write the type signature of the following function in the type notation:
\begin{lstlisting}
def fmap[A, B](f: A => B): Option[A] => Option[B]
\end{lstlisting}


\subparagraph{Solution}

This is a curried function, so we first rewrite the type signature
as:
\begin{lstlisting}
def fmap[A, B]: (A => B) => Option[A] => Option[B]
\end{lstlisting}
The type notation for \lstinline!Option[A]! is $\bbnum 1+A$. Now
we can write the type signature of \lstinline!fmap! as:
\begin{align*}
 & \text{fmap}^{A,B}:\left(A\rightarrow B\right)\rightarrow\bbnum 1+A\rightarrow\bbnum 1+B\quad,\\
{\color{greenunder}\text{or equivalently}:}\quad & \text{fmap}:\forall(A,B).\,\left(A\rightarrow B\right)\rightarrow\bbnum 1+A\rightarrow\bbnum 1+B\quad.
\end{align*}
We do not put parentheses around $\bbnum 1+A$ and $\bbnum 1+B$ because
the function arrow ($\rightarrow$) groups weaker than the other type
operations. But parentheses around $\left(A\rightarrow B\right)$
are required.

We will usually prefer to write type parameters in superscripts rather
than under type quantifiers. So, for example, we will write $\text{id}^{A}\triangleq x^{:A}\rightarrow x$
rather than $\text{id}\triangleq\forall A.\,x^{:A}\rightarrow x$.

\subsection{Exercises: Type notation\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-type-notation-1}\ref{subsec:Exercise-type-notation-1}}

Define a Scala disjunctive type \lstinline!Q[T, A]! corresponding
to this type notation:
\[
Q^{T,A}\triangleq\bbnum 1+T\times A+\text{Int}\times(T\rightarrow T)+\text{String}\times A\quad.
\]


\subsubsection{Exercise \label{subsec:Exercise-type-notation-2}\ref{subsec:Exercise-type-notation-2}}

Convert the type \lstinline!Either[(A, Int), Either[(A, Char), (A, Float)]]!
from Scala syntax to the type notation. 

\subsubsection{Exercise \label{subsec:Exercise-type-notation-3}\ref{subsec:Exercise-type-notation-3}}

Define a Scala type \lstinline!Opt2[A, B]! written in the type notation
as $\bbnum 1+A+B$.

\subsubsection{Exercise \label{subsec:Exercise-type-notation-4}\ref{subsec:Exercise-type-notation-4}}

Write a Scala type signature for the fully parametric function:
\[
\text{flatMap}^{A,B}:\bbnum 1+A\rightarrow\left(A\rightarrow\bbnum 1+B\right)\rightarrow\bbnum 1+B
\]
and implement this function, preserving information as much as possible.

\section{The logic of ${\cal CH}$-propositions}

\subsection{Motivation and first examples\label{subsec:ch-Motivation-and-first-examples}}

So far, we were able to convert statements such as \textsf{``}\emph{a fully
parametric function can compute values of type} $A$\textsf{''} into logical
propositions that we called ${\cal CH}$-propositions. The next step
is to determine the proof rules suitable for reasoning about ${\cal CH}$-propositions.

Formal logic uses axioms and derivation rules for proving that certain
formulas are true or false. We will use Greek letters ($\alpha$,
$\beta$, etc.) to denote propositions in logic.

We will often need logical formulas that talk about properties of
\emph{arbitrary} propositions. This is denoted by the \textbf{universal
quantifier}\index{universal quantifier (forall)@universal quantifier ($\forall$)}
symbol ($\forall$), which means \textsf{``}for all\textsf{''}. The universal quantifier
will be usually located in front of the formula, for example:
\[
\forall(\alpha,\beta).\,\left(\alpha\Rightarrow\beta\right)\Rightarrow\alpha\Rightarrow\alpha\quad.
\]
The symbol $\Rightarrow$ denotes \index{implication (in logic)}\index{logical implication}\textbf{implication}:
$\alpha\Rightarrow\beta$ means that \emph{if} $\alpha$ is proved
true \emph{then} $\beta$ will be proved true.

Formulas whose propositions are universally quantified correspond
to type signatures that are made entirely from type parameters. For
instance, the formula shown above corresponds to the following type
signature:
\begin{lstlisting}
def f[A, B]: (A => B) => A => A
\end{lstlisting}
The universal quantifier $\forall(\alpha,\beta)$ corresponds to the
fact that the function \lstinline!f! works with any choice of types
\lstinline!A! and \lstinline!B!.

A simple example of a true logical formula is \textsf{``}any proposition $\alpha$
follows from itself\textsf{''}:
\begin{equation}
\forall\alpha.\,\alpha\Rightarrow\alpha\quad.\label{eq:ch-type-sig-1a}
\end{equation}
If the proposition $\alpha$ is a ${\cal CH}$-proposition, that is,
if $\alpha\triangleq{\cal CH}(A)$ for some type $A$, we obtain from
Eq.~(\ref{eq:ch-type-sig-1a}) the formula:
\begin{equation}
\forall A.\,{\cal CH}(A)\Rightarrow{\cal CH}(A)\quad.\label{eq:ch-type-sig-1}
\end{equation}
We expect true ${\cal CH}$-propositions to correspond to types that
\emph{can} be computed in a fully parametric function. Let us see
if this example fits our expectations. We can rewrite Eq.~(\ref{eq:ch-type-sig-1})
as:
\begin{align*}
 & \forall A.\,\gunderline{{\cal CH}(A)\Rightarrow{\cal CH}(A)}\\
{\color{greenunder}\text{rule for function types}:}\quad & =\gunderline{\forall A}.\,{\cal CH}\left(A\rightarrow A\right)\\
{\color{greenunder}\text{rule for parameterized types}:}\quad & ={\cal CH}\left(\forall A.\,A\rightarrow A\right)\quad.
\end{align*}
The last line shows a ${\cal CH}$-proposition that corresponds to
the type $\forall A.\,A\rightarrow A$. Translating this type notation
into a Scala type signature, we get:
\begin{lstlisting}
def f[A]: A => A
\end{lstlisting}
This type signature can be implemented by an identity function:
\begin{lstlisting}
def f[A]: A => A = { x => x }
\end{lstlisting}
This example shows a true ${\cal CH}$-proposition that corresponds
to a type signature of a function \lstinline!f!, and we see that
\lstinline!f! \emph{can} be implemented in code.

While the correctness of the formula $\forall\alpha.\,\alpha\Rightarrow\alpha$
may be self-evident, the point of using formal logic is to have a
set of axioms and proof rules that allow us to verify \emph{all} true
formulas systematically, without guessing or testing. What axioms
and proof rules are suitable for proving ${\cal CH}$-propositions?

A set of axioms and proof rules defines a \textbf{formal logic}\index{formal logic}.
Mathematicians have studied many different logics that are useful
for solving different problems. We are now looking for a specific
formal logic that gives correct answers when reasoning about ${\cal CH}$-propositions.\footnote{\label{fn:Bornat-proof-book}\index{Richard Bornat}For an overview
and more details about that logic and the necessary proof techniques,
see the book by R.~Bornat, \textsf{``}Proof and disproof in formal logic:
an introduction for programmers\textsf{''}. An early draft version of that
book is available at \texttt{\href{https://homepages.phonecoop.coop/randj/richard/books/ProofandDisproof.pdf}{https://homepages.phonecoop.coop/randj/richard/books/ProofandDisproof.pdf}}}

\subsection{The rules of proof for ${\cal CH}$-propositions\label{subsec:The-rules-of-proof}}

To derive the suitable logical axioms and proof rules systematically,
let us examine what could make a sequent with ${\cal CH}$-propositions
true.

A sequent ${\cal CH}(A)\vdash{\cal CH}(X)$ is true when a value of
type $X$ can be computed by fully parametric code that may only use
a given value of type $A$. To describe all possible ways of computing
a value of type $X$, we need to enumerate all possible ways of \emph{writing
code} for a fully parametric function body. The requirement of full
parametricity means that we are not allowed to use any specific types
such as \lstinline!Int! or \lstinline!String!, any concrete values
such as \lstinline!123! or \lstinline!"hello"!, or any library functions
that work with specific (non-parametric) types. We are only allowed
to work with values of unknown types described by the given type parameters.
However, we are permitted to use fully parametric types such as \lstinline!Either[A, B]!
or \lstinline!Option[A]!. 

In fact, we can enumerate all the allowed constructions that may be
used by fully parametric code implementing a ${\cal CH}$-proposition.
There are \emph{eight} code constructions\index{eight code constructions}\index{fully parametric!code constructions}
as illustrated by this code:
\begin{lstlisting}
def f[A, B, ...](a: A, b: B): X = {   // Any given type signature.
  val x1: Unit = ()                   // 1) Use a value of type Unit.
  val x2: A = a                       // 2) Use a given argument value.
  val x3 = { x: A => b }              // 3) Create a function.
  val x4: D = x3(x2)                  // 4) Use a function.
  val x5: (A, B) = (a, b)             // 5) Create a tuple.
  val x6: B = x5._2                   // 6) Use a tuple.
  val x7: Either[A, B] = Right(x6)    // 7) Create values of a disjunctive type.
  val x8 = x7 match { ... }           // 8) Use values of a disjunctive type.
}     // 9) Call f() itself recursively. Not included here because recursion is not supported by CH-propositions.
\end{lstlisting}
The proposition ${\cal CH}(X)$ is true if we can create a sequence
of computed values such as \lstinline!x1!, \lstinline!x2!, ...,
\lstinline!xN!, each using one of these eight code constructs, with
\lstinline!xN! having type $X$. 

So, each of the eight code constructs will give a proof rule in the
logic.

It is important that there are only a finite number of allowed code
constructions. This defines rigorously the concept of \textsf{``}fully parametric
code\textsf{''}\index{fully parametric!code|textit} and allows us to \emph{prove}
${\cal CH}$-propositions.

Reasoning about proof rules is easier with a compact notation. To
obtain that notation, we first need to translate the eight code constructs
into the notation of sequents. A proof of a sequent, e.g., $\mathcal{CH}(A)\vdash\mathcal{CH}(X)$,
will consist of applying some of those proof rules. We will then combine
the code constructs corresponding to each rule and obtain some code
that computes a value of type $X$ using an argument of type $A$. 

Conversely, any fully parametric (and non-recursive) code computing
a value of type $X$ must be a combination of some of the eight code
constructs\index{eight code constructions}. That code combination
can be translated into a combination of logic rules, which will produce
a proof of the proposition ${\cal CH}(X)$.

In this way, we will get a correspondence between fully parametric
programs and proofs of sequents. This is the second part of the Curry-Howard
correspondence.

In the following text, we will need to write ${\cal CH}$-propositions
such as Eq.~(\ref{eq:ch-CH-proposition-def}) as sequents such as
Eq.~(\ref{eq:ch-example-sequent}). As we have seen, ${\cal CH}$-propositions
involving complicated type expressions can be always rewritten via
${\cal CH}$-propositions for individual type parameters (${\cal CH}(A)$,
${\cal CH}(B)$, etc.). So, we will only need sequents involving such
${\cal CH}$-propositions. For brevity, we denote $\alpha\triangleq{\cal CH}(A)$,
$\beta\triangleq{\cal CH}(B)$, etc. We will use the letter $\Gamma$
to stand for a set of premises and write a shorter formula $\Gamma\vdash\alpha$
instead of the sequent~(\ref{eq:ch-example-sequent}).

With these notations, we list the rules for proving ${\cal CH}$-propositions
and the corresponding code:

\paragraph{1) Use the \texttt{Unit} value}

At any place in the code, we may write the expression \lstinline!()!
of type \lstinline!Unit!. This expression corresponds to a proof
of the proposition ${\cal CH}(\bbnum 1)$ with any set $\Gamma$ of
premises (even with an empty set of premises). So, the sequent $\Gamma\vdash{\cal CH}(\bbnum 1)$
is always true. The code corresponding to the proof of this sequent
is an expression that creates a value of the \lstinline!Unit! type:
\[
\text{Proof}\,\big(\Gamma\vdash{\cal CH}(\bbnum 1)\big)=1\quad,
\]
where we denoted by $1$ the value \lstinline!()!.

In formal logic, a sequent that is always true, such as our $\Gamma\vdash{\cal CH}(\bbnum 1)$,
is called an \textbf{axiom}\index{logical axiom} and is written in
the following notation:
\[
\frac{}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{create unit})\quad\quad.
\]
The \textsf{``}fraction with a label\textsf{''} represents a proof rule. The denominator
of the fraction is the target sequent that we need to prove. The numerator
of the fraction can have zero or more other sequents that need to
be proved before the target sequent can be proved. In this case, the
set of previous sequents is empty: the target sequent is an axiom
and so requires no previous sequents for its proof. The label \textsf{``}$\text{create unit}$\textsf{''}
is an arbitrary name used to refer to the rule.

\paragraph{2) Use a given value}

At any place within the code of a fully parametric function, we may
use one of the function\textsf{'}s arguments, say $x^{:A}$. If some argument
has type $A$, it means that we already have a value of type $A$.
So, the corresponding proposition, $\alpha\triangleq{\cal CH}(A)$,
belongs to the set of premises of the sequent we are trying to prove.
To indicate this, we write the set of premises as \textsf{``}$\Gamma,\alpha$\textsf{''}.
The code construct \lstinline!x: A! computes a value of type $A$,
i.e., shows that $\alpha$ is true with premises $\Gamma,\alpha$.
This is expressed by the sequent $\Gamma,\alpha\vdash\alpha$. The
proof code for this sequent is an expression that just returns the
given value (which we denoted by $x^{:A}$):
\[
\text{Proof}\,\big(\Gamma,\alpha\vdash\alpha\big)_{\text{given }x^{:A}}=x\quad.
\]
This sequent is an axiom since its proof requires no previous sequents;
a value of type $A$ is already given in the premises. We denote this
axiom by:
\[
\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use value})\quad\quad.
\]


\paragraph{3) Create a function}

At any place in the code, we may compute a nameless function of type,
say, $A\rightarrow B$, by writing \lstinline!(x:A) => expr! as long
as a value \lstinline!expr! of type $B$ can be computed in the inner
scope of the function. The code for \lstinline!expr! is also required
to be fully parametric; it may use \lstinline!x! and/or other values
visible in that scope. So, we now need to answer the question of whether
a fully parametric function can compute a value of type $B$, given
an argument of type $A$ as well as all other arguments previously
given to the parent function. This question is answered by a sequent
whose premises contain one more proposition, ${\cal CH}(A)$, in addition
to all previously available premises. Translating this into the language
of ${\cal CH}$-propositions, we find that we will prove the sequent:
\[
\Gamma\vdash{\cal CH}(A\rightarrow B)\quad=\quad\Gamma\vdash{\cal CH}(A)\Rightarrow{\cal CH}(B)\quad=\quad\Gamma\vdash\alpha\Rightarrow\beta
\]
if we can prove the sequent $\Gamma,{\cal CH}(A)\vdash{\cal CH}(B)=\Gamma,\alpha\vdash\beta$.
In the notation of formal logic, this is a \textbf{derivation rule}\index{derivation rule}
(rather than an axiom) and is written as:
\[
\frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})\quad\quad.
\]
The \textbf{turnstile}\index{0@$\vdash$ (turnstile) symbol}\index{turnstile (vdash) symbol@turnstile ($\vdash$) symbol}
symbol ($\vdash$) groups weaker than other operators. So, we can
write sequents such as $(\Gamma,\alpha)\vdash(\beta\Rightarrow\gamma)$
with fewer parentheses: $\Gamma,\alpha\vdash\beta\Rightarrow\gamma$.

What code corresponds to the \textsf{``}$\text{create function}$\textsf{''} rule?
The proof of $\Gamma\vdash\alpha\Rightarrow\beta$ depends on a proof
of another sequent. So, the corresponding code must be a \emph{function}
that takes a proof of the previous sequent as an argument and returns
a proof of the new sequent. We call that function a \index{Curry-Howard correspondence!proof transformer}\index{proof transformer}\textbf{proof
transformer}.

By the CH correspondence, a proof of a sequent corresponds to a code
expression of the type given by the goal of the sequent. That expression
may use arguments of types corresponding to the premises of the sequent.
So, a proof of the sequent $\Gamma,\alpha\vdash\beta$ is an expression
\lstinline!exprB! of type $B$ that may use a given value of type
$A$ as well as any other arguments given previously. Then we can
write the proof code for the sequent $\Gamma\vdash\alpha\Rightarrow\beta$
as the nameless function \lstinline!(x: A) => exprB!. This function
has type $A\rightarrow B$ and requires us to already have a suitable
expression \lstinline!exprB!. This exactly corresponds to the proof
rule \textsf{``}$\text{create function}$\textsf{''}. That rule\textsf{'}s proof transformer
is:
\[
\text{Proof}\,\big(\Gamma\vdash\alpha\Rightarrow\beta\big)=x^{:A}\rightarrow\text{Proof}\,\big(\Gamma,\alpha\vdash\beta\big)_{\text{given }x^{:A}}\quad.
\]
Here, the subscript \textsf{``}given $x^{:A}$\textsf{''} indicates that the value
$x^{:A}$ must come from the premises. (In this case, the set of premises
is $\Gamma,\alpha$ and so the proposition $\alpha$ must have been
already proved. The proof of $\alpha$ will give a value $x^{:A}$.)

\paragraph{4) Use a function}

At any place in the code, we may apply an already defined function
of type $A\rightarrow B$ to an already computed value of type $A$.
The result will be a value of type $B$. This corresponds to assuming
${\cal CH}(A\rightarrow B)$ and ${\cal CH}(A)$, and then deriving
${\cal CH}(B)$. The notation for this proof rule is:
\[
\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})\quad\quad.
\]
The code corresponding to this proof rule takes previously computed
values \lstinline!x:A! and \lstinline!f:A => B!, and writes the
expression \lstinline!f(x)!. This can be written as a function application:
\[
\text{Proof}\,(\Gamma\vdash\beta)=\text{Proof}\left(\Gamma\vdash\alpha\Rightarrow\beta\right)(\text{Proof}\,(\Gamma\vdash\alpha))\quad.
\]


\paragraph{5) Create a tuple}

If we have already computed some values \lstinline!a: A! and \lstinline!b: B!,
we may write the expression \lstinline!(a, b)! and so compute a value
of the tuple type \lstinline!(A, B)!. The proof rule is:
\[
\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})\quad\quad.
\]
Writing $a\times b$ to mean the pair \lstinline!(a, b)!, we can
write the corresponding code expression as:
\[
\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)=\text{Proof}\left(\Gamma\vdash\alpha\right)\times\text{Proof}\left(\Gamma\vdash\beta\right)\quad.
\]

This rule describes creating a tuple of $2$ values. A larger tuple,
such as \lstinline!(w, x, y, z)!, can be expressed via nested pairs,
e.g., as \lstinline!(w, (x, (y, z)))!. So, it suffices to have a
derivation rule for creating pairs. That rule allows us to express
the rules for creating all larger tuples, and so we do not need to
define separate rules for, say, $\Gamma\vdash\alpha\wedge\beta\wedge\gamma$.

\paragraph{6) Use a tuple}

If we already have a value \lstinline!t:(A,B)! of a tuple type $A\times B$,
we can extract one of the parts of the tuple and obtain a value of
type \lstinline!A! or a value of type \lstinline!B!. The code is
\lstinline!t._1! and \lstinline!t._2! respectively, and the corresponding
sequent proof rules are:
\[
\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)\quad\quad,\quad\quad\quad\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\beta}\quad(\text{use tuple-}2)\quad\quad.
\]
The proof code can be written as:
\[
\text{Proof}\left(\Gamma\vdash\alpha\right)=\pi_{1}\left(\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)\right)\quad,\quad\text{Proof}\left(\Gamma\vdash\beta\right)=\pi_{2}\left(\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)\right)\quad,
\]
where we introduced the notation $\pi_{1}$ and $\pi_{2}$ to mean
the Scala code \lstinline!_._1! and \lstinline!_._2!.

Since all tuples can be expressed through pairs, it is sufficient
to have proof rules for pairs.

\paragraph{7) Create a disjunctive value}

The type \lstinline!Either[A, B]! corresponding to the disjunction
$\alpha\vee\beta$ can be used to define any other disjunctive type;
e.g., a disjunctive type with three parts can be expressed as \lstinline!Either[A, Either[B, C]]!.
So, it suffices to have proof rules for a disjunction of \emph{two}
propositions.

There are two ways of creating a value of the type \lstinline!Either[A, B]!:
the code expressions are \lstinline!Left(x: A)! and \lstinline!Right(y: B)!.
The values \lstinline!x: A! or \lstinline!y: B! must have been computed
previously (and correspond to previously proved sequents). So, the
sequent proof rules are:
\[
\frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Left})\quad\quad\quad\quad\quad\frac{\Gamma\vdash\beta}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Right})\quad\quad.
\]
The corresponding proof transformers can be written using the case
class names \lstinline!Left! and \lstinline!Right! as:
\begin{align*}
\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right) & =\text{Left}\,(\text{Proof}\left(\Gamma\vdash\alpha\right))\quad,\\
\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right) & =\text{Right}\,(\text{Proof}\left(\Gamma\vdash\beta\right))\quad.
\end{align*}


\paragraph{8) Use a disjunctive value}

Pattern matching is the basic way of using a value of type \lstinline!Either[A, B]!:
\begin{lstlisting}
val result: C = (e: Either[A, B]) match {
  case Left(x: A)    => expr1(x)
  case Right(y: B)   => expr2(y)
}
\end{lstlisting}
Here, \lstinline!expr1(x)! is an expression of type \lstinline!C!
computed using \lstinline!x: A! and any previously computed values.
Similarly, \lstinline!expr2(y)! is computed using \lstinline!y: B!
and previous values. The values used in computation correspond to
the premises of a sequent. So, \lstinline!expr1(x)! represents a
proof of a sequent with an additional premise of type \lstinline!A!.
Denoting $\gamma\triangleq{\cal CH}(C)$, we write that sequent as:
$\Gamma,\alpha\vdash\gamma$. Similarly, \lstinline!expr2(y)! is
a proof of the sequent $\Gamma,\beta\vdash\gamma$. We can compute
\lstinline!result! only if we can compute \lstinline!e!, \lstinline!expr1!,
and \lstinline!expr2!. So, the proof rule is: 
\[
\frac{\Gamma\vdash\alpha\vee\beta\quad\quad\Gamma,\alpha\vdash\gamma\quad\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use Either})\quad\quad.
\]
The corresponding code can be written as:
\[
\text{Proof}\left(\Gamma\vdash\gamma\right)=\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right)\text{ match }\begin{cases}
\text{case }a^{:A}\rightarrow & \text{Proof}\left(\Gamma,\alpha\vdash\gamma\right)_{\text{given }a}\\
\text{case }b^{:B}\rightarrow & \text{Proof}\left(\Gamma,\beta\vdash\gamma\right)_{\text{given }b}
\end{cases}\quad.
\]

We found eight proof rules shown in Table~\ref{tab:Proof-rules-for-constructive-logic}.
These rules define the \textbf{\index{intuitionistic propositional logic}intuitionistic
propositional logic}, also called \textbf{\index{constructive logic}constructive
propositional logic}. We will call this logic \textsf{``}constructive\textsf{''} for
short.

\begin{table}
\begin{centering}
{\small{}}%
\fbox{\begin{minipage}[t]{0.75\columnwidth}%
{\small{}
\begin{align*}
{\color{greenunder}\text{axioms}:}\quad & \frac{~}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{use unit})\quad\quad\quad\quad\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use value})\\
{\color{greenunder}\text{derivation rules}:}\quad & \frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})\\
 & \frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})\\
 & \frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})\\
 & \frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)\quad\quad\quad\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\beta}\quad(\text{use tuple-}2)\\
 & \frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create \texttt{Left}})\quad\quad\quad\frac{\Gamma\vdash\beta}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create \texttt{Right}})\\
 & \frac{\Gamma\vdash\alpha\vee\beta\quad\quad\Gamma,\alpha\vdash\gamma\quad\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use \texttt{Either}})
\end{align*}
}%
\end{minipage}}{\small\par}
\par\end{centering}
\caption{Proof rules for the constructive logic.\label{tab:Proof-rules-for-constructive-logic}}
\end{table}


\subsection{Examples: Deriving code from proofs of ${\cal CH}$-propositions\label{subsec:Example:-Proving-a-ch-proposition}}

Using the proof rules of Table~\ref{tab:Proof-rules-for-constructive-logic},
we can (in principle) derive code from type signatures of fully parametric
functions. We will now show two simple examples and we perform such
derivations step by step.

\subsubsection{Example \label{subsec:Example-derive-code-1}\ref{subsec:Example-derive-code-1}}

Derive the code for the type signature:
\begin{lstlisting}
def d[X]: X => (X, X)
\end{lstlisting}


\subparagraph{Solution}

First, we formulate the task as proving the proposition \textsf{``}For any
type $X$, we can have a value of type $X\rightarrow X\times X$\textsf{''}.
This corresponds to the proposition ${\cal CH}(\forall X.\,X\rightarrow X\times X)$.
That proposition will be the goal of a sequent. The function has no
arguments, so there are no premises for the sequent. We denote an
empty set of premises by the symbol $\emptyset$ \index{0@$\emptyset$ (empty set)}.
So, the sequent is written as:
\[
\emptyset\vdash{\cal CH}(\forall X.\,X\rightarrow X\times X)\quad.
\]
We denote $\chi\triangleq{\cal CH}(X)$ and rewrite this sequent using
the rules of Table~\ref{tab:ch-correspondence-type-notation-CH-propositions}.
The result is a sequent involving just $\chi$:
\[
\forall\chi.\,\emptyset\vdash\chi\Rightarrow\chi\wedge\chi\quad.
\]

Next, we look for a proof of this sequent. For brevity, we will omit
the quantifier $\forall\chi$ since it will be present in front of
every sequent.

We search through the proof rules in Table~\ref{tab:Proof-rules-for-constructive-logic},
looking for \textsf{``}denominators\textsf{''} that match our current sequent. If
we find such a rule, we will apply that rule to our sequent. Then
we will need to prove the sequents in the rule\textsf{'}s \textsf{``}numerator\textsf{''}.

Beginning with $\emptyset\vdash\chi\Rightarrow\chi\wedge\chi$, we
find a match with the rule \textsf{``}create function\textsf{''}: 
\[
\frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})
\]
The denominator of that rule is $\Gamma\vdash\alpha\Rightarrow\beta$.
This pattern will match our sequent ($\emptyset\vdash\chi\Rightarrow\chi\wedge\chi$)
if we set $\Gamma=\emptyset$, $\alpha=\chi$, and $\beta=\chi\wedge\chi$.
So, we are allowed to apply the rule \textsf{``}create function\textsf{''} with these
assignments.

After these assignments, the rule \textsf{``}create function\textsf{''} becomes:
\[
\frac{\emptyset,\chi\vdash\chi\wedge\chi}{\emptyset\vdash\chi\Rightarrow\chi\wedge\chi}\quad.
\]
Now the rule says: we will prove the denominator ($\emptyset\vdash\chi\Rightarrow\chi\wedge\chi$)
if we first prove the numerator ($\emptyset,\chi\vdash\chi\wedge\chi$).

The set of premises $\emptyset,\chi$ is the union of an empty set
and the set having a single premise $\chi$. So, we can write the
last sequent also as $\chi\vdash\chi\wedge\chi$ if we like.

To prove that sequent, we again look for a rule whose denominator
matches our sequent. That rule is \textsf{``}create tuple\textsf{''}:
\[
\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})
\]
The denominator ($\Gamma\vdash\alpha\wedge\beta$) will match our
sequent ($\chi\vdash\chi\wedge\chi$) if we assign $\Gamma=\chi$,
$\alpha=\chi$, $\beta=\chi$. With these assignments, the rule says
that we need to prove two sequents ($\Gamma\vdash\alpha$ and $\Gamma\vdash\beta$),
which are in fact the same sequent ($\chi\vdash\chi$).

To prove that sequent, we apply the axiom \textsf{``}use value\textsf{''}:
\[
\frac{}{\Gamma,\alpha\vdash\alpha}\quad.
\]
The denominator of that axiom matches $\emptyset,\chi\vdash\chi$
if we set $\Gamma=\emptyset$ and $\alpha=\chi$. Then the axiom \textsf{``}use
value\textsf{''} becomes:
\[
\frac{}{\emptyset,\chi\vdash\chi}\quad.
\]
This axiom says that the sequent $\emptyset,\chi\vdash\chi$ (or equivalently
$\chi\vdash\chi$) is already true with nothing more needed to prove.
So, the proof is finished.

We may visualize the proof as a tree shown in Figure~\ref{fig:Proof-of-the-sequent-example-1}.
The tree starts with the initial sequent and applies rules that require
us to prove other sequents. The tree stops with axioms in leaf positions.

\begin{figure}
\begin{centering}
{\footnotesize{}}%
\fbox{\begin{minipage}[t]{0.7\columnwidth}%
\begin{center}
{\footnotesize{}}{\footnotesize{}\Tree[ .$\emptyset\vdash\chi\Rightarrow\chi\wedge\chi$ [ .{rule \textsf{``}$\text{create function}$\textsf{''}} [ .$\chi\vdash\chi\wedge\chi$ [ .{rule \textsf{``}$\text{create tuple}$\textsf{''}} [ .$\chi\vdash\chi$ [ .{axiom \textsf{``}$\text{use value}$\textsf{''}}   ] ] [ .$\chi\vdash\chi$ {axiom \textsf{``}$\text{use value}$\textsf{''}} ] ] ] ] ]}
\par\end{center}%
\end{minipage}}{\footnotesize\par}
\par\end{centering}
\caption{Proof tree for the sequent $\emptyset\vdash\chi\Rightarrow\chi\wedge\chi$.\label{fig:Proof-of-the-sequent-example-1}}
\end{figure}

Now we need to extract code from the proof. We begin with the leaves
of the tree and trace back the proof towards the top. The axiom \textsf{``}use
value\textsf{''} has the proof code $x$, where $x$ is given in the premises:
\[
\text{Proof}\,\big(\Gamma,\alpha\vdash\alpha\big)_{\text{given }x^{:A}}=x\quad.
\]
The proof used this axiom twice with $\alpha=\chi$. Recall that $\chi$
denotes ${\cal CH}(X)$, and so the value $x$ must have type $X$.
So, we write:
\[
\text{Proof}\,\big(\chi\vdash\chi\big)_{\text{given }x^{:X}}=x\quad.
\]

The previous rule used by the proof was \textsf{``}create tuple\textsf{''}. Its proof
code is:
\[
\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)=\text{Proof}\left(\Gamma\vdash\alpha\right)\times\text{Proof}\left(\Gamma\vdash\beta\right)\quad.
\]
That rule was used with $\Gamma=\chi$, $\alpha=\chi$, and $\beta=\chi$.
So, the proof code becomes:
\begin{align*}
\text{Proof}\left(\chi\vdash\chi\wedge\chi\right)_{\text{given }x^{:X}} & =\text{Proof}\left(\chi\vdash\chi\right)_{\text{given }x^{:X}}\times\text{Proof}\left(\chi\vdash\chi\right)_{\text{given }x^{:X}}\\
 & =x\times x\quad.
\end{align*}

Finally, the first rule \textsf{``}create function\textsf{''} has the proof code:
\[
\text{Proof}\,\big(\Gamma\vdash\alpha\Rightarrow\beta\big)=x^{:A}\rightarrow\text{Proof}\,\big(\Gamma,\alpha\vdash\beta\big)_{\text{given }x^{:A}}\quad.
\]
That rule was used with $\Gamma=\emptyset$, $A=X$, $\alpha=\chi$,
and $\beta=\chi\wedge\chi$. Using these assignments, we obtain the
code:
\[
\text{Proof}\,\big(\emptyset\vdash\chi\Rightarrow\chi\wedge\chi\big)=x^{:X}\rightarrow\text{Proof}\,\big(\chi\vdash\chi\times\chi\big)_{\text{given }x^{:X}}=x^{:X}\rightarrow x\times x\quad.
\]
In Scala, this code is:
\begin{lstlisting}
def d[X]: X => (X, X) = (x: X) => (x, x) 
\end{lstlisting}
$\square$

\subsubsection{Example \label{subsec:Example-derive-code-2}\ref{subsec:Example-derive-code-2}}

Derive the code for the type signature:
\begin{lstlisting}
def s[A, B]: ((A => A) => B) => B
\end{lstlisting}


\subparagraph{Solution}

The task is to compute a value of type $((A\rightarrow A)\rightarrow B)\rightarrow B$
for arbitrary types $A$, $B$ without any arguments. This is written
as the sequent: 
\[
\emptyset\vdash{\cal CH}\big(\forall(A,B).\,((A\rightarrow A)\rightarrow B)\rightarrow B\big)\quad.
\]
Denote $\alpha\triangleq{\cal CH}(A)$ and $\beta\triangleq{\cal CH}(B)$,
and rewrite the sequent using the rules of Table~\ref{tab:ch-correspondence-type-notation-CH-propositions}
to obtain a logic formula that involves just $\alpha$ and $\beta$:
\begin{equation}
\forall(\alpha,\beta).~\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta\quad.\label{eq:ch-example-sequent-2}
\end{equation}

The next step is to prove the sequent~(\ref{eq:ch-example-sequent-2}).
For brevity, we will omit the quantifier $\forall(\alpha,\beta)$
since it will be present in front of every sequent.

Begin by looking for a proof rule whose \textsf{``}denominator\textsf{''} has a sequent
similar to Eq.~(\ref{eq:ch-example-sequent-2}), i.e., has an implication
($p\Rightarrow q$) in the goal. We have only one rule with the \textsf{``}denominator\textsf{''}
of the form $\Gamma\vdash(p\Rightarrow q$); this is the rule \textsf{``}$\text{create function}$\textsf{''},
which we will rewrite as:
\[
\frac{\Gamma,p\vdash q}{\Gamma\vdash p\Rightarrow q}\quad(\text{create function})
\]
To match the denominator, we use this rule with the assignments $\Gamma=\emptyset$,
$p\triangleq(\alpha\Rightarrow\alpha)\Rightarrow\beta$ and $q\triangleq\beta$:
\[
\frac{(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta}{\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta}\quad\text{(create function)}
\]
The rule\textsf{'}s numerator now requires us to prove the sequent $(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta$.
We may write that sequent as as $\gamma\vdash\beta$, where we defined
$\gamma\triangleq(\alpha\Rightarrow\alpha)\Rightarrow\beta$ for brevity.

So, the next step is to prove the sequent $\gamma\vdash\beta$. The
premise ($\gamma$) contains an implication. But there is no proof
rule whose denominator has a \emph{premise} in the form of an implication
($p\Rightarrow q$). Instead, we have the rule \textsf{``}$\text{use function}$\textsf{''}
whose denominator contains an arbitrary sequent:
\[
\frac{\Gamma\vdash p\quad\quad\Gamma\vdash p\Rightarrow q}{\Gamma\vdash q}\quad(\text{use function})
\]
This rule\textsf{'}s denominator matches $\gamma\vdash\beta$ if we set $\Gamma=\gamma$
and $q=\beta$. But it is not clear how to choose $p$. After some
trial and error, one finds that the proof will work if we set $p=(\alpha\Rightarrow\alpha)$:
\[
\frac{\gamma\vdash\alpha\Rightarrow\alpha\quad\quad\gamma\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta}{\gamma\vdash\beta}\quad\text{(use function)}
\]
This rule\textsf{'}s numerator now requires us to prove two new sequents: $\gamma\vdash\alpha\Rightarrow\alpha$
and $\gamma\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$. To prove
the first of these sequents, apply the rule \textsf{``}$\text{create function}$\textsf{''}
like this:
\[
\frac{\gamma,\alpha\vdash\alpha}{\gamma\vdash\alpha\Rightarrow\alpha}\quad\text{(create function)}
\]
The sequent in the numerator $\gamma,\alpha\vdash\alpha$ is proved
directly by the axiom \textsf{``}$\text{use value}$\textsf{''}. The sequent $\gamma\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$
is the same as $\gamma\vdash\gamma$ and is also proved by the axiom
\textsf{``}$\text{use value}$\textsf{''}.

The proof of the sequent~(\ref{eq:ch-example-sequent-2}) is now
complete and can be drawn as a tree (see Figure~\ref{fig:Proof-of-the-sequent-example-2}).
The next step is to convert that proof to Scala code.

To do that, we combine the code expressions that correspond to each
of the proof rules we used. We need to retrace the proof backwards,
starting from the leaves of the tree and going towards the root. We
will then combine the corresponding $\text{Proof}\left(...\right)$
code expressions.

\begin{figure}
\begin{centering}
{\footnotesize{}}%
\fbox{\begin{minipage}[t]{0.7\columnwidth}%
\begin{center}
{\footnotesize{}}{\footnotesize{}\Tree[ .$\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta $ [ .{rule \textsf{``}$\text{create function}$\textsf{''}} [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta$ [ .{rule \textsf{``}$\text{use function}$\textsf{''}} [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\alpha\Rightarrow\alpha$ [ .{rule \textsf{``}$\text{create function}$\textsf{''}} [ .$\gamma,\alpha\vdash\alpha$ {axiom \textsf{``}$\text{use value}$\textsf{''}} ] ] ] [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$ {axiom \textsf{``}$\text{use value}$\textsf{''}} ] ] ] ] ]}
\par\end{center}%
\end{minipage}}{\footnotesize\par}
\par\end{centering}
\caption{Proof tree for sequent~(\ref{eq:ch-example-sequent-2}).\label{fig:Proof-of-the-sequent-example-2}}
\end{figure}

Begin with the left-most leaf: \textsf{``}$\text{use value}$\textsf{''}. That rule
gives the code $x^{:A}$:
\[
\text{Proof}\left(\gamma,\alpha\vdash\alpha\right)_{\text{given }x^{:A}}=x^{:A}\quad.
\]
Here \textsf{``}given $x^{:A}$\textsf{''} means that $x^{:A}$ must be a proof of
the premise $\alpha$ in the sequent $\gamma,\alpha\vdash\alpha$
(recall that $\alpha$ denotes ${\cal CH}(A)$, and so $x$ has type
$A$). We need to use the same $x^{:A}$ when we write the code for
the previous rule, \textsf{``}$\text{create function}$\textsf{''}:
\[
\text{Proof}\left(\gamma\vdash\alpha\Rightarrow\alpha\right)=\big(x^{:A}\rightarrow\text{Proof}\left(\gamma,\alpha\vdash\alpha\right)_{\text{given }x^{:A}}\big)=(x^{:A}\rightarrow x)\quad.
\]

Note that in this code we are able to use a value $x$ of type $A$
even though no such value is given as an argument of our function
\lstinline!s[A, B]!. The reason is that the sequent $\gamma,\alpha\vdash\alpha$
has an extra premise $\alpha$ added to the set of premises only for
this step of the proof. Once we are finished with this step, we again
will not have any values of type $A$ available. In the code, this
corresponds to the local scoping of the bound value $x^{:A}$ in the
function $x^{:A}\rightarrow x$. 

We continue tracing the proof tree bottom-up. The right-most leaf
\textsf{``}$\text{use value}$\textsf{''} corresponds to the code $f^{:(A\rightarrow A)\rightarrow B}$,
where $f$ is the code corresponding to the premise $\gamma=(\alpha\Rightarrow\alpha)\Rightarrow\beta$.
So, we can write:
\[
\text{Proof}\left(\gamma\vdash\gamma\right)_{\text{given }f^{:(A\rightarrow A)\rightarrow B}}=f^{:(A\rightarrow A)\rightarrow B}\quad.
\]
The previous rule (\textsf{``}$\text{use function}$\textsf{''}) combines the two
preceding proofs:
\begin{align*}
 & \text{Proof}\left((\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta\right)_{\text{given }f^{:\left(A\rightarrow A\right)\rightarrow B}}\\
 & =\text{Proof}\,(\gamma\vdash\gamma)\left(\text{Proof}\,(\gamma\vdash\alpha\Rightarrow\alpha)\right)_{\text{given }f^{:\left(A\rightarrow A\right)\rightarrow B}}\\
 & =f(x^{:A}\rightarrow x)\quad.
\end{align*}
Going further backwards, we find that the rule applied before \textsf{``}$\text{use function}$\textsf{''}
was \textsf{``}$\text{create function}$\textsf{''}. We need to provide the same $f^{:\left(A\rightarrow A\right)\rightarrow B}$
as in the premise above, and so we obtain the code:
\begin{align*}
 & \text{Proof}\left(\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta\right)\\
 & =f^{:\left(A\rightarrow A\right)\rightarrow B}\rightarrow\text{Proof}\left((\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta\right)_{\text{given }f^{:\left(A\rightarrow A\right)\rightarrow B}}\\
 & =f^{:\left(A\rightarrow A\right)\rightarrow B}\rightarrow f(x^{:A}\rightarrow x)\quad.
\end{align*}
This is the final code expression that implements the type $((A\rightarrow A)\rightarrow B)\rightarrow B$.
In this way, we have systematically derived the code from the type
signature of a function. That code can be written in Scala as:
\begin{lstlisting}
def s[A, B]: ((A => A) => B) => B = { (f : A => A) => f(x => x) }
\end{lstlisting}
$\square$

We found the proof tree in Figure~\ref{fig:Proof-of-the-sequent-example-2}
by combining various proof rules that match our sequents. But we had
to guess how to apply the \textsf{``}use function\textsf{''} rule: it was not obvious
how to assign the rule\textsf{'}s variable $p$. If we \emph{somehow} find
a proof tree for a sequent, we can derive the corresponding code (perform
\textsf{``}code inference\textsf{''} from type). As we have seen, choosing the proof
rules from Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
requires guessing or trying different possibilities. 

In other words, the rules in Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
do not provide an algorithm for finding a proof tree automatically.
It turns out that one can replace the rules in Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
by a different but equivalent set of derivation rules that \emph{do}
give an algorithm (called the \textsf{``}\index{LJT algorithm}LJT algorithm\textsf{''},
see Section~\ref{app:The-LJT-algorithm} below). That algorithm either
finds that the given formula cannot be proved, or it finds a proof
and infers code that has the given type signature.

The library \index{curryhoward library@\texttt{curryhoward} library}\lstinline!curryhoward!\texttt{}\footnote{\texttt{\href{https://github.com/Chymyst/curryhoward}{https://github.com/Chymyst/curryhoward}}}
implements the LJT algorithm. Here are some examples of using this
library for \index{code inference}code inference. We will run the
\lstinline!ammonite!\texttt{}\footnote{\texttt{\href{http://ammonite.io/\#Ammonite-Shell}{http://ammonite.io/\#Ammonite-Shell}}}
shell to load the library more easily.

As a non-trivial (but artificial) example, consider the type signature:
\[
\forall(A,B).\,\left(\left(\left(\left(A\rightarrow B\right)\rightarrow A\right)\rightarrow A\right)\rightarrow B\right)\rightarrow B\quad.
\]
It is not obvious whether a function with this type signature exists.
The LJT algorithm can figure that out and derive the code automatically.
The library does this via the method \lstinline!implement!:
\begin{lstlisting}
@ import $ivy.`io.chymyst::curryhoward:0.3.8`, io.chymyst.ch._

@ def f[A, B]: ((((A => B) => A) => A) => B) => B  =  implement
defined function f

@ println(f.lambdaTerm.prettyPrint)
a => a (b => b (c => a (d => c)))
\end{lstlisting}
The code $a\rightarrow a\left(b\rightarrow b\left(c\rightarrow a\left(d\rightarrow c\right)\right)\right)$
was produced automatically for the function \lstinline!f!. The function
\lstinline!f! has been compiled and is ready to be used in any subsequent
code.

A compile-time error occurs when no fully parametric function has
the given type signature:
\begin{lstlisting}
@ def g[A, B]: ((A => B) => A) => A  =  implement
cmd3.sc:1: type ((A => B) => A) => A cannot be implemented
\end{lstlisting}
The logical formula corresponding to this type signature is:
\begin{equation}
\forall(\alpha,\beta).\,\left(\left(\alpha\Rightarrow\beta\right)\Rightarrow\alpha\right)\Rightarrow\alpha\quad.\label{eq:ch-example-3-peirce-law}
\end{equation}
This formula is known as \textbf{Peirce\textsf{'}s law}\index{Peirce\textsf{'}s law}\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Peirce\%27s_law}{https://en.wikipedia.org/wiki/Peirce\%27s\_law}}}
and gives an example showing that the logic of types in functional
programming languages is not Boolean (other examples are shown in
Sections~\ref{subsec:Example:-Failure-of-Boolean-logic} and~\ref{subsec:Relationship-between-Boolean}).
Peirce\textsf{'}s law is true in Boolean logic but does not hold in the constructive
logic, i.e., it cannot be derived using the proof rules in Table~\ref{tab:Proof-rules-of-constructive-and-boolean}.
If we try to implement \lstinline!g[A, B]! with the type signature
shown above via fully parametric code, we will fail to write code
that compiles without type errors. This is because no such code exists,
\textemdash{} not because we are insufficiently clever. The LJT algorithm
can \emph{prove} that the given type signature cannot be implemented.
The \lstinline!curryhoward! library will then print an error message,
and compilation will fail.

As another example, let us verify that the type signature from Section~\ref{subsec:Motivation-and-outlook}
is not implementable:
\begin{lstlisting}
@ def bad2[A, B, C](g: A => Either[B, C]): Either[A => B, A => C] = implement
cmd4.sc:1: type (A => Either[B, C]) => Either[A => B, A => C] cannot be implemented
\end{lstlisting}

The LJT algorithm will sometimes find several inequivalent proofs
of the same logic formula. In that case, each of the different proofs
will be automatically translated into code. The \lstinline!curryhoward!
library tries to select the code that has the least information loss,
according to several heuristics. In many cases, the heuristics select
the implementation that is most useful to the programmer.

The rules of constructive logic and the LJT algorithm define rigorously
what it means to write code \textsf{``}guided by the types\textsf{''}. However, in
order to use the LJT algorithm well, a programmer needs to learn how
to infer code from types by hand. We will practice doing that throughout
the book.

\subsection{The LJT algorithm\label{app:The-LJT-algorithm}}

The LJT algorithm solves an important problem: namely, that the logic
rules in Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
do not provide an algorithm for finding a proof for a given sequent.
In the previous section, we saw an example showing that searching
for a proof of a sequent via Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
sometimes requires guessing. To illustrate this difficulty on another
example, let us try proving the sequent:
\[
A,B\vee C\vdash(A\wedge B)\vee C\quad.
\]
We expect that this sequent is provable because we can write the corresponding
Scala code:
\begin{lstlisting}
def f[A, B, C](a: A): Either[B, C] => Either[(A, B), C] = {
  case Left(b)    => Left((a, b))
  case Right(c)   => Right(c)
}
\end{lstlisting}
How can we obtain a proof of this sequent via Table~\ref{tab:Proof-rules-of-constructive-and-boolean}?
We could potentially apply the rules \textsf{``}create \lstinline!Left!\textsf{''},
\textsf{``}create \lstinline!Right!\textsf{''}, \textsf{``}use \lstinline!Either!\textsf{''}, and
\textsf{``}use function\textsf{''}. But we will get stuck at the next step, no matter
what rule we choose. Let us see why:

To apply \textsf{``}create \lstinline!Left!\textsf{''}, we first need to prove the
sequent $A,B\vee C\vdash A\wedge B$. But this sequent cannot be proved:
we do not necessarily have values of both types $A$ and $B$ if we
are only given values of type $A$ and of type \lstinline!Either[B, C]!.
To apply \textsf{``}create \lstinline!Right!\textsf{''}, we need to prove the sequent
$A,B\vee C\vdash C$. Again, we find that this sequent cannot be proved.
The next choice is the rule \textsf{``}use \lstinline!Either!\textsf{''} that matches
any goal of the sequent as the proposition $\gamma$. But we are then
required to choose two new propositions ($\alpha$ and $\beta$) such
that we can prove $A,B\vee C\vdash\alpha\vee\beta$ as well as $A,B\vee C,\alpha\vdash(A\wedge B)\vee C$
and $A,B\vee C,\beta\vdash(A\wedge B)\vee C$. It is not clear how
we should choose $\alpha$ and $\beta$ in order to make progress
with the proof. The remaining rule, \textsf{``}use function\textsf{''}, similarly
requires us to choose a new proposition $\alpha$ such that we can
prove $A,B\vee C\vdash\alpha$ and $A,B\vee C\vdash\alpha\Rightarrow((A\wedge B)\vee C)$.
The rules give us no guidance for choosing $\alpha$ appropriately.

The rules in Table~\ref{tab:Proof-rules-for-constructive-logic}
are not helpful for proof search because the rules \textsf{``}use function\textsf{''}
and \textsf{``}use \lstinline!Either!\textsf{''} require us to choose new unknown
propositions and to prove sequents more complicated than the ones
we had before. For instance, the rule \textsf{``}use function\textsf{''} gives a proof
of $\Gamma\vdash\beta$ only if we first choose some other proposition
$\alpha$ and prove the sequents $\Gamma\vdash\alpha$ and $\Gamma\vdash\alpha\Rightarrow\beta$.
The rule does not tell us how to choose the proposition $\alpha$
correctly. We need to guess the correct $\alpha$ by trial and error.
Even after choosing $\alpha$ in some way, we will have to prove a
more complicated sequent ($\Gamma\vdash\alpha\Rightarrow\beta$).
It is not guaranteed that we are getting closer to finding the proof
of the initial sequent ($\Gamma\vdash\beta$). 

It is far from obvious how to overcome that difficulty. Mathematicians
have studied the constructive logic for more than 60 years, trying
to replace the rules in Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
by a different but equivalent set of derivation rules that require
no guessing when looking for a proof. The first partial success came
in 1935 with an algorithm called \textsf{``}LJ\textsf{''}.\footnote{See \texttt{\href{https://en.wikipedia.org/wiki/Sequent_calculus\#Overview}{https://en.wikipedia.org/wiki/Sequent\_calculus\#Overview}}}
The LJ algorithm works in many cases but still has a significant problem:
one of its derivation rules may be applied infinitely many times,
leading to an infinite loop. So, the LJ algorithm is not guaranteed
to terminate without some heuristics for avoiding infinite loops.
This problem is solved by a modification of the LJ algorithm, called
\index{LJT algorithm}LJT, first formulated in 1992.\footnote{An often cited paper by R.~Dyckhoff\index{Roy Dyckhoff} is \texttt{\href{https://philpapers.org/rec/DYCCSC}{https://philpapers.org/rec/DYCCSC}}.
For the history of that research, see \texttt{\href{https://research-repository.st-andrews.ac.uk/handle/10023/8824}{https://research-repository.st-andrews.ac.uk/handle/10023/8824}}} 

We will first present the LJ algorithm. Although that algorithm does
not guarantee termination, it is simpler to understand and to apply
by hand. Then we will show how to modify the LJ algorithm in order
to obtain the always-terminating LJT algorithm.

\subsubsection*{The LJ algorithm}

Figure~\ref{fig:Rules-of-the-LJ-algorithm} shows the LJ algorithm\textsf{'}s
axioms and derivation rules. Each rule says that the bottom sequent
will be proved if proofs are given for sequent(s) at the top. For
each possible sub-expression (conjunction $X\wedge Y$, disjunction
$X\vee Y$, and implication $X\Rightarrow Y$) there is one rule where
that sub-expression is a premise (at \textsf{``}left\textsf{''}) and one rule where
that sub-expression is the goal (at \textsf{``}right\textsf{''}). Those sub-expressions
are shown in blue in Figure~\ref{fig:Rules-of-the-LJ-algorithm}
to help us look for a proof. To find out which rules apply, we match
some part of the sequent with a blue sub-expression.

\begin{figure}
\begin{centering}
\fbox{\begin{minipage}[t]{0.7\linewidth}%
\begin{align*}
\frac{}{\Gamma,X\vdash{\color{blue}X}}~(\text{Id})\qquad & \qquad\frac{}{\Gamma\vdash{\color{blue}\top}}~(\text{True})\\
\frac{\Gamma,A\Rightarrow B\vdash A\quad\Gamma,B\vdash C}{\Gamma,{\color{blue}A\Rightarrow B}\vdash C}~(\text{Left}\Rightarrow)\qquad & \qquad\frac{\Gamma,A\vdash B}{\Gamma\vdash{\color{blue}A\Rightarrow B}}~(\text{Right}\Rightarrow)\\
\frac{\Gamma,A_{i}\vdash C}{\Gamma,{\color{blue}A_{1}\wedge A_{2}}\vdash C}~(\text{Left}\wedge_{i})\qquad & \qquad\frac{\Gamma\vdash A\quad\quad\Gamma\vdash B}{\Gamma\vdash{\color{blue}A\wedge B}}~(\text{Right}\wedge)\\
\frac{\Gamma,A\vdash C\quad\Gamma,B\vdash C}{\Gamma,{\color{blue}A\vee B}\vdash C}~(\text{Left}\vee)\qquad & \qquad\frac{\Gamma\vdash A_{i}}{\Gamma\vdash{\color{blue}A_{1}\vee A_{2}}}~(\text{Right}\vee_{i})
\end{align*}
%
\end{minipage}}
\par\end{centering}
\caption{Axioms and derivation rules of the LJ algorithm. Each of the rules
\textquotedblleft ($\text{Left}\wedge_{i}$)\textquotedblright{} and
\textquotedblleft ($\text{Right}\vee_{i}$)\textquotedblright{} have
two versions, with $i=1$ or $i=2$. \label{fig:Rules-of-the-LJ-algorithm}}
\end{figure}

It turns out that the rules in Figure~\ref{fig:Rules-of-the-LJ-algorithm}
are \emph{equivalent} to the rules in Table~\ref{tab:Proof-rules-for-constructive-logic}.
The proof is beyond the scope of this book. We only remark that this
equivalence is far from obvious. To prove it, one needs to demonstrate
that any sequent derived through the first set of rules is also derivable
through the second set, and vice versa.

To illustrate the LJ algorithm, let us prove the sequent~(\ref{eq:ch-example-sequent-2}).
Denote that sequent by $S_{0}$:
\[
S_{0}\triangleq\emptyset\vdash\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\Rightarrow\beta\quad.
\]
 Since the goal of $S_{0}$ contains an implication, we use the rule
\textsf{``}($\text{Right}\Rightarrow$)\textsf{''} and get a sequent $S_{1}$:
\[
S_{1}\triangleq\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\vdash\beta\quad.
\]
Now the implication is in the premise, so we use the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}
and get two new sequents:
\[
S_{2}\triangleq\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\vdash\alpha\Rightarrow\alpha\quad,\quad\quad S_{3}\triangleq\beta\vdash\beta\quad.
\]
Sequent $S_{3}$ follows from the \textsf{``}(Id)\textsf{''} axiom, so it remains
to prove $S_{2}$. Since $S_{2}$ contains an implication both as
a premise and as the goal, we may apply either the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}
or the rule \textsf{``}($\text{Right}\Rightarrow$)\textsf{''}. We choose to apply
\textsf{``}($\text{Left}\Rightarrow$)\textsf{''} and get two new sequents:
\[
S_{4}\triangleq\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\vdash\alpha\Rightarrow\alpha\quad,\quad\quad S_{5}:\beta\vdash\alpha\Rightarrow\alpha\quad.
\]
Notice that $S_{4}=S_{2}$. So, our proof search is getting into an
infinite loop trying to prove the same sequent $S_{2}$ over and over
again. We can prove $S_{5}$ but this will not help us break the loop.

Once we recognize the problem, we backtrack to the point where we
chose to apply \textsf{``}($\text{Left}\Rightarrow$)\textsf{''} to $S_{2}$. That
was a bad choice, so let us instead apply \textsf{``}($\text{Right}\Rightarrow$)\textsf{''}
to $S_{2}$. This yields a new sequent $S_{6}$:
\[
S_{6}\triangleq\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta,\alpha\vdash\alpha\quad.
\]
This sequent follows from the \textsf{``}(Id)\textsf{''} axiom. There are no more
sequents to prove, so the proof of $S_{0}$ is finished. It can be
drawn as a \index{proof tree}\textbf{proof tree} like this:
\[
\xymatrix{\xyScaleY{1.5pc}\xyScaleX{4pc} &  &  & (\text{Id})\\
\ar[r]\sp(0.35){S_{0}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.5){S_{1}} & (\text{Left}\Rightarrow)\ar[r]\sp(0.5){S_{2}}\ar[ru]\sp(0.6){S_{3}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.65){S_{6}} & (\text{Id})
}
\]
The nodes of the proof tree are axioms or derivation rules, and the
edges are intermediate sequents required by the rules. Some rule nodes
branch into several sequents because some rules require more than
one new sequent to be proved. The leaves of the tree are axioms that
do not require proving any further sequents. 

\subsubsection*{Extracting code from proofs}

According to the Curry-Howard correspondence, a sequent (such as $A,B,...,C\vdash X$)
represents the task of writing a fully parametric code expression
of type $X$ that uses some given values of types $A$, $B$, ...,
$C$. The sequent is true (i.e., can be proved) if that code expression
can be found. So, the code serves as an \textsf{``}evidence of proof\textsf{''} for
the sequent.

\begin{figure}
\begin{centering}
{\small{}}%
\noindent\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
{\small{}
\begin{align*}
\frac{}{\Gamma,A\vdash{\color{blue}A}}~(\text{Id})\quad & \quad\text{Proof}\,(\Gamma,A\vdash A)_{\text{given }p^{:\Gamma},x^{:A}}=x\\
\frac{}{\Gamma\vdash{\color{blue}\top}}~(\text{True})\quad & \quad\text{Proof}\,(\Gamma\vdash\top)_{\text{given }p^{:\Gamma}}=1\\
\frac{\Gamma,A\Rightarrow B\vdash A\quad\Gamma,B\vdash C}{\Gamma,{\color{blue}A\Rightarrow B}\vdash C}~(\text{Left}\Rightarrow)\quad & \quad\text{Proof}\,(\Gamma,A\Rightarrow B\vdash C)_{\text{given }p^{:\Gamma},q^{:A\rightarrow B}}\\
 & \quad\quad=\text{Proof}\,(\Gamma,B\vdash C)_{\text{given }p,b^{:B}}\\
\text{where} & \quad b^{:B}\triangleq q\big(\text{Proof}\,(\Gamma,A\Rightarrow B\vdash A)_{\text{given }p,q}\big)\\
\frac{\Gamma,A\vdash B}{\Gamma\vdash{\color{blue}A\Rightarrow B}}~(\text{Right}\Rightarrow)\quad & \quad\text{Proof}\,(\Gamma\vdash A\Rightarrow B)_{\text{given }p^{:\Gamma}}\\
 & \quad\quad=x^{:A}\rightarrow\text{Proof}\,(\Gamma,A\vdash B)_{\text{given }p^{:\Gamma},x^{:A}}\\
\frac{\Gamma,A\vdash C}{\Gamma,{\color{blue}A\wedge B}\vdash C}~(\text{Left}\wedge_{1})\quad & \quad\text{Proof}\,(\Gamma,A\wedge B\vdash C)_{\text{given }p^{:\Gamma},(a^{:A}\times b^{:B})}\\
 & \quad\quad=\text{Proof}\,(\Gamma,A\vdash C)_{\text{given }p^{:\Gamma},a^{:A}}\\
\frac{\Gamma,B\vdash C}{\Gamma,{\color{blue}A\wedge B}\vdash C}~(\text{Left}\wedge_{2})\quad & \quad\text{Proof}\,(\Gamma,A\wedge B\vdash C)_{\text{given }p^{:\Gamma},(a^{:A}\times b^{:B})}\\
 & \quad\quad=\text{Proof}\,(\Gamma,B\vdash C)_{\text{given }p^{:\Gamma},b^{:B}}\\
\frac{\Gamma\vdash A\quad\quad\Gamma\vdash B}{\Gamma\vdash{\color{blue}A\wedge B}}~(\text{Right}\wedge)\quad & \quad\text{Proof}\,(\Gamma\vdash A\wedge B)_{\text{given }p^{:\Gamma}}\\
 & \quad\quad=\text{Proof}\,(\Gamma\vdash A)_{\text{given }p^{:\Gamma}}\\
 & \quad\quad\quad\times\text{Proof}\,(\Gamma\vdash B)_{\text{given }p^{:\Gamma}}\\
\frac{\Gamma,A\vdash C\quad\quad\Gamma,B\vdash C}{\Gamma,{\color{blue}A\vee B}\vdash C}~(\text{Left}\vee)\quad & \quad\text{Proof}\,(\Gamma,A\vee B\vdash C)_{\text{given }p^{:\Gamma},q^{:A+B}}\\
 & \quad\quad=q\triangleright\begin{array}{|c||c|}
 & C\\
\hline A & x^{:A}\rightarrow\text{Proof}\,(\Gamma,A\vdash C)_{\text{given }p,x}\\
B & y^{:B}\rightarrow\text{Proof}\,(\Gamma,B\vdash C)_{\text{given }p,y}
\end{array}\\
\frac{\Gamma\vdash A}{\Gamma\vdash{\color{blue}A\vee B}}~(\text{Right}\vee_{1})\quad & \quad\text{Proof}\,(\Gamma\vdash A\vee B)_{\text{given }p^{:\Gamma}}=\text{Proof}\,(\Gamma\vdash A)+\bbnum 0^{:B}\\
\frac{\Gamma\vdash B}{\Gamma\vdash{\color{blue}A\vee B}}~(\text{Right}\vee_{2})\quad & \quad\text{Proof}\,(\Gamma\vdash A\vee B)_{\text{given }p^{:\Gamma}}=\bbnum 0^{:A}+\text{Proof}\,(\Gamma\vdash B)
\end{align*}
\medskip{}
}%
\end{minipage}}{\small\par}
\par\end{centering}
\caption{\label{fig:proof-transformers-for-LJ-rules}Proof transformers for
the rules of the LJ algorithm.}
\end{figure}

In the previous subsection, we have found a proof of the sequent $S_{0}$,
which represents the task of writing a fully parametric function with
type signature $(\left(A\rightarrow A\right)\rightarrow B)\rightarrow B$).
Let us now see how we can extract the code of that function from the
proof of the sequent $S_{0}$.

We start from the leaves of the proof tree and move step by step towards
the initial sequent. At each step, we shorten the proof tree by replacing
some sequent by its corresponding evidence-of-proof code. Eventually
we will replace the initial sequent by its corresponding code. Let
us see how this procedure works for the proof tree of the sequent
$S_{0}$ shown in the previous section.

Since the leaves are axioms, let us write the code corresponding to
each axiom of LJ:
\begin{align*}
 & \frac{}{\Gamma,X\vdash X}~(\text{Id})\quad:\quad\quad\text{Proof}\,(\Gamma,X\vdash X)_{\text{given }p^{:\Gamma},\,x^{:X}}=x\quad;\\
 & \frac{}{\Gamma\vdash\top}~(\text{True})\quad:\quad\quad\text{Proof}\,(\Gamma\vdash\top)_{\text{given }p^{:\Gamma}}=1\quad.
\end{align*}
Here we denote explicitly the values (such as $p$ and $x$) given
as premises to the sequent. The notation $p^{:\Gamma}$ means all
values given in the set of premises $\Gamma$. Below we will assume
that the propositions $\alpha$ and $\beta$ correspond to types $A$
and $B$; that is, $\alpha\triangleq{\cal CH}(A)$ and $\beta\triangleq{\cal CH}(B)$.

The leaves in the proof tree for $S_{0}$ are the \textsf{``}($\text{Id}$)\textsf{''}
axioms used to prove the sequents $S_{3}$ and $S_{6}$. Let us write
the code that serves as the \textsf{``}evidence of proof\textsf{''} for these sequents.
For brevity, we denote $\gamma\triangleq\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta$
and $C\triangleq\left(A\rightarrow A\right)\rightarrow B$, so that
$\gamma={\cal CH}(C)$. Then we can write:
\begin{align*}
 & S_{3}\triangleq\beta\vdash\beta\quad,\quad\quad\text{Proof}\,(S_{3})_{\text{given }y^{:B}}=y\quad,\\
 & S_{6}\triangleq\gamma,\alpha\vdash\alpha\quad,\quad\quad\text{Proof}\,(S_{6})_{\text{given }q^{:C},\,x^{:A}}=x\quad.
\end{align*}
Note that the proof of $S_{6}$ does not use the first given value
$q^{:C}$ (corresponding to the premise $\gamma$).

We now shorten the proof tree by replacing the sequents $S_{3}$ and
$S_{6}$ by their \textsf{``}evidence of proof\textsf{''}:
\[
\xymatrix{\xyScaleY{1.5pc}\xyScaleX{4.2pc} &  &  & \square\\
\ar[r]\sp(0.35){S_{0}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.5){S_{1}} & (\text{Left}\Rightarrow)\ar[r]\sp(0.5){S_{2}}\ar[ru]\sp(0.6){(y)_{\text{given }y^{:B}}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.65){(x)_{\text{given }q^{:C},x^{:A}}} & \square
}
\]

The next step is to consider the proof of $S_{2}$, which is found
by applying the rule \textsf{``}($\text{Right}\Rightarrow$)\textsf{''}. This rule
promises to give a proof of $S_{2}$ if we have a proof of $S_{6}$.
In order to extract code from that rule, we can write a function that
transforms a proof of $S_{6}$ into a proof of $S_{2}$. That function
is the proof transformer corresponding to the rule \textsf{``}($\text{Right}\Rightarrow$)\textsf{''}.
That rule and its transformer are defined as:
\begin{align*}
\frac{\Gamma,A\vdash B}{\Gamma\vdash A\Rightarrow B}~(\text{Right}\Rightarrow)\quad: & \quad\quad\text{Proof}\,(\Gamma\vdash A\Rightarrow B)_{\text{given }p^{:\Gamma}}\\
 & \quad\quad=x^{:A}\rightarrow\text{Proof}\,(\Gamma,A\vdash B)_{\text{given }p^{:\Gamma},\,x^{:A}}\quad.
\end{align*}
Applying the proof transformer to the known proof of $S_{6}$, we
obtain a proof of $S_{2}$:
\[
\text{Proof}\,(S_{2})_{\text{given }q^{:C}}=x^{:A}\rightarrow\text{Proof}\,(S_{6})_{\text{given }q^{:C},\,x^{:A}}=(x^{:A}\rightarrow x)_{\text{given }q^{:C}}\quad.
\]
The proof tree can be now shortened to:
\[
\xymatrix{\xyScaleY{1.5pc}\xyScaleX{3.5pc} &  &  & \square\\
\ar[r]\sp(0.35){S_{0}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.5){S_{1}} & (\text{Left}\Rightarrow)\ar[rr]\sp(0.62){(x^{:A}\rightarrow x)_{\text{given }q^{:C}}}\ar[ru]\sp(0.6){(y)_{\text{given }y^{:B}}} &  & \square
}
\]

The next step is to get the proof of $S_{1}$ obtained by applying
the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}. That rule requires two
previous sequents, so its transformer is a function of two previously
obtained proofs:
\begin{align*}
 & \frac{\Gamma,A\Rightarrow B\vdash A\quad\quad\Gamma,B\vdash C}{\Gamma,A\Rightarrow B\vdash C}~(\text{Left}\Rightarrow)\quad:\\
 & \text{Proof}\,(\Gamma,A\Rightarrow B\vdash C)_{\text{given }p^{:\Gamma},q^{:A\rightarrow B}}=\text{Proof}\,(\Gamma,B\vdash C)_{\text{given }p^{:\Gamma},b^{:B}}\\
 & \quad\quad\text{where}\quad b^{:B}\triangleq q\big(\text{Proof}\,(\Gamma,A\Rightarrow B\vdash A)_{\text{given }p^{:\Gamma},q^{:A\rightarrow B}}\big)\quad.
\end{align*}
In the proof tree shown above, we obtain a proof of $S_{1}$ by applying
that proof transformer to the proofs of $S_{2}$ and $S_{3}$:
\begin{align*}
 & \text{Proof}\,(S_{1})_{\text{given }q^{:C}}=\text{Proof}\,(S_{3})_{\text{given }b^{:B}}\text{ where }b^{:B}\triangleq q(\text{Proof}\,(S_{2}))_{\text{given }q^{:C}}\\
 & \quad=b\text{ where }b^{:B}\triangleq q(x^{:A}\rightarrow x)_{\text{given }q^{:C}}=q(x^{:A}\rightarrow x)_{\text{given }q^{:C}}\quad.
\end{align*}
Substituting this proof into the proof tree, we shorten the tree to:
\[
\xymatrix{\xyScaleY{1.5pc}\xyScaleX{5.5pc}\ar[r]\sp(0.35){S_{0}} & (\text{Right}\Rightarrow)\ar[r]\sp(0.65){q(x^{:A}\rightarrow x)_{\text{given }q^{:C}}} & \square}
\]

It remains to obtain the proof of $S_{0}$ by applying the proof transformer
of the rule \textsf{``}($\text{Right}\Rightarrow$)\textsf{''}:
\begin{align*}
 & \text{Proof}\,(S_{0})=\text{Proof}\,(\emptyset\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta)\\
 & =q^{:(A\rightarrow A)\rightarrow B}\rightarrow\text{Proof}\,(S_{1})_{\text{given }q^{:C}}=q^{:(A\rightarrow A)\rightarrow B}\rightarrow q(x^{:A}\rightarrow x)\quad.
\end{align*}
The proof tree is now shortened to just the code $q^{:(A\rightarrow A)\rightarrow B}\rightarrow q(x^{:A}\rightarrow x)$,
which has type $\left(\left(A\rightarrow A\right)\rightarrow B\right)\rightarrow B$.
So, that code is an evidence of proof for $S_{0}$. In this way, we
have derived the code of a fully parametric function from its type
signature.

Figure~\ref{fig:proof-transformers-for-LJ-rules} shows the proof
transformers for all the rules of the LJ algorithm. Apart from the
special rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}, all other rules have
proof transformers using just one of the code constructions (\textsf{``}create
function\textsf{''}, \textsf{``}create tuple\textsf{''}, \textsf{``}use tuple\textsf{''}, etc.) allowed within
fully parametric code.

\subsubsection*{The LJT algorithm}

As we have seen, the LJ algorithm enters a loop if the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}
gives a sequent we already had at a previous step. That rule requires
us to prove two new sequents:
\[
\frac{\Gamma,A\Rightarrow B\vdash A\quad\quad\Gamma,B\vdash C}{\Gamma,A\Rightarrow B\vdash C}~(\text{Left}\Rightarrow)\quad\quad.
\]
A sign of trouble is that the first of these sequents ($\Gamma,A\Rightarrow B\vdash A$)
does not have a simpler form than the initial sequent ($\Gamma,A\Rightarrow B\vdash C$).
So, it is not clear that we are getting closer to completing the proof.
If $A=C$, the new sequent will simply repeat the initial sequent,
immediately creating a loop.

In some cases, a repeated sequent will occur after more than one step.
It is not easy to formulate rigorous conditions for stopping the loop
or for avoiding the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}.

The LJT algorithm solves this problem by removing the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}
from the LJ algorithm. Instead, \emph{four} new rules are introduced.
Each of these rules contains a different pattern instead of $A$ in
the premise $A\Rightarrow C$:
\begin{align*}
\text{(}A\text{ is atomic)\,}\frac{\Gamma,A,B\vdash D}{\Gamma,A,{\color{blue}A\Rightarrow B}\vdash D}~(\text{Left}\Rightarrow_{A})\qquad & \quad\frac{\Gamma,A\Rightarrow B\Rightarrow C\vdash D}{\Gamma,{\color{blue}(A\wedge B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\wedge})\\
\frac{\Gamma,B\Rightarrow C\vdash A\Rightarrow B\quad\quad\Gamma,C\vdash D}{\Gamma,{\color{blue}(A\Rightarrow B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\Rightarrow})\qquad & \quad\frac{\Gamma,A\Rightarrow C,B\Rightarrow C\vdash D}{\Gamma,{\color{blue}(A\vee B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\vee})
\end{align*}
The rule \textsf{``}$\text{Left}\Rightarrow_{A}$\textsf{''} applies only if the implication
starts with an \textsf{``}atomic\textsf{''} type expression, i.e., a single type parameter
or a unit type. In all other cases, the implication must start with
a conjunction, a disjunction, or an implication, which means that
one of the three remaining rules will apply.

The LJT algorithm retains all the rules in Figure~\ref{fig:proof-transformers-for-LJ-rules}
except the rule \textsf{``}($\text{Left}\Rightarrow$)\textsf{''}, which is replaced
by the four new rules. It is far from obvious that the new rules are
equivalent to the old ones. It took mathematicians several decades
to come up with the LJT rules and to prove their validity. This book
will rely on that result and will not attempt to prove it.

The proof transformers for the new rules are shown in Figure~\ref{fig:proof-transformers-for-LJT-rules}.
Figures~\ref{fig:proof-transformers-for-LJ-rules}\textendash \ref{fig:proof-transformers-for-LJT-rules}
define the set of proof transformers sufficient for using the LJT
algorithm in practice. The \index{curryhoward library@\texttt{curryhoward} library}\lstinline!curryhoward!
library\texttt{}\footnote{See \texttt{\href{https://github.com/Chymyst/curryhoward}{https://github.com/Chymyst/curryhoward}}}
implements these proof transformers.

The most complicated of the new rules is the rule \textsf{``}($\text{Left}\Rightarrow_{\Rightarrow}$)\textsf{''}.
It is far from obvious why the rule $\text{Left}\Rightarrow_{\Rightarrow}$
is useful or even correct. This rule is based on a non-trivial logic
identity: 
\[
\left(\left(A\rightarrow B\right)\rightarrow C\right)\rightarrow\left(A\rightarrow B\right)\,\Longleftrightarrow\,\left(B\rightarrow C\right)\rightarrow\left(A\rightarrow B\right)\quad.
\]
Consider the type at the left-hand side of this identity:
\[
\left(\left(A\rightarrow B\right)\rightarrow C\right)\rightarrow B\rightarrow C\quad.
\]
A function with that type can be written as: 
\[
f=k^{:\left(A\rightarrow B\right)\rightarrow C}\rightarrow b^{:B}\rightarrow k\,(\_^{:A}\rightarrow b)\quad.
\]
The function $f$ occurs in the proof transformer for the rule $\text{Left}\Rightarrow_{\Rightarrow}$
(shown below in Table~\ref{fig:proof-transformers-for-LJT-rules}).
Note that this $f$ applies $k$ to a function $(\_\rightarrow b)$
that ignores its argument. We expect to be able to simplify the resulting
expression at the place when $(\_\rightarrow b)$ is applied to some
argument expression, which we can then ignore. For this reason, applying
the transformer for the rule $\text{Left}\Rightarrow_{\Rightarrow}$
results in evidence-of-proof code that is longer than the code obtained
via LJ\textsf{'}s rule transformers. The code obtained via the LJT algorithm
needs to be simplified symbolically.

As an example of using the LJT algorithm, we again prove the sequent
from the previous section: $S_{0}=\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta$.
At each step, only one LJT rule applies to each sequent. The initial
part of the proof tree looks like this:\vspace{-1\baselineskip}
\[
\xymatrix{\xyScaleY{1.5pc}\xyScaleX{6pc} &  &  & ~\\
\ar[r]\sp(0.4){\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta} & (\text{Right}\Rightarrow)\ar[r]\sp(0.5){(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta} & (\text{Left}\Rightarrow_{\Rightarrow})\ar[ru]\sp(0.65){\beta\vdash\beta}\ar[r]\sp(0.65){\alpha\Rightarrow\beta\vdash\alpha\Rightarrow\alpha} & ~
}
\]
The proofs for the sequents $\beta\vdash\beta$ and $\alpha\Rightarrow\beta\vdash\alpha\Rightarrow\alpha$
are the same as before:
\[
\text{Proof}\,(\beta\vdash\beta)_{\text{given }y^{:B}}=y\quad,\quad\quad\text{Proof}\,(\alpha\Rightarrow\beta\vdash\alpha\Rightarrow\alpha)_{\text{given }r^{:A\rightarrow B}}=x^{:A}\rightarrow x\quad.
\]
Substituting these proofs into the proof transformer of the rule \textsf{``}($\text{Left}\Rightarrow_{\Rightarrow}$)\textsf{''}
produces this code:
\begin{align*}
 & \text{Proof}\,((\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta)_{\text{given }q^{:(A\rightarrow A)\rightarrow B}}=q\big(\text{Proof}\,(\alpha\Rightarrow\beta\vdash\alpha\Rightarrow\alpha)_{\text{given }r^{:A\rightarrow B}}\big)\\
 & \quad\quad\text{where }r^{:A\rightarrow B}=a^{:A}\rightarrow q(\_^{:A}\rightarrow a)\\
 & =q(x^{:A}\rightarrow x)\quad.
\end{align*}
The proof of $\alpha\Rightarrow\beta\vdash\alpha\Rightarrow\alpha$
does not actually use the intermediate value $r^{:A\rightarrow B}$
provided by the proof transformer. As a symbolic simplification step,
we may simply omit the code of $r$. The \lstinline!curryhoward!
library always performs symbolic simplification after applying the
LJT algorithm. 

\begin{figure}
\begin{centering}
{\small{}}%
\noindent\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
{\small{}
\begin{align*}
\frac{\Gamma,A,B\vdash D}{\Gamma,A,{\color{blue}A\Rightarrow B}\vdash D}~(\text{Left}\Rightarrow_{A})\quad & \quad\text{Proof}\,(\Gamma,A,A\Rightarrow B\vdash D)_{\text{given }p^{:\Gamma},x^{:A},q^{:A\rightarrow B}}\\
 & \quad\quad=\text{Proof}\,(\Gamma,A,B\vdash D)_{\text{given }p,x,q(x)}\\
\frac{\Gamma,A\Rightarrow B\Rightarrow C\vdash D}{\Gamma,{\color{blue}(A\wedge B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\wedge})\quad & \quad\text{Proof}\,(\Gamma,(A\wedge B)\Rightarrow C\vdash D)_{\text{given }p^{:\Gamma},q^{:A\times B\rightarrow C}}\\
 & \quad\quad=\text{Proof}\,(\Gamma,\\
 & \quad\quad\quad A\Rightarrow B\Rightarrow C\vdash D)_{\text{given }p,(a^{:A}\rightarrow b^{:B}\rightarrow q(a\times b))}\\
\frac{\Gamma,A\Rightarrow C,B\Rightarrow C\vdash D}{\Gamma,{\color{blue}(A\vee B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\vee})\quad & \quad\text{Proof}\,(\Gamma,(A\vee B)\Rightarrow C\vdash D)_{\text{given }p^{:\Gamma},q^{:A+B\rightarrow C}}\\
 & \quad\quad=\text{Proof}\,(\Gamma,A\Rightarrow C,B\Rightarrow C\vdash D)_{\text{given }p,r,s}\\
 & \quad\quad\text{where}~r\triangleq a^{:A}\rightarrow q(a+\bbnum 0)\\
 & \quad\quad\text{ and }s\triangleq b^{:B}\rightarrow q(\bbnum 0+b)\\
\frac{\Gamma,B\Rightarrow C\vdash A\Rightarrow B\quad\Gamma,C\vdash D}{\Gamma,{\color{blue}(A\Rightarrow B)\Rightarrow C}\vdash D}~(\text{Left}\Rightarrow_{\Rightarrow})\quad & \quad\text{Proof}\,(\Gamma,(A\Rightarrow B)\Rightarrow C\vdash D)_{\text{given }p^{:\Gamma},q^{:\left(A\rightarrow B\right)\rightarrow C}}\\
 & \quad\quad=\text{Proof}\,(\Gamma,C\vdash D)_{\text{given }p,c}\\
 & \quad\quad\text{ where}~c^{:C}\triangleq q\big(\text{Proof}\,(\Gamma,\\
 & \quad\quad\quad\quad\quad\quad\quad\quad B\Rightarrow C\vdash A\Rightarrow B)_{\text{given }p,r}\big)\\
 & \quad\quad\text{ and }r^{:B\rightarrow C}\triangleq b^{:B}\rightarrow q(\_^{:A}\rightarrow b)
\end{align*}
\medskip{}
}%
\end{minipage}}{\small\par}
\par\end{centering}
\caption{\label{fig:proof-transformers-for-LJT-rules}Proof transformers for
the four new rules of the \index{LJT algorithm|textit}LJT algorithm.}
\end{figure}

The reason the LJT algorithm terminates is that each rule replaces
a given sequent by one or more sequents with simpler premises or goals.\footnote{The paper \texttt{\href{http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2618}{http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2618}}
shows that the LJT algorithm terminates by giving an explicit decreasing
measure on proof trees.} This guarantees that the proof search will terminate either with
a complete proof or with a sequent to which no more rules apply. An
example of such a \textsf{``}dead-end\textsf{''} sequent is $\alpha\vdash\beta$ where
$\alpha$ and $\beta$ are different, unrelated propositions. When
no more rules apply, the LJT algorithm concludes that the initial
sequent cannot be proved.

To \emph{prove} that there is no proof, one needs to use methods
that are beyond the scope of this book. An introduction to the required
techniques is in the already mentioned book \textsf{``}Proof and Disproof
in Formal Logic\textsf{''} by R.~Bornat\index{Richard Bornat}.

\subsection{Failure of Boolean logic in reasoning about $\mathcal{CH}$-propositions\label{subsec:Example:-Failure-of-Boolean-logic}}

Programmers are familiar with the \index{Boolean logic}Boolean logic
whose operations are written in Scala as \lstinline!x && y! (conjunction),
\lstinline!x || y! (disjunction), and \lstinline*!x* (negation).
However, it turns out that the Boolean logic does \emph{not} always
produce correct conclusions when reasoning about ${\cal CH}$-propositions
and implementable type signatures. For correct reasoning about those
questions, one needs to use the constructive logic.

Let us nevertheless briefly look at how Boolean logic would handle
that reasoning. In the Boolean logic, each proposition ($\alpha$,
$\beta$, ...) is either $True$ or $False$. The operations are $\alpha\wedge\beta$
(conjunction), $\alpha\vee\beta$ (disjunction), and $\neg\alpha$
(negation). The \textbf{implication} \index{implication (in logic)}
($\Rightarrow$) is defined through other operations by:
\begin{equation}
\left(\alpha\Rightarrow\beta\right)\triangleq\left((\neg\alpha)\vee\beta\right)\quad.\label{eq:ch-definition-of-implication-in-Boolean-logic}
\end{equation}

To verify whether a formula is true in the Boolean logic, we can substitute
either $True$ or $False$ into every variable and check if the formula
has the value $True$ in all possible cases. The result can be arranged
into a \index{truth table}truth table. The formula is true if all
values in its truth table are $True$.

Disjunction, conjunction, negation, and implication operations have
the following truth table:
\begin{center}
{\small{}}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
{\small{}$\alpha$} & {\small{}$\beta$} & \textbf{\small{}$\alpha\vee\beta$} & \textbf{\small{}$\alpha\wedge\beta$} & \textbf{\small{}$\neg\alpha$} & \textbf{\small{}$\alpha\Rightarrow\beta$}\tabularnewline
\hline 
\hline 
{\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$True$} & {\small{}$False$} & {\small{}$True$} & {\small{}$False$} & {\small{}$False$} & {\small{}$False$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$True$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
\end{tabular}{\small\par}
\par\end{center}

Using this table, we find that the formula $\alpha\Rightarrow\alpha$
has the value $True$ in all cases, whether $\alpha$ itself is $True$
or $False$. This check is sufficient to show that $\forall\alpha.\,\alpha\Rightarrow\alpha$
is true in Boolean logic.

Here is the truth table for the formulas $\forall(\alpha,\beta).\,(\alpha\wedge\beta)\Rightarrow\alpha$
and $\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\wedge\beta)$.
The first formula is true since all values in its column are $True$,
while the second formula is not true since one value in the last column
is $False$:
\begin{center}
{\small{}}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
{\small{}$\alpha$} & {\small{}$\beta$} & \textbf{\small{}$\alpha\wedge\beta$} & {\small{}$(\alpha\wedge\beta)\Rightarrow\alpha$} & {\small{}$\alpha\Rightarrow(\alpha\wedge\beta)$}\tabularnewline
\hline 
\hline 
{\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$True$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$} & {\small{}$False$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
\end{tabular}{\small\par}
\par\end{center}

Table~\ref{tab:Logical-formulas-Boolean-theorems} shows more examples
of logical formulas that are true in Boolean logic. Each formula is
first written in terms of ${\cal CH}$-propositions (we denote $\alpha\triangleq{\cal CH}(A)$
and $\beta\triangleq{\cal CH}(B)$ for brevity) and then as a Scala
type signature of a function. So, all these type signatures \emph{can}
be implemented.

\begin{table}[h]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\small{}Logic formula} & \textbf{\small{}Type formula} & \textbf{\small{}Scala code}\tabularnewline
\hline 
\hline 
{\footnotesize{}$\forall\alpha.\,\alpha\Rightarrow\alpha$} & {\footnotesize{}$\forall A.\,A\rightarrow A$} & \lstinline!def id[A](x: A): A = x!\tabularnewline
\hline 
{\footnotesize{}$\forall\alpha.\,\alpha\Rightarrow True$} & {\footnotesize{}$\forall A.\,A\rightarrow\bbnum 1$} & \lstinline!def toUnit[A](x: A): Unit = ()!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\vee\beta)$} & {\footnotesize{}$\forall(A,B).\,A\rightarrow A+B$} & \lstinline!def f[A, B](x: A): Either[A, B] = Left(x)!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\wedge\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,A\times B\rightarrow A$} & \lstinline!def f[A, B](p: (A, B)): A = p._1!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\beta\Rightarrow\alpha)$} & {\footnotesize{}$\forall(A,B).\,A\rightarrow(B\rightarrow A)$} & \lstinline!def f[A, B](x: A): B => A = (_ => x)!\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Examples of logical formulas that are true theorems in Boolean logic.\label{tab:Logical-formulas-Boolean-theorems}}
\end{table}

Table~\ref{tab:Logical-formulas-not-Boolean-theorems} shows some
examples of formulas that are \emph{not true} in Boolean logic. Translated
into type formulas and then into Scala, these formulas yield type
signatures that \emph{cannot} be implemented by fully parametric functions.

\begin{table}[h]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\small{}Logic formula} & \textbf{\small{}Type formula} & \textbf{\small{}Scala type signature}\tabularnewline
\hline 
\hline 
{\footnotesize{}$\forall\alpha.\,True\Rightarrow\alpha$} & {\footnotesize{}$\forall A.\,\bbnum 1\rightarrow A$} & \lstinline!def f[A](x: Unit): A!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\vee\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,A+B\rightarrow A$} & \lstinline!def f[A,B](x: Either[A, B]): A!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\wedge\beta)$} & {\footnotesize{}$\forall(A,B).\,A\rightarrow A\times B$} & \lstinline!def f[A,B](p: A): (A, B)!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\Rightarrow\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,(A\rightarrow B)\rightarrow A$} & \lstinline!def f[A,B](x: A => B): A!\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Examples of logical formulas that are \emph{not} true in Boolean logic.\label{tab:Logical-formulas-not-Boolean-theorems}}
\end{table}

At first sight, it may appear from these examples that whenever a
logical formula is true in Boolean logic, the corresponding type signature
can be implemented in code, and vice versa. However, this is \emph{incorrect}:
the rules of Boolean logic are not fully suitable for reasoning about
types in a functional language. We will now show some examples of
formulas that are true in Boolean logic but correspond to unimplementable
type signatures.

The first example is given by the following type:
\begin{equation}
\forall(A,B,C).\,\left(A\rightarrow B+C\right)\rightarrow\left(A\rightarrow B\right)+\left(A\rightarrow C\right)\quad,\label{eq:ch-example-boolean-bad-type}
\end{equation}
which corresponds to the Scala type signature (shown in Section~\ref{subsec:Motivation-and-outlook}):
\begin{lstlisting}
def bad2[A, B, C](g: A => Either[B, C]): Either[A => B, A => C] = ???
\end{lstlisting}
The function \lstinline!bad2! \emph{cannot} be implemented via fully
parametric code. To see why, consider that the only available data
is a function $g^{:A\rightarrow B+C}$, which returns values of type
$B$ or $C$ depending (in some unknown way) on the input value of
type $A$. The function \lstinline!bad2! must return either a function
of type $A\rightarrow B$ or a function of type $A\rightarrow C$.
How can the code of \lstinline!bad2! make that decision? The only
input data is the function $g$ that takes an argument of type $A$.
We could imagine applying $g$ to various arguments of type $A$ and
to see whether $g$ returns a $B$ or a $C$. However, the type $A$
is arbitrary, and a fully parametric function cannot produce a value
of type $A$ in order to apply $g$ to it. So, the decision about
whether to return $A\rightarrow B$ or $A\rightarrow C$ must be independent
of $g$. That decision must be hard-coded in the function \lstinline!bad2!.

Suppose we hard-coded the decision to return a function of type $A\rightarrow B$.
How would we create a function of type $A\rightarrow B$ in the body
of \lstinline!bad2!? Given a value $x^{:A}$ of type $A$, we would
need to compute some value of type $B$. Since the type $B$ is arbitrary
(it is a type parameter), we cannot produce a value of type $B$ from
scratch. The only potential source of values of type $B$ is the given
function $g$. The only way of using $g$ is to apply it to $x^{:A}$.
However, for some $x$, the value $g(x)$ may be of the form \lstinline!Right(c)!,
where \lstinline!c! is of type $C$. In that case, we will have a
value of type $C$, not $B$. So, in general, we cannot guarantee
that we can always obtain a value of type $B$ from a given value
$x^{:A}$. This means we cannot build a function of type $A\rightarrow B$
out of the function $g$. Similarly, we cannot build a function of
type $A\rightarrow C$ out of $g$. 

Whether we decide to return $A\rightarrow B$ or $A\rightarrow C$,
we will not be able to return a value of the required type, as we
just saw. We must conclude that we cannot implement \lstinline!bad!
as a fully parametric function.

We could try to switch between $A\rightarrow B$ and $A\rightarrow C$
depending on a given value of type $A$. This idea, however, means
that we are working with a different type signature: 
\[
\forall(A,B,C).\,\left(A\rightarrow B+C\right)\rightarrow A\rightarrow\left(A\rightarrow B\right)+\left(A\rightarrow C\right)\quad.
\]
This type signature \emph{can} be implemented, for instance, by this
Scala code:
\begin{lstlisting}
def q[A, B, C](g: A => Either[B, C]): A => Either[A => B, A => C] = { a =>
  g(a) match {
    case Left(b) => Left(_ => b)
    case Right(c) => Right(_ => c)
  }
}
\end{lstlisting}
But this is not the required type signature~(\ref{eq:ch-example-boolean-bad-type}).

Now let us convert the type signature~(\ref{eq:ch-example-boolean-bad-type})
into a ${\cal CH}$-proposition:
\begin{align}
 & \forall(\alpha,\beta,\gamma).\,\left(\alpha\Rightarrow\left(\beta\vee\gamma\right)\right)\Rightarrow\left(\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)\right)\quad,\label{eq:abc-example-classical-logic-bad}\\
{\color{greenunder}\text{where we denoted}:}\quad & \alpha\triangleq{\cal CH}(A),\quad\beta\triangleq{\cal CH}(B),\quad\gamma\triangleq{\cal CH}(C)\quad.\nonumber 
\end{align}
It turns out that this formula is true in Boolean logic. To prove
this, we need to show that Eq.~(\ref{eq:abc-example-classical-logic-bad})
is equal to $True$ for any Boolean values of the variables $\alpha$,
$\beta$, $\gamma$. One way is to rewrite the expression~(\ref{eq:abc-example-classical-logic-bad})
using the rules of Boolean logic, such as Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic}):
\begin{align*}
 & \gunderline{\alpha\Rightarrow}\left(\beta\vee\gamma\right)\\
{\color{greenunder}\text{definition of }\Rightarrow\text{ via Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & \quad=(\neg\alpha)\vee\beta\vee\gamma\quad,\\
 & \gunderline{\left(\alpha\Rightarrow\beta\right)}\vee\gunderline{\left(\alpha\Rightarrow\gamma\right)}\\
{\color{greenunder}\text{definition of }\Rightarrow\text{ via Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & \quad=\gunderline{(\neg\alpha)}\vee\beta\vee\gunderline{(\neg\alpha)}\vee\gamma\\
{\color{greenunder}\text{property }x\vee x=x\text{ in Boolean logic}:}\quad & \quad=(\neg\alpha)\vee\beta\vee\gamma\quad,
\end{align*}
showing that $\alpha\Rightarrow(\beta\vee\gamma)$ is in fact \emph{equal}
to $\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)$
in Boolean logic.

Let us also give a proof by truth-value reasoning. The only possibility
for an implication $X\Rightarrow Y$ to be $False$ is when $X=True$
and $Y=False$. So, Eq.~(\ref{eq:abc-example-classical-logic-bad})
can be $False$ only if $\left(\alpha\Rightarrow(\beta\vee\gamma)\right)=True$
and $\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)=False$.
A disjunction can be false only when both parts are false; so we must
have both $\left(\alpha\Rightarrow\beta\right)=False$ and $\left(\alpha\Rightarrow\gamma\right)=False$.
This is only possible if $\alpha=True$ and $\beta=\gamma=False$.
But, with these value assignments, we find $\left(\alpha\Rightarrow(\beta\vee\gamma)\right)=False$
rather than $True$ as we assumed. It follows that we cannot ever
make Eq.~(\ref{eq:abc-example-classical-logic-bad}) equal to $False$.
So, Eq.~(\ref{eq:abc-example-classical-logic-bad}) is true in Boolean
logic.

\section{Equivalence of types}

We found a correspondence between types, code, logical propositions,
and proofs, which is known as the \textbf{Curry-Howard correspondence}\index{Curry-Howard correspondence}.
An example of the CH correspondence is that a proof of the logical
proposition:
\begin{equation}
\forall(\alpha,\beta).\,\alpha\Rightarrow\left(\beta\Rightarrow\alpha\right)\label{eq:ch-proposition-example-2}
\end{equation}
corresponds to the code of the following function:
\begin{lstlisting}
def f[A, B]: A => (B => A) = { x => _ => x }
\end{lstlisting}
With the CH correspondence in mind, we may say that the \emph{existence}
of the code \lstinline!x => _ => x! with the type $A\rightarrow(B\rightarrow A)$
\textsf{``}is\textsf{''} a proof of the logical formula~(\ref{eq:ch-proposition-example-2}),
because it shows how to compute a value of type $\forall(A,B).\,A\rightarrow B\rightarrow A$.

The Curry-Howard correspondence maps logic formulas such as $(\alpha\vee\beta)\wedge\gamma$
into type expressions such as $\left(A+B\right)\times C$. We have
seen that types behave similarly to logic formulas in one respect:
A logic formula is a true theorem of constructive logic when the corresponding
type signature can be implemented as a fully parametric function,
and vice versa.

It turns out that the similarity ends here. In other respects, type
expressions behave as \emph{arithmetic} expressions and not as logic
formulas. For this reason, the type notation used in this book denotes
disjunctive types by $A+B$ and tuples by $A\times B$, which is designed
to remind us of arithmetic expressions (such as $1+2$ and $2\times3$)
rather than of logical formulas (such as $A\vee B$ and $A\wedge B$). 

An important use of the type notation is for writing equations with
types. Can we use the arithmetic intuition for writing type equations
such as:
\begin{equation}
\left(A+B\right)\times C=A\times C+B\times C\quad?\label{eq:ch-example-distributive}
\end{equation}
In this section, we will learn how to check whether one type expression
is equivalent to another.

\subsection{Logical identity does not correspond to type equivalence\label{subsec:Logical-identity-not-type-equivalence}}

The CH correspondence maps Eq.~(\ref{eq:ch-example-distributive})
into the logic formula:
\begin{equation}
\forall(A,B,C).\,\left(A\vee B\right)\wedge C=\left(A\wedge C\right)\vee\left(B\wedge C\right)\quad.\label{eq:ch-example-distributive-1}
\end{equation}
This formula is a well-known distributive law\footnote{See \texttt{\href{https://en.wikipedia.org/wiki/Distributive_property\#Rule_of_replacement}{https://en.wikipedia.org/wiki/Distributive\_property\#Rule\_of\_replacement}}}
valid in Boolean logic as well as in the constructive logic. Since
a logical equation $P=Q$ means $P\Rightarrow Q$ and $Q\Rightarrow P$,
the distributive law~(\ref{eq:ch-example-distributive-1}) means
that the two formulas hold:
\begin{align}
 & \forall(A,B,C).\,\left(A\vee B\right)\wedge C\Rightarrow\left(A\wedge C\right)\vee\left(B\wedge C\right)\quad,\label{eq:ch-example-distributive-1a}\\
 & \forall(A,B,C).\,\left(A\wedge C\right)\vee\left(B\wedge C\right)\Rightarrow\left(A\vee B\right)\wedge C\quad.\label{eq:ch-example-distributive-1b}
\end{align}
The CH correspondence maps these logical formulas to fully parametric
functions with types:
\begin{lstlisting}
def f1[A, B, C]: ((Either[A, B], C))    => Either[(A, C), (B, C)] = ???
def f2[A, B, C]: Either[(A, C), (B, C)] => (Either[A, B], C)      = ???
\end{lstlisting}
In the type notation, these type signatures are written as:
\begin{align*}
 & f_{1}^{A,B,C}:\left(A+B\right)\times C\rightarrow A\times C+B\times C\quad,\\
 & f_{2}^{A,B,C}:A\times C+B\times C\rightarrow\left(A+B\right)\times C\quad.
\end{align*}
Since the two logical formulas (\ref{eq:ch-example-distributive-1a})\textendash (\ref{eq:ch-example-distributive-1b})
are true theorems in constructive logic, we expect to be able to implement
the functions \lstinline!f1! and \lstinline!f2!. We could use the
proof rules of the LJT algorithm to obtain proofs of Eqs.~(\ref{eq:ch-example-distributive-1a})\textendash (\ref{eq:ch-example-distributive-1b})
and to derive implementations of \lstinline!f1! and \lstinline!f2!.
Instead, let us exercise our intuition and write the Scala code directly.

To implement \lstinline!f1!, we need to perform pattern matching
on the argument:
\begin{lstlisting}
def f1[A, B, C]: ((Either[A, B], C)) => Either[(A, C), (B, C)] = {
  case (Left(a), c)   => Left((a, c))  // No other choice here.
  case (Right(b), c)  => Right((b, c)) // No other choice here.
}
\end{lstlisting}
In both cases, we have only one possible expression of the correct
type.

Similarly, the implementation of \lstinline!f2! leaves us no choices:
\begin{lstlisting}
def f2[A, B, C]: Either[(A, C), (B, C)] => (Either[A, B], C) = {
  case Left((a, c))   => (Left(a), c)  // No other choice here.
  case Right((b, c))  => (Right(b), c) // No other choice here.
}
\end{lstlisting}
The code of \lstinline!f1! and \lstinline!f2! never discards any
given values; in other words, these functions appear to preserve information.
We can formulate this property rigorously as a requirement that an
arbitrary value \lstinline!x: (Either[A, B], C)! be mapped by \lstinline!f1!
to some value \lstinline!y: Either[(A, C), (B, C)]! and then mapped
by \lstinline!f2! back to \emph{the same} value \lstinline!x!. Similarly,
any value \lstinline!y! of type \lstinline!Either[(A, C), (B, C)]!
should be transformed by \lstinline!f2! and then by \lstinline!f1!
back to the same value \lstinline!y!.

Let us write those conditions as equations:
\[
\forall x^{:(A+B)\times C}.\,f_{2}(f_{1}(x))=x\quad,\quad\quad\forall y^{:A\times C+B\times C}.\,f_{1}\left(f_{2}(y)\right)=y\quad.
\]
If these equations hold, it means that all the information in a value
$x^{:(A+B)\times C}$ is completely preserved inside the value $y\triangleq f_{1}(x)$;
the original value $x$ can be recovered as $x=f_{2}(y)$. Then the
function $f_{1}$ is the \textbf{inverse}\index{inverse function}
of $f_{2}$. Conversely, all the information in a value $y^{:A\times C+B\times C}$
is preserved inside $x\triangleq f_{2}(y)$ and can be recovered by
applying $f_{1}$. Since the values $x^{:(A+B)\times C}$ and $y^{:A\times C+B\times C}$
are arbitrary, it will follow that the \emph{data types} themselves,
$\left(A+B\right)\times C$ and $A\times C+B\times C$, carry equivalent
information. Such types are called equivalent\index{types!equivalent}
or isomorphic\index{types!isomorphic}\index{isomorphic types}.

Generally, we say that types $P$ and $Q$ are \textbf{equivalent}
or \textbf{isomorphic} (denoted $P\cong Q$) \index{type equivalence}when
there exist functions $f_{1}:P\rightarrow Q$ and $f_{2}:Q\rightarrow P$
that are inverses of each other. We can write that using the notation
$(f_{1}\bef f_{2})(x)\triangleq f_{2}(f_{1}(x))$ as:
\[
f_{1}\bef f_{2}=\text{id}\quad,\quad\quad f_{2}\bef f_{1}=\text{id}\quad.
\]
(In Scala, the forward composition $f_{1}\bef f_{2}$ is the function
\lstinline!f1 andThen f2!. We omit type annotations since we already
checked that the types match.) If these conditions hold, there is
a one-to-one correspondence between values of types $P$ and $Q$.
This is the same as to say that the data types $P$ and $Q$ \textsf{``}carry
equivalent information\textsf{''}.

To verify that the Scala functions \lstinline!f1! and \lstinline!f2!
defined above are inverses of each other, we first check if $f_{1}\bef f_{2}=\text{id}$.
Applying $f_{1}\bef f_{2}$ means to apply $f_{1}$ and then to apply
$f_{2}$ to the result. Begin by applying $f_{1}$ to an arbitrary
value $x^{:(A+B)\times C}$. A value $x$ of that type can be in only
one of the two disjoint cases: a tuple \lstinline!(Left(a), c)! or
a tuple \lstinline!(Right(b), c)!, for some values \lstinline!a:A!,
\lstinline!b:B!, and \lstinline!c:C!. The Scala code of \lstinline!f1!
maps these tuples to \lstinline!Left((a, c))! and to \lstinline!Right((b, c))!
respectively; we can see this directly from the code of \lstinline!f1!.
We then apply $f_{2}$ to those values, which maps them back to a
tuple \lstinline!(Left(a), c)! or to a tuple \lstinline!(Right(b), c)!
respectively, according to the code of \lstinline!f2!. These tuples
are exactly the value $x$ we started with. So, applying $f_{1}\bef f_{2}$
to an arbitrary $x^{:(A+B)\times C}$ returns that value $x$. This
is the same as to say that $f_{1}\bef f_{2}=\text{id}$.

To check whether $f_{2}\bef f_{1}=\text{id}$, we apply $f_{2}$ to
an arbitrary value $y^{:A\times C+B\times C}$, which must be one
of the two disjoint cases, \lstinline!Left((a, c))! or \lstinline!Right((b, c))!.
The code of \lstinline!f2! maps these two cases into tuples \lstinline!(Left(a), c)!
and \lstinline!(Right(b), c)! respectively. Then we apply \lstinline!f1!
and map these tuples back to \lstinline!Left((a, c))! and \lstinline!Right((b, c))!
respectively. It follows that applying $f_{2}$ and then $f_{1}$
will always return the initial value. As a formula, this is written
as $f_{2}\bef f_{1}=\text{id}$.

By looking at the code of \lstinline!f1! and \lstinline!f2!, we
can directly observe that these functions are inverses of each other:
the tuple pattern \lstinline!(Left(a), c)! is mapped to \lstinline!Left((a, c))!,
and the pattern \lstinline!(Right(b), c)! to \lstinline!Right((b, c))!,
or vice versa. It is visually clear that no information is lost and
that the original values are returned by function compositions $f_{1}\bef f_{2}$
or $f_{2}\bef f_{1}$.

We find that the logical identity~(\ref{eq:ch-example-distributive-1})
leads to an equivalence of the corresponding types:
\begin{equation}
\left(A+B\right)\times C\cong A\times C+B\times C\quad.\label{eq:ch-distributive-law-types}
\end{equation}
To get Eq.~(\ref{eq:ch-distributive-law-types}) from Eq.~(\ref{eq:ch-example-distributive-1}),
we need to convert a logical formula to an arithmetic expression by
replacing the disjunction operations $\vee$ by $+$ and the conjunctions
$\wedge$ by $\times$ everywhere.

As another example of a logical identity, consider the associativity
law for conjunction:
\begin{equation}
\left(\alpha\wedge\beta\right)\wedge\gamma=\alpha\wedge\left(\beta\wedge\gamma\right)\quad.\label{eq:ch-example-associativity-conjunction}
\end{equation}
The corresponding types are $(A\times B)\times C$ and $A\times(B\times C)$;
in Scala, \lstinline!((A, B), C)! and \lstinline!(A, (B, C))!. We
can define functions that convert between these types without information
loss\index{information loss}:
\begin{lstlisting}
def f3[A, B, C]: (((A, B), C)) => (A, (B, C)) = { case ((a, b), c) =>
                                                       (a, (b, c)) }
def f4[A, B, C]: (A, (B, C)) => (((A, B), C)) = { case (a, (b, c)) =>
                                                       ((a, b), c) }
\end{lstlisting}
By applying these functions to arbitrary values of types \lstinline!((A, B), C)!
and \lstinline!(A, (B, C))!, it is easy to see that the functions
\lstinline!f3! and \lstinline!f4! are inverses of each other. This
is also directly visible in the code: the nested tuple pattern \lstinline!((a, b), c)!
is mapped to the pattern \lstinline!(a, (b, c))! and back. So, the
types $\left(A\times B\right)\times C$ and $A\times\left(B\times C\right)$
are equivalent. We will often write $A\times B\times C$ without parentheses.

Does a logical identity always correspond to an equivalence of types?
This turns out to be \emph{not} so. A simple example of a logical
identity that does not correspond to a type equivalence is:
\begin{equation}
True\vee\alpha=True\quad.\label{eq:ch-example-logic-identity-2}
\end{equation}
Since the CH correspondence maps the logical constant $True$ into
the unit type $\bbnum 1$, the type equivalence corresponding to Eq.~(\ref{eq:ch-example-logic-identity-2})
is $\bbnum 1+A\cong\bbnum 1$. The type denoted by $\bbnum 1+A$ means
\lstinline!Option[A]! in Scala, so the corresponding equivalence
is \lstinline!Option[A]!$\cong$\lstinline!Unit!. Intuitively, this
type equivalence should not hold: an \lstinline!Option[A]! may carry
a value of type \lstinline!A!, which cannot possibly be stored in
a value of type \lstinline!Unit!. We can verify this intuition rigorously
by proving that any fully parametric functions with type signatures
$g_{1}:\bbnum 1+A\rightarrow\bbnum 1$ and $g_{2}:\bbnum 1\rightarrow\bbnum 1+A$
will not satisfy $g_{1}\bef g_{2}=\text{id}$. To verify this, we
note that $g_{2}:\bbnum 1\rightarrow\bbnum 1+A$ must have this type
signature:
\begin{lstlisting}
def g2[A]: Unit => Option[A] = ???
\end{lstlisting}
This function must always return \lstinline!None!, since a fully
parametric function cannot produce values of an arbitrary type \lstinline!A!
from scratch. Therefore, $g_{1}\bef g_{2}$ is also a function that
always returns \lstinline!None!. The function $g_{1}\bef g_{2}$
has type signature $\bbnum 1+A\rightarrow\bbnum 1+A$ or, in Scala
syntax, \lstinline!Option[A] => Option[A]!, and is not equal to the
identity function, because the identity function does \emph{not} always
return \lstinline!None!.

Another example of a logical identity that does not correspond to
a type equivalence is the distributive law:
\begin{equation}
\forall(A,B,C).\,\left(A\wedge B\right)\vee C=\left(A\vee C\right)\wedge\left(B\vee C\right)\quad,\label{eq:ch-example-distributive-2}
\end{equation}
which is \textsf{``}dual\textsf{''} to the law~(\ref{eq:ch-example-distributive-1}),
i.e., it is obtained from Eq.~(\ref{eq:ch-example-distributive-1})
by swapping all conjunctions ($\wedge$) with disjunctions ($\vee$).
In logic, a dual formula to an identity is also an identity. The CH
correspondence maps Eq.~(\ref{eq:ch-example-distributive-2}) into
the type equation:
\begin{equation}
\forall(A,B,C).\,\left(A\times B\right)+C=\left(A+C\right)\times\left(B+C\right)\quad.\label{eq:ch-example-incorrect-identity-2}
\end{equation}
However, the types $A\times B+C$ and $\left(A+C\right)\times\left(B+C\right)$
are \emph{not} equivalent. To see why, look at the possible code of
a function $g_{3}:\left(A+C\right)\times\left(B+C\right)\rightarrow A\times B+C$:
\begin{lstlisting}[numbers=left]
def g3[A, B, C]: ((Either[A, C], Either[B, C])) => Either[(A, B), C] = {
  case (Left(a), Left(b))      => Left((a, b)) // No other choice.
  case (Left(a), Right(c))     => Right(c)     // No other choice.
  case (Right(c), Left(b))     => Right(c)     // No other choice.
  case (Right(c1), Right(c2))  => Right(c1)    // Must discard c1 or c2 here!
}   // May return Right(c2) instead of Right(c1) in the last line.
\end{lstlisting}
In line 5, we have a choice of returning \lstinline!Right(c1)! or
\lstinline!Right(c2)!. Whichever we choose, we will lose information\index{information loss}
because we will have discarded one of the given values \lstinline!c1!,
\lstinline!c2!. After evaluating $g_{3}$, we will not be able to
restore \emph{both} \lstinline!c1! and \lstinline!c2!, no matter
what code we write. So, the composition $g_{3}\bef g_{4}$ with any
$g_{4}$ cannot be equal to the identity function. The type equation~(\ref{eq:ch-example-incorrect-identity-2})
is incorrect.

We find that a logical identity ${\cal CH}(P)={\cal CH}(Q)$ guarantees,
via the CH correspondence, that we can implement \emph{some} fully
parametric functions of types $P\rightarrow Q$ and $Q\rightarrow P$.
However, it is not guaranteed that these functions are inverses of
each other, i.e., that the type conversions $P\rightarrow Q$ or $Q\rightarrow P$
have no information loss\index{information loss}. So, the type equivalence
$P\cong Q$ does not automatically follow from the logical identity
${\cal CH}(P)={\cal CH}(Q)$.

The CH correspondence means that for true propositions ${\cal CH}(X)$
we can compute \emph{some} value $x$ of type $X$. However, the CH
correspondence does not guarantee that the computed value $x^{:X}$
will satisfy any additional properties or laws.

\subsection{Arithmetic identity corresponds to type equivalence}

Looking at the examples of equivalent types, we notice that correct
type equivalences correspond to \emph{arithmetical} identities rather
than \emph{logical} identities. For instance, the logical identity
in Eq.~(\ref{eq:ch-example-distributive-1}) leads to the type equivalence~(\ref{eq:ch-distributive-law-types}),
which looks like a standard identity of arithmetic, such as:
\[
(1+10)\times20=1\times20+10\times20\quad.
\]
The logical identity in Eq.~(\ref{eq:ch-example-distributive-2}),
which does \emph{not} yield a type equivalence, leads to an incorrect
arithmetic equation~(\ref{eq:ch-example-incorrect-identity-2}),
e.g., $\left(1\times10\right)+20\neq\left(1+20\right)\times\left(10+20\right)$.
Similarly, the associativity law~(\ref{eq:ch-example-associativity-conjunction})
leads to a type equivalence and to the arithmetic identity:
\[
\left(a\times b\right)\times c=a\times\left(b\times c\right)\quad,
\]
The logical identity in Eq.~(\ref{eq:ch-example-logic-identity-2}),
which does not yield a type equivalence, leads to an incorrect arithmetic
statement (\textsf{``}$1+a=1$ for all $a$\textsf{''}).

Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction}
summarizes these and other examples of logical identities and the
corresponding type equivalences. In all rows, quantifiers such as
$\forall\alpha$ or $\forall(A,B)$ are implied as necessary.

Because we chose the type notation to be similar to the ordinary arithmetic
notation, it is easy to translate a possible type equivalence into
an arithmetic equation. In all cases, valid arithmetic identities
correspond to type equivalences, and failures to obtain a type equivalence
correspond to incorrect arithmetic identities. With regard to type
equivalence, types such as $A+B$ and $A\times B$ behave similarly
to arithmetic expressions such as $10+20$ and $10\times20$ and not
similarly to logical formulas such as $\alpha\vee\beta$ and $\alpha\wedge\beta$.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|}
\hline 
\textbf{\small{}Logical identity} & \textbf{\small{}Type equivalence (if it holds)}\tabularnewline
\hline 
\hline 
{\small{}$True\vee\alpha=True$} & {\small{}$\bbnum 1+A\not\cong\bbnum 1$}\tabularnewline
\hline 
{\small{}$True\wedge\alpha=\alpha$} & {\small{}$\bbnum 1\times A\cong A$}\tabularnewline
\hline 
{\small{}$False\vee\alpha=\alpha$} & {\small{}$\bbnum 0+A\cong A$}\tabularnewline
\hline 
{\small{}$False\wedge\alpha=False$} & {\small{}$\bbnum 0\times A\cong\bbnum 0$}\tabularnewline
\hline 
{\small{}$\alpha\vee\beta=\beta\vee\alpha$} & {\small{}$A+B\cong B+A$}\tabularnewline
\hline 
{\small{}$\alpha\wedge\beta=\beta\wedge\alpha$} & {\small{}$A\times B\cong B\times A$}\tabularnewline
\hline 
{\small{}$\left(\alpha\vee\beta\right)\vee\gamma=\alpha\vee\left(\beta\vee\gamma\right)$} & {\small{}$\left(A+B\right)+C\cong A+\left(B+C\right)$}\tabularnewline
\hline 
{\small{}$\left(\alpha\wedge\beta\right)\wedge\gamma=\alpha\wedge\left(\beta\wedge\gamma\right)$} & {\small{}$\left(A\times B\right)\times C\cong A\times\left(B\times C\right)$}\tabularnewline
\hline 
{\small{}$\left(\alpha\vee\beta\right)\wedge\gamma=\left(\alpha\wedge\gamma\right)\vee\left(\beta\wedge\gamma\right)$} & {\small{}$\left(A+B\right)\times C\cong A\times C+B\times C$}\tabularnewline
\hline 
{\small{}$\left(\alpha\wedge\beta\right)\vee\gamma=\left(\alpha\vee\gamma\right)\wedge\left(\beta\vee\gamma\right)$} & {\small{}$\left(A\times B\right)+C\not\cong\left(A+C\right)\times\left(B+C\right)$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logic identities with disjunction and conjunction, and the possible
type equivalences.\label{tab:Logical-identities-with-disjunction-and-conjunction}}
\end{table}

We already verified the first line and the last three lines of Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction}.
Other identities are verified in a similar way. Let us begin with
lines 3 and 4 of Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction},
which involve the proposition $False$ and the corresponding \index{void type}void
type $\bbnum 0$ (Scala\textsf{'}s \lstinline!Nothing!). Reasoning about the
void type needs a special technique that we will now develop while
verifying the type isomorphisms $\bbnum 0\times A\cong\bbnum 0$ and
$\bbnum 0+A\cong A$.

\subsubsection{Example \label{subsec:ch-Example-0-times-A}\ref{subsec:ch-Example-0-times-A}\index{examples (with code)}}

Verify the type equivalence $\bbnum 0\times A\cong\bbnum 0$.

\subparagraph{Solution}

Recall that the type notation $\bbnum 0\times A$ represents the Scala
tuple type \lstinline!(Nothing, A)!. To demonstrate that the type
\lstinline!(Nothing, A)! is equivalent to the type \lstinline!Nothing!,
we need to show that the type \lstinline!(Nothing, A)! has \emph{no}
values. Indeed, how could we create a value of type, say, \lstinline!(Nothing, Int)!?
We would need to fill \emph{both} parts of the tuple. We have values
of type \lstinline!Int!, but we can never get a value of type \lstinline!Nothing!.
So, regardless of the type \lstinline!A!, it is impossible to create
any values of type \lstinline!(Nothing, A)!. In other words, the
set of values of the type \lstinline!(Nothing, A)! is empty. But
that is the definition of the void type \lstinline!Nothing!. The
types \lstinline!(Nothing, A)! (denoted by $\bbnum 0\times A$) and
\lstinline!Nothing! (denoted by $\bbnum 0$) are both void and therefore
equivalent.

\subsubsection{Example \label{subsec:ch-Example-0-plus-A}\ref{subsec:ch-Example-0-plus-A}}

Verify the type equivalence $\bbnum 0+A\cong A$.

\subparagraph{Solution}

The type notation $\bbnum 0+A$ corresponds to the Scala type \lstinline!Either[Nothing, A]!.
We need to show that any value of that type can be mapped without
loss of information to a value of type \lstinline!A!, and vice versa.
This means implementing functions $f_{1}:\bbnum 0+A\rightarrow A$
and $f_{2}:A\rightarrow\bbnum 0+A$ such that $f_{1}\bef f_{2}=\text{id}$
and $f_{2}\bef f_{1}=\text{id}$.

The argument of $f_{1}$ is of type \lstinline!Either[Nothing, A]!.
How can we create a value of that type? Our only choices are to create
a \lstinline!Left(x)! with \lstinline!x:Nothing!, or to create a
\lstinline!Right(y)! with \lstinline!y:A!. However, we cannot create
a value \lstinline!x! of type \lstinline!Nothing! because the type
\lstinline!Nothing! has \emph{no} values. We cannot create a \lstinline!Left(x)!.
The only remaining possibility is to create a \lstinline!Right(y)!
with some value \lstinline!y! of type \lstinline!A!. So, any values
of type $\bbnum 0+A$ must be of the form \lstinline!Right(y)!, and
we can extract that \lstinline!y! to obtain a value of type \lstinline!A!:
\begin{lstlisting}
def f1[A]: Either[Nothing, A] => A = {
  case Right(y) => y
  // No need for `case Left(x) => ...` since no `x` can ever be given as `Left(x)`.
}
\end{lstlisting}
For the same reason, there is only one implementation of the function
\lstinline!f2!:
\begin{lstlisting}
def f2[A]: A => Either[Nothing, A] = { y => Right(y) }
\end{lstlisting}
It is clear from the code that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other.

We have just seen that a value of type $\bbnum 0+A$ is a \lstinline!Right(y)!
with some \lstinline!y: A!. Similarly, a value of type $A+\bbnum 0$
is always a \lstinline!Left(x)! with some \lstinline!x: A!. So,
we will use the notation $A+\bbnum 0$ and $\bbnum 0+A$ to \emph{denote}
the \lstinline!Left! and the \lstinline!Right! parts of the disjunctive
type \lstinline!Either!. This notation agrees with the behavior of
the Scala compiler, which will infer the types \lstinline!Either[A, Nothing] !or
\lstinline!Either[Nothing, A]! for these parts:
\begin{lstlisting}
def toLeft[A, B]: A => Either[A, B] = x => Left(x)
def toRight[A, B]: B => Either[A, B] = y => Right(y)

scala> toLeft(123)
res0: Either[Int, Nothing] = Left(123)

scala> toRight("abc")
res1: Either[Nothing, String] = Right("abc")
\end{lstlisting}
We can write the functions \lstinline!toLeft! and \lstinline!toRight!
in a code notation as:
\[
\text{toLeft}^{A,B}\triangleq x^{:A}\rightarrow x+\bbnum 0^{:B}\quad,\quad\quad\text{toRight}^{A,B}\triangleq y^{:B}\rightarrow\bbnum 0^{:A}+y\quad.
\]
In this notation, a value of the disjunctive type is shown without
using Scala class names such as \lstinline!Either!, \lstinline!Right!,
and \lstinline!Left!. This shortens the writing and speeds up code
reasoning. 

The type annotation $\bbnum 0^{:A}$ is helpful to remind ourselves
about the type parameter $A$ used, e.g., by the disjunctive value
$\bbnum 0^{:A}+y^{:B}$ in the body of \lstinline!toRight[A, B]!.
Without this type annotation, $\bbnum 0+y^{:B}$ means a value of
type \lstinline!Either[A, B]! where the parameter $A$ should be
determined by matching the types of other expressions. When it is
clear what types are being used, we may omit type annotations and
write simply $\bbnum 0+y$ instead of $\bbnum 0^{:A}+y^{:B}$.

In the notation $\bbnum 0+y^{:B}$, we use the symbol $\bbnum 0$
rather than an ordinary zero ($0$), to avoid suggesting that $0$
is a value of type $\bbnum 0$. The void type $\bbnum 0$ has \emph{no}
values, unlike the \lstinline!Unit! type, $\bbnum 1$, which has
a value denoted by $1$ in the code notation.

\subsubsection{Example \label{subsec:ch-Example-1xA}\ref{subsec:ch-Example-1xA}}

Verify the type equivalence $A\times\bbnum 1\cong A$.

\subparagraph{Solution}

The corresponding Scala types are the tuple \lstinline!(A, Unit)!
and the type \lstinline!A!. We need to implement functions $f_{1}:\forall A.\,A\times\bbnum 1\rightarrow A$
and $f_{2}:\forall A.\,A\rightarrow A\times\bbnum 1$ and to demonstrate
that they are inverses of each other. The Scala code for these functions
is:
\begin{lstlisting}
def f1[A]: ((A, Unit)) => A = { case (a, ()) => a }
def f2[A]: A => (A, Unit) = { a => (a, ()) }
\end{lstlisting}
Let us first write a proof by reasoning directly with Scala code:
\begin{lstlisting}
(f1 andThen f2)((a,())) == f2(f1((a,())) == f2(a) == (a, ())
(f2 andThen f1)(a) == f1(f2(a)) == f1((a, ())) = a
\end{lstlisting}
Now let us write a proof in the code notation. The codes of $f_{1}$
and $f_{2}$ are:
\[
f_{1}\triangleq a^{:A}\times1\rightarrow a\quad,\quad\quad f_{2}\triangleq a^{:A}\rightarrow a\times1\quad,
\]
where we denoted by $1$ the value \lstinline!()! of the \lstinline!Unit!
type. We find:
\begin{align*}
(f_{1}\bef f_{2})(a^{:A}\times1) & =f_{2}\left(f_{1}(a\times1)\right)=f_{2}\left(a\right)=a\times1\quad,\\
(f_{2}\bef f_{1})(a^{:A}) & =f_{1}(f_{2}(a))=f_{1}(a\times1)=a\quad.
\end{align*}
This shows that both compositions are identity functions. Another
way of writing the proof is by computing the function compositions
symbolically, without applying to a value $a^{:A}$:
\begin{align*}
f_{1}\bef f_{2} & =\left(a\times1\rightarrow a\right)\bef\left(a\rightarrow a\times1\right)=\left(a\times1\rightarrow a\times1\right)=\text{id}^{A\times\bbnum 1}\quad,\\
f_{2}\bef f_{1} & =\left(a\rightarrow a\times1\right)\bef\left(a\times1\rightarrow a\right)=\left(a\rightarrow a\right)=\text{id}^{A}\quad.
\end{align*}


\subsubsection{Example \label{subsec:ch-Example-A+B}\ref{subsec:ch-Example-A+B}}

Verify the type equivalence $A+B\cong B+A$.

\subparagraph{Solution}

The corresponding Scala types are \lstinline!Either[A, B]! and \lstinline!Either[B, A]!.
Use pattern matching to implement the functions required for the type
equivalence:
\begin{lstlisting}
def f1[A, B]: Either[A, B] => Either[B, A] = {
  case Left(a)    => Right(a) // No other choice here.
  case Right(b)   => Left(b)  // No other choice here.
}
def f2[A, B]: Either[B, A] => Either[A, B] = f1[B, A]
\end{lstlisting}
The functions \lstinline!f1! and \lstinline!f2! are implemented
by code that can be derived unambiguously from the type signatures.
For instance, the line \lstinline!case Left(a) => ...! is required
to return a value of type \lstinline!Either[B, A]! by using a given
value \lstinline!a:A!. The only way of doing that is by returning
\lstinline!Right(a)!.

It is clear from the code that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other. To verify that rigorously, we need to
show that \lstinline!f1 andThen f2! is equal to an identity function.
The function \lstinline!f1 andThen f2! applies \lstinline!f2! to
the result of \lstinline!f1!. The code of \lstinline!f1! contains
two \lstinline!case ...! lines, each returning a result. So, we need
to apply \lstinline!f2! separately in each line. Evaluate the code
symbolically:
\begin{lstlisting}
(f1 andThen f2) == {
  case Left(a)    => f2(Right(a))
  case Right(b)   => f2(Left(b))
} == {
  case Left(a)    => Left(a)
  case Right(b)   => Right(b)
}
\end{lstlisting}
The result is a function of type \lstinline!Either[A, B] => Either[A, B]!
that does not change its argument; so, it is equal to the identity
function. 

Let us now write the function \lstinline!f1! in the code notation
and perform the same derivation. We will also develop a useful notation
for functions operating on disjunctive types.

The pattern matching construction in the Scala code of \lstinline!f1!
is similar to a pair of functions with types \lstinline!A => Either[B, A]!
and \lstinline!B => Either[B, A]!. One of these functions is applied
depending on whether the argument of \lstinline!f1! has type $A+\bbnum 0$
or $\bbnum 0+B$. So, we may write the code of \lstinline!f1! as:
\[
f_{1}\triangleq x^{:A+B}\rightarrow\begin{cases}
\quad\text{if }x=a^{:A}+\bbnum 0^{:B}\quad: & \bbnum 0^{:B}+a^{:A}\\
\quad\text{if }x=\bbnum 0^{:A}+b^{:B}\quad: & b^{:B}+\bbnum 0^{:A}
\end{cases}
\]
Since both the argument and the result of $f_{1}$ are disjunctive
types with $2$ parts each, it is convenient to write the code of
$f_{1}$ as a $2\times2$ \emph{matrix} that maps the input parts
to the output parts:\index{disjunctive type!in matrix notation}\index{matrix notation}
\begin{lstlisting}
def f1[A, B]: Either[A, B] => Either[B, A] = {
  case Left(a)    => Right(a)
  case Right(b)   => Left(b)
}
\end{lstlisting}
\[
f_{1}\triangleq\,\begin{array}{|c||cc|}
 & B & A\\
\hline A & \bbnum 0 & a^{:A}\rightarrow a\\
B & b^{:B}\rightarrow b & \bbnum 0
\end{array}\quad.
\]
The rows of the matrix correspond to the \lstinline!case! rows in
the Scala code. There is one row for each part of the disjunctive
type of the argument. The columns of the matrix correspond to the
parts of the disjunctive type of the result.\index{pattern matching!in matrix notation}
The matrix element in row $A$ and column $A$ is a function of type
$A\rightarrow A$ that corresponds to the line \lstinline!case Left(a) => Right(a)!
in the Scala code. The matrix element in row $A$ and column $B$
is written as $\bbnum 0$ because no value of that type is returned.
In this way, we translate all lines of the \lstinline!match!/\lstinline!case!
expression into a code matrix.

The code of $f_{2}$ is written similarly. Let us rename arguments
for clarity:\hfill{}
\begin{lstlisting}
def f2[A, B]: Either[B, A] => Either[A, B] = {
  case Left(y)    => Right(y)
  case Right(x)   => Left(x)
}
\end{lstlisting}
\[
f_{2}\triangleq\,\begin{array}{|c||cc|}
 & A & B\\
\hline B & \bbnum 0 & y^{:B}\rightarrow y\\
A & x^{:A}\rightarrow x & \bbnum 0
\end{array}\quad.
\]
The forward composition $f_{1}\bef f_{2}$ is computed by the standard
rules of row-by-column matrix multiplication.\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Matrix_multiplication}{https://en.wikipedia.org/wiki/Matrix\_multiplication}}}
Any terms containing $\bbnum 0$ are omitted, and the remaining functions
are composed:
\begin{align*}
 & f_{1}\bef f_{2}=\,\begin{array}{|c||cc|}
 & B & A\\
\hline A & \bbnum 0 & a^{:A}\rightarrow a\\
B & b^{:B}\rightarrow b & \bbnum 0
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & A & B\\
\hline B & \bbnum 0 & y^{:B}\rightarrow y\\
A & x^{:A}\rightarrow x & \bbnum 0
\end{array}\\
{\color{greenunder}\text{matrix composition}:}\quad & =\,\,\begin{array}{|c||cc|}
 & A & B\\
\hline A & \gunderline{(a^{:A}\rightarrow a)\bef(x^{:A}\rightarrow x)} & \bbnum 0\\
B & \bbnum 0 & \gunderline{(b^{:B}\rightarrow b)\bef(y^{:B}\rightarrow y)}
\end{array}
\end{align*}
\begin{align*}
{\color{greenunder}\text{function composition}:}\quad & =\,\begin{array}{|c||cc|}
 & A & B\\
\hline A & \text{id} & \bbnum 0\\
B & \bbnum 0 & \text{id}
\end{array}\,=\text{id}^{:A+B\rightarrow A+B}\quad.
\end{align*}

Several features of the matrix notation are helpful in such calculations.
The parts of the code of $f_{1}$ are automatically composed with
the corresponding parts of the code of $f_{2}$. To check that the
types match in the function composition, we just need to compare the
types in the output row $\,\begin{array}{||cc|}
B & A\end{array}\,$ of $f_{1}$ with the input column $\,\begin{array}{|c||}
B\\
A
\end{array}\,$ of $f_{2}$. Once we verified that all types match, we may omit the
type annotations and write the same derivation more concisely as:
\begin{align*}
f_{1}\bef f_{2} & =\,\begin{array}{||cc|}
\bbnum 0 & a^{:A}\rightarrow a\\
b^{:B}\rightarrow b & \bbnum 0
\end{array}\,\bef\,\begin{array}{||cc|}
\bbnum 0 & y^{:B}\rightarrow y\\
x^{:A}\rightarrow x & \bbnum 0
\end{array}\\
{\color{greenunder}\text{matrix composition}:}\quad & =\,\,\begin{array}{||cc|}
\gunderline{(a^{:A}\rightarrow a)\bef(x^{:A}\rightarrow x)} & \bbnum 0\\
\bbnum 0 & \gunderline{(b^{:B}\rightarrow b)\bef(y^{:B}\rightarrow y)}
\end{array}\\
{\color{greenunder}\text{function composition}:}\quad & =\,\begin{array}{||cc|}
\text{id} & \bbnum 0\\
\bbnum 0 & \text{id}
\end{array}\,=\text{id}\quad.
\end{align*}
The identity function is represented by the diagonal matrix: $\,\begin{array}{||cc|}
\text{id} & \bbnum 0\\
\bbnum 0 & \text{id}
\end{array}$~.

\subsubsection{Exercise \label{subsec:ch-Exercise-AxB}\ref{subsec:ch-Exercise-AxB}\index{exercises}}

Verify the type equivalence $A\times B\cong B\times A$.

\subsubsection{Exercise \label{subsec:ch-Exercise-A+B+C}\ref{subsec:ch-Exercise-A+B+C}}

Verify the type equivalence $\left(A+B\right)+C\cong A+\left(B+C\right)$.
Since Section~\ref{subsec:Logical-identity-not-type-equivalence}
proved the equivalences $\left(A+B\right)+C\cong A+\left(B+C\right)$
and $\left(A\times B\right)\times C\cong A\times\left(B\times C\right)$,
we may write $A+B+C$ and $A\times B\times C$ without any parentheses.

\subsubsection{Exercise \label{subsec:ch-Exercise-A+B2}\ref{subsec:ch-Exercise-A+B2}}

Verify the type equivalence:
\[
\left(A+B\right)\times\left(A+B\right)=A\times A+\bbnum 2\times A\times B+B\times B\quad,
\]
where $\bbnum 2$ denotes the \lstinline!Boolean! type\index{2@$\bbnum 2$ (the \texttt{Boolean} type)}
(which may be defined as $\bbnum 2\triangleq\bbnum 1+\bbnum 1$).

\subsection{Type cardinalities and type equivalence}

To understand why type equivalences are related to arithmetic identities,
consider the question of how many different values a given type can
have.

Begin by counting the number of distinct values for simple types.
For example, the \lstinline!Unit! type has only one distinct value;
the type \lstinline!Nothing! has zero values; the \lstinline!Boolean!
type has two distinct values (\lstinline!true! and \lstinline!false!);
and the type \lstinline!Int! has $2^{32}$ distinct values.

It is more difficult to count the number of distinct values in a type
such as \lstinline!String!, which is equivalent to a list of unknown
length, \lstinline!List[Char]!. However, each computer\textsf{'}s memory is
limited, so there will exist a maximum length for values of type \lstinline!String!.
So, the total number of possible different strings will be finite
(but will depend on the computer).

For a given type $A$, let us denote by $\left|A\right|$ the number
of distinct values of type $A$. The number $\left|A\right|$ is called
the \index{cardinality}\textbf{cardinality} of type $A$. This is
the same as the number of elements in the set of all values of type
$A$. Since any computer\textsf{'}s memory is finite, there will be \emph{finitely}
many different values of a given type $A$ that can exist in the computer.
So, we may assume that $\left|A\right|$ is always a finite integer
value. This assumption will simplify our reasoning. We will not actually
need to compute the precise number of, say, all the different possible
strings. It is sufficient to know that the set of all strings is finite,
so that we can denote its cardinality by $|\text{String}|$.

The next step is to consider the cardinality of types such as $A\times B$
and $A+B$. If the types $A$ and $B$ have cardinalities $\left|A\right|$
and $\left|B\right|$, it follows that the set of all distinct pairs
\lstinline!(A, B)! has $\left|A\right|\times\left|B\right|$ elements.
So, the cardinality of the type $A\times B$ is equal to the (arithmetic)
product of the cardinalities of $A$ and $B$. The set of all pairs,
denoted in mathematics by:
\[
\left\{ (a,b)\,\,|\,\,a\in A,b\in B\right\} \quad,
\]
is called the \index{Cartesian product}\textbf{Cartesian product}
of sets $A$ and $B$, and is denoted by $A\times B$. For this reason,
the tuple type is also called the \index{product type}\textbf{product
type}. Accordingly, the type notation adopts the symbol $\times$
for the product type.

The set of all distinct values of the type $A+B$, i.e., of the Scala
type \lstinline!Either[A, B]!, is a \index{labeled union}labeled
union of the set of values of the form \lstinline!Left(a)! and the
set of values of the form \lstinline!Right(b)!. It is clear that
the cardinalities of these two sets are equal to $\left|A\right|$
and $\left|B\right|$ respectively. So, the cardinality of the type
\lstinline!Either[A, B]! is equal to $\left|A\right|+\left|B\right|$.
For this reason, disjunctive types are also called \index{sum type!see \textsf{``}disjunctive type\textsf{''}}\textbf{sum
types}, and the type notation adopts the symbol $+$ for these types.

We can write our conclusions as:
\begin{align*}
\left|A\times B\right| & =\left|A\right|\times\left|B\right|\quad,\\
\left|A+B\right| & =\left|A\right|+\left|B\right|\quad.
\end{align*}
The type notation, $A\times B$ for \lstinline!(A,B)! and $A+B$
for \lstinline!Either[A, B]!, translates directly into type cardinalities.

The last step is to notice that two types can be equivalent, $P\cong Q$,
only if their cardinalities are equal, $\left|P\right|=\left|Q\right|$.
When the cardinalities are not equal, $\left|P\right|\neq\left|Q\right|$,
it will be impossible to have a one-to-one correspondence between
the sets of values of type $P$ and values of type $Q$. So, it will
be impossible to convert values from type $P$ to type $Q$ and back
without loss of information.

We conclude that types are equivalent when a logical identity \emph{and}
an arithmetic identity hold.

The presence of both identities does not automatically guarantee a
useful type equivalence. The fact that information in one type can
be identically stored in another type does not necessarily mean that
it is helpful to do so in a given application.

For example, the types \lstinline!Option[Option[A]]! and \lstinline!Either[Boolean, A]!
are equivalent because both types contain $2+\left|A\right|$ distinct
values. The short notation for these types is $\bbnum 1+\bbnum 1+A$
and $\bbnum 2+A$ respectively. The type Boolean is denoted by $\bbnum 2$
since it has only \emph{two} distinct values. 

One could write code for converting between these types without loss
of information:
\begin{lstlisting}
def f1[A]: Option[Option[A]] => Either[Boolean, A] = {
  case None           => Left(false) // Or maybe Left(true)?
  case Some(None)     => Left(true)
  case Some(Some(x))  => Right(x)
}

def f2[A]: Either[Boolean, A] => Option[Option[A]] = {
  case Left(false)    => None
  case Left(true)     => Some(None)
  case Right(x)       => Some(Some(x))
}
\end{lstlisting}
The presence of an arbitrary choice in this code is a warning sign.
In \lstinline!f1!, we could map \lstinline!None! to \lstinline!Left(false)!
or to \lstinline!Left(true)! and adjust the rest of the code accordingly.
The type equivalence holds with either choice. So, these types \emph{are}
equivalent, but there is no natural choice of the conversion functions
\lstinline!f1! and \lstinline!f2! because the meaning of those data
types will be application-dependent. We call this type equivalence
\textbf{accidental}\index{type equivalence!accidental}.

\subsubsection{Example \label{subsec:ch-Example-cardinality-option-either}\ref{subsec:ch-Example-cardinality-option-either}\index{examples (with code)}}

Are the types \lstinline!Option[A]! and \lstinline!Either[Unit, A]!
equivalent? Check whether the corresponding logic identity and arithmetic
identity hold.

\paragraph{Solution}

Begin by writing the given types in the type notation: \lstinline!Option[A]!
is written as $\bbnum 1+A$, and \lstinline!Either[Unit, A]! is written
also as $\bbnum 1+A$. The notation already indicates that the types
are equivalent. But let us verify explicitly that the type notation
is not misleading us here.

To establish the type equivalence, we need to implement two functions:
\begin{lstlisting}
def f1[A]: Option[A] => Either[Unit, A] = ???
def f2[A]: Either[Unit, A] => Option[A] = ???
\end{lstlisting}
These functions must satisfy $f_{1}\bef f_{2}=\text{id}$ and $f_{2}\bef f_{1}=\text{id}$.
It is straightforward to implement \lstinline!f1! and \lstinline!f2!:
\begin{lstlisting}
def f1[A]: Option[A] => Either[Unit, A] = {
  case None      => Left(())
  case Some(x)   => Right(x)
}
def f2[A]: Either[Unit, A] => Option[A] = {
  case Left(())   => None
  case Right(x)   => Some(x)
}
\end{lstlisting}
The code clearly shows that \lstinline!f1! and \lstinline!f2! are
inverses of each other. This verifies the type equivalence.

The logic identity is $True\vee A=True\vee A$ and holds trivially.
It remains to check the arithmetic identity, which relates the cardinalities
of types \lstinline!Option[A]! and \lstinline!Either[Unit, A]!.
Assume that the cardinality of type \lstinline!A! is $\left|A\right|$.
Any possible value of type \lstinline!Option[A]! must be either \lstinline!None!
or \lstinline!Some(x)!, where \lstinline!x! is a value of type \lstinline!A!.
So, the number of distinct values of type \lstinline!Option[A]! is
$1+\left|A\right|$. All possible values of type \lstinline!Either[Unit, A]!
are of the form \lstinline!Left(())! or \lstinline!Right(x)!, where
\lstinline!x! is a value of type \lstinline!A!. So, the cardinality
of type \lstinline!Either[Unit, A]! is $1+\left|A\right|$. We see
that the arithmetic identity holds: the types \lstinline!Option[A]!
and \lstinline!Either[Unit, A]! have equally many distinct values.

This example shows that the type notation is helpful for reasoning
about type equivalences. The answer was found immediately when we
wrote the type notation ($\bbnum 1+A$) for the given types.

\subsection{Type equivalence involving function types}

Until now, we have looked at product types and disjunctive types.
We now turn to type constructions involving function types.

Consider two types $A$ and $B$ having known cardinalities $\left|A\right|$
and $\left|B\right|$. How many distinct values does the function
type $A\rightarrow B$ have? A function \lstinline!f: A => B! needs
to select a value of type $B$ for each possible value of type $A$.
Therefore, the number of different functions \lstinline!f: A => B!
is $\left|B\right|^{\left|A\right|}$. 

Here $\left|B\right|^{\left|A\right|}$ denotes the \textbf{numeric
exponent}\index{exponent}, that is, $\left|B\right|$ to the power
$\left|A\right|$. We use the numeric exponent notation ($a^{b}$)
only when computing with numbers. When denoting types and code, this
book uses superscripts for type parameters and type annotations.

For the types $A=B=\text{Int}$, we have $\left|A\right|=\left|B\right|=2^{32}$,
and the exponential formula gives:
\[
\left|A\rightarrow B\right|=(2^{32})^{\left(2^{32}\right)}=2^{32\times2^{32}}=2^{2^{37}}\approx10^{4.1\times10^{10}}\quad.
\]
This number greatly exceeds the number of atoms in the observable
Universe.\footnote{Estimated in \texttt{\href{https://www.universetoday.com/36302/atoms-in-the-universe/amp/}{https://www.universetoday.com/36302/atoms-in-the-universe/amp/}}
to be between $10^{78}$ and $10^{82}$.} However, almost all of those functions will map integers to integers
in extremely complicated (and practically useless) ways. The code
of those functions will be much larger than the available memory of
a realistic computer. So, the number of practically implementable
functions of type $A\rightarrow B$ can be much smaller than $\left|B\right|^{\left|A\right|}$.
Since the code of a function is a string of bytes that needs to fit
into the computer\textsf{'}s memory, the number of implementable functions
is no larger than the number of possible byte strings.

Nevertheless, the formula $\left|B\right|^{\left|A\right|}$ is useful
since it shows the number of distinct functions that are possible
in principle. When types $A$ and $B$ have only a small number of
distinct values (for example, with $A=$ \lstinline!Option[Boolean]]!
and $B=$ \lstinline!Either[Boolean, Boolean]!), the formula $\left|B\right|^{\left|A\right|}$
gives an exact and practically relevant answer.

Let us now look for logic identities and arithmetic identities involving
function types. Table~\ref{tab:Logical-identities-with-function-types}
lists the available identities and the corresponding type equivalences.
(In the last column, we defined $a\triangleq\left|A\right|$, $b\triangleq\left|B\right|$,
and $c\triangleq\left|C\right|$ for brevity.) 

It is notable that no logic identity is available for the formula
$\alpha\Rightarrow\left(\beta\vee\gamma\right)$, and correspondingly
no type equivalence is available for the type expression $A\rightarrow B+C$
(although there is an identity for $A\rightarrow B\times C$). Reasoning
about types of the form $A\rightarrow B+C$ is more complicated because
those types usually cannot be rewritten as simpler types.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\footnotesize{}Logical identity (if holds)} & \textbf{\footnotesize{}Type equivalence} & \textbf{\footnotesize{}Arithmetic identity}\tabularnewline
\hline 
\hline 
{\footnotesize{}$\left(True\Rightarrow\alpha\right)=\alpha$} & {\footnotesize{}$\bbnum 1\rightarrow A\cong A$} & {\footnotesize{}$a^{1}=a$}\tabularnewline
\hline 
{\footnotesize{}$\left(False\Rightarrow\alpha\right)=True$} & {\footnotesize{}$\bbnum 0\rightarrow A\cong\bbnum 1$} & {\footnotesize{}$a^{0}=1$}\tabularnewline
\hline 
{\footnotesize{}$\left(\alpha\Rightarrow True\right)=True$} & {\footnotesize{}$A\rightarrow\bbnum 1\cong\bbnum 1$} & {\footnotesize{}$1^{a}=1$}\tabularnewline
\hline 
{\footnotesize{}$\left(\alpha\Rightarrow False\right)\neq False$} & {\footnotesize{}$A\rightarrow\bbnum 0\not\cong\bbnum 0$} & {\footnotesize{}$0^{a}\neq0$}\tabularnewline
\hline 
{\footnotesize{}$\left(\alpha\vee\beta\right)\Rightarrow\gamma=\left(\alpha\Rightarrow\gamma\right)\wedge\left(\beta\Rightarrow\gamma\right)$} & {\footnotesize{}$A+B\rightarrow C\cong\left(A\rightarrow C\right)\times\left(B\rightarrow C\right)$} & {\footnotesize{}$c^{a+b}=c^{a}\times c^{b}$}\tabularnewline
\hline 
{\footnotesize{}$(\alpha\wedge\beta)\Rightarrow\gamma=\alpha\Rightarrow\left(\beta\Rightarrow\gamma\right)$} & {\footnotesize{}$A\times B\rightarrow C\cong A\rightarrow B\rightarrow C$} & {\footnotesize{}$c^{a\times b}=(c^{b})^{a}$}\tabularnewline
\hline 
{\footnotesize{}$\alpha\Rightarrow\left(\beta\wedge\gamma\right)=\left(\alpha\Rightarrow\beta\right)\wedge\left(\alpha\Rightarrow\gamma\right)$} & {\footnotesize{}$A\rightarrow B\times C\cong\left(A\rightarrow B\right)\times\left(A\rightarrow C\right)$} & {\footnotesize{}$\left(b\times c\right)^{a}=b^{a}\times c^{a}$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logical identities with implication, and the corresponding type equivalences
and arithmetic identities.\label{tab:Logical-identities-with-function-types}}
\end{table}

We will now prove some of the type identities in Table~\ref{tab:Logical-identities-with-function-types}.

\subsubsection{Example \label{subsec:ch-Example-type-identity-f}\ref{subsec:ch-Example-type-identity-f}\index{examples (with code)}}

Verify the type equivalence $\bbnum 1\rightarrow A\cong A$.

\subparagraph{Solution}

Recall that the type notation $\bbnum 1\rightarrow A$ means the Scala
function type \lstinline!Unit => A!. There is only one value of type
\lstinline!Unit!. The choice of a function of type \lstinline!Unit => A!
is the same as the choice of a value of type \lstinline!A!. So, the
type $\bbnum 1\rightarrow A$ has $\left|A\right|$ distinct values,
and the arithmetic identity holds.

To verify the type equivalence explicitly, we need to implement two
functions:
\begin{lstlisting}
def f1[A]: (Unit => A) => A = ???
def f2[A]: A => Unit => A = ???
\end{lstlisting}
The first function needs to produce a value of type \lstinline!A!,
given an argument of the function type \lstinline!Unit => A!. The
only possibility is to apply that function to the value of type \lstinline!Unit!.
We produce that value as \lstinline!()!:
\begin{lstlisting}
def f1[A]: (Unit => A) => A = (h: Unit => A) => h(())
\end{lstlisting}
Implementing \lstinline!f2! is straightforward. We can just discard
the \lstinline!Unit! argument:
\begin{lstlisting}
def f2[A]: A => Unit => A = (x: A) => _ => x
\end{lstlisting}
It remains to show that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other. Let us show the proof using Scala code
and then using the code notation.

Writing Scala code, compute \lstinline!f1(f2(x))! for an arbitrary
\lstinline!x:A!. Using the code of \lstinline!f1! and \lstinline!f2!,
we get:
\begin{lstlisting}
f1(f2(x)) == f1(_ => x) == (_ => x)(()) == x
\end{lstlisting}
Now compute \lstinline!f2(f1(h))! for arbitrary \lstinline!h: Unit => A!
in Scala code:
\begin{lstlisting}
f2(f1(h)) == f2(h(())) == { _ => h(()) }
\end{lstlisting}
How can we show that the function \lstinline!{_ => h(())}! is equal
to \lstinline!h!? Whenever we apply equal functions to equal arguments,
they return equal results. In our case, the argument of \lstinline!h!
is of type \lstinline!Unit!, so we only need to verify that the result
of applying \lstinline!h! to the value \lstinline!()! is the same
as the result of applying \lstinline!{_ => h(())}! to \lstinline!()!.
In other words, we need to apply both sides to an additional argument
\lstinline!()!:
\begin{lstlisting}
f2(f1(h))(()) == { _ => h(()) } (()) == h(())
\end{lstlisting}
This completes the proof.

For comparison, let us show the same proof in the code notation. The
functions $f_{1}$ and $f_{2}$ are:
\[
f_{1}\triangleq h^{:\bbnum 1\rightarrow A}\rightarrow h(1)\quad,\quad\quad f_{2}\triangleq x^{:A}\rightarrow1\rightarrow x\quad.
\]
Now write the function compositions in both directions:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{id}:}\quad & f_{1}\bef f_{2}=(h^{:\bbnum 1\rightarrow A}\rightarrow h(1))\bef(x^{:A}\rightarrow1\rightarrow x)\\
{\color{greenunder}\text{compute composition}:}\quad & \quad=h^{:\bbnum 1\rightarrow A}\rightarrow\gunderline{1\rightarrow h(1)}\\
{\color{greenunder}\text{note that }1\rightarrow h(1)\text{ is the same as }h:}\quad & \quad=(h^{:\bbnum 1\rightarrow A}\rightarrow h)=\text{id}\quad.
\end{align*}
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{id}:}\quad & f_{2}\bef f_{1}=(x^{:A}\rightarrow1\rightarrow x)\bef(h^{:\bbnum 1\rightarrow A}\rightarrow h(1))\\
{\color{greenunder}\text{compute composition}:}\quad & \quad=x^{:A}\rightarrow\gunderline{(1\rightarrow x)(1)}\\
{\color{greenunder}\text{apply function}:}\quad & \quad=(x^{:A}\rightarrow x)=\text{id}\quad.
\end{align*}

The type $\bbnum 1\rightarrow A$ is equivalent to the type $A$ in
the sense of carrying the same information, but these types are not
exactly the same. An important difference between these types is that
a value of type $A$ is available immediately, while a value of type
$\bbnum 1\rightarrow A$ is a function that still needs to be applied
to an argument (of type $\bbnum 1$) before a value of type $A$ is
obtained. The type $\bbnum 1\rightarrow A$ may represent an \textsf{``}on-call\textsf{''}\index{on-call value}
value of type $A$. That value is computed on demand every time it
is requested. (See Section~\ref{subsec:Lazy-values-iterators-and-streams}
for more details about \textsf{``}on-call\textsf{''} values.)

The void type\index{void type} $\bbnum 0$ needs special reasoning,
as the next examples show:

\subsubsection{Example \label{subsec:ch-Example-type-identity-0-to-A}\ref{subsec:ch-Example-type-identity-0-to-A}}

Verify the type equivalence $\bbnum 0\rightarrow A\cong\bbnum 1$.

\subparagraph{Solution}

To verify that a type $X$ is equivalent to the \lstinline!Unit!
type, we need to show that there is only one distinct value of type
$X$. So, let us find out how many values the type $\bbnum 0\rightarrow A$
has. Consider a value of that type, which is a function $f^{:\bbnum 0\rightarrow A}$
from the type $\bbnum 0$ to a type $A$. Since there exist no values
of type $\bbnum 0$, the function $f$ will never be applied to any
arguments and so \emph{does not need} to compute any actual values
of type $A$. So, $f$ is a function whose body may be \textsf{``}empty\textsf{''}.
At least, $f$\textsf{'}s body does not need to contain any expressions of
type $A$. In Scala, such a function can be written as:
\begin{lstlisting}
def absurd[A]: Nothing => A = { ??? }
\end{lstlisting}
This code will compile without type errors. An equivalent code is:
\begin{lstlisting}
def absurd[A]: Nothing => A = { x => ??? }
\end{lstlisting}
The symbol \lstinline!???! is defined in the Scala library and represents
code that is \textsf{``}not implemented\textsf{''}. Trying to evaluate this symbol
will produce an error:
\begin{lstlisting}
scala> val x = ???
scala.NotImplementedError: an implementation is missing
\end{lstlisting}
Since the function \lstinline!absurd! can never be applied to an
argument, this error will never happen. So, one can pretend that the
result value (which will never be computed) has any required type,
e.g., the type $A$. In this way, the compiler will accept the definition
of \lstinline!absurd!.

Let us now verify that there exists \emph{only one} distinct function
of type $\bbnum 0\rightarrow A$. Take any two functions of that type,
$f^{:\bbnum 0\rightarrow A}$ and $g^{:\bbnum 0\rightarrow A}$. Are
they different? The only way of showing that $f$ and $g$ are different
is by finding a value $x$ such that $f(x)\neq g(x)$. But then $x$
would be of type $\bbnum 0$, and there are \emph{no} \emph{values}
of type $\bbnum 0$. So, we will never be able to find the required
value $x$. It follows that any two functions $f$ and $g$ of type
$\bbnum 0\rightarrow A$ are equal, $f=g$. In other words, there
exists only one distinct value of type $\bbnum 0\rightarrow A$. Since
the cardinality of the type $\bbnum 0\rightarrow A$ is $1$, we obtain
the type equivalence $\bbnum 0\rightarrow A\cong\bbnum 1$.

\subsubsection{Example \label{subsec:ch-Example-type-identity-A-0}\ref{subsec:ch-Example-type-identity-A-0}}

Show that $A\rightarrow\bbnum 0\not\cong\bbnum 0$ and $A\rightarrow\bbnum 0\not\cong\bbnum 1$,
where $A$ is an arbitrary type.

\subparagraph{Solution}

To prove that two types are \emph{not} equivalent, it is sufficient
to show that their cardinalities are different. Let us determine the
cardinality of the type $A\rightarrow\bbnum 0$, assuming that the
cardinality of $A$ is known. We note that a function of type, say,
$\text{Int}\rightarrow\bbnum 0$ is impossible to implement. (If we
had such a function $f^{:\text{Int}\rightarrow\bbnum 0}$, we could
evaluate, say, $x\triangleq f(123)$ and obtain a value $x$ of type
$\bbnum 0$, which is impossible by definition of the type $\bbnum 0$.
It follows that $\left|\text{Int}\rightarrow\bbnum 0\right|=0$. However,
Example~\ref{subsec:ch-Example-type-identity-0-to-A} shows that
$\bbnum 0\rightarrow\bbnum 0$ has cardinality $1$. So, we find that
$\left|A\rightarrow\bbnum 0\right|=1$ if the type $A$ is itself
$\bbnum 0$ but $\left|A\rightarrow\bbnum 0\right|=0$ for all other
types $A$. We conclude that the type $A\rightarrow\bbnum 0$ is not
equivalent to $\bbnum 0$ or $\bbnum 1$ when $A$ is an unknown type.
The type $A\rightarrow\bbnum 0$ is void for non-void types $A$,
and vice versa.

\subsubsection{Example \label{subsec:ch-Example-type-identity-2}\ref{subsec:ch-Example-type-identity-2}}

Verify the type equivalence $A\rightarrow\bbnum 1\cong\bbnum 1$.

\subparagraph{Solution}

There is only one fully parametric function that returns $\bbnum 1$:
\begin{lstlisting}
def f[A]: A => Unit = { _ => () }
\end{lstlisting}
The function $f$ must return the fixed value \lstinline!()! of type
\lstinline!Unit!. The argument of type $A$ is of no use for that.
So, the code of $f$ \emph{must} discard its argument and return the
value \lstinline!()!. In the code notation, this function is written
as:
\[
f^{:A\rightarrow\bbnum 1}\triangleq\_\rightarrow1\quad.
\]
We can show that there exists only \emph{one} distinct function of
type $A\rightarrow\bbnum 1$ (that is, the type $A\rightarrow\bbnum 1$
has cardinality $1$). Assume that $f$ and $g$ are two such functions,
and try to find a value $x^{:A}$ such that $f(x)\neq g(x)$. We cannot
find any such $x$ because $f(x)=1$ and $g(x)=1$ for all $x$. So,
any two functions $f$ and $g$ of type $A\rightarrow\bbnum 1$ must
be equal to each other. The cardinality of the type $A\rightarrow\bbnum 1$
is $1$.

Any type having cardinality $1$ is equivalent to the \lstinline!Unit!
type ($\bbnum 1$). So, $A\rightarrow\bbnum 1\cong\bbnum 1$.

\subsubsection{Example \label{subsec:ch-Example-type-identity-6-1}\ref{subsec:ch-Example-type-identity-6-1}}

Denote by $\_^{:A}\rightarrow B$ the type of \index{constant function}constant
functions of type $A\rightarrow B$ (functions that ignore their argument).
Show that the type $\_^{:A}\rightarrow B$ is equivalent to the type
$B$, as long as $A\neq\bbnum 0$.

\subparagraph{Solution}

An isomorphism between the types $B$ and $\_^{:A}\rightarrow B$
is given by the two functions:
\begin{align*}
 & f_{1}:B\rightarrow\_^{:A}\rightarrow B\quad,\quad\quad f_{1}\triangleq b\rightarrow\_\rightarrow b\quad;\\
 & f_{2}:(\_^{:A}\rightarrow B)\rightarrow B\quad,\quad\quad f_{2}\triangleq k^{:\_\rightarrow B}\rightarrow k(x^{:A})\quad,
\end{align*}
where $x$ is any value of type $A$. That value exists since the
type $A$ is not void. The function $f_{2}$ does not depend on the
choice of $x$ because $k$ is a constant function, so $k(x)$ is
the same for all $x$. In other words, the function $k$ satisfies
$k=(\_\rightarrow k(x))$ with any chosen $x$. To prove that $f_{1}$
and $f_{2}$ are inverses:
\begin{align*}
 & f_{1}\bef f_{2}=(b\rightarrow\_\rightarrow b)\bef(k\rightarrow k(x))=b\rightarrow(\_\rightarrow b)(x)=(b\rightarrow b)=\text{id}\quad,\\
 & f_{2}\bef f_{1}=(k\rightarrow k(x))\bef(b\rightarrow\_\rightarrow b)=k\rightarrow\_\rightarrow k(x)=k\rightarrow k=\text{id}\quad.
\end{align*}


\subsubsection{Example \label{subsec:ch-Example-type-identity-5}\ref{subsec:ch-Example-type-identity-5}}

Verify the following type equivalence:
\[
A+B\rightarrow C\cong(A\rightarrow C)\times(B\rightarrow C)\quad.
\]


\subparagraph{Solution}

Begin by implementing two functions with type signatures:
\begin{lstlisting}
def f1[A, B, C]: (Either[A, B] => C) => (A => C, B => C) = ???
def f2[A, B, C]: ((A => C, B => C)) => Either[A, B] => C = ???
\end{lstlisting}
The code can be derived unambiguously from the type signatures. For
the first function, we need to produce a pair of functions of type
\lstinline!(A => C, B => C)!. Can we produce the first part of that
pair? Computing a function of type \lstinline!A => C! means that
we need to produce a value of type \lstinline!C! given an arbitrary
value \lstinline!a:A!. The available data is a function of type \lstinline!Either[A, B] => C!
called, say, \lstinline!h!. We can apply that function to \lstinline!Left(a)!
and obtain a value of type \lstinline!C! as required. So, a function
of type \lstinline!A => C! is computed as \lstinline!a => h(Left(a))!.
We can produce a function of type \lstinline!B => C! similarly. The
code is:
\begin{lstlisting}
def f1[A, B, C]: (Either[A, B] => C) => (A => C, B => C) =
  (h: Either[A, B] => C) => (a => h(Left(a)), b => h(Right(b)))
\end{lstlisting}
We write this function in the code notation like this:
\begin{align*}
 & f_{1}:\left(A+B\rightarrow C\right)\rightarrow\left(A\rightarrow C\right)\times\left(B\rightarrow C\right)\quad,\\
 & f_{1}\triangleq h^{:A+B\rightarrow C}\rightarrow\big(a^{:A}\rightarrow h(a+\bbnum 0^{:B})\big)\times\big(b^{:B}\rightarrow h(\bbnum 0^{:A}+b)\big)\quad.
\end{align*}

For the function \lstinline!f2!, we need to apply pattern matching
to both curried arguments and then return a value of type \lstinline!C!.
This can be achieved in only one way:
\begin{lstlisting}
def f2[A, B, C](f: A => C, g: B => C): Either[A, B] => C = {
  case Left(a)    => f(a)
  case Right(b)   => g(b)
}
\end{lstlisting}
We write this function in the code notation like this:
\begin{align*}
 & f_{2}:\left(A\rightarrow C\right)\times\left(B\rightarrow C\right)\rightarrow A+B\rightarrow C\quad,\\
 & f_{2}\triangleq f^{:A\rightarrow C}\times g^{:B\rightarrow C}\rightarrow\,\begin{array}{|c||c|}
 & C\\
\hline A & a\rightarrow f(a)\\
B & b\rightarrow g(b)
\end{array}\quad.
\end{align*}
The matrix in the last line has only one column because the result
type, $C$, is not known to be a disjunctive type. We may also simplify
the functions, e.g., replace $a\rightarrow f(a)$ by just $f$, and
write:
\[
f_{2}\triangleq f^{:A\rightarrow C}\times g^{:B\rightarrow C}\rightarrow\,\begin{array}{|c||c|}
 & C\\
\hline A & f\\
B & g
\end{array}\quad.
\]

It remains to verify that $f_{1}\bef f_{2}=\text{id}$ and $f_{2}\bef f_{1}=\text{id}$.
To compute $f_{1}\bef f_{2}$, we write (omitting types):
\begin{align*}
f_{1}\bef f_{2}= & \big(h\rightarrow(a\rightarrow h(a+\bbnum 0))\times(b\rightarrow h(\bbnum 0+b))\big)\bef\bigg(f\times g\rightarrow\,\begin{array}{||c|}
f\\
g
\end{array}\,\bigg)\\
{\color{greenunder}\text{compute composition}:}\quad & =h\rightarrow\,\begin{array}{||c|}
a\rightarrow h(a+\bbnum 0)\\
b\rightarrow h(\bbnum 0+b)
\end{array}\quad.
\end{align*}
To proceed, we need to simplify the expressions $h(a+\bbnum 0)$ and
$h(\bbnum 0+b)$. We rewrite the argument $h$ (an arbitrary function
of type $A+B\rightarrow C$) in the matrix notation:
\[
h\triangleq\,\begin{array}{|c||c|}
 & C\\
\hline A & a\rightarrow p(a)\\
B & b\rightarrow q(b)
\end{array}\,=\,\begin{array}{|c||c|}
 & C\\
\hline A & p\\
B & q
\end{array}\quad,
\]
where $p^{:A\rightarrow C}$ and $q^{:B\rightarrow C}$ are new arbitrary
functions. Since we already checked the types, we can omit all type
annotations and write $h$ as:
\[
h\triangleq\,\begin{array}{||c|}
p\\
q
\end{array}\quad.
\]
To evaluate expressions such as $h(a+\bbnum 0)$ and $h(\bbnum 0+b)$,
we need to use one of the rows of this matrix. The correct row will
be selected \emph{automatically} by the rules of matrix multiplication
if we place a row vector to the left of the matrix and use the convention
of omitting terms containing $\bbnum 0$:
\[
\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}\,=a\triangleright p\quad,\quad\quad\begin{array}{|cc|}
\bbnum 0 & b\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}\,=b\triangleright q\quad.
\]
Here we used the symbol $\triangleright$ to separate an argument
from a function when the argument is written to the \emph{left} of
the function. The symbol $\triangleright$ (pronounced \textsf{``}pipe\textsf{''}\index{pipe notation}\index{0@$\triangleright$-notation!see \textsf{``}pipe notation\textsf{''}})
is defined by $x\triangleright f\triangleq f(x)$. In Scala, this
operation is available as \lstinline!x.pipe(f)! as of Scala 2.13.

We can write values of disjunctive types, such as $a+\bbnum 0$, as
row vectors $\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,$:
\begin{equation}
h(a+\bbnum 0)=(a+\bbnum 0)\triangleright h=\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\triangleright\,h\quad.\label{eq:forward-notation-}
\end{equation}
With these notations, we compute further. Omit all terms applying
$\bbnum 0$ or applying something to $\bbnum 0$:
\[
h(a+\bbnum 0)=\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\triangleright\,h=\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}\,=a\triangleright p=p(a)\quad,
\]
\[
h(\bbnum 0+b)=\,\begin{array}{|cc|}
\bbnum 0 & b\end{array}\,\triangleright\,h=\,\,\begin{array}{|cc|}
\bbnum 0 & b\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}\,=b\triangleright q=q(b)\quad.
\]
Now we can complete the proof of $f_{1}\bef f_{2}=\text{id}$:
\begin{align*}
 & f_{1}\bef f_{2}=h\rightarrow\,\begin{array}{||c|}
a\rightarrow h(a+\bbnum 0)\\
b\rightarrow h(\bbnum 0+b)
\end{array}\\
{\color{greenunder}\text{previous equations}:}\quad & =\,\begin{array}{||c|}
p\\
q
\end{array}\,\rightarrow\,\begin{array}{||c|}
a\rightarrow p(a)\\
b\rightarrow q(b)
\end{array}\\
{\color{greenunder}\text{simplify functions}:}\quad & =\,\,\begin{array}{||c|}
p\\
q
\end{array}\,\rightarrow\,\begin{array}{||c|}
p\\
q
\end{array}\,=\text{id}\quad.
\end{align*}

To prove that $f_{2}\bef f_{1}=\text{id}$, use the notation~(\ref{eq:forward-notation-}):
\begin{align*}
f_{2}\bef f_{1}= & \bigg(f\times g\rightarrow\,\begin{array}{||c|}
f\\
g
\end{array}\,\bigg)\bef\big(h\rightarrow(a\rightarrow(a+\bbnum 0)\triangleright h)\times(b\rightarrow(\bbnum 0+b)\triangleright h)\big)\\
{\color{greenunder}\text{composition}:}\quad & =f\times g\rightarrow\big(a\rightarrow\gunderline{\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\,\triangleright}\,\begin{array}{||c|}
f\\
g
\end{array}\,\big)\times\big(b\rightarrow\gunderline{\,\begin{array}{|cc|}
\bbnum 0 & b\end{array}\,\,\triangleright}\,\begin{array}{||c|}
f\\
g
\end{array}\,\big)\\
{\color{greenunder}\text{apply functions}:}\quad & =f\times g\rightarrow(a\rightarrow\gunderline{a\triangleright f})\times(b\rightarrow\gunderline{b\triangleright g})\\
{\color{greenunder}\text{definition of }\triangleright:}\quad & =f\times g\rightarrow\gunderline{\left(a\rightarrow f(a)\right)}\times\gunderline{\left(b\rightarrow g(b)\right)}\\
{\color{greenunder}\text{simplify functions}:}\quad & =\left(f\times g\rightarrow f\times g\right)=\text{id}\quad.
\end{align*}

In this way, we have proved that $f_{1}$ and $f_{2}$ are mutual
inverses. The proofs appear long because we took time to motivate
and introduce new notation for applying matrices to row vectors. Once
this notation is understood, a proof of $f_{1}\bef f_{2}=\text{id}$
can be written as:
\begin{align*}
f_{1}\bef f_{2} & =\left(h\rightarrow(a\rightarrow(a+\bbnum 0)\triangleright h)\times(b\rightarrow(\bbnum 0+b)\triangleright h)\right)\bef\bigg(f\times g\rightarrow\,\begin{array}{||c|}
f\\
g
\end{array}\bigg)\\
{\color{greenunder}\text{composition}:}\quad & =h\rightarrow\,\begin{array}{||c|}
\,a\,\rightarrow\,\left|\begin{array}{cc}
a & \bbnum 0\end{array}\right|\triangleright h\\
b\rightarrow\left|\begin{array}{cc}
\bbnum 0 & b\end{array}\right|\triangleright h
\end{array}\,=\,\begin{array}{||c|}
p\\
q
\end{array}\rightarrow\,\begin{array}{||c|}
a\rightarrow\,\begin{array}{|cc|}
a & \bbnum 0\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}\\
b\,\rightarrow\,\begin{array}{|cc|}
\bbnum 0 & b\,\,\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}
\end{array}\\
{\color{greenunder}\text{apply functions}:}\quad & =\,\begin{array}{||c|}
p\\
q
\end{array}\,\rightarrow\,\begin{array}{||c|}
a\rightarrow a\triangleright p\\
b\rightarrow b\triangleright q
\end{array}\,=\,\begin{array}{||c|}
p\\
q
\end{array}\,\rightarrow\,\begin{array}{||c|}
p\\
q
\end{array}\,=\text{id}\quad.
\end{align*}
Proofs in the code notation are shorter than in Scala syntax because
certain names and keywords (such as \lstinline!Left!, \lstinline!Right!,
\lstinline!case!, \lstinline!match!, etc.) are omitted. From now
on, we will prefer to use the code notation in proofs, keeping in
mind that one can always convert the code notation to Scala.

Note that the function arrow ($\rightarrow$) binds weaker than the
pipe operation ($\triangleright$), so the code notation $x\rightarrow y\triangleright z$
means $x\rightarrow(y\triangleright z)$. We will review the code
notation more systematically in Chapter~\ref{chap:Reasoning-about-code}.

\subsubsection{Example \label{subsec:ch-Example-type-identity-6}\ref{subsec:ch-Example-type-identity-6}}

Verify the type equivalence:
\[
A\times B\rightarrow C\cong A\rightarrow B\rightarrow C\quad.
\]


\subparagraph{Solution}

Begin by implementing the two functions:
\begin{lstlisting}
def f1[A, B, C]: (((A, B)) => C) => A => B => C = ???
def f2[A, B, C]: (A => B => C) => ((A, B)) => C = ???
\end{lstlisting}
The Scala code can be derived from the type signatures unambiguously:
\begin{lstlisting}
def f1[A,B,C]: (((A, B)) => C) => A => B => C = g => a => b => g((a, b))
def f2[A,B,C]: (A => B => C) => ((A, B)) => C = h => { case (a, b) => h(a)(b) }
\end{lstlisting}
Write these functions in the code notation:
\begin{align*}
 & f_{1}=g^{:A\times B\rightarrow C}\rightarrow a^{:A}\rightarrow b^{:B}\rightarrow g(a\times b)\quad,\\
 & f_{2}=h^{:A\rightarrow B\rightarrow C}\rightarrow\left(a\times b\right)^{:A\times B}\rightarrow h(a)(b)\quad.
\end{align*}
We denote by $\left(a\times b\right)^{:A\times B}$ the argument of
type \lstinline!(A, B)! with pattern matching implied. This notation
allows us to write shorter code formulas involving tupled arguments.

Compute the function composition $f_{1}\bef f_{2}$ and show that
it is equal to an identity function:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{id}:}\quad & f_{1}\bef f_{2}=(g\rightarrow\gunderline{a\rightarrow b\rightarrow g(a\times b)})\bef\left(h\rightarrow a\times b\rightarrow h(a)(b)\right)\\
{\color{greenunder}\text{composition}:}\quad & \quad=g\rightarrow\gunderline{a\times b\rightarrow g(a\times b)}\\
{\color{greenunder}\text{simplify function}:}\quad & \quad=\left(g\rightarrow g\right)=\text{id}\quad.
\end{align*}
Compute the function composition $f_{2}\bef f_{1}$ and show that
it is equal to an identity function:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{id}:}\quad & f_{2}\bef f_{1}=(h\rightarrow\gunderline{a\times b\rightarrow h(a)(b)})\bef\left(g\rightarrow a\rightarrow b\rightarrow g(a\times b)\right)\\
{\color{greenunder}\text{composition}:}\quad & \quad=h\rightarrow a\rightarrow\gunderline{b\rightarrow h(a)(b)}\\
{\color{greenunder}\text{simplify }b\rightarrow h(a)(b):}\quad & \quad=h\rightarrow\gunderline{a\rightarrow h(a)}\\
{\color{greenunder}\text{simplify }a\rightarrow h(a)\text{ to }h:}\quad & \quad=\left(h\rightarrow h\right)=\text{id}\quad.
\end{align*}


\section{Summary}

What tasks can we perform now?
\begin{itemize}
\item Convert a fully parametric type signature into a logical formula and:
\begin{itemize}
\item Decide whether the type signature can be implemented in code.
\item If possible, derive the code using the CH correspondence.
\end{itemize}
\item Use the type notation (Table~\ref{tab:ch-correspondence-type-notation-CH-propositions})
for reasoning about types to:
\begin{itemize}
\item Decide type equivalence using the rules in Tables~\ref{tab:Logical-identities-with-disjunction-and-conjunction}\textendash \ref{tab:Logical-identities-with-function-types}.
\item Simplify type expressions before writing code.
\end{itemize}
\item Use the matrix notation and the pipe notation to write code that works
on disjunctive types.
\end{itemize}
What tasks \emph{cannot} be performed with these tools?
\begin{itemize}
\item Automatically generate code for \emph{recursive} functions. The CH
correspondence is based on propositional logic, which cannot describe
recursion. Accordingly, recursion is absent from the eight code constructions
of Section~\ref{subsec:The-rules-of-proof}. Recursive functions
need to be coded by hand.
\item Automatically generate code satisfying a property (e.g., isomorphism).
We may generate some code, but the CH correspondence does not guarantee
that properties will hold. We need to verify the required properties
manually, after deriving the code.
\item Express complicated conditions (e.g., \textsf{``}array is sorted\textsf{''}) in a
type signature. This can be done using \textbf{dependent types}\index{dependent type}
(i.e., types that directly depend on values in some way). This is
an advanced technique beyond the scope of this book. Programming languages
such as Coq, Agda, and Idris fully support dependent types, while
Scala has only limited support.
\item Generate code using type constructors with known methods (e.g., the
\lstinline!map! method).
\end{itemize}
As an example of using type constructors with known methods, consider
this type signature:
\begin{lstlisting}
def q[A]: Array[A] => (A => Option[B]) => Array[Option[B]]
\end{lstlisting}
Can we generate the code of this function from its type signature?
We know that the Scala library defines a \lstinline!map! method on
the \lstinline!Array! class. So, an implementation of \lstinline!q!
is:
\begin{lstlisting}
def q[A]: Array[A] => (A => Option[B]) => Array[Option[B]] = { arr => f => arr.map(f) }
\end{lstlisting}
However, it is hard to create an \emph{algorithm} that can derive
this implementation automatically from the type signature of \lstinline!q!
via the Curry-Howard correspondence. The algorithm would have to convert
the type signature of \lstinline!q! into this logical formula:
\begin{equation}
{\cal CH}(\text{Array}^{A})\Rightarrow{\cal CH}(A\rightarrow\text{Opt}^{B})\Rightarrow{\cal CH}(\text{Array}^{\text{Opt}^{B}})\quad.\label{eq:ch-example-quantified-proposition}
\end{equation}
To derive an implementation, the algorithm would need to use the available
\lstinline!map! method for \lstinline!Array!. That method has the
type signature:
\[
\text{map}:\forall(A,B).\,\text{Array}^{A}\rightarrow\left(A\rightarrow B\right)\rightarrow\text{Array}^{B}\quad.
\]
To derive the ${\cal CH}$-proposition~(\ref{eq:ch-example-quantified-proposition}),
the algorithm will need to assume that the ${\cal CH}$-proposition:
\begin{equation}
{\cal CH}\,\big(\forall(A,B).\,\text{Array}^{A}\rightarrow\left(A\rightarrow B\right)\rightarrow\text{Array}^{B}\big)\label{eq:ch-example-quantified-proposition-2}
\end{equation}
already holds. In other words, Eq.~(\ref{eq:ch-example-quantified-proposition-2})
must be one of the premises of a sequent. Reasoning about premises
such as Eq.~(\ref{eq:ch-example-quantified-proposition-2}) requires
\index{first-order logic}\textbf{first-order logic} \textemdash{}
a logic whose proof rules can handle quantified types such as $\forall(A,B)$\emph{
}inside premises. However, first-order logic is \textbf{undecidable}\index{undecidable logic}:
no algorithm can find a proof (or verify the absence of a proof) in
all cases. 

The constructive propositional logic with the rules listed in Table~\ref{tab:Proof-rules-for-constructive-logic}
is \textbf{decidable},\index{decidable logic} i.e., it has an algorithm
that either finds a proof or disproves any given formula. However,
that logic cannot handle type constructors such as $\text{Array}^{A}$.
It also cannot handle premises containing type quantifiers such as
$\forall(A,B)$, because all the available logic rules have the quantifiers
placed \emph{outside} the premises. 

So, code for functions such as \lstinline!q! can only be derived
by trial and error, informed by intuition. This book will help programmers
to acquire the necessary intuition and technique.

\subsection{Examples\index{examples (with code)}}

\subsubsection{Example \label{subsec:ch-solvedExample-1}\ref{subsec:ch-solvedExample-1}}

Find the cardinality of the type \lstinline!P = Option[Option[Boolean] => Boolean]!.
Write \lstinline!P! in the type notation.

\subparagraph{Solution}

Begin with the type \lstinline!Option[Boolean]!, which can be either
\lstinline!None! or \lstinline!Some(x)! with some value \lstinline!x: Boolean!.
Because the type \lstinline!Boolean! has $2$ possible values, the
type \lstinline!Option[Boolean]! has $3$ possible values:
\[
|\text{Opt}^{\text{Boolean}}|=\left|\bbnum 1+\text{Boolean}\right|=1+\left|\text{Boolean}\right|=1+2=3\quad.
\]
In the type notation, \lstinline!Boolean! is denoted by the symbol
$\bbnum 2$ and \lstinline!Option[Boolean]! by $\bbnum 1+\bbnum 2$.
So, the type notation $\bbnum 1+\bbnum 2$ is consistent with the
cardinality $3$ of that type:
\[
\left|\bbnum 1+\text{Boolean}\right|=\left|\bbnum 1+\bbnum 2\right|=1+2=3\quad.
\]

The function type \lstinline!Option[Boolean] => Boolean! is denoted
by $\bbnum 1+\bbnum 2\rightarrow\bbnum 2$. Compute its cardinality
as:
\[
|\text{Opt}^{\text{Boolean}}\rightarrow\text{Boolean}|=\left|\bbnum 1+\bbnum 2\rightarrow\bbnum 2\right|=\left|\bbnum 2\right|^{\left|\bbnum 1+\bbnum 2\right|}=2^{3}=8\quad.
\]
Finally, the we write \lstinline!P! in the type notation as $P=\bbnum 1+\left(\bbnum 1+\bbnum 2\rightarrow\bbnum 2\right)$
and find:
\[
\left|P\right|=\left|\bbnum 1+\left(\bbnum 1+\bbnum 2\rightarrow\bbnum 2\right)\right|=1+\left|\bbnum 1+\bbnum 2\rightarrow\bbnum 2\right|=1+8=9\quad.
\]


\subsubsection{Example \label{subsec:ch-solvedExample-2}\ref{subsec:ch-solvedExample-2}}

Implement a Scala type \lstinline!P[A]! given by this type notation:
\[
P^{A}\triangleq1+A+\text{Int}\times A+(\text{String}\rightarrow A)\quad.
\]


\subparagraph{Solution}

To translate type notation into Scala code, begin by defining the
disjunctive types as case classes, choosing class names for convenience.
In this case, $P^{A}$ is a disjunctive type with four parts, so we
need four case classes:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A](???) extends P[A]
final case class P2[A](???) extends P[A]
final case class P3[A](???) extends P[A]
final case class P4[A](???) extends P[A]
\end{lstlisting}
Each of the case classes represents one part of the disjunctive type.
Now we write the contents for each of the case classes, in order to
implement the data in each of the disjunctive parts:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A]()                 extends P[A]
final case class P2[A](x: A)             extends P[A]
final case class P3[A](n: Int, x: A)     extends P[A]
final case class P4[A](f: String => A)   extends P[A]
\end{lstlisting}


\subsubsection{Example \label{subsec:ch-solvedExample-2a}\ref{subsec:ch-solvedExample-2a}}

Find an equivalent disjunctive type for the type \lstinline!P = (Either[A, B], Either[C, D])!.

\subparagraph{Solution}

Begin by writing the given type in the type notation. The tuple becomes
a product type, and \lstinline!Either! becomes a disjunctive type:
\[
P\triangleq(A+B)\times(C+D)\quad.
\]
By the usual rules of arithmetic, we expand brackets and obtain an
equivalent type:
\[
P\cong A\times C+A\times D+B\times C+B\times D\quad.
\]
This is a disjunctive type with $4$ parts.

\subsubsection{Example \label{subsec:ch-solvedExample-3}\ref{subsec:ch-solvedExample-3}}

Show that the following type equivalences do \emph{not} hold: $A+A\not\cong A$
and $A\times A\not\cong A$, although the corresponding logical identities
hold.

\subparagraph{Solution}

The arithmetic equalities do not hold, $A+A\neq A$ and $A\times A\ne A$.
This already indicates that the types are not equivalent. To build
further intuition, consider that a value of type $A+A$ (in Scala,
\lstinline!Either[A, A]!) is a \lstinline!Left(a)! or a \lstinline!Right(a)!
for some \lstinline!a:A!. In the code notation, it is either $a^{:A}+\bbnum 0$
or $\bbnum 0+a^{:A}$. So, a value of type $A+A$ contains a value
of type $A$ with the additional information about whether it is the
first or the second part of the disjunctive type. We cannot represent
that information in a single value of type $A$. 

Similarly, a value of type $A\times A$ contains two (possibly different)
values of type $A$, which cannot be represented by a single value
of type $A$ without loss of information.

However, the corresponding logical identities $\alpha\vee\alpha=\alpha$
and $\alpha\wedge\alpha=\alpha$ hold. To see that, we could derive
the four formulas:
\[
\alpha\vee\alpha\Rightarrow\alpha\quad,\quad\quad\alpha\Rightarrow\alpha\vee\alpha\quad,\quad\quad\alpha\wedge\alpha\Rightarrow\alpha\quad,\quad\quad\alpha\Rightarrow\alpha\wedge\alpha\quad,
\]
using the proof rules of Section~\ref{subsec:The-rules-of-proof}.
Alternatively, we may use the CH correspondence and show that the
type signatures:
\[
\forall A.\,A+A\rightarrow A\quad,\quad\quad\forall A.\,A\rightarrow A+A\quad,\quad\quad\forall A.\,A\times A\rightarrow A\quad,\quad\quad\forall A.\,A\rightarrow A\times A\quad
\]
can be implemented via fully parametric functions. For a programmer,
it is easier to write code than to guess the correct sequence of proof
rules. For the first pair of type signatures, we find:
\begin{lstlisting}
def f1[A]: Either[A, A] => A = {
  case Left(a)    => a   // No other choice here.
  case Right(a)   => a   // No other choice here.
}
def f2[A]: A => Either[A, A] = { a => Left(a) } // Can be also Right(a).
\end{lstlisting}
The presence of an arbitrary choice, to return \lstinline!Left(a)!
or \lstinline!Right(a)!, is a warning sign showing that additional
information is required to create a value of type \lstinline!Either[A, A]!.
This is precisely the information present in the type $A+A$ but missing
in the type $A$.

The code notation for these functions is:
\begin{align*}
f_{1} & \triangleq\,\begin{array}{|c||c|}
 & A\\
\hline A & a^{:A}\rightarrow a\\
A & a^{:A}\rightarrow a
\end{array}\,=\,\begin{array}{|c||c|}
 & A\\
\hline A & \text{id}\\
A & \text{id}
\end{array}\quad,\\
f_{2} & \triangleq a^{:A}\rightarrow a+\bbnum 0^{:A}=\,\begin{array}{|c||cc|}
 & A & A\\
\hline A & a^{:A}\rightarrow a & \bbnum 0
\end{array}\,=\,\begin{array}{|c||cc|}
 & A & A\\
\hline A & \text{id} & \bbnum 0
\end{array}\quad.
\end{align*}
The composition of these functions is \emph{not} equal to identity:
\[
f_{1}\bef f_{2}=\,\begin{array}{||c|}
\text{id}\\
\text{id}
\end{array}\,\bef\,\begin{array}{||cc|}
\text{id} & \bbnum 0\end{array}\,=\,\begin{array}{||cc|}
\text{id} & \bbnum 0\\
\text{id} & \bbnum 0
\end{array}\,\quad,\quad\text{while we have}\quad\text{id}^{:A+A\rightarrow A+A}=\,\begin{array}{||cc|}
\text{id} & \bbnum 0\\
\bbnum 0 & \text{id}
\end{array}\quad.
\]

For the second pair of type signatures, the code is:
\begin{lstlisting}
def f1[A]: ((A, A)) => A = { case (a1, a2) => a1 }   // Could be also `a2`.
cef f2[A]: A => (A, A)   = { a => (a, a) }           // No other choice here.
\end{lstlisting}
It is clear that the first function loses information when it returns
\lstinline!a1! and discards \lstinline!a2! (or vice versa).

The code notation for the functions \lstinline!f1! and \lstinline!f2!
is:
\[
f_{1}\triangleq a_{1}^{:A}\times a_{2}^{:A}\rightarrow a_{1}=\pi_{1}^{:A\times A\rightarrow A}\quad,\quad\quad f_{2}\triangleq a^{:A}\rightarrow a\times a=\Delta^{:A\rightarrow A\times A}\quad.
\]
Computing the compositions of these functions, we find that $f_{2}\bef f_{1}=\text{id}$
while $f_{1}\bef f_{2}\ne\text{id}$:
\begin{align*}
f_{1}\bef f_{2} & =\left(a_{1}\times a_{2}\rightarrow a_{1}\right)\bef\left(a\rightarrow a\times a\right)\\
 & =\left(a_{1}\times a_{2}\rightarrow a_{1}\times a_{1}\right)\neq\text{id}=\left(a_{1}\times a_{2}\rightarrow a_{1}\times a_{2}\right)\quad.
\end{align*}

We have implemented all four type signatures as fully parametric functions,
which shows that the corresponding logical formulas are all true (i.e.,
can be derived using the proof rules). However, the functions cannot
be inverses of each other. So, the type equivalences do not hold.

\subsubsection{Example \label{subsec:ch-solvedExample-4}\ref{subsec:ch-solvedExample-4}}

Show that $\left(\left(A\wedge B\right)\Rightarrow C\right)\neq(A\Rightarrow C)\vee(B\Rightarrow C)$
in the constructive logic, but the equality holds in Boolean logic.
This is another example where the Boolean reasoning fails to give
correct answers about implementability of type signatures.

\subparagraph{Solution}

Begin by rewriting the logical equality as two implications:
\begin{align*}
(A\wedge B & \Rightarrow C)\Rightarrow(A\Rightarrow C)\vee(B\Rightarrow C)\\
\quad\text{ and }\quad & \left((A\Rightarrow C)\vee(B\Rightarrow C)\right)\Rightarrow\left(\left(A\wedge B\right)\Rightarrow C\right)\quad.
\end{align*}
It is sufficient to show that one of these implications is incorrect.
Rather than looking for a proof tree in the constructive logic (which
would be difficult, since we need to demonstrate that \emph{no} proof
exists), let us use the CH correspondence. According to the CH correspondence,
an equivalent task is to implement fully parametric functions with
the type signatures:
\[
(A\times B\rightarrow C)\rightarrow(A\rightarrow C)+(B\rightarrow C)\quad\text{ and }\quad(A\rightarrow C)+(B\rightarrow C)\rightarrow A\times B\rightarrow C\quad.
\]
For the first type signature, the Scala code is:
\begin{lstlisting}
def f1[A, B, C]: (((A, B)) => C) => Either[A => C, B => C] = { k => ??? }
\end{lstlisting}
We are required to return either a \lstinline!Left(g)! with \lstinline!g: A => C!,
or a \lstinline!Right(h)! with \lstinline!h: B => C!. The only given
data is a function \lstinline!k! of type $A\times B\rightarrow C$,
so the decision of whether to return a \lstinline!Left! or a \lstinline!Right!
must be hard-coded in the function \lstinline!f1! independently of
\lstinline!k!. Can we produce a function \lstinline!g! of type \lstinline!A => C!?
Given a value of type \lstinline!A!, we would need to return a value
of type \lstinline!C!. The only way to obtain a value of type \lstinline!C!
is by applying \lstinline!k! to some arguments. But to apply \lstinline!k!,
we need a value of type \lstinline!B!, which we do not have. So,
we cannot produce a \lstinline!g: A => C!. Similarly, we cannot produce
a function \lstinline!h! of type \lstinline!B => C!.

We repeat the same argument in the type notation. Obtaining a value
of type $(A\rightarrow C)+(B\rightarrow C)$ means to compute either
$g^{:A\rightarrow C}+\bbnum 0$ or $\bbnum 0+h^{:B\rightarrow C}$.
This decision must be hard-coded since the only data is a function
$k^{:A\times B\rightarrow C}$. We can compute $g^{:A\rightarrow C}$
only by partially applying $k^{:A\times B\rightarrow C}$ to a value
of type $B$. However, we have no values of type $B$. Similarly,
we cannot compute a value $h^{:B\rightarrow C}$.

The inverse type signature \emph{can} be implemented:
\begin{lstlisting}
def f2[A, B, C]: Either[A => C, B => C] => ((A, B)) => C = {
  case Left(g)    =>  { case (a, b) => g(a) }
  case Right(h)   =>  { case (a, b) => h(b) }
}
\end{lstlisting}
\[
f_{2}\triangleq\,\begin{array}{|c||c|}
 & A\times B\rightarrow C\\
\hline A\rightarrow C & g^{:A\rightarrow C}\rightarrow a\times b\rightarrow g(a)\\
B\rightarrow C & h^{:B\rightarrow C}\rightarrow a\times b\rightarrow h(b)
\end{array}\quad.
\]

Let us now show that the logical identity:
\begin{equation}
((\alpha\wedge\beta)\Rightarrow\gamma)=((\alpha\Rightarrow\gamma)\vee(\beta\Rightarrow\gamma))\label{eq:ch-example-identity-boolean-not-constructive}
\end{equation}
holds in Boolean logic. A straightforward calculation is to simplify
the Boolean expression using Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic}),
which only holds in Boolean logic (but not in the constructive logic).
We find:
\begin{align*}
{\color{greenunder}\text{left-hand side of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})}:}\quad & \left(\alpha\wedge\beta\right)\gunderline{\Rightarrow}\,\gamma\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & \quad=\gunderline{\neg(\alpha\wedge\beta)}\vee\gamma\\
{\color{greenunder}\text{use de Morgan\textsf{'}s law}:}\quad & \quad=\neg\alpha\vee\neg\beta\vee\gamma\quad.\\
{\color{greenunder}\text{right-hand side of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})}:}\quad & (\gunderline{\alpha\Rightarrow\gamma})\vee(\gunderline{\beta\Rightarrow\gamma})\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & \quad=\neg\alpha\vee\gunderline{\gamma}\vee\neg\beta\vee\gunderline{\gamma}\\
{\color{greenunder}\text{use identity }\gamma\vee\gamma=\gamma:}\quad & \quad=\neg\alpha\vee\neg\beta\vee\gamma\quad.
\end{align*}
Both sides of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})
are equal to the same formula, $\neg\alpha\vee\neg\beta\vee\gamma$,
so the identity holds.

This calculation does not work in the constructive logic because its
proof rules can derive neither the Boolean formula~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})
nor the \textbf{law of de Morgan}\index{law of de Morgan}, $\neg(\alpha\wedge\beta)=\left(\neg\alpha\vee\neg\beta\right)$.

Another way of proving the Boolean identity~(\ref{eq:ch-example-identity-boolean-not-constructive})
is to enumerate all possible truth values for the variables $\alpha$,
$\beta$, and $\gamma$. The left-hand side, $\left(\alpha\wedge\beta\right)\Rightarrow\gamma$,
can be $False$ only if $\alpha\wedge\beta=True$ (that is, both $\alpha$
and $\beta$ are $True$) and $\gamma=False$. For all other truth
values of $\alpha$, $\beta$, and $\gamma$, the formula $\left(\alpha\wedge\beta\right)\Rightarrow\gamma$
is $True$. Let us determine when the right-hand side, $(\alpha\Rightarrow\gamma)\vee(\beta\Rightarrow\gamma)$,
can be $False$. This can happen only if both parts of the disjunction
are $False$. That means $\alpha=True$, $\beta=True$, and $\gamma=False$.
So, the two sides of the identity~(\ref{eq:ch-example-identity-boolean-not-constructive})
are both $True$ or both $False$ with any choice of truth values
of $\alpha$, $\beta$, and $\gamma$. In Boolean logic, this is sufficient
to prove the identity~(\ref{eq:ch-example-identity-boolean-not-constructive}).
$\square$

The following example shows how to use the formulas from Tables~\ref{tab:Logical-identities-with-disjunction-and-conjunction}\textendash \ref{tab:Logical-identities-with-function-types}
to derive the type equivalence of complicated type expressions without
need for proofs.

\subsubsection{Example \label{subsec:ch-solvedExample-5-2}\ref{subsec:ch-solvedExample-5-2}}

Use known formulas to verify the type equivalences without direct
proofs:

\textbf{(a)} $A\times\left(A+\bbnum 1\right)\times\left(A+\bbnum 1+\bbnum 1\right)\cong A\times\left(\bbnum 1+\bbnum 1+A\times\left(\bbnum 1+\bbnum 1+\bbnum 1+A\right)\right)$.

\textbf{(b)} $\bbnum 1+A+B\rightarrow\bbnum 1\times B\cong\left(B\rightarrow B\right)\times\left(A\rightarrow B\right)\times B$.

\subparagraph{Solution}

\textbf{(a)} We can expand brackets in type expressions as in arithmetic:
\begin{align*}
A\times\left(A+\bbnum 1\right) & \cong A\times A+A\times\bbnum 1\cong A\times A+A\quad,\\
A\times\left(A+\bbnum 1\right)\times\left(A+\bbnum 1+\bbnum 1\right) & \cong\left(A\times A+A\right)\times\left(A+\bbnum 1+\bbnum 1\right)\\
 & \cong A\times A\times A+A\times A+A\times A\times\left(\bbnum 1+\bbnum 1\right)+A\times\left(\bbnum 1+\bbnum 1\right)\\
 & \cong A\times A\times A+A\times A\times\left(\bbnum 1+\bbnum 1+\bbnum 1\right)+A\times\left(\bbnum 1+\bbnum 1\right)\quad.
\end{align*}
The result looks like a polynomial in $A$, which we can now rearrange
into the required form:
\[
A\times A\times A+A\times A\times\left(\bbnum 1+\bbnum 1+\bbnum 1\right)+A\times\left(\bbnum 1+\bbnum 1\right)\cong A\times\left(\bbnum 1+\bbnum 1+A\times\left(\bbnum 1+\bbnum 1+\bbnum 1+A\right)\right)\quad.
\]

\textbf{(b)} Keep in mind that the conventions of the type notation
make the function arrow $\left(\rightarrow\right)$ group weaker than
other type operations. So, the type expression $\bbnum 1+A+B\rightarrow\bbnum 1\times B$
means a function from $\bbnum 1+A+B$ to $\bbnum 1\times B$. 

Begin by using the equivalence $\bbnum 1\times B\cong B$ to obtain
$\bbnum 1+A+B\rightarrow B$. Now we use another rule:
\[
A+B\rightarrow C\cong\left(A\rightarrow C\right)\times\left(B\rightarrow C\right)
\]
and derive the equivalence:
\[
\bbnum 1+A+B\rightarrow B\cong\left(\bbnum 1\rightarrow B\right)\times\left(A\rightarrow B\right)\times\left(B\rightarrow B\right)\quad.
\]
Finally, we note that $\bbnum 1\rightarrow B\cong B$ and that the
type product is commutative, so we can rearrange the last type expression
into the required form:
\[
B\times\left(A\rightarrow B\right)\times\left(B\rightarrow B\right)\cong\left(B\rightarrow B\right)\times\left(A\rightarrow B\right)\times B\quad.
\]
We obtain the required type expression: $\left(B\rightarrow B\right)\times\left(A\rightarrow B\right)\times B$.

\subsubsection{Example \label{subsec:ch-solvedExample-5}\ref{subsec:ch-solvedExample-5}}

Denote $\text{Reader}^{E,A}\triangleq E\rightarrow A$ and implement
fully parametric functions with types $A\rightarrow\text{Reader}^{E,A}$
and $\text{Reader}^{E,A}\rightarrow(A\rightarrow B)\rightarrow\text{Reader}^{E,B}$.

\subparagraph{Solution}

Begin by defining a type alias for the type constructor $\text{Reader}^{E,A}$:
\begin{lstlisting}
type Reader[E, A] = E => A
\end{lstlisting}
The first type signature has only one implementation:
\begin{lstlisting}
def p[E, A]: A => Reader[E, A] = { x => _ => x }
\end{lstlisting}
We \emph{must} discard the argument of type $E$; we cannot use it
for computing a value of type \lstinline!A!.

The second type signature has three type parameters. It is the curried
version of the function \lstinline!map!:
\begin{lstlisting}
def map[E, A, B]: Reader[E, A] => (A => B) => Reader[E, B] = ???
\end{lstlisting}
Expanding the type alias, we see that the two curried arguments are
functions of types $E\rightarrow A$ and $A\rightarrow B$. The forward
composition of these functions is a function of type $E\rightarrow B$,
or $\text{Reader}^{E,B}$, which is exactly what we are required to
return. So, the code can be written as:

\begin{lstlisting}
def map[E, A, B]: (E => A) => (A => B) => E => B = { r => f => r andThen f }
\end{lstlisting}
If we did not notice this shortcut, we would reason differently: We
are required to compute a value of type $B$ given \emph{three} curried
arguments $r^{:E\rightarrow A}$, $f^{:A\rightarrow B}$, and $e^{:E}$.
Write this requirement as:
\[
\text{map}\triangleq r^{:E\rightarrow A}\rightarrow f^{:A\rightarrow B}\rightarrow e^{:E}\rightarrow???^{:B}\quad,
\]
The symbol $\text{???}^{:B}$ denotes a \index{typed hole}\textbf{typed
hole}. It stands for a value that we are still figuring out how to
compute, but whose type is already known. Typed holes are supported
in Scala by an experimental compiler plugin.\footnote{See \texttt{\href{https://github.com/cb372/scala-typed-holes}{https://github.com/cb372/scala-typed-holes}}}
The plugin will print the known information about the typed hole.

To fill the typed hole $\text{???}^{:B}$, we need a value of type
$B$. Since no arguments have type $B$, the only way of getting a
value of type $B$ is to apply $f^{:A\rightarrow B}$ to some value
of type $A$. So, we write:
\[
\text{map}\triangleq r^{:E\rightarrow A}\rightarrow f^{:A\rightarrow B}\rightarrow e^{:E}\rightarrow f(???^{:A})\quad.
\]
The only way of getting an $A$ is to apply $r$ to some value of
type $E$:
\[
\text{map}\triangleq r^{:E\rightarrow A}\rightarrow f^{:A\rightarrow B}\rightarrow e^{:E}\rightarrow f(r(???^{:E}))\quad.
\]
We have exactly one value of type $E$, namely $e^{:E}$. So, the
code must be:
\[
\text{map}^{E,A,B}\triangleq r^{:E\rightarrow A}\rightarrow f^{:A\rightarrow B}\rightarrow e^{:E}\rightarrow f(r(e))\quad.
\]
Translate this to the Scala syntax:
\begin{lstlisting}
def map[E, A, B]: (E => A) => (A => B) => E => B = { r => f => e => f(r(e)) }
\end{lstlisting}
We may now notice that the expression $e\rightarrow f(r(e))$ is a
function composition $r\bef f$ applied to $e$, and simplify the
code accordingly.

\subsubsection{Example \label{subsec:ch-solvedExample-6}\ref{subsec:ch-solvedExample-6}}

Show that one cannot implement the type signature \lstinline!Reader[A, T] => (A => B) => Reader[B, T]!
by a fully parametric function.

\subparagraph{Solution}

Expand the type signature and try implementing this function:
\begin{lstlisting}
def m[A, B, T] : (A => T) => (A => B) => B => T = { r => f => b => ??? }
\end{lstlisting}
Given values $r^{:A\rightarrow T}$, $f^{:A\rightarrow B}$, and $b^{:B}$,
we need to compute a value of type $T$:
\[
m=r^{:A\rightarrow T}\rightarrow f^{:A\rightarrow B}\rightarrow b^{:B}\rightarrow???^{:T}\quad.
\]
The only way of getting a value of type $T$ is to apply $r$ to some
value of type $A$:
\[
m=r^{:A\rightarrow T}\rightarrow f^{:A\rightarrow B}\rightarrow b^{:B}\rightarrow r(???^{:A})\quad.
\]
However, we do not have any values of type $A$. We have a function
$f^{:A\rightarrow B}$ that \emph{consumes} values of type $A$, and
we cannot use $f$ to produce any values of type $A$. So, it seems
that we are unable to fill the typed hole $\text{???}^{:A}$ and implement
the function \lstinline!m!.

In order to verify that \lstinline!m! is unimplementable, we need
to prove that the logical formula:
\begin{equation}
\forall(\alpha,\beta,\tau).\,(\alpha\Rightarrow\tau)\Rightarrow(\alpha\Rightarrow\beta)\Rightarrow(\beta\Rightarrow\tau)\label{eq:ch-example-boolean-formula-3}
\end{equation}
is not true in the constructive logic. We could use the \lstinline!curryhoward!
library for that:
\begin{lstlisting}
@ def m[A, B, T] : (A => T) => (A => B) => B => T = implement
cmd1.sc:1: type (A => T) => (A => B) => B => T cannot be implemented
def m[A, B, T] : (A => T) => (A => B) => B => T = implement
                                                ^
Compilation Failed
\end{lstlisting}
Another way is to check whether this formula is true in Boolean logic.
A formula that holds in constructive logic will always hold in Boolean
logic, because all rules shown in Section~\ref{subsec:The-rules-of-proof}
preserve Boolean truth values (see Section~\ref{subsec:Relationship-between-Boolean}
for a proof). It follows that any formula that fails to hold in Boolean
logic will also not hold in constructive logic. 

It is relatively easy to check whether a given Boolean formula is
always equal to $True$. Simplifying Eq.~(\ref{eq:ch-example-boolean-formula-3})
with the rules of Boolean logic, we find:
\begin{align*}
 & (\alpha\Rightarrow\tau)\,\gunderline{\Rightarrow}\,(\alpha\Rightarrow\beta)\,\gunderline{\Rightarrow}\,(\beta\Rightarrow\tau)\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\neg(\gunderline{\alpha\Rightarrow\tau})\vee\neg(\gunderline{\alpha\Rightarrow\beta})\vee(\gunderline{\beta\Rightarrow\tau})\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{\neg(\neg\alpha\vee\tau)}\vee\gunderline{\neg(\neg\alpha\vee\beta)}\vee(\neg\beta\vee\tau)\\
{\color{greenunder}\text{use de Morgan\textsf{'}s law}:}\quad & =\left(\alpha\wedge\neg\tau\right)\vee\gunderline{\left(\alpha\wedge\neg\beta\right)\vee\neg\beta}\vee\tau\\
{\color{greenunder}\text{use identity }(p\wedge q)\vee q=q:}\quad & =\gunderline{\left(\alpha\wedge\neg\tau\right)}\vee\neg\beta\vee\gunderline{\tau}\\
{\color{greenunder}\text{use identity }(p\wedge\neg q)\vee q=p\vee q:}\quad & =\alpha\vee\neg\beta\vee\tau\quad.
\end{align*}
This formula is not identically $True$: it is $False$ when $\alpha=\tau=False$
and $\beta=True$. So, Eq.~(\ref{eq:ch-example-boolean-formula-3})
is not true in Boolean logic, therefore it is also not true in constructive
logic. By the CH correspondence, we conclude that the type signature
of \lstinline!m! cannot be implemented by a fully parametric function.

\subsubsection{Example \label{subsec:ch-solvedExample-7}\ref{subsec:ch-solvedExample-7}}

Define the type constructor $P^{A}\triangleq\bbnum 1+A+A$ and implement
\lstinline!map! for it, with the type signature $\text{map}^{A,B}:P^{A}\rightarrow(A\rightarrow B)\rightarrow P^{B}$.
To check that \lstinline!map! preserves information, verify the law
\lstinline!map(p)(x => x) == p! for all \lstinline!p: P[A]!.

\subparagraph{Solution}

It is implied that \lstinline!map! should be fully parametric and
information-preserving. Begin by defining a Scala type constructor
for the notation $P^{A}\triangleq\bbnum 1+A+A$:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A]()     extends P[A]
final case class P2[A](x: A) extends P[A]
final case class P3[A](x: A) extends P[A]
\end{lstlisting}
Now we can write code to implement the required type signature. Each
time we have several choices of an implementation, we will choose
to preserve information as much as possible.

\begin{lstlisting}
def map[A, B]: P[A] => (A => B) => P[B] =
  p => f => p match {
    case P1()    => P1() // No other choice.
    case P2(x)   => ???
    case P3(x)   => ???
  }
\end{lstlisting}
In the case \lstinline!P2(x)!, we are required to produce a value
of type $P^{B}$ from a value $x^{:A}$ and a function $f^{:A\rightarrow B}$.
Since $P^{B}$ is a disjunctive type with three parts, we can produce
a value of type $P^{B}$ in three different ways: \lstinline!P1()!,
\lstinline!P2(...)!, and \lstinline!P3(...)!. If we return \lstinline!P1()!,
we will lose the information about the value \lstinline!x!. If we
return \lstinline!P3(...)!, we will preserve the information about
\lstinline!x! but lose the information\index{information loss} that
the input value was a \lstinline!P2! rather than a \lstinline!P3!.
By returning \lstinline!P2(...)! in that scope, we preserve the entire
input information. 

The value under \lstinline!P2(...)! must be of type $B$, and the
only way of getting a value of type $B$ is to apply $f$ to $x$.
So, we return \lstinline!P2(f(x))!.

Similarly, in the case \lstinline!P3(x)!, we should return \lstinline!P3(f(x))!.
The final code of \lstinline!map! is:
\begin{lstlisting}
def map[A, B]: P[A] => (A => B) => P[B] = p => f => p match {
  case P1()    => P1()        // No other choice here.
  case P2(x)   => P2(f(x))    // Preserve information.
  case P3(x)   => P3(f(x))    // Preserve information.
}
\end{lstlisting}

To verify the given law, we first write a matrix notation for \lstinline!map!:
\[
\text{map}^{A,B}\triangleq p^{:\bbnum 1+A+A}\rightarrow f^{:A\rightarrow B}\rightarrow p\triangleright\,\begin{array}{|c||ccc|}
 & \bbnum 1 & B & B\\
\hline \bbnum 1 & \text{id} & \bbnum 0 & \bbnum 0\\
A & \bbnum 0 & f & \bbnum 0\\
A & \bbnum 0 & \bbnum 0 & f
\end{array}\quad.
\]
The required law is written as an equation $\text{map}\left(p\right)(\text{id})=p$,
called the\index{identity laws!of map@of \texttt{map}} \textbf{identity
law}. Substituting the code notation for \lstinline!map!, we verify
the law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{map}\left(p\right)(\text{id})\\
{\color{greenunder}\text{apply \texttt{map()()} to arguments}:}\quad & =p\triangleright\,\begin{array}{||ccc|}
\text{id} & \bbnum 0 & \bbnum 0\\
\bbnum 0 & \text{id} & \bbnum 0\\
\bbnum 0 & \bbnum 0 & \text{id}
\end{array}\\
{\color{greenunder}\text{identity function in matrix notation}:}\quad & =p\triangleright\text{id}\\
{\color{greenunder}\triangleright\text{-notation}:}\quad & =\text{id}\left(p\right)=p\quad.
\end{align*}


\subsubsection{Example \label{subsec:ch-solvedExample-8}\ref{subsec:ch-solvedExample-8}}

Implement \lstinline!map! and \lstinline!flatMap! for \lstinline!Either[L, R]!,
applied to the type parameter \lstinline!L!.

\subparagraph{Solution}

For a type constructor, say, $P^{A}$, the standard type signatures
for \lstinline!map! and \lstinline!flatMap! are:
\[
\text{map}:P^{A}\rightarrow(A\rightarrow B)\rightarrow P^{B}\quad,\quad\quad\text{flatMap}:P^{A}\rightarrow(A\rightarrow P^{B})\rightarrow P^{B}\quad.
\]
If a type constructor has more than one type parameter, e.g., $P^{A,S,T}$,
one can define the functions \lstinline!map! and \lstinline!flatMap!
applied to a chosen type parameter. For example, when applied to the
type parameter $A$, the type signatures are:
\begin{align*}
\text{map} & :P^{A,S,T}\rightarrow(A\rightarrow B)\rightarrow P^{B,S,T}\quad,\\
\text{flatMap} & :P^{A,S,T}\rightarrow(A\rightarrow P^{B,S,T})\rightarrow P^{B,S,T}\quad.
\end{align*}
Being \textsf{``}applied to the type parameter $A$\textsf{''} means that the other
type parameters $S,T$ in $P^{A,S,T}$ remain fixed while the type
parameter $A$ is replaced by $B$ in the type signatures of \lstinline!map!
and \lstinline!flatMap!.

For the type \lstinline!Either[L, R]! (in the type notation, $L+R$),
we keep the type parameter $R$ fixed while $L$ is replaced by $M$.
So, we obtain the type signatures:
\begin{align*}
\text{map} & :L+R\rightarrow(L\rightarrow M)\rightarrow M+R\quad,\\
\text{flatMap} & :L+R\rightarrow(L\rightarrow M+R)\rightarrow M+R\quad.
\end{align*}
Implementing these functions is straightforward:
\begin{lstlisting}
def map[L,M,R]: Either[L, R] => (L => M) => Either[M, R] = e => f => e match {
  case Left(x)    => Left(f(x))
  case Right(y)   => Right(y)
}

def flatMap[L, M, R]: Either[L, R] => (L => Either[M, R]) => Either[M, R] = e => f => e match {
  case Left(x)    => f(x)
  case Right(y)   => Right(y)
}
\end{lstlisting}
The code notation for these functions is:
\begin{align*}
\text{map} & \triangleq e^{:L+R}\rightarrow f^{:L\rightarrow M}\rightarrow e\triangleright\,\begin{array}{|c||cc|}
 & M & R\\
\hline L & f & \bbnum 0\\
R & \bbnum 0 & \text{id}
\end{array}\quad,\\
\text{flatMap} & \triangleq e^{:L+R}\rightarrow f^{:L\rightarrow M+R}\rightarrow e\triangleright\,\begin{array}{|c||c|}
 & M+R\\
\hline L & f\\
R & y^{:R}\rightarrow\bbnum 0^{:M}+y
\end{array}\quad.
\end{align*}
Note that the code matrix for \lstinline!flatMap! cannot be split
into the $M$ and $R$ columns because we do not know in advance which
part of the disjunctive type $M+R$ will be returned when we evaluate
$f(x^{:L})$.

\subsubsection{Example \label{subsec:ch-solvedExample-9}\ref{subsec:ch-solvedExample-9}}

Define a type constructor $\text{State}^{S,A}\equiv S\rightarrow A\times S$
and implement the functions:

\textbf{(a)} $\text{pure}^{S,A}:A\rightarrow\text{State}^{S,A}\quad.$

\textbf{(b)} $\text{map}^{S,A,B}:\text{State}^{S,A}\rightarrow(A\rightarrow B)\rightarrow\text{State}^{S,B}\quad.$

\textbf{(c)} $\text{flatMap}^{S,A,B}:\text{State}^{S,A}\rightarrow(A\rightarrow\text{State}^{S,B})\rightarrow\text{State}^{S,B}\quad.$

\subparagraph{Solution}

It is assumed that all functions must be fully parametric and preserve
as much information as possible. We define the type alias:
\begin{lstlisting}
type State[S, A] = S => (A, S)
\end{lstlisting}

\textbf{(a)} The type signature is $A\rightarrow S\rightarrow A\times S$,
and there is only one implementation:
\begin{lstlisting}
def pure[S, A]: A => State[S, A] = a => s => (a, s)
\end{lstlisting}
In the code notation, this is written as:
\[
\text{pu}^{S,A}\triangleq a^{:A}\rightarrow s^{:S}\rightarrow a\times s\quad.
\]

\textbf{(b)} The type signature is:
\[
\text{map}^{S,A,B}:(S\rightarrow A\times S)\rightarrow(A\rightarrow B)\rightarrow S\rightarrow B\times S\quad.
\]
Begin writing a Scala implementation:
\begin{lstlisting}
def map[S, A, B]: State[S, A] => (A => B) => State[S, B] = { t => f => s => ??? }
\end{lstlisting}
We need to compute a value of $B\times S$ from the curried arguments
$t^{:S\rightarrow A\times S}$, $f^{:A\rightarrow B}$, and $s^{:S}$.
We begin writing the code of \lstinline!map! using a typed hole:
\[
\text{map}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow B}\rightarrow s^{:S}\rightarrow\text{???}^{:B}\times\text{???}^{:S}\quad.
\]
The only way of getting a value of type $B$ is by applying $f$ to
a value of type $A$:
\[
\text{map}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow B}\rightarrow s^{:S}\rightarrow f(\text{???}^{:A})\times\text{???}^{:S}\quad.
\]
The only possibility of filling the typed hole $\text{???}^{:A}$
is to apply $t$ to a value of type $S$. We already have such a value,
$s^{:S}$. Computing $t(s)$ yields a pair of type $A\times S$, from
which we may take the first part (of type $A$) to fill the typed
hole $\text{???}^{:A}$. The second part of the pair is a value of
type $S$ that we may use to fill the second typed hole, $\text{???}^{:S}$.
So, the Scala code is:
\begin{lstlisting}[numbers=left]
def map[S, A, B]: State[S, A] => (A => B) => State[S, B] = {
  t => f => s =>
    val (a, s2) = t(s)
    (f(a), s2)     // We could also return `(f(a), s)` here.
}
\end{lstlisting}
Why not return the original value \lstinline!s! in the tuple $B\times S$,
instead of the new value \lstinline!s2!? The reason is that we would
like to preserve information as much as possible. If we return \lstinline!(f(a), s)!
in line 4, we will have discarded the computed value \lstinline!s2!,
which is a loss of information.

To write the code notation for \lstinline!map!, we need to destructure
the pair that $t(s)$ returns. We can write explicit destructuring
code like this:
\[
\text{map}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow B}\rightarrow s^{:S}\rightarrow(a^{:A}\times s_{2}^{:S}\rightarrow f(a)\times s_{2})(t(s))\quad.
\]
If we temporarily denote by $q$ the following destructuring function:
\[
q\triangleq(a^{:A}\times s_{2}^{:S}\rightarrow f(a)\times s_{2})\quad,
\]
we will notice that the expression $s\rightarrow q(t(s))$ is a function
composition applied to $s$. So, we rewrite $s\rightarrow q(t(s))$
as the composition $t\bef q$ and obtain shorter code:
\[
\text{map}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow B}\rightarrow t\bef(a^{:A}\times s^{:S}\rightarrow f(a)\times s)\quad.
\]
Shorter formulas are often easier to reason about in derivations,
although not necessarily easier to read when converted to program
code.

\textbf{(c)} The required type signature is:
\[
\text{flatMap}^{S,A,B}:(S\rightarrow A\times S)\rightarrow(A\rightarrow S\rightarrow B\times S)\rightarrow S\rightarrow B\times S\quad.
\]
We perform code reasoning with typed holes:
\[
\text{flatMap}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow S\rightarrow B\times S}\rightarrow s^{:S}\rightarrow\text{???}^{:B\times S}\quad.
\]
To fill $\text{???}^{:B\times S}$, we need to apply $f$ to some
arguments, since $f$ is the only function that returns any values
of type $B$. Applying $f$ to two values will yield a value of type
$B\times S$, just as we need:
\[
\text{flatMap}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow S\rightarrow B\times S}\rightarrow s^{:S}\rightarrow f(\text{???}^{:A})(\text{???}^{:S})\quad.
\]
To fill the new typed holes, we need to apply $t$ to an argument
of type $S$. We have only one given value $s^{:S}$ of type $S$,
so we must compute $t(s)$ and destructure it:
\[
\text{flatMap}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow S\rightarrow B\times S}\rightarrow s^{:S}\rightarrow\left(a\times s_{2}\rightarrow f(a)(s_{2})\right)(t(s))\quad.
\]
Translating this notation into Scala code, we obtain:
\begin{lstlisting}
def flatMap[S, A, B]: State[S, A] => (A => State[S, B]) => State[S, B] = {
  t => f => s =>
    val (a, s2) = t(s)
    f(a)(s2)            // We could also return `f(a)(s)` here, but that would lose information.
}
\end{lstlisting}
In order to preserve information, we choose not to discard the computed
value \lstinline!s2!.

The code notation for this \lstinline!flatMap! can be simplified
to:
\[
\text{flatMap}\triangleq t^{:S\rightarrow A\times S}\rightarrow f^{:A\rightarrow S\rightarrow B\times S}\rightarrow t\bef\left(a\times s\rightarrow f(a)(s)\right)\quad.
\]


\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:ch-Exercise-0}\ref{subsec:ch-Exercise-0}}

Find the cardinality of the following Scala type: 
\begin{lstlisting}
type P = Option[Boolean => Option[Boolean]]
\end{lstlisting}
Show that \lstinline!P! is equivalent to \lstinline!Option[Boolean] => Boolean!,
but the equivalence is accidental\index{type equivalence!accidental}
and not \textsf{``}natural\textsf{''}.

\subsubsection{Exercise \label{subsec:ch-Exercise-1-a}\ref{subsec:ch-Exercise-1-a}}

Verify the type equivalences $A+A\cong\bbnum 2\times A$ and $A\times A\cong\bbnum 2\rightarrow A$,
where $\bbnum 2$ denotes the \lstinline!Boolean! type.

\subsubsection{Exercise \label{subsec:ch-Exercise-1}\ref{subsec:ch-Exercise-1}}

Show that $A\Rightarrow(B\vee C)\neq(A\Rightarrow B)\wedge(A\Rightarrow C)$
in constructive and Boolean logic.

\subsubsection{Exercise \label{subsec:ch-solvedExample-5-1}\ref{subsec:ch-solvedExample-5-1}}

Verify the type equivalence $\left(A\rightarrow B\times C\right)\cong\left(A\rightarrow B\right)\times\left(A\rightarrow C\right)$
with full proofs.

\subsubsection{Exercise \label{subsec:ch-Exercise-type-identity-4}\ref{subsec:ch-Exercise-type-identity-4}}

Use known rules to verify the type equivalences without need for proofs:

\textbf{(a)} $\left(A+B\right)\times\left(A\rightarrow B\right)\cong A\times\left(A\rightarrow B\right)+\left(\bbnum 1+A\rightarrow B\right)\quad.$

\textbf{(b)} $\left(A\times(\bbnum 1+A)\rightarrow B\right)\cong\left(A\rightarrow B\right)\times\left(A\rightarrow A\rightarrow B\right)\quad.$

\textbf{(c)} $A\rightarrow\left(\bbnum 1+B\right)\rightarrow C\times D\cong\left(A\rightarrow C\right)\times\left(A\rightarrow D\right)\times\left(A\times B\rightarrow C\right)\times\left(A\times B\rightarrow D\right)\quad.$

\subsubsection{Exercise \label{subsec:ch-Exercise-2}\ref{subsec:ch-Exercise-2}}

Write the type notation for \lstinline!Either[(A, Int), Either[(A, Char), (A, Float)]]!.
Transform this type into an equivalent type of the form $A\times(...)$.

\subsubsection{Exercise \label{subsec:ch-Exercise-3}\ref{subsec:ch-Exercise-3}}

Define a type $\text{OptE}^{T,A}\triangleq\bbnum 1+T+A$ and implement
information-preserving \lstinline!map! and \lstinline!flatMap! for
it, applied to the type parameter $A$. Get the same result using
the equivalent type $(\bbnum 1+A)+T$, i.e., \lstinline!Either[Option[A], T]!.
The required type signatures are:
\begin{align*}
\text{map}^{A,B,T} & :\text{OptE}^{T,A}\rightarrow\left(A\rightarrow B\right)\rightarrow\text{OptE}^{T,B}\quad,\\
\text{flatMap}^{A,B,T} & :\text{OptE}^{T,A}\rightarrow(A\rightarrow\text{OptE}^{T,B})\rightarrow\text{OptE}^{T,B}\quad.
\end{align*}


\subsubsection{Exercise \label{subsec:ch-Exercise-4}\ref{subsec:ch-Exercise-4}}

Implement the \lstinline!map! function for the type constructor \lstinline!P[A]!
from Example~\ref{subsec:ch-solvedExample-2}. The required type
signature is $P^{A}\rightarrow\left(A\rightarrow B\right)\rightarrow P^{B}$.
Preserve information as much as possible.

\subsubsection{Exercise \label{subsec:ch-Exercise-4-1}\ref{subsec:ch-Exercise-4-1}}

For the type constructor $Q^{T,A}$ defined in Exercise~\ref{subsec:Exercise-type-notation-1},
define the \lstinline!map! function, preserving information as much
as possible:
\[
\text{map}^{T,A,B}:Q^{T,A}\rightarrow\left(A\rightarrow B\right)\rightarrow Q^{T,B}\quad.
\]


\subsubsection{Exercise \label{subsec:Exercise-disjunctive-6}\ref{subsec:Exercise-disjunctive-6}}

Define a recursive type constructor $\text{Tr}_{3}$ as $\text{Tr}_{3}{}^{A}\triangleq\bbnum 1+A\times A\times A\times\text{Tr}_{3}{}^{A}$
and implement the \lstinline!map! function for it, with the standard
type signature: $\text{map}^{A,B}:\text{Tr}_{3}{}^{A}\rightarrow\left(A\rightarrow B\right)\rightarrow\text{Tr}_{3}{}^{B}\quad.$

\subsubsection{Exercise \label{subsec:ch-Exercise-5}\ref{subsec:ch-Exercise-5}}

Implement fully parametric, information-preserving functions with
the following type signatures:

\textbf{(a)} $\left(A\rightarrow B\rightarrow C\right)\rightarrow A\rightarrow B\rightarrow C\quad.$

\textbf{(b)} $\left(A\rightarrow C\right)\rightarrow\left(B\rightarrow D\right)\rightarrow A+B\rightarrow C+D\quad.$

\textbf{(c)} $\left(A\rightarrow C\right)\rightarrow\left(B\rightarrow D\right)\rightarrow A\times B\rightarrow C\times D\quad.$

\textbf{(d)} $((A\rightarrow A)\rightarrow A)\rightarrow A\quad.$

\textbf{(e)} $\left(\left(A\rightarrow B\right)\rightarrow C\right)\rightarrow B\rightarrow C\quad.$

\textbf{(f)} $((A\rightarrow B)\rightarrow A)\rightarrow\left(A\rightarrow B\right)\rightarrow B\quad.$

\textbf{(g)} $(A\rightarrow B+C)\rightarrow(B\rightarrow C)\rightarrow A\rightarrow C\quad.$

\textbf{(h)} $(A+B\rightarrow C)\rightarrow(B\rightarrow C)\rightarrow A\rightarrow C\quad.$

\textbf{(i)} $\text{Reader}^{E,A}\rightarrow(A\rightarrow\text{Reader}^{E,B})\rightarrow\text{Reader}^{E,B}\quad.$

\textbf{(j)} $\text{Reader}^{E,A}\times\text{Reader}^{E,B}\rightarrow(A\times B\rightarrow C)\rightarrow\text{Reader}^{E,C}\quad.$

\textbf{(k)} $\text{State}^{S,A}\rightarrow\left(S\times A\rightarrow B\right)\rightarrow\text{State}^{S,B}\quad.$

\textbf{(l)} $A+Z\rightarrow B+Z\rightarrow(A\rightarrow B\rightarrow C)\rightarrow C+Z\quad.$

\textbf{(m)} $P+A\times A\rightarrow(A\rightarrow B)\rightarrow(P\rightarrow A+Q)\rightarrow Q+B\times B\quad.$

\subsubsection{Exercise \label{subsec:ch-Exercise-7}\ref{subsec:ch-Exercise-7}}

Denote $\text{Cont}^{R,T}\triangleq\left(T\rightarrow R\right)\rightarrow R$
and implement the functions:

\textbf{(a)} $\text{map}^{R,T,U}:\text{Cont}^{R,T}\rightarrow(T\rightarrow U)\rightarrow\text{Cont}^{R,U}\quad.$

\textbf{(b)} $\text{flatMap}^{R,T,U}:\text{Cont}^{R,T}\rightarrow(T\rightarrow\text{Cont}^{R,U})\rightarrow\text{Cont}^{R,U}\quad.$

\subsubsection{Exercise \label{subsec:ch-Exercise-8}\ref{subsec:ch-Exercise-8}}

Denote $\text{Sel}^{Z,T}\triangleq\left(T\rightarrow Z\right)\rightarrow T$
and implement the functions:

\textbf{(a)} $\text{map}^{Z,A,B}:\text{Sel}^{Z,A}\rightarrow\left(A\rightarrow B\right)\rightarrow\text{Sel}^{Z,B}\quad.$

\textbf{(b)} $\text{flatMap}^{Z,A,B}:\text{Sel}^{Z,A}\rightarrow(A\rightarrow\text{Sel}^{Z,B})\rightarrow\text{Sel}^{Z,B}\quad.$

\section{Discussion and further developments}

\subsection{Using the Curry-Howard correspondence for writing code}

The CH correspondence is used in two practically important reasoning
tasks: checking whether a type signature can be implemented as a fully
parametric function, and determining whether two types are equivalent.
For the first task, we map type expressions into formulas in the constructive
logic and apply the proof rules of that logic. For the second task,
we map type expressions into \emph{arithmetic} formulas and apply
the ordinary rules of arithmetic.

Although tools such as the \lstinline!curryhoward! library can sometimes
derive code from types, it is beneficial if a programmer is able to
derive an implementation by hand or to determine that an implementation
is impossible. For instance, the programmer should recognize that
the type signature:
\begin{lstlisting}
def f[A, B]: A => (A => B) => B
\end{lstlisting}
has only one fully parametric implementation, while the following
two type signatures have none:
\begin{lstlisting}
def g[A, B]: A => (B => A) => B
def h[A, B]: ((A => B) => A) => A
\end{lstlisting}
Exercises in this chapter help to build up the required technique
and intuition. The two main guidelines for code derivation are: \textsf{``}values
of parametric types cannot be constructed from scratch\textsf{''} and \textsf{``}one
must hard-code the decision to return a chosen part of a disjunctive
type when no other disjunctive value is given\textsf{''}. These guidelines
can be justified by referring to the rigorous rules of proof (Table~\ref{tab:Proof-rules-for-constructive-logic}).
Sequents producing a value of type $A$ can be proved only if there
is a premise containing $A$ or a function that returns a value of
type $A$.\footnote{This is proved rigorously by R.~Dyckhoff\index{Roy Dyckhoff} as
the \textsf{``}Theorem\textsf{''} in section 6 (\textsf{``}Goal-directed pruning\textsf{''}), see
\texttt{\href{https://research-repository.st-andrews.ac.uk/handle/10023/8824}{https://research-repository.st-andrews.ac.uk/handle/10023/8824}}} One can derive a disjunction without hard-coding only if one already
has a disjunction in the premises (and then the rule \textsf{``}use \lstinline!Either!\textsf{''}
could apply).

Throughout this chapter, we require all code to be fully parametric.
This is because the CH correspondence gives useful, non-trivial results
only for parameterized types and fully parametric code. For concrete,
non-parameterized types (\lstinline!Int!, \lstinline!String!, etc.),
one can always produce \emph{some} values even with no previous data.
So, the propositions $\mathcal{CH}(\text{Int})$ or $\mathcal{CH}(\text{String})$
are always true.

Consider the function \lstinline!(x:Int) => x + 1!. Its type signature,
\lstinline!Int => Int!, may be implemented by many other functions,
such as \lstinline!x => x - 1!, \lstinline!x => x * 2!, etc. So,
the type signature \lstinline!Int => Int! is insufficient to specify
the code of the function, and deriving code from that type is not
a meaningful task. Only a fully parametric type signature, such as
$A\rightarrow\left(A\rightarrow B\right)\rightarrow B$, could give
enough information for deriving the function\textsf{'}s code. Additionally,
we must require the code of functions to be fully parametric. Otherwise
we will be unable to reason about code derivation from type signatures.

Validity of a ${\cal CH}$-proposition ${\cal CH}(T)$ means that
we can implement \emph{some} value of the given type $T$. But this
does not give any information about the properties of that value,
such as whether it satisfies any laws. This is why type equivalence
(which requires the laws of isomorphisms) is not determined by an
equivalence of logical formulas.

It is useful for programmers to be able to transform type expressions
to equivalent simpler types before starting to write code. The type
notation introduced in this book is designed to help programmers to
recognize patterns in type expressions and to reason about them more
easily. We have shown that a type equivalence corresponds to \emph{each}
standard arithmetic identity such as $\left(a+b\right)+c=a+\left(b+c\right)$,
$\left(a\times b\right)\times c=a\times(b\times c)$, $1\times a=a$,
$\left(a+b\right)\times c=a\times c+b\times c$, and so on. Because
of this, we are allowed to transform and simplify types as if they
were arithmetic expressions, e.g., to rewrite:
\[
\bbnum 1\times\left(A+B\right)\times C+D\cong D+A\times C+B\times C\quad.
\]
The type notation makes this reasoning more intuitive (for people
familiar with arithmetic). 

These results apply to all type expressions built up using product
types, disjunctive types (also called \textsf{``}sum\textsf{''} types because they
correspond to arithmetic sums), and function types (also called \textsf{``}exponential\textsf{''}
types because they correspond to arithmetic exponentials). Type expressions
that contain only products and sum types are called \textbf{polynomial}\index{polynomial type}\index{types!polynomial types}.\footnote{These types are often called \textsf{``}algebraic data types\index{algebraic data types}\textsf{''}
but this book prefers the more precise term \textsf{``}polynomial types\textsf{''}.} Type expressions that also contain function types are called \textbf{exponential-polynomial}\index{exponential-polynomial type}\index{types!exponential-polynomial types}.
We focus on exponential-polynomial types because they are sufficient
for almost all design patterns used in functional programming.

There are no types corresponding to subtraction or division, so arithmetic
equations such as:
\begin{align*}
\left(1-t\right)\times\left(1+t\right) & =1-t\times t\quad,\quad\text{ and }\quad\frac{t+t\times t}{t}=1+t\quad,
\end{align*}
do not directly yield any type equivalences. However, consider this
well-known formula:
\[
\frac{1}{1-t}=1+t+t^{2}+t^{3}+...+t^{n}+...\quad.
\]
At first sight, this formula appears to involve subtraction, division,
and an infinite series, and so cannot be directly translated into
a type equivalence. However, the formula can be rewritten as:
\begin{equation}
\frac{1}{1-t}=L(t)\quad\text{ where }\quad L(t)\triangleq1+t+t^{2}+t^{3}+...+t^{n}\times L(t)\quad.\label{eq:ch-example-type-formula-list}
\end{equation}
The definition of $L(t)$ is finite and only contains additions and
multiplications. So, Eq.~(\ref{eq:ch-example-type-formula-list})
can be translated into a type equivalence:
\begin{equation}
L^{A}\cong1+A+A\times A+A\times A\times A+...+\underbrace{A\times...\times A}_{n\text{ times}}\times\,L^{A}\quad.\label{eq:ch-example-type-expansion-list}
\end{equation}
This type formula (with $n=1$) is equivalent to a recursive definition
of the type constructor \lstinline!List!:
\[
\text{List}^{A}\triangleq1+A\times\text{List}^{A}\quad.
\]
The type equivalence~(\ref{eq:ch-example-type-expansion-list}) suggests
that we may view the recursive type \lstinline!List! heuristically
as an \textsf{``}infinite disjunction\textsf{''} describing lists of zero, one, two,
etc., elements.

\subsection{Implications for designing new programming languages}

Today\textsf{'}s functional programming practice assumes, at the minimum, that
programmers will use the six standard type constructions (Section~\ref{subsec:Type-notation-and-standard-type-constructions})
and the eight standard code constructions (Section~\ref{subsec:The-rules-of-proof}).
These constructions are foundational in the sense that they are used
to express all design patterns of functional programming. A language
that does not directly support all of those constructions cannot be
considered a functional programming language.

A remarkable result of the CH correspondence is that the type system
of any given programming language (functional or not) is mapped into
a \emph{certain} \emph{logic}, i.e., a system of logical operations
and proof rules. A logical operation will correspond to each of the
\emph{type} constructions available in the programming language. A
proof rule will correspond to each of the available \emph{code} constructions.
Programming languages that support all the standard type and code
constructions \textemdash{} for instance, OCaml, Haskell, F\#, Scala,
Swift, Rust, \textemdash{} are mapped into the constructive logic
with all standard logical operations available ($True$, $False$,
disjunction, conjunction, and implication).

Languages such as C, C++, Java, C\#, Go are mapped into logics that
do not have the disjunction operation or the constants $True$ and
$False$. In other words, these languages are mapped into \emph{incomplete}
logics where some true formulas cannot be proved. Incompleteness of
the logic of types will make a programming language unable to express
certain computations, e.g., directly handle data that belongs to a
disjoint domain. 

Languages that do not enforce type checking (e.g., Python or JavaScript)
are mapped to inconsistent logics where any proposition can be proved
\textemdash{} even propositions normally considered $False$. The
CH correspondence will map such absurd proofs to code that \emph{appears}
to compute a certain value (since the $\mathcal{CH}$-proposition
was proved to be $True$) although that value is not actually available.
In practice, such code will crash because of a value that has a wrong
type or is \textsf{``}null\textsf{''} (a pointer to an invalid memory location). Those
errors cannot happen in a programming language whose logic of types
is consistent and whose compiler checks all types at compile time. 

So, the CH correspondence gives a mathematically justified procedure
for designing new programming languages. The procedure has the following
steps:
\begin{itemize}
\item Choose a formal logic that is complete and free of inconsistencies.
\item For each logical operation, provide a type construction in the language.
\item For each axiom and proof rule of the logic, provide a code construction
in the language.
\end{itemize}
Mathematicians have studied different logics, such as modal logic,
temporal logic, or linear logic. Compared with the constructive logic,
those other logics have some additional type operations. For instance,
modal logic adds the operations \textsf{``}necessarily\textsf{''} and \textsf{``}possibly\textsf{''},
and temporal logic adds the operation \textsf{``}until\textsf{''}. For each logic,
mathematicians have determined the minimal complete sets of operations,
axioms, and proof rules that do not lead to inconsistency. Programming
language designers can use this mathematical knowledge by choosing
a logic and translating it into a minimal \textsf{``}core\textsf{''} of a programming
language. Code in that language will be guaranteed \emph{never to
crash} as long as all types match. This mathematical guarantee (known
as \index{type safety}\textbf{type safety}) is a powerful help for
programmers since it automatically prevents a large number of coding
errors. So, programmers will benefit if they use languages designed
using the CH correspondence.

Practically useful programming languages will of course need more
features than the minimal set of mathematically necessary features
derived from a chosen logic. Language designers need to make sure
that all added features are consistent with the core language. 

At present, it is still not fully understood how a practical programming
language could use, say, modal or linear logic as its logic of types.
Experience suggests that, at least, the operations of the plain constructive
logic should be available. So, it appears that the six type constructions
and the eight code constructions will remain available in all future
languages of functional programming. 

It is possible to apply the FP paradigm while writing code in any
programming language. However, some languages lack certain features
that make FP techniques easier to use in practice. For example, in
a language such as C++ or Java, one can easily use the map/reduce
operations but not disjunctive types. More advanced FP constructions
(such as typeclasses) are impractical in those languages: the required
code becomes too hard to read and to write without errors, which negates
the advantages of rigorous reasoning about functional programs.

Some programming languages, such as Haskell and OCaml, were designed
specifically for advanced use and exploration of the FP paradigm.
Other languages, such as F\#, Scala, Swift, and Rust, have different
design goals but still support enough FP features to be considered
FP languages. This book uses Scala, but the same constructions may
be implemented in other FP languages in a similar way. Differences
between OCaml, Haskell, F\#, Scala, Swift, Rust, and other FP languages
do not play a significant role at the level of detail needed in this
book.

\subsection{Practical uses of the void type (Scala\textsf{'}s \texttt{Nothing})}

The \index{void type}void type\footnote{The \textsf{``}void\textsf{''} type is a type with no values. It is \emph{not} the
same as the \lstinline!void! keyword in Java or C that denotes functions
returning \textsf{``}no value\textsf{''}. Those functions are equivalent to Scala
functions returning the (unique) value of \lstinline!Unit! type.} (Scala\textsf{'}s \lstinline!Nothing!) corresponds to the logical constant
$False$. (The proposition \textsf{``}\emph{the code can compute a value of
the void type}\textsf{''} is always false.) The void type is used in some
theoretical proofs but has few practical uses. One use case is for
a branch of a \lstinline!match!/\lstinline!case! expression that
throws an \index{exception}exception instead of returning a value.
In this sense, returning a value of the void type corresponds to a
crash in the program. So, a \lstinline!throw! expression is defined
as if it returns a value of type \lstinline!Nothing!. We can then
pretend to convert that \textsf{``}value\textsf{''} (which will never be actually
returned) into a value of any other type. Example~\ref{subsec:ch-Example-type-identity-0-to-A}
shows how to write a function \lstinline!absurd[A]! of type \lstinline!Nothing => A!.

To see how this trick is used, consider this code defining a value
\lstinline!x!:
\begin{lstlisting}
val x: Double = if (t >= 0.0) math.sqrt(t) else throw new Exception("error")
\end{lstlisting}
The \lstinline!else! branch does not return a value, but \lstinline!x!
is declared to have type \lstinline!Double!. For this code to type-check,
both branches must return values of the same type. So, the compiler
needs to pretend that the \lstinline!else! branch also returns a
value of type \lstinline!Double!. The compiler first assigns the
type \lstinline!Nothing! to the expression \lstinline!throw ...!
and then automatically uses the conversion \lstinline!Nothing => Double!
to convert that type to \lstinline!Double!. In this way, types will
match in the definition of the value \lstinline!x!. 

This book does not discuss exceptions in much detail. The functional
programming paradigm does not use exceptions because their presence
prevents mathematical reasoning about code.

As another example of using the void type, suppose an external library
implements a function:
\begin{lstlisting}
def parallel_run[E, A, B](f: A => Either[E, B]): Either[E, B] = ???
\end{lstlisting}
We may imagine that \lstinline!parallel_run(f)! performs some parallel
computations using a given function $f$. In general, functions $f^{:A\rightarrow E+B}$
may return an error of type $E$ or a result of type $B$. Suppose
we know that a particular function $f$ never fails to compute its
result. To express that knowledge in code, we may explicitly set the
type parameter $E$ to the void type \lstinline!Nothing! when applying
\lstinline!parallel_run!:
\begin{lstlisting}
parallel_run[Nothing, A, B](f)  // Types match only when values f(a) always are of the form Right(b). 
\end{lstlisting}
Returning an error is now impossible (the type \lstinline!Nothing!
has no values). If the function \lstinline!parallel_run! is fully
parametric, it will work in the same way with all types $E$, including
$E=\bbnum 0$. The code implements our intention via type parameters,
giving a compile-time guarantee of correct results.

So far, none of our examples involved the logical \textbf{negation}\index{negation (in logic)}
operation. It is defined as:
\[
\neg\alpha\triangleq(\alpha\Rightarrow False)\quad.
\]
Its practical use in functional programming is as limited as that
of $False$ and the void type. However, logical negation plays an
important role in Boolean logic.

\subsection{Relationship between Boolean logic and constructive logic\label{subsec:Relationship-between-Boolean} }

We have seen that some true theorems of Boolean logic are not true
in constructive logic. For example, the Boolean identities $\neg\left(\neg\alpha\right)=\alpha$
and $\left(\alpha\Rightarrow\beta\right)=(\neg\alpha\vee\beta)$ do
not hold in the constructive logic. However, as we will now show,
any theorem of constructive logic is also a theorem of Boolean logic.
The reason is that all eight rules of constructive logic (Section~\ref{subsec:The-rules-of-proof})
also hold in Boolean logic.

To verify that a formula is true in Boolean logic, it is sufficient
 to check that the value of the formula is $True$ for all possible
truth values ($True$ or $False$) of its variables. A sequent such
as $\alpha,\beta\vdash\gamma$ is true in Boolean logic if and only
if $\gamma=True$ under the assumption that $\alpha=\beta=True$.
So, the sequent $\alpha,\beta\vdash\gamma$ is translated into the
Boolean formula:
\[
\alpha,\beta\vdash\gamma=\left(\left(\alpha\wedge\beta\right)\Rightarrow\gamma\right)=\left(\neg\alpha\vee\neg\beta\vee\gamma\right)\quad.
\]
Table~\ref{tab:Proof-rules-of-constructive-and-boolean} translates
all proof rules of Section~\ref{subsec:The-rules-of-proof} into
Boolean formulas. The first two lines are axioms, while the subsequent
lines are Boolean theorems that can be verified by calculation.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|}
\hline 
\textbf{\small{}Constructive logic} & \textbf{\small{}Boolean logic}\tabularnewline
\hline 
\hline 
{\small{}$\frac{}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{create unit})$} & {\small{}$\neg\Gamma\vee True=True$}\tabularnewline
\hline 
{\small{}$\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use value})$} & {\small{}$\neg\Gamma\vee\neg\alpha\vee\alpha=True$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})$} & {\small{}$\left(\neg\Gamma\vee\neg\alpha\vee\beta\right)=\left(\neg\Gamma\vee\left(\alpha\Rightarrow\beta\right)\right)$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma\vdash\alpha\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})$} & {\small{}$\left(\left(\neg\Gamma\vee\alpha\right)\wedge\left(\neg\Gamma\vee\left(\alpha\Rightarrow\beta\right)\right)\right)\Rightarrow\left(\neg\Gamma\vee\beta\right)$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma\vdash\alpha\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})$} & {\small{}$\left(\neg\Gamma\vee\alpha\right)\wedge\left(\neg\Gamma\vee\beta\right)=\left(\neg\Gamma\vee\left(\alpha\wedge\beta\right)\right)$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)$} & {\small{}$\left(\neg\Gamma\vee\left(\alpha\wedge\beta\right)\right)\Rightarrow\left(\neg\Gamma\vee\alpha\right)$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create \texttt{Left}})$} & {\small{}$\left(\neg\Gamma\vee\alpha\right)\Rightarrow\left(\neg\Gamma\vee\left(\alpha\vee\beta\right)\right)$}\tabularnewline
\hline 
{\small{}$\frac{\Gamma\vdash\alpha\vee\beta\quad\Gamma,\alpha\vdash\gamma\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use \texttt{Either}})$} & {\small{}$\left(\neg\Gamma\vee\alpha\vee\beta\right)\wedge\left(\neg\Gamma\vee\neg\alpha\vee\gamma\right)\quad\quad$}\tabularnewline
 & $\quad\quad\wedge\left(\neg\Gamma\vee\neg\beta\vee\gamma\right)\Rightarrow\left(\neg\Gamma\vee\gamma\right)$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Proof rules of constructive logic are true also in the Boolean logic.\label{tab:Proof-rules-of-constructive-and-boolean}}
\end{table}

To simplify the calculations, note that all terms in the formulas
contain the operation $\left(\neg\Gamma\vee...\right)$ corresponding
to the context $\Gamma$. Now, if $\Gamma$ is $False$, the entire
formula becomes automatically $True$, and there is nothing else to
check. So, it remains to verify the formula in case $\Gamma=True$,
and then we can simply omit all instances of $\neg\Gamma$ in the
formulas. Let us show the Boolean derivations for the rules \textsf{``}$\text{use function}$\textsf{''}
and \textsf{``}use~\lstinline!Either!\textsf{''}; other formulas are checked in
a similar way:
\begin{align*}
{\color{greenunder}\text{formula \textsf{``}use function\textsf{''}}:}\quad & \left(\alpha\wedge\left(\alpha\Rightarrow\beta\right)\right)\Rightarrow\beta\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{\neg}(\alpha\,\gunderline{\wedge}\,(\neg\alpha\,\gunderline{\vee}\,\beta))\vee\beta\\
{\color{greenunder}\text{de Morgan\textsf{'}s laws}:}\quad & =\gunderline{\neg\alpha\vee(\alpha\wedge\neg\beta)}\vee\beta
\end{align*}
\begin{align*}
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q\text{ with }p=\neg\alpha\text{ and }q=\beta:}\quad & =\neg\alpha\vee\gunderline{\neg\beta\vee\beta}\\
{\color{greenunder}\text{axiom \textsf{``}use value\textsf{''}}:}\quad & =True\quad.
\end{align*}
\begin{align*}
{\color{greenunder}\text{formula \textsf{``}use \texttt{Either}\textsf{''}}:}\quad & \left(\left(\alpha\vee\beta\right)\wedge\left(\alpha\Rightarrow\gamma\right)\wedge\left(\beta\Rightarrow\gamma\right)\right)\Rightarrow\gamma\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\neg\left(\left(\alpha\vee\beta\right)\wedge\left(\neg\alpha\vee\gamma\right)\wedge\left(\neg\beta\vee\gamma\right)\right)\vee\gamma\\
{\color{greenunder}\text{de Morgan\textsf{'}s laws}:}\quad & =\left(\neg\alpha\wedge\neg\beta\right)\vee\gunderline{\left(\alpha\wedge\neg\gamma\right)}\vee\gunderline{\left(\beta\wedge\neg\gamma\right)}\vee\gamma\\
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q:}\quad & =\gunderline{\left(\neg\alpha\wedge\neg\beta\right)\vee\alpha}\vee\beta\vee\gamma\\
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q:}\quad & =\gunderline{\neg\alpha\vee\alpha}\vee\beta\vee\gamma\\
{\color{greenunder}\text{axiom \textsf{``}use value\textsf{''}}:}\quad & =True\quad.
\end{align*}
Since each proof rule of the constructive logic is translated into
a true formula in Boolean logic, it follows that a proof tree in the
constructive logic will be translated into a tree of Boolean formulas
that have value $True$ for each axiom or proof rule. The result is
that any constructive proof for a sequent such as $\emptyset\vdash f(\alpha,\beta,\gamma)$
is translated into a chain of Boolean implications that look like
this:
\[
True=(...)\Rightarrow(...)\Rightarrow...\Rightarrow f(\alpha,\beta,\gamma)\quad.
\]
Since $\left(True\Rightarrow\alpha\right)=\alpha$, this chain proves
the Boolean formula $f(\alpha,\beta,\gamma)$.

For example, the proof tree shown in Figure~\ref{fig:Proof-of-the-sequent-example-2}
is translated into:
\begin{align*}
{\color{greenunder}\text{axiom \textsf{``}use valuue\textsf{''}}:}\quad & True=\left(\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\neg\alpha\vee\alpha\right)\\
{\color{greenunder}\text{rule \textsf{``}create function\textsf{''}}:}\quad & \quad\Rightarrow\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\alpha\right)\quad.\\
{\color{greenunder}\text{axiom \textsf{``}use value\textsf{''}}:}\quad & True=\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\quad.\\
{\color{greenunder}\text{rule \textsf{``}use function\textsf{''}}:}\quad & True\Rightarrow\left(\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\beta\right)\\
{\color{greenunder}\text{rule \textsf{``}create function\textsf{''}}:}\quad & \quad\Rightarrow\left(\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\Rightarrow\beta\right)\quad.
\end{align*}

It is easier to check Boolean truth tables than to find a proof tree
in constructive logic (or to establish that no proof tree exists).
If we find that a formula is \emph{not} true in Boolean logic, we
know it is also not true in constructive logic. This gives us a quick
way of proving that some type signatures are \emph{not} implementable
as fully parametric functions. However, if a formula is true in Boolean
logic, it does not follow that the formula is also true in the constructive
logic.

In addition to formulas shown in Table~\ref{tab:Logical-formulas-not-Boolean-theorems}
(Section~\ref{subsec:ch-Motivation-and-first-examples}), here are
three more examples of formulas that are \emph{not} true in Boolean
logic:
\[
\forall\alpha.\,\alpha\quad,\quad\quad\forall(\alpha,\beta).\,\alpha\Rightarrow\beta\quad,\quad\quad\forall(\alpha,\beta).\,(\alpha\Rightarrow\beta)\Rightarrow\beta\quad.
\]
These formulas are also \emph{not} true in the constructive logic.

\subsection{The constructive logic and the law of excluded middle}

Computations in the Boolean logic are often performed using truth
tables. It is perhaps surprising that the proof rules of the constructive
logic are \emph{not} equivalent to checking whether some propositions
are $True$ or $False$ via a truth table. A general form of this
statement was proved by K.~G\"odel\index{Kurt@Kurt G\"odel} in
1932.\footnote{See \texttt{\href{https://plato.stanford.edu/entries/intuitionistic-logic-development/\#SomeEarlResu}{plato.stanford.edu/entries/intuitionistic-logic-development/}}}
In this sense, constructive logic does \emph{not} imply that every
proposition is either $True$ or $False$. This is not intuitive and
requires getting used to. Reasoning in the constructive logic must
use the axioms and derivation rules directly, instead of truth tables.

The Boolean logic can use truth tables because every Boolean proposition
may be assumed in advance to be either $True$ or $False$. This can
be written as the formula $\forall\alpha.\,(\neg\alpha\vee\alpha=True)$.
Table~\ref{tab:Proof-rules-of-constructive-and-boolean} uses the
Boolean identity $\left(\alpha\Rightarrow\beta\right)=(\neg\alpha\vee\beta)$,
which does not hold in the constructive logic, to translate the constructive
axiom \textsf{``}$\text{use value}$\textsf{''} into the Boolean axiom $\neg\alpha\vee\alpha=True$.
The formula $\forall\alpha.\,\neg\alpha\vee\alpha=True$ is known
as the \textbf{law of excluded middle}.\footnote{See \texttt{\href{https://en.wikipedia.org/wiki/Law_of_excluded_middle}{https://en.wikipedia.org/wiki/Law\_of\_excluded\_middle}}}\index{law of excluded middle}
It is remarkable that the constructive logic \emph{does not have}
the law of excluded middle. It is neither an axiom nor a derived theorem
in constructive logic.

To see why, translate the constructive logic formula $\forall\alpha.\,\neg\alpha\vee\alpha=True$
into a type. The negation operation ($\neg\alpha$) is defined as
the implication $\alpha\Rightarrow False$. So, the formula $\forall\alpha.\,\neg\alpha\vee\alpha$
corresponds to the type $\forall A.\,\left(A\rightarrow\bbnum 0\right)+A$.
Can we compute a value of this type via fully parametric code? For
that, we need to compute either a value of type $A\rightarrow\bbnum 0$
or a value of type $A$. This decision needs to be made in advance
independently of $A$, because the code of a fully parametric function
must operate in the same way for all types. As we have seen in Example~\ref{subsec:ch-Example-type-identity-A-0},
a value of type $A\rightarrow\bbnum 0$ exists if the type $A$ is
itself $\bbnum 0$. But we do not know in advance whether $A=\bbnum 0$.
Since there are no values of type $\bbnum 0$, and the type parameter
$A$ could be, say, \lstinline!Int!, we cannot compute a value of
type $A\rightarrow\bbnum 0$. For similar reasons, we cannot compute
a value of type $A$ either.

Is it really impossible to implement a value of the type $\left(A\rightarrow\bbnum 0\right)+A$?
We could reason like this: the type $A$ is either void or not void.
If $A$ is void then $\left(A\rightarrow\bbnum 0\right)\cong\bbnum 1$
is not void (as Example~\ref{subsec:ch-Example-type-identity-A-0}
shows). So, one of the types in the disjunction $\left(A\rightarrow\bbnum 0\right)+A$
should be non-void and have values that we can compute.

While this argument is true, it does not help implementing a value
of type $\left(A\rightarrow\bbnum 0\right)+A$ via fully parametric
code. It is not enough to know that one of the two values \textsf{``}should
exist\textsf{''}. We need to know \emph{which} of the two values exists, and
we need to write code that computes that value. That code may not
decide what to do depending on whether the type $A$ is void, because
the code must work in the same way for all types $A$ (void or not).
As we have seen, that code is impossible to write.

In Boolean logic, one may prove that a value \textsf{``}should exist\textsf{''} by
showing that the non-existence of a value is contradictory in some
way. However, any practically useful program needs to \textsf{``}construct\textsf{''}
(i.e., to compute) actual values. The \textsf{``}constructive\index{constructive logic}\textsf{''}
logic got its name from this requirement. So, it is the constructive
logic (not the Boolean logic) that provides correct reasoning about
the types of values computable by fully parametric functional programs.

If we drop the requirement of full parametricity, we \emph{could}
implement the law of excluded middle\index{law of excluded middle}.
Special features of Scala (reflection, type tags, and type casts)
allow programmers to compare types as values and to determine what
type was given to a type parameter when a function is applied:
\begin{lstlisting}[mathescape=true]
import scala.reflect.runtime.universe._
   // Convert the type parameter T into a special value.
def getType[T: TypeTag]: Type = weakTypeOf[T]
   // Compare types A and B.
def equalTypes[A: TypeTag, B: TypeTag]: Boolean = getType[A] =:= getType[B]

  // excludedMiddle has type ${\color{dkgreen}\forall A.\left(A\rightarrow\bbnum 0\right)+A}$.
def excludedMiddle[A: TypeTag]: Either[A, A => Nothing] =
   if (equalTypes[A, Nothing]) Right((identity _).asInstanceOf[A => Nothing])    // Return ${\color{dkgreen}\text{id}:\bbnum 0\rightarrow\bbnum 0}$.
   else if (equalTypes[A, Int]) Left(123.asInstanceOf[A])      // Produce some value of type Int.
   else if (equalTypes[A, Boolean]) Left(true.asInstanceOf[A]) // Produce some value of type Boolean.
   else ???        // Write more definitions to support all other Scala types.

scala> excludedMiddle[Int]
res0: Either[Int,Int => Nothing] = Left(123)

scala> excludedMiddle[Nothing]
res1: Either[Nothing,Nothing => Nothing] = Right(<function1>) 
\end{lstlisting}
In this code, we check whether $A=\bbnum 0$. If so, we can implement
$A\rightarrow\bbnum 0$ as an identity function of type $\bbnum 0\rightarrow\bbnum 0$.
Otherwise, we know that $A$ is one of the existing Scala types (\lstinline!Int!,
\lstinline!Boolean!, etc.), which are not void and have values that
we can simply write down one by one in the subsequent code. 

Explicit\index{type casts} \textbf{type casts}, such as \lstinline!123.asInstanceOf[A]!,
are needed because the Scala compiler cannot know that \lstinline!A!
is \lstinline!Int! in the scope where we return \lstinline!Left(123)!.
Without a type cast, the compiler will not accept \lstinline!123!
as a value of type \lstinline!A! in that scope.

The method \lstinline!asInstanceOf! is dangerous because the code
\lstinline!x.asInstanceOf[T]! disables the type checking for the
value \lstinline!x!. This tells the Scala compiler to believe that
\lstinline!x! has type \lstinline!T! even when the type \lstinline!T!
is inconsistent with the actually given code of \lstinline!x!. The
resulting programs compile but may give unexpected results or crash.
These errors would have been prevented if we did not disable the type
checking. In this book, we will avoid writing such code whenever possible.%
\begin{comment}
showing here this is equivalent in Scala just different syntax importantly
non theorems cannot be implemented in code some on theorems are statements
in logic that cannot be derived statements that are false or undereye
verbal examples of these statements are these for all a from one follows
a now this is certainly suspicious in terms of logic what if a were
false then we would have it from true false false that\textsf{'}s very obviously
wrong and we cannot implement a function of this type to implement
it we would have to take a unit argument and produce a value of type
a where a is arbitrary type but how can we produce a value of type
a of the type that we don't even know what it is and there is no data
for us to produce that value so it is impossible another example of
an impossible type is this type so from a plus B follows a if you
wanted to implement this function you would have to take a value of
disjunction type a plus B and return a value of type a but how can
you do that what exodus Junction type happens to contain B and no
a just B it cannot contain a if it contains a B it\textsf{'}s a disjunction
so then we don't have an A and then we again cannot produce any and
having a B which is a completely different arbitrary type doesn't
help us to produce me exactly the same reason shows why we cannot
produce an A a and B given a because that requires a B we cannot produce
and also this is not implementable because we are required to produce
an A but all we have is a function from A to B this function will
consume an A if given only this function cannot possibly produce an
A for us but we are required to produce an A as a result so we cannot
and also there is no proof of this formula in the logic so these examples
actually lead us to a natural question how can we decide given a certain
formula whether it is a theorem in logic and therefore whether it
can be implemented in code it is not obvious consider this example
can we write a function with this type in Scala it is not obvious
can we prove this formula it is not clear not quite obvious right
now suppose I were of the opinion that this cannot be proved but how
do I show that this cannot be proved I certainly cannot just try all
possible proofs that would be infinitely many possible proofs that
would give me all kinds of other formulas and that would give me nothing
that I can stand oh how to answer these questions so it is really
a very hard question we are not going to try to answer it on our own
we were going to use the results of mathematicians they have studied
these questions for many many years for centuries logic has been studied
since ancient Greece more than 2,000 years of study all we need to
do is to find out by what name mathematicians call this logic they
are probably already studied it what kind of logic is this that we
are using that follows from the type constructions remember and the
very beginning of our consideration we started with the type constructions
that our programming languages have so that\textsf{'}s set of type constructions
specifies the set of rules of derivation of the logic mathematicians
call this logic intuitionistic propositional logic or IPL also they
call it constructive propositional logic but it is less frequently
used most frequently used name is this and mathematicians also call
this a non classical logic because this logic is actually different
from the boolean logic that we are familiar with the logic of the
values true and false and their truth tables I assume that you are
familiar with those computations using truth tables and operations
and or not in the boolean logic so actually this logic the logic of
types as I call it or intuitionistic propositional logic is very different
from boolean logic in certain ways it\textsf{'}s similar in other ways disjunction
for instance works very differently here\textsf{'}s an example consider this
sequence if it has given that from a follows B plus C then either
from a follows B or from a follows C it sounds right from the common-sense
point of it if if B plus C Falls a B or C if I was I'm using plus
as a logical or so if B or C follows then it kind of makes sense either
B follows or C Falls indeed this is correct in the boolean logic which
we can find out by writing the truth table so we enumerate all the
possibilities for a B and C to be true or false or eight such possibilities
and for each of those possibilities we write the truth value of this
the truth value of this and we see from the table that whenever this
is true then this is also true in the boolean logic but this does
not hold in the intuitionistic logic for the logic of types well why
does it not hold that\textsf{'}s counterintuitive well in fact there is very
little that\textsf{'}s intuitive about this so-called intuitionistic logic
actually we need to think differently about this logic we need to
think can we implement an expression of this sequent so implementing
it would mean if we're given this expression we can build an expression
of this type so we're given an expression of type A to B plus C let\textsf{'}s
say some F of this type can we build an expression of this type we
can this differently by asking can we implement a function that takes
this as an argument and returns this well we know that this is equivalent
one of our derivation rules is that if you have this sequence then
you can also have a sequence that is a function type from this to
this so for the programmer it is easier to reason about a function
taking this as an argument and returning this so how can we implement
this function this function takes F and needs to return a value of
this type so the body of this function if we could implement it and
have to construct a value of type either of something there are only
two ways of constructing a value of type either one is to construct
the left value second is to construct the right value how do we decide
whether to construct the left value or the right value we have to
decide it somehow on the basis of what information can we decide it
we don't actually have any such information what we have here is a
function from a to either BC so given some value of a of type a we
could compute f of that value and then we would have either B or C
we could decide them whether to we could take them that B or that
C but that\textsf{'}s not what we need to return we don't need to return either
of BC we need to return either of this function or that function and
that function is not yet applied to any a it is it is too late for
us to ask what is the a we already have to return the left of this
or a right of that in other words this type either of something-something
is not itself a function of a it contains functions away but itself
it cannot be decided on the basis of any assets too late so we need
to supply a left or right so here right away immediately we have to
decide whether this will return a left or a right and we cannot really
decide that if we decide we return the left we must then return a
function from A to B so there\textsf{'}s no way for us to construct this function
if we're given this function because this function could sometimes
return C instead of B and then we'll be stuck we cannot do this and
we can also return we cannot also return the right either so it is
impossible to implement a function of this type implication also works
a little differently in the intuitionistic logic here\textsf{'}s an example
this holds in boolean logic but not in intuitionistic logic again
let\textsf{'}s see why how can we compute this given this this function will
give us an e only when given an argument of this type but how can
we produce a value of this type we cannot we don't have information
that will allow us to produce a value of this type a and B are some
arbitrary types remember there is universal quantifier outside of
all this for all a and for all B we're supposed to produce this and
that is impossible we don't have enough data to produce some values
type a and so we cannot implement this function conjunction works
kind of the same as in boolean logic so here\textsf{'}s an example this implemented
and this is also in boolean logic a true theorem now in boolean logic
the usual way of deciding whether something is true or something is
a theorem is to write a truth table unfortunately the intuitionistic
logic cannot have a truth table it cannot have a fixed number of truth
values even if you allow more than two truth values such that the
validity of formulas the truth of theorems can be decided on the basis
of the truth table this was shown by noodle and this means we should
not actually try to reason about this logic using truth values it
is not very useful even an infinite infinite number of truth values
will not help instead however it turns out that this logic has a decision
procedure or an algorithm and this algorithm is guaranteed either
to find the proof for any given formula of the internation intuitionistic
logic or to determine that there is no proof for that formula the
algorithm can also find several in equivalent proofs if there is a
theorem so a theorem could have several in equivalent proofs and since
each proof could be automatically translated into code of that type
it means we could generate several in equivalent expressions of some
type sometimes so that is the situation with this logic which we discover
if we write if we read papers about intuitionistic propositional logic
that are available in the literature and their open source projects
on the web such as the gen GHC which is a compiler plugin for haskell
this is another project doing the same thing and for Scala are implemented
occurred the Clary Howard library both of these Scala and Haskell
all of these color and Haskell projects do the same thing they take
a type of some expression for function and generate code for it automatic
by translating the type into sequence finding a proof in this logic
using the algorithm and translating that proof back into code in the
way that we have seen in an example it is interesting that all these
provers and there\textsf{'}s a few others there\textsf{'}s one more for the idris language
I did not mention here they all used the same decision procedure or
the same basic algorithm which is called ljt which was explained in
a paper by dick off here they all side the same paper and I believe
this is so because most other papers on this subject are unreadable
to non-specialists they are written in a very complicated way or they
describe algorithms that are too complicated so I will show how this
works in the rest of this tutorial in order to find out how to get
an algorithm we need to ask well first of all do we have the rules
of derivation that allow us to create an algorithm already here is
a summary of the axioms and the rules of derivation that we have found
so far these are direct translations of the cold expressions that
we held in the programming language in the notation of sequence now
there\textsf{'}s one other notation for derivation rules which looks like a
fraction like this the numerator is one or more sequins and the denominator
is a sequence and this notation means in order to derive what is in
the denominator you have to present proofs for what is in the numerator
so this is the convention in the literature this fraction like syntax
or notation now we keep in mind that proofs of sequence are actually
just called expressions that have these types as some variables and
this type is the entire expression so these are directly responding
to proofs of this sequence and to the proofs of these derivation rules
and so if we have a proof that operates by combining some of these
axioms and some of these generation rules which directly translate
that back into code now the question is do these rules give us an
algorithm for finding a proof the answer is no how can we use these
rules to obtain an algorithm well suppose we need to prove some sequence
like this in order to prove it we could first see if the sequence
is one of the axioms if so then we have already proved if we know
what expression to write now in this case none of the axioms match
this so much means maybe a is a times B so B here is C and then on
the Left we must have C or you must have a times B now we don't you
don't have C on the left as we have because even that\textsf{'}s not the same
we also don't have a times B at the premise we have a but we don't
have a times B so these rules don't match the other rules don't match
the premises and the goal either but also these rules so how can we
use them well when the writer must be an implication we don't have
an application on the right here we could try to delete some of the
premises because it\textsf{'}s unused well actually it doesn't look like a
good idea could you read a for example and we end up with an really
hopeless sequence from B plus C we cannot get an A ever and so but
sounds hopeless so this doesn't seem to help and changing the order
doesn't seem to help much either and so we cannot find matching rules
but actually this sequence is provable just a clever combination of
what axiom to start with and what role to use and then again some
axiom and so on it will give us that time sure because I know how
to write code for this this is not difficult you have a function with
two arguments one of them is a the other is B plus C so disjunction
of either B C and we are supposed to produce a disjunction of tuple
a B or C that\textsf{'}s easy look at this disjunction if we have a B in this
disjunction then we can produce a left of the tuple a B because we
always have an A anyway if we have a see in this disjunction then
we could return this part of the disjunction in the right of C and
we're done but unfortunately we see that the rules here do not give
us an algorithm for deciding this we need a better formulation of
the logic again mathematicians need to save us from the situation
and they have done so mathematicians have studied this logic for a
long time starting from the early 20th of the last century the first
algorithmic formulation of the logic that was found is due to Jensen
who published what he called the calculus just ignore the word calculus
it means not very much complete and sound calculus means that he came
up with some rules of derivation which are summarized here such that
they are equivalent to these they derive all the same theorems and
only the same theorems so they derive all the stuff that is right
and only that stuff they don't derive any wrong statements it\textsf{'}s very
hard to come up with such a system of axioms and derivation rules
that are equivalent to another one in this sense also it\textsf{'}s very hard
to prove that these are actually the rules that will give you all
the theorems that could be right in this logic that you can actually
derive all the theorems that are right yet work is already done by
mathematicians so we're not going to try to do it ourselves we're
just going to understand how these rules work now the syntax here
is slightly enhanced compared with this the enhancement is that their
names pretty cool now these are just labels they don't really do anything
in terms of sequence these help us identify which we all have has
been applied to which sequence and that\textsf{'}s all we do so other than
that it is the same notation so the fraction such as this one means
that there is a sequence in the denominator which we will prove if
there are proofs given for sequence in the numerator in this rule
there are two sequence of them in the numerator other rules may have
one sequence in the numerator or no sequence in the numerator so these
rules that will have no previous sequence required those are axioms
this axiom means if you have an atomic X in other words it\textsf{'}s a variable
it\textsf{'}s a type variables not not a complicated expression just attack
variable and you can derive that same variable this is our accion
right here now why is it important that this is atomic that this is
type variable and not a more complicated expression actually not important
but it\textsf{'}s the simplest rule that you can come up with and mathematicians
always like the most minimal set of rules so that\textsf{'}s why they say let\textsf{'}s
only consider this rule for the type variables X not for more complicated
expressions but we can consider this rule for any expression of course
the identity axiom well here is a truth truth axiom net which derives
the truth which is the ste symbol which I denote it by one the format
in logical notation this is the T symbol well let\textsf{'}s just call this
one for clarity so that can be derived from any premises with no previous
sequence necessary none of these other rules now what do these other
rules do they do an interesting thing actually each of these rules
is either about something in the sequence on the left to the trans
time or something in the sequence to the right of the transplant which
I here shown in blue so these are the interesting parts of the sequence
that are being worked on or transformed by the rule so here\textsf{'}s an example
this rule is actually two rules the eyes the index so I is one or
two another two rules just written for gravity like this with index
I and each of them says you will prove this if you prove one of if
you prove this so for example you will prove C given if you're given
a a one A two if you will prove C given just a one which makes sense
because if you can prove C given a one you don't need a two we can
ignore this a T we can already proved C from anyone so in this way
it would be proved and so all these rules work in this way you can
prove what\textsf{'}s on the bottom of the seat of the of the fraction if you're
given proofs for what\textsf{'}s on the top so these are eight derivation rules
and two axioms we can use this now to make a proof search how do we
do that I start with a sequence we see which rule matches that sequence
so the sequence must have something on the left and something on the
right well at least one of these it cannot be empty so it must be
something somewhere and there are only four kinds of expressions in
our logic type variables conjunctions implications and disjunctions
now notice I'm using this arithmetic arithmetic all notation for logic
just because I like it better and I will show that it has advantages
later so we take a sequence we see which rule matches one of them
won't match because either in the premise we have one of these expressions
were in the goal we have one of these expressions and then we find
the rule of match that matches we apply that rule so we now have new
sequence one or more that we will need to be proved and if they're
true then we fork the tree and now we have to prove both of them son-in
we continue doing that for each of the sequence until we hit axioms
so the tree will and this leaf or we hit a sequence to which no rule
applies in which case we cannot prove it and the entire thing is unprovable
so in the search tree there will be sequence at the nodes of the tree
and proofs will be at the edges of the tree so each node sends its
proof to the root of the tree this calculus is guaranteed by mathematicians
to be such that indeed if you cannot find a rule that applies that
means the sequence cannot be proved which was not the case here the
sequence can be proved and yet we cannot find a rule that applies
so in this calculus we can use bottom-up approach to make a proof
search as a tree here we cannot that is the advantage capitalizing
on the mathematicians results let us look at an example suppose we
want to prove this formula this theorem so first step we need to write
a sequence and this needs to be proved from no premises so we write
a sequence s0 which has an empty set of premises this is a single
now what rule applies to this sequence with your bottom up so in other
words we look at these rules and they refine which denominator matches
our sequential and our cylinders empty set on the left so all the
rules on the left cannot be applied but on the right we have an expression
which is an implication at the top level of this expression there
is this implies that so this is of the form a implies B so this rule
applies we have a sequence of the form something in our case this
is an empty set and then a implies B so we apply this rule which is
the right implication and we get a new sequence which is that what
was here before the implication is now put on the left to the trans
of the to the left of the trans time and it means that this expression
needs to be now to the left of the turnstile so now this is the sequence
s1 now we need to prove s1 well we see what rule applies to us one
well on the right there is just Q so nothing can be done of these
rules and Q is not truth so we cannot use the axiom either so let\textsf{'}s
look at their left rules on the Left we have now an implication so
this is let\textsf{'}s say a and this is B so we have a rule which has a implication
B on the left this is the row left implication let\textsf{'}s apply it that
law will give us two new sequence so these two new sequence are s2
and s3 no these ones as you can check if you match a location B against
this implication Q so this is a this is B so then you get these two
sequence now we have to prove these two sequence as 2 and s 3 s 3
is easy it is just the axiom of identity it is this now as 2 again
has an implication on the left let\textsf{'}s again apply the rule left implication
to that we get two more sequence as foreign s5 as for is this because
5 is this so now actually we are in trouble because as 2 and s 4 is
are the same sequence as 5 actually we could prove with some more
work but that won't help because we are in a situation when to prove
as two we need to prove again s 2 so that\textsf{'}s it that\textsf{'}s a loop that
will never give us anything it means we applied the wrong rule so
we need to backtrack this step when we apply the rule left implication
to s 2 we erase is 4 in this 5 and try a different rule to apply to
s 2 which rule can apply to s 2 well as to is this it actually has
implication on the right so we can use the right implication rule
and if we do that we get a sequence s 6 which is this and this sequence
immediately follows from the identity axiom because it has promise
are on the left and premise are and goal are on the right and that
is this axiom whatever other premises and the premise X on the left
premise X on the right and that is a type variable so that\textsf{'}s perfect
we have done the proof as 6 follows from the axiom and therefore we
have proved s0 no more sequins need to be proved and because sequence
s0 shows this to be derived from no premises than this formula is
the theorem that\textsf{'}s what the theorem means in the logic so that is
how we use this calculus to do proof search now we notice that we
were a bit stuck at some point we had a loop now if we are in the
loop we don't know what to do maybe we need to continue applying the
same rule maybe some new sequence come up or maybe we should not continue
it is not clear what to do and just looking at the rule left implication
shows us that it\textsf{'}s copying this premise a implication B it is copied
into the premises of the new sequence and so it will generate a loop
assuredly after the second time you apply it however this sequence
might be new so we might need to apply it second time we don't know
that so that is a problem it will do now there have been a lot of
work trying to fix this problem and literally decades from research
by mathematicians the main ones I found were what are the off we published
in the Soviet Union who de Meyer and dick Hoff who published in the
United States over this time discovered gradually a new set of rules
which is called ljt or the calculus ljt which cures this problem of
looping the way it clears this problem is by replacing this rule left
implication through four new rules which are listed here all other
rules are kept the same from this calculus except the rule left implication
which is replaced in what way so left implication was applying it
applied to a sequence when the sequin had an implication among the
premises or on the left to the left of the turnstile the new rules
look in more detail at what is that implication so that implication
could have one of the four expressions as the argument of the implication
it could have an atomic expression as the argument it would have a
conjunction as the argument could have a disjunction as the argument
or it could have an implication as the argument in our logic there
are no more expressions except these four atomic variables conjunctions
disjunction and implications and so we have here enumerated all the
possibilities for what could be to the left of the implication in
this premise which I have here shown in the blue in blue and so for
each of these we do certain things replacing this sequence with one
or more other sequence again it\textsf{'}s quite a lot of work to prove that
these rules are equivalent to these and also that the new rules are
somehow better they are not giving loops a lot of work which I am
NOT going to go through because that\textsf{'}s far too complicated for the
scope so what we need suffice it to say that we have very smart people
who published on this and it is reasonably sure that this is correct
so the T in the name lgt starts stands for terminating so if we use
these rules in the same way by by creating a proof tree the proof
tree will have no loops and will terminate after a finite number of
steps and there is actually this paper that is also helpful for understanding
how to implement this algorithm and this paper shows explicitly how
to construct an integer function from sequence to integers which is
a measure of the complexity of the sequence and this measure decreases
every time you apply a rule so it strictly decreases and since this
is a strictly decreasing measure on the proof tree it means that all
the next nodes in the proof tree will have a smaller value of this
measure so eventually it will hit zero and the proof tree will terminate
at that leaf either that or you have no more rules to apply and if
you have no more laws to apply then again mathematicians have proved
it means our sequence cannot be proved so this is an important result
that we are going to use and note that this this rule is quite complicated
it does a very interesting thing it takes this expression which has
implication inside an implication and it transforms this expression
in a weird way namely the B here is separated from the C by parenthesis
but here it is not separated so this transformation is highly non-trivial
and unexpected and its validity is based on this theorem that this
in the intuitionistic logic is equivalent to this equivalent means
they're both following from the other so from this promos that and
from there follows this so this key theorem was attributed to rob
you off my dick off in this paper and this is this lemma 2 which says
that if this sorry that the this derivation is if and only if that
derivations will have these two equivalences and the proof is trivial
and the 34 is a reference to to borrow be off now when a mathematician
says that something is trivial doesn't mean that a statement is easy
to understand it doesn't mean that the proof is easy to find or that
it has trees easy to understand it means none of these things it just
means that right now for this mathematician it is not interesting
to talk about how it is done that\textsf{'}s all it means could be for any
number of reasons for example mathematicians could just be lazy or
have no time to again explain this and so they say it\textsf{'}s trivial don't
be don't be deceived when you see somebody says that something is
trivial in a mathematical text so to prove this one stepping stone
could be to prove this first this is an easier theorem and if you
prove this then clearly from here you can get B to C B to C you can
substitute in here you can get a to B and then you have here a to
B so in this way you can show this equivalence in one direction now
the proof of this statement is obviously trivial in order to show
the expression of this type I will use my short notation so this is
F which has this type the first argument of the function the second
is B which is at this type then we need to produce a see how do we
produce a C we apply F to an argument of this type the argument of
this type is a function that takes a and returns a B so we take some
X of type a and we return a B which was this B so we ignore this X
we just returned that B and that\textsf{'}s the argument of F so this expression
is the proof of this sequence in other words this is the code that
has this type and therefore the proof must be available somehow so
the details of proving this theorem are left as an exercise for the
reader again when you see in a mathematical text that something is
left as an exercise for the reader it does not mean that it is easy
to do it does not mean that for you it would be a useful exercise
to do it also does not mean that the author knows how to do it it
means none of these things it just means the author doesn't feel like
doing it right now and showing it to you for whatever reason could
be because they are lazy it could be because I don't know how to do
it could be because they feel that they should know how to do it but
they don't really do know how to do it could be any of these reasons
don't be deceived when you see something like this but of course I
had to actually produce an expression function of this type in order
to implement my curry forward language because as I will show in a
moment we need to be able to implement all these has code in order
to help approver so why is that we believe the mathematicians that
the new rules are equivalent to the old rules which means that if
you find a proof using these rules somehow you should be able to find
the proof also using our initial rules which means that if you found
that proof it would easily translate that to code because each step
here is directly corresponding to a certain code expression as we
have seen at the beginning of this tutorial these cold expressions
from each of these operations so in order to do this with new rules
in other words in order to create code from proof using new rules
we need to show equivalence or we need to show how to get code out
of each of the new rules now proof of a sequence means that we have
some expression let\textsf{'}s say T what uses variables a B and C of these
types and expression itself has type G and also as I have shown this
could be conveniently seen as a function the T as a function from
a B and C from these three arguments to the type G so for each sequencing
a proof we should be able to show either that it follows from an axiom
one of these or that it show it follows from a derivation rule and
the derivations all transforms one proof into another the axioms are
just fixed expressions as we had before the axiom that actually didn't
change between our initial formulation of the logic and the new calculus
lgt they actually did not change the derivation rules changed each
new derivation rule means that you're given expressions that prove
the sequence in the numerator one or more and you are out of these
expressions somehow you have to construct an expression that proves
this sequence now when I say an expression proves the sequence what
it means is that expression has the type that is described by the
sequence it\textsf{'}s the same thing because we described types of expressions
through sequence and only those sequence that correspond to valid
and existing expressions in the programming language only those sequence
can be proved by the logic this is by construction so now we need
to just find what are these expressions that corresponds to each of
the derivation rules in each rule has a proof transformer function
as I call it and the proof transfer function is explicitly a function
that takes one or more expressions that are in the numerator and converts
that to the expression in the denominator that has this type so it
has an expression as it has an explicit function we need to write
down for each of the derivation rules so let\textsf{'}s see how this is done
for these two examples of derivation laws first example have a rule
that says if you want to derive this sequence we need to derive these
two sequence now this sequence represents an expression of type C
which uses an expression of type A plus B so let\textsf{'}s represent this
as a function from a plus B to C now we will be able to just ignore
these other premises which are common arguments and all these functions
we just pass them and we don't write them out what is the proof transformer
for this derivation rule the proof transformer for it is a function
that has two arguments t1 which is the proof of this must be a function
of type II to see and t2 which is a proof of this sequence which must
be a function of type B to see now earlier I said that sequence represent
expressions that use certain variables but equivalently we can say
these are functions that take these variables and return these expressions
that\textsf{'}s more convenient when you implement this in code so what we
need is a function that takes a to C and B to C and returns a function
from a plus B to C and this is the code that does it we take an argument
of type a plus B and we return a match expression if it\textsf{'}s in the left
we applied t1 to that value and we get to see if it\textsf{'}s in the right
we apply t2 to that value and we get a C so in any case we get a syllabus
so this is a function from a plus B to C as required another example
is the proof transformer for this rule this rule has one sequence
going to one sequence so in order to transform is proof into this
we need a function that takes argument of type A to B to C to D and
returns a function of type tuple a B going to C to D so here\textsf{'}s the
code we take a function f of type A to B to C to D we return a function
that takes a G of this type shown here in blue and return we need
to return a D so how do we get a deal we apply F to a function of
type A to B to C so we create that function out of G X of type a going
to Y of type B going to G of x1 so this is a function of type A to
B to C which is the argument of F as required and the result is of
type D so that is what we write so this kind of code is the proof
transformer for this derivation arrow and we need to produce this
proof transformers for every rule of the calculus lgt and I have done
it because I have implemented the Korea Howard library that uses LG
T so I'll must have done it for each flow this is a bit tedious because
there are many of those rules and you need to implement all this machinery
of passing arguments no matter how many in this gamma which are emitted
from this notation for brevity but in of course in the real code you
have to deal with all that too so let\textsf{'}s see how this works on an example
because once the proof tree is found we need to start backwards from
the leaves of the tree back to the root on each step we take the proof
expression apply the proof transformer to ative according to the rule
that was used on that step we get a new proof expression and so on
so for each sequence we will get a proof expression and at the end
we'll have a proof expression for the root sequence and that will
be the answer so I will denote denote by T I the proof expressions
for the sequence s hi so starting from s6 s6 was this sequence in
our proof so I mean yes just just going through the proof example
it was here backwards from a 6 back to a 0 s-six was this it followed
from axiom identity it\textsf{'}s proof expression t6 is a function of two
variables these two variables of these two types and this function
just returns the second variable so it\textsf{'}s a function of RR q and r
and just denote this by our argued and Garibaldi\textsf{'}s types r RQ variable
of this type is hard here so this function is very simple just ignores
the first argument and returns or so that is what the axiom does the
next sequence was as to as to was obtained by rule our implication
or right implication from s 6 so the proof transformer for right implication
let\textsf{'}s look at the right implication and see what the proof transformer
must be so we are given this sequence for this expression which is
the function body the function body that uses a variable of type a
somehow out of this we need to produce a function expression that
takes an argument of type a and returns that functional body so this
is the code which is just writing a new argument returning the function
body that was our proof transformer we need to convert function body
into a function so we just write that argument and arrow in the function
body so in our case we need this as a function body and so our t2
is a function of our Q and this function is this the sequence s 3
followed from the axiom and so it was just this function this is just
the identity function then we used the left implication so this was
actually still done in the calculus algae but the same thing works
in the calculus lgt I'm just using algae because it\textsf{'}s simpler for
example here proof transformer for the left implication is a little
more complicated and so if you look at it what what does it have to
be it takes these two expressions and returns this expression so it
takes a function from A to B to a and from B to C and it returns a
function from A to B to see how does it do it given a function a to
b you use this to derive a from it then you substitute that a into
the function into B you get a B when you use this to derive see from
that B and that\textsf{'}s your C so you use this function a to be twice you
put it in here once and then you get an A and substitute back into
the same function when you get a B then you use that and that\textsf{'}s exactly
what the proof transformer does it takes this rrq and it uses it twice
substitutes into it something that was obtained from one of the terms
and then uses the second term on the result so then this is the proof
transformer for the rule left implication the result of the proof
transformation is the proof for the sequence s1 finally we use the
right implication again which is just this function construction and
we get the proof expression for the sequence s0 now this proof expression
is written through these t1 t2 t3 we have to substitute all this back
in order to get the final expression so if we substitute first of
all we find this is our our cubone going to tea one of our cutie one
of our queue is this so we have to put it here now t3 is just identity
so we can just remove that so that gets you riq going to our Q of
T 2 T 2 is less if I have to put it in T 6 is just identity on R so
this is our going to our and so finally you have this expression so
that is the final code that has the required type notice that we have
derived this code completely algorithmic to it there was no guessing
we found which rules applied to the sequence with transformed sequence
according to the rules once we found the proof which was if we use
the calculus ljt the proof will be just a finite tree with no loops
it will terminate you can get an exhaustive depth-first search for
it for example and you find all the possible proofs if you want as
well well you will find many in any case in some for some expressions
and then we use the proof transformers which are fixed functions that
you can upfront compute for each these expressions are proof transformers
applied to the previous proofs so these are completely fixed algorithmically
fixed so we have derived this code completely algorithmically given
this expression this type so it is in this way that the career Howard
correspondence allows us to derive the code of functions from there
type signatures another important application of the correspondence
is to analyze type by some morphisms or type equivalences and I was
led to this by asking the question so in this logic or in the types
are these operations plus and times as I denoted them more like logic
more like the disjunction and conjunction or are they more like arithmetic
plus and times because this is kind of not so clear right away our
logic is this intuitionistic logic it in any case this is different
from boolean logic so what are the properties of these types really
so are the properties such that it is better to think about these
operations as plus and times rather than logical conjunction and disjunction
can answer this question I looked at identities that we have in the
water these are some identities from simple ones obvious ones to less
obvious identities like this the equal sign here stands for implication
in both directions so both this implies that and vice versa because
of this each of the implications means a function so since these are
all identities in logic it means that for example the implication
from here to here is a theorem of logic and so it can be implemented
as we know all our identities in logic can be implemented in code
and we even have an algorithm now that can automatically produce proofs
and automatically produce code so that means for any of these identities
that has some ik some expression X on the left and some Y on the right
so some kind of X equals y we have X implies Y and y implies X if
we convert that to code we will have a pair of functions function
from X to one and the function from Y to X what do these functions
do well they convert values in some ways from type X to type Y and
back so do these functions Express the equivalence of the types x
and y so that any value of type X can be converted to some equivalent
value type while and back without any loss of information is that
so that was the question I asked I looked at some examples well first
what does it mean more rigorously that types are equivalent for as
mathematicians say isomorphic the types are isomorphic and we will
use this notation for that if there is a one-to-one correspondence
between the sets of values of these types and in order to demonstrate
that we need a pair of functions one going from A to B the other going
from B to a such that the composition of these functions in both directions
is equal to identity function so F compose G or F value G will give
you from A to B and then from B to a is back so that would be identity
of a to a this will be identity of B to B if this is true if the composition
is identity it means we indeed did not lose any information let\textsf{'}s
consider an example this is an identity in the logic a conjunction
with one is equal to a in Scala the types responding to the left and
the right hand sides of this conjunction all of this are equivalent
are the conjunction of a and unit and a itself now we need functions
with these types indeed we can write functions is having these types
a pair of a and unit we need to produce an a out of that we'll just
take the first element of the pair you are done take an X of type
a will produce tuple of a and unit very easy just put a unit value
in the tuple in here done and it\textsf{'}s easy to verify that composition
of these functions will not change any values so it will be identity
in both directions another example this is an identity in logic if
this is understood as a disjunction one or a or true or a is true
that is an identity in logic for theorem in the logic are the types
equivalent though the type for 1 plus a is the option in Scala it
is option in Haskell at is called maybe this type is standard library
type in pretty much every functional programming language now option
of a is a disjunction of one or unit and a it is certainly not equivalent
to just unit because this type could contain a value of a in it but
this could not so there is no way that you could transform this type
to this and then back without losing information you could transform
so since this is a theorem you have functions from this type to this
type and back some functions you have them but these functions do
not compose to identity they cannot because what if you had a here
you must map it into unit from this unit back you must map into this
unit you cannot get an a out of unit and so that will erase this information
and that cannot become isomorphism so we see that some logic identities
do yield isomorphism types but others do not why is that let\textsf{'}s look
at some more examples to figure out why in all these examples we can
implement functions F 1 and F 2 between the two sets to two types
in both directions and then we can check we certainly can implement
them because these are logical identities but then we can check if
the compositions are identity functions and if so the types are isomorphic
but we find that in the first three examples we can do it but in this
last example we can note now I have written the logical identities
logical theorems with the arithmetic notation I call this arithmetical
notation because this suggests arithmetic operations plus and times
and if you look at these identities this looks like a well-known algebraic
identity from the school algebra in this too but this certainly seen
your own as an arithmetic as an as an arithmetic identity this is
certainly not true in arithmetic it is true in logical if you replace
this with disjunction and this with conjunction this is an identity
in logic so this suggests an interesting thing if you replace disjunction
by plus and conjunction by x and the result is an identity in arithmetic
then it is an isomorphism of types otherwise it is not let\textsf{'}s see why
this is so indeed this is so I call this the arithmetic arithmetic
oh very hard correspondence to see how it works let\textsf{'}s consider only
the types without loss of generation of generality that have a finite
set of possible values for example a boolean type has only two possible
true and false integer let\textsf{'}s say in the computers all the integers
are fine nights ago so those types have a finite set of possible values
and this does not limit our generality because in the computer everything
is finite all types have a finite set of possible values now let\textsf{'}s
consider how many values a given type has so that would be the size
of the type or using the mathematical terminology it\textsf{'}s called a cardinality
of the type so let\textsf{'}s see what is the cardinality of various type constructions
the sum type for example if the cardinality of types a and B is known
and the cardinality of a plus B the sum type the disjunction of a
and B is the sum of the two cardinalities or sizes this is because
a value of the disjunction type is constructed as either a value of
the first part or a value of the second part and so you cannot have
both together and so obviously the different number of values is just
the sum of the two sizes that the number of different values of the
sum type is just the sum of the numbers of different values of types
a and B for the product type again we have an interesting thing it\textsf{'}s
the arithmetic product of the sizes of a and B because for every a
value you could have an arbitrary B value so this is a direct product
or transient product of sets and we have school level identities about
the operations plus and times such as these identities or these all
of these identities are valid for arithmetic and they show if you
translate that into statements about the sizes of types they show
that the size of the type on the left is equal to the size of the
type on the right and that is very suggestive in other words if you
take a identity like this and you compute the size of the type on
the left and the size of the type on the right you get an arithmetic
identity of the sizes but you don't get that identity here because
the earth medical formula is not right this is very suggestive if
the sizes are equal and maybe the types are equivalent or isomorphic
when the sizes are not equal then certainly they cannot be equivalent
the function type very interestingly also is described in the same
way it provides the set of all maps between the two sets of values
so for example from integer to boolean that would be all the functions
that take some integer and return some boolean so that\textsf{'}s and a number
of boolean values \textasciicircum{} the number of integer values
that\textsf{'}s how many different functions you can have as a combinatorial
number so it\textsf{'}s an exponential and so the size of the type of function
a to be is the size of the type of B \textasciicircum{} the size of
type of a and again we have all the school identities about powers
and how to multiply powers and so on and they are directly translated
into these three identities if you take the sizes of the types on
the left and on the right the sizes will be equal due to these three
identities since the sizes are equal it\textsf{'}s very likely that the type
our actual equivalent so far haven't seen any counter examples to
this in these constructions so this gives us a meaning of the Curie
Howard correspondence so far we have seen three facets of the curly
Howard correspondence one is the correspondence between types and
logical formulas two is the correspondence between code and proofs
and three the correspondence between the cardinality of a type or
the set size of the type and the arithmetic identities that we have
in the school algebra about these types so arithmetical identities
signify type equivalence or isomorphism while logic identities only
talk about how you create some value of this type out of value of
another type so that does not guarantee that it preserves information
it just guarantees that you can implement some function of that type
it doesn't tell you that the function will be an isomorphism so if
one type is logically equivalent to another it means are equally implementable
if one is implementable another is also implementable but no more
than that whereas arithmetical identities actually tell you about
isomorphism of types therefore if you look at types and write them
using my preferred notation which is using the arithmetic all symbols
instead of logical symbols instead of these I'll use these symbols
if I do that this is very suggestive of a possible isomorphism of
types then it becomes very easy for me to reason about types I can
see right away that these two are isomorphic types or that these two
are isomorphic types because I am used to looking at school algebra
it\textsf{'}s very obvious then that this is not an isomorphism of types because
this doesn't make sense in the school algebra so reasoning about isomorphic
types is basically school level algebra involving polynomials and
powers so if you are familiar with all these identities as you should
be it will be very easy for you the reason about what types are equivalent
as long as all these types are made up of constants or primitive types
disjunctions tuples or conjunctions and functions which will then
directly be translated into exponential polynomial expressions constants
sums products and expand powers or Exponential\textsf{'}s so I call these exponential
polynomial types that is types built up from these type constructions
so all we have been talking about in this tutorial is what I call
exponential polynomial types these are the basic type constructions
that I started with tuple product function exponential disjunction
some unit constant or 1 now just one comment that in the functional
programming community today there is a terminology algebraic types
so people usually call algebraic types the types that are made from
constant types sums and products excluding Exponential\textsf{'}s I do not
find this terminology it\textsf{'}s very helpful I find it confusing because
what is particularly an algebraic about these identities these are
identities of school algebra the properties of the function type are
described by algebraic identities like this so it would be strange
to call the function type not algebraic whereas these types are algebraic
they are very similar to each other in terms of their properties being
described by identity is known from school algebra so instead of algebraic
types I would prefer to say polynomial types this is much more descriptive
and precise and if you want to talk about function types as well then
you just can you can just say exponential polynomial types or exfoli
types for short so by way of summarizing what we have done so far
what are the practical implications of the career Howard correspondence
so one set of implications is actually for writing code and reason
and eternal code one thing we can do now is if we're given a function
with some type and usually this will be typed with type parameters
all type trainers fully parametric types such as the function we have
been considering here all these functions do not have any types that
are specific like integer or string all the types are fully parametric
and then there are some constructions some type expressions made out
of these types so these are what I call fully parametric functions
for these functions we have a decision procedure an algorithm that
based on the ljt calculus which decides whether this function can
be implemented in code and computer scientists a type is inhabited
if you can produce a value of this type in your program so CH of T
is this proposition which they call type is inhabited and I prefer
to call it just that you can compute a value of this type or code
has the type O code can create a value of this type and so we have
a algorithm that can also generate the code from type when it is possible
if it is not possible the algorithm will tell you so often not always
but often this algorithm can be used actually to generate the code
you want we can also use what I call the arithmetic of glory Harvard
correspondence to reason about type isomorphisms and to transform
types isomorphic we simplify type expressions just like we simplify
expressions in school level algebra by expanding brackets by permuting
the order of terms like a plus B is equal to B plus a or associativity
a times B all times C can be expanded and so on so this allows us
once we have written types in the short notation in the notation that
I prefer which resembles school algebra because it uses the plus and
times symbols instead of the logic symbols so once we rewrite our
types and this notation which I have been doing consistently in this
tutorial it enables us the reason very easily but which types are
equal or isomorphic because we are all familiar with the school level
algebra what are the problems that we cannot solve using this knowledge
one thing we cannot do is to generate code automatically such that
it will be an isomorphism so for instance in an example here we are
able to generate automatically the code of these functions but it
will not be an isomorphism and the lgt algorithm cannot check that
this is nice a morphism that\textsf{'}s the important thing this algorithm
does not know about equations or isomorphisms it only knows that it
found some code that has the type you wanted whether this code is
useful to you or not we don't know the algorithm doesn't know this
also if the algorithm finds several such several proofs of a sequence
it will generate several not in equivalent versions of your code it
doesn't know which one is is useful maybe some of them are useless
maybe not the algorithm cannot automatically decide that in general
another thing we cannot do is to express complicated conditions via
types such as that array is sorted the type system is not powerful
enough in all the languages I listed you need a much more powerful
type system such as that in the programming language interests or
add them or cook those are much more powerful type systems that can
express such complicated conditions but for those type systems there
is no algorithm that will generate code another thing we cannot do
is to generate code that has type constructors such as the map function
here\textsf{'}s an example in Scala this is a map function on a list so there\textsf{'}s
the list of a a is a type parameter and then we say dot map and map
has another type frame to be it takes a function from A to B for any
B so a is fixed but now from any B we can take a function from A to
B and generate a list of B so if we wrote this formula in the short
type notation this would look something like this I'm writing subscript
a because this is a type parameter so this is like an argument or
a type parameter I'm writing it like this and then from this this
is the first argument of the function and then there is a second argument
which is this F and that is another quantifier for B inside parentheses
so this formula has a quantifier inside so far we have been dealing
with formulas that have all quantifiers outside and so we never write
quantifiers explicitly but here we have to write them inside this
is a more powerful logic which is called first-order logic in other
words this is a logic where you have quantifiers anywhere in the formula
including inside the formula unfortunately this logic is undecidable
so there is no algorithm that we can use either to find the proof
and therefore code freedom type or to show that there is no proof
no code so we're kind of stuck in all these directions some more remarks
about the curry Harvard correspondence first is that only with parameterize
types we can get some interesting information out of it if we take
concrete types like integer then the proposition CH event meaning
that our code can have a value of type int it that\textsf{'}s always true can
always write any some integer value we don't need any previous data
for it so for all specific types all these propositions are always
choice completely void of information the only interesting part comes
when we start considering type variables if we start asking can we
make a type which is either of a B going to a going to B in soon for
all a B once we start doing this with type parameters a B and so on
then we get interesting information as we have seen in this tutorial
another remark is that functions like this one are not sufficiently
described by their type so that this is the type of integer going
to integer now looking at this type we can put this into a sequence
but we'll never get enough information to actually get this function
so only certain class of functions which are fully typed biometric
their type signature is informative enough so that we can derive code
automatically only in much more powerful type systems you can have
type information that is enough to specify fully a code like this
another caveat is that I don't know the proof that arithmetic identity
guarantees the type equivalence it is certainly a necessary condition
because if two types have different cardinality or different size
of their sets of values that they cannot be equivalent or they cannot
be isomorphic so this is a necessary condition but it\textsf{'}s not a sufficient
condition it looks like I don't know if this is sufficient I haven't
seen any counter examples so far final remarks about type correspondence
the logical constant false did not appear in any of my slides so far
this was on purpose it has extremely limited practical use in programming
languages because actually we have types corresponding to false Scala
has type called nothing Haskell has type usually called void that
corresponds to the logical constant false what does it mean CH of
nothing is false it means your code can never have a value of type
nothing or in Haskell void you can never compute a value of this type
so clearly it has a very limited practical significance you will never
be able to compute any values of this type ever in any program it\textsf{'}s
identically falseness this constant so if you want to add it to the
logic it\textsf{'}s very easy you just have one rule and you're not done you
can derive things with it if you want but they will have almost never
any use in practical code also we did not talk about negation none
of the calculus calculate that I should have in logical negation as
in operation again for the same reason we do not have a programming
language construction that represents logical negation negation by
definition is like this is an application from 8 to 4 so that\textsf{'}s not
a not a means from a follows falsehood now since you cannot ever get
false in a programming language you cannot really implement this function
in any useful sense and so i have seen some haskell library that used
this type void as a type parameter in some way but certainly it\textsf{'}s
a very limited and rare use and so it is not really lumen 18 to include
negation it could probably find some very esoteric uses of it but
almost never useful and finally there is another set of important
implications from the Kurihara correspondence these are implications
for people who want to design new programming languages as we have
seen the Karaka with correspondence maps the type system of a programming
language into a certain logical system where prepositions follow from
each other or can be proved from each other and this enables us to
reason about programmed to see what kind of code can be written if
some other kind of code can be written and logical reasoning is very
powerful it\textsf{'}s simpler than trying to write code and it gives you algorithms
and all kinds of mathematical results that have been found over the
centuries so languages like those listed here have all the five type
constructions that I wasted in the beginning of this tutorial and
mapping them into logic gives a full constructive logic or full intuitionistic
logic with all logical operations and or so conjunction disjunction
implication and the truth constant whereas languages such as C C++
Java and c-sharp and so on they're mapped to incomplete logics because
they do not have some of these operations for instance they do not
have type constructions of correspond to disjunction we also do not
have the true constant or the false constant so they are mapped to
a logic that lacks some of the foundational logical operation so it
can be only fewer theorems can be proved in that logic and so your
reasoning about theory types is hampered languages called scripting
languages sometimes such as Python or JavaScript will be and so on
also our belongs there in that line those languages only have one
type they actually don't check types at compile time and so they're
mapped to logics with only one proposition those logics are extremely
small in terms of what kind of things you can reason about and so
if you write a program in these languages you are completely unable
to reason at the level of types whereas in these languages you are
able to reason but in a limited way you're not having a complete logic
so this suggests a principle for designing the type system in a new
programming language the first step would be to choose a good and
complete logic that is free of inconsistency mathematicians have studied
all kinds of logics and they are always interested in questions such
as is this logic consistent consistent means you cannot derive false
from true is this logic complete can you derive all things that are
true are there enough axioms and rules of derivation or maybe there
are too many axioms and rules of derivation you can delete some of
them and have fewer mathematicians have always been interested in
such questions they found all kinds of interesting logics where you
can derive a lot of interesting theorems non trivial theorems and
they found the minimum sets of axioms and rules of derivations for
these logics use their results take one of the logics that they do
them and develop such as intuitionistic logic model logic temporal
logic linear logic and so on take one of these logics for each of
the basic operations of this logic provide type constructions in your
programming language that are easy to use for instance your logic
has disjunction implication or something else provide a type constructor
for each of them that\textsf{'}s easy to use easy to write down such as provided
by the languages we have seen then every type will be mapped to a
logical form of the OPF logical formula for every type and there will
be a type for every logical formula and then for each rule of the
new logic for each derivation rule there should be a construct in
the code that corresponds to it so that you could transform proofs
in logic into code and code into proofs if you do that your language
will be faithful to the scorecard correspondence you will be able
to use logic to reason about your language and one important result
at this level while we have seen that you can sometimes generate code
that is maybe nice but a very important result is that if your logic
is free of inconsistency it means that no program will ever be able
to derive an inconsistent an inconsistent type means that you had
a function that requires some type a but it was called with a different
type beam which is incompatible and that basically crashes so in languages
like C and C++ we have all kinds of crashes like a segmentation fault
in Java the exceptions nullpointerexception or class cast exception
which happens when you call a function on the wrong type of argument
and that happens if your logic is inconsistent if your logic can derive
incorrect statements from correct premises then if you translate that
derivation into code and the that code will derive incompatible type
at the wrong place and it will crash the crash will happen at runtime
the compiler will not catch this inconsistency because the compiler
only checks the logic of types and the logic checks out you have followed
the rules of derivation of the logic the compiler can check out all
these logical rules but the compiler does not know that your logic
is inconsistent maybe and then it will deep have derived an inconsistent
result falsehood from truth for example and that will crash at runtime
now we know that crashing at runtime is not a good outcome so in fact
languages like Oh camel have been studied and for other languages
some subsets of Haskell I believe called safe Haskell have been studied
and it has been shown that they cannot crash and they're the way to
show it mathematically is to use the fact that they are based on a
complete and consistent logic and then all you need to show is that
your compiler does not have some critical bugs that allow it to oversee
that you have not followed the derivation rules of the logic that
is an extremely valuable feature of functional programming languages
that are based on the Curie habit correspondence you can prove their
safety at compile time or at least exclude a large number of possible
bugs and errors certainly these languages are quite large and they
include features that are not covered by the Carey Hart correspondence
type constructors that I have not considered in this tutorial and
those might may not be safe but at least the foundation of these languages
the foundation of the type system will be safe so that is the final
lesson from the great Howard correspondence this concludes the tutorial
\end{comment}

