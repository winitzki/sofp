\global\long\def\gunderline#1{\mathunderline{greenunder}{#1}}%
\global\long\def\bef{\forwardcompose}%
\global\long\def\bbnum#1{\custombb{#1}}%
\global\long\def\pplus{{\displaystyle }{+\negmedspace+}}%


\chapter{Traversable functors\label{chap:9-Traversable-functors-and}}

\section{Motivation}

The \lstinline!map!/\lstinline!reduce! style\index{map/reduce programming style@\texttt{map}/\texttt{reduce} programming style}
is a major industrial use case of functional programming. Previous
chapters examined systematically the properties of functions used
in that style (\lstinline!map!, \lstinline!filter!, \lstinline!flatMap!,
and \lstinline!zip!) and generalized them to many different type
constructors. This chapter adopts the same approach to study and generalize
the \lstinline!reduce! method. In this way, we will conclude the
study of the \lstinline!map!/\lstinline!reduce! programming style.

\subsection{From \texttt{reduce} and \texttt{foldLeft} to \texttt{foldMap} and
\texttt{traverse}\label{subsec:From-reduce-and-foldleft-to-foldmap}}

The type signatures of \lstinline!reduce! and \lstinline!foldLeft!
for a sequence of type \lstinline!Seq[A]! may be written as:
\begin{lstlisting}
def reduce[A](xs: Seq[A])(update: (A, A) => A): A
def foldLeft[A, B](xs: Seq[A])(b0: B)(update: (B, A) => B): B
\end{lstlisting}
\begin{align*}
 & \text{reduce}:\text{Seq}^{A}\rightarrow(A\times A\rightarrow A)\rightarrow A\quad,\\
 & \text{foldLeft}:\text{Seq}^{A}\rightarrow B\rightarrow(B\times A\rightarrow B)\rightarrow B\quad.
\end{align*}
In \lstinline!foldLeft!, the accumulated result is of a different
type ($B$) than the type of sequence elements ($A$). So, \lstinline!foldLeft!
is a more general version of \lstinline!reduce!. We can implement
\lstinline!reduce! through \lstinline!foldLeft!:
\begin{lstlisting}
def reduce[A](xs: Seq[A])(update: (A, A) => A): A = xs.tail.foldLeft(xs.head)(update)
\end{lstlisting}

The \lstinline!foldLeft! operation goes over each element of a sequence
and accumulates the updates, returning the final updated value. We
have seen many examples of using \lstinline!foldLeft! in Chapter~\ref{chap:2-Mathematical-induction}.
Now we would like to generalize \lstinline!foldLeft! to data structures
other than sequences. Such a generalization must be based on imposing
suitable laws, but it is not obvious what laws \lstinline!foldLeft!
must satisfy (other than naturality laws that will hold automatically
for any fully parametric code). To make progress, we will generalize
\lstinline!foldLeft! further to obtain a new function called \lstinline!traverse!,
for which we can motivate non-trivial laws. This generalization goes
in two steps, first transforming \lstinline!foldLeft! to a function
called \lstinline!foldMap! and then to \lstinline!traverse!. 

The way from \lstinline!foldLeft! to \lstinline!foldMap! \index{foldMap function@\texttt{foldMap} function}starts
by changing the type of \lstinline!update: (B, A) => B! to an equivalent
curried form with a flipped order of arguments:
\[
\text{upd}:A\rightarrow B\rightarrow B\quad.
\]
So, $\text{upd}\,(x^{:A})$ has type $B\rightarrow B$. The key observation
is that the set of all functions of type $B\rightarrow B$ is a monoid
(denoted by \lstinline!monoidFunc! in Example~\ref{subsec:tc-Example-Monoids}
and Section~\ref{subsec:Monoids-constructions}).\index{monoid}
For brevity, we will here denote that monoid by \lstinline!MF!. The
empty element of that monoid is the identity function ($e_{\text{MF}}\triangleq\text{id}^{:B\rightarrow B}$),
and the \lstinline!compose! operation is the function composition
($f^{:B\rightarrow B}\oplus_{\text{MF}}g^{:B\rightarrow B}\triangleq f\bef g$).
Now the evaluation of a \lstinline!foldLeft! operation can be visualized
like this:
\begin{align*}
 & \text{foldLeft}\left(\left[x_{1},x_{2},x_{3}\right]\right)\left(b_{0}\right)\left(\text{upd}\right)=b_{0}\triangleright\text{upd}\left(x_{1}\right)\triangleright\text{upd}\left(x_{2}\right)\triangleright\text{upd}\left(x_{3}\right)\\
 & =b_{0}\triangleright\text{upd}\left(x_{1}\right)\bef\text{upd}\left(x_{2}\right)\bef\text{upd}\left(x_{3}\right)=b_{0}\triangleright\big(\text{upd}\left(x_{1}\right)\oplus_{\text{MF}}\text{upd}\left(x_{2}\right)\oplus_{\text{MF}}\text{upd}\left(x_{3}\right)\big)\quad.
\end{align*}
This formulation suggests that we could replace the specific monoid
(\lstinline!MF!) used in \lstinline!foldLeft! by an arbitrary monoid
($M$). The new, more general function is called \lstinline!foldMap!.
To obtain that function, we first change the order of the curried
arguments for convenience:
\[
\text{foldMap}\,(\text{upd})(\text{xs})\left(b_{0}\right)\triangleq\text{foldLeft}\,(\text{xs})\left(b_{0}\right)(\text{upd})\quad.
\]
Omitting the argument $b_{0}$, we visualize the computation performed
by \lstinline!foldMap! as:
\[
\text{foldMap}\,(\text{upd})\left(\left[x_{1},x_{2},x_{3}\right]\right)=\text{upd}\left(x_{1}\right)\oplus_{M}\text{upd}\left(x_{2}\right)\oplus_{M}\text{upd}\left(x_{3}\right)\quad.
\]
The type of \lstinline!upd! is now $A\rightarrow M$ instead of $A\rightarrow B\rightarrow B$.
So, the type signature of \lstinline!foldMap! must be:
\[
\text{foldMap}:(A\rightarrow M)\rightarrow\text{Seq}^{A}\rightarrow M\quad.
\]
This type signature assumes that $M$ is a monoid type. We can now
implement \lstinline!foldMap! via \lstinline!foldLeft!:
\begin{lstlisting}
def foldMap[M: Monoid, A](f: A => M): Seq[A] => M =
  _.foldLeft(Monoid[M].empty) { (m, a) => m |+| f(a) }
\end{lstlisting}
We can also implement \lstinline!foldLeft! via \lstinline!foldMap!
by using the monoid instance of \lstinline!MF! (see Example~\ref{subsec:tc-Example-Monoids}):
\begin{lstlisting}
implicit def monoidFunc[A]: Monoid[A => A] = ...
def foldLeft[A, B](xs: Seq[A])(b0: B)(update: (B, A) => B): B =
  foldMap[B => B, A](a => b => update(b, a))(xs)(b0)
\end{lstlisting}
We will prove later that \lstinline!reduce!, \lstinline!foldLeft!,
and \lstinline!foldMap! are in fact equivalent.

The type signature of \lstinline!foldMap! suggests that we could
replace \lstinline!Seq! by an arbitrary functor \lstinline!L!. We
call a functor \lstinline!L! \textbf{foldable}\index{foldable functor}
if a \lstinline!foldMap! method is defined:
\begin{lstlisting}
def foldMap_L[M: Monoid, A](f: A => M): L[A] => M
\end{lstlisting}
\[
\text{foldMap}_{L}:\left(A\rightarrow M\right)\rightarrow L^{A}\rightarrow M\quad,\quad\text{assuming that }M\text{ is a monoid}\quad.
\]
The function $\text{foldMap}_{L}$ works in the same way for all monoids
$M$ (but differently for each $L$). 

The final step towards the \lstinline!traverse! method is to replace
an arbitrary monoid $M$ by an arbitrary \emph{applicative} functor
$F^{B}$ with a new type parameter $B$. This step is not straightforward:
values of applicative functor types are not monoids (we do not necessarily
have an operation of type $F^{B}\times F^{B}\rightarrow F^{B}$).
So, we will give an independent motivation for the \lstinline!traverse!
method in the next subsection.

\subsection{The \texttt{traverse} operation\label{subsec:The-traverse-operation}}

Consider the task of waiting for several concurrent operations to
finish. As an example, suppose we have a list of type \lstinline!List[A]!
and a processing function of type \lstinline!A => Future[B]!. We
need to apply the processing function in parallel to all items from
the list and wait until the entire data set is processed.

The Scala standard library has a special method (\lstinline!Future.traverse!)
to use in this situation:
\begin{lstlisting}
val data: List[A] = ???
val processing: A => Future[B] = ???
implicit val ec: ExecutionContext = ???
val results: Future[List[B]] = Future.traverse(data)(processing)
\end{lstlisting}
Since the processing of each element is asynchronous, the result is
wrapped in a \lstinline!Future! type constructor. The \lstinline!traverse!
method will mark that \lstinline!Future! value as finished when all
processing is done.

If we ignore the implicit argument of type \lstinline!ExecutionContext!
(which is specific to the \lstinline!Future! class) and also flip
the first two curried arguments, we will arrive at the following type
signature of \lstinline!traverse!:
\[
\text{traverse}:(A\rightarrow\text{Future}^{B})\rightarrow\text{List}^{A}\rightarrow\text{Future}^{\text{List}^{B}}\quad.
\]
Generalizing to other type constructors $F$, $L$ instead of \lstinline!Future!
and \lstinline!List!, we obtain:
\begin{lstlisting}
def traverse[A, B, F[_]: Applicative: Functor](f: A => F[B]): L[A] => F[L[B]]
\end{lstlisting}
\[
\text{trav}_{L}^{F,A}:(A\rightarrow F^{B})\rightarrow L^{A}\rightarrow F^{L^{B}}\quad.
\]
In this type signature, we have chosen the parameter names $F$ and
$L$ specifically to help us avoid errors (e.g., writing $L^{F^{B}}$
instead of $F^{L^{B}}$) by recalling the example with $F=$ \lstinline!Future!
and $L=$ \lstinline!List!.

It turns out that the \lstinline!traverse! method can work in the
same way for any \emph{applicative} functor $F$. A functor $L$ having
a \lstinline!traverse! method of this type is called \index{traversable functor}\textbf{traversable}.

The type signatures of \lstinline!traverse! and \lstinline!foldMap!
are similar except that $M$ is replaced by $F^{B}$ or $F^{L^{B}}$
as needed. To see that \lstinline!foldMap! is a special case of \lstinline!traverse!,
take a constant functor $F^{A}\triangleq M$, which is applicative
if $M$ is a monoid (as shown in Section~\ref{subsec:Constructions-of-applicative-functors}).
With this choice of $F$, the type signature of \lstinline!traverse!
reduces to that of \lstinline!foldMap!.

Before studying the properties of \lstinline!traverse!, we will look
at some examples of implementing that function for various type constructors
and using it in practice.

\section{Practical use of folding and traversing operations}

\subsection{Implementing \texttt{traverse} for various data types}

To understand how \lstinline!traverse! works, let us implement it
for some data types, including lists and trees.

\subsubsection{Example \label{subsec:Example-traverse-for-1+a*a}\ref{subsec:Example-traverse-for-1+a*a}\index{examples (with code)}}

Implement \lstinline!traverse! for the type constructor $L$ defined
by:

\begin{wrapfigure}{l}{0.4\columnwidth}%
\vspace{-0.5\baselineskip}
\begin{lstlisting}
type L[A] = Option[(A, A)]
\end{lstlisting}
\vspace{-0.5\baselineskip}
\end{wrapfigure}%

~\vspace{-0.5\baselineskip}
\[
L^{A}\triangleq\bbnum 1+A\times A\quad.
\]


\subparagraph{Solution}

For the given type constructor $L$, write the type signature of \lstinline!trav!
as:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B]): Option[(A, A)] => F[Option[(B, B)]]
\end{lstlisting}
In the type notation, this type signature is written as:
\[
\text{trav}_{L}:(A\rightarrow F^{B})\rightarrow(\bbnum 1+A\times A)\rightarrow F^{\bbnum 1+B\times B}\quad.
\]
 Let us implement \lstinline!trav!, aiming to preserve information.
We need to compute a value of type $F^{\bbnum 1+B\times B}$. Begin
by pattern matching on the argument of type $\bbnum 1+A\times A$:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B]): Option[(A, A)] => F[Option[(B, B)]] = {
  case None             => Applicative[F].pure(None)  // No other choice here.
  case Some((a1, a2))   => ???                        // Need a value of type F[Some[(B, B)]] here.
}
\end{lstlisting}
The only way of getting values of type $B$ is by applying \lstinline!f!
to the values \lstinline!a1! and \lstinline!a2!. We will then get
two values of type $F^{B}$. We may merge them into a single value
of type $F^{B\times B}$ via $F$\textsf{'}s \lstinline!zip! method. It remains
to use $F$\textsf{'}s \lstinline!map! in order to transform $F^{B\times B}$
to the required type $F^{\bbnum 1+B\times B}$:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B]): Option[(A, A)] => F[Option[(B, B)]] = {
  case None             => Applicative[F].pure(None)
  case Some((a1, a2))   => Applicative[F].zip(f(a1), f(a2)).map { case (b1, b2)  => Some((b1, b2)) }
}
\end{lstlisting}
In the code notation, this is written as:
\[
\text{trav}_{L}(f^{:A\rightarrow F^{B}})\triangleq\,\begin{array}{|c||c|}
 & F^{\bbnum 1+B\times B}\\
\hline \bbnum 1 & \_\rightarrow\text{pu}_{F}(1+\bbnum 0^{:B\times B})\\
A\times A & (f\boxtimes f)\bef\text{zip}_{F}\bef(b_{1}^{:B}\times b_{2}^{:B}\rightarrow\bbnum 0^{:\bbnum 1}+b_{1}\times b_{2})^{\uparrow F}
\end{array}\quad.
\]
The code must use $F$\textsf{'}s methods \lstinline!map!, \lstinline!pure!,
and \lstinline!zip!. This makes it clear why \lstinline!traverse!
requires $F$ to be an applicative functor. Apart from that requirement,
the code works in the same way for all $F$.

\subsubsection{Example \label{subsec:Example-traversable-seq}\ref{subsec:Example-traversable-seq}}

Implement \lstinline!traverse! for Scala\textsf{'}s \lstinline!List! type
constructor. 

\subparagraph{Solution}

We need to implement the following type signature:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B]): List[A] => F[List[B]]
\end{lstlisting}
A list may be empty, or may have a head and a tail. Similarly to the
code in Example~\ref{subsec:Example-traverse-for-1+a*a}, we use
$F$\textsf{'}s methods \lstinline!pure!, \lstinline!zip!, and \lstinline!map!
to compute a suitable value of type \lstinline!F[Seq[B]]! in these
cases.
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B])(la: List[A]): F[List[B]] = la match {
  case Nil            => Applicative[F].pure(Nil)
  case head :: tail   => (f(head) zip trav(f)(tail)).map { case (headB, tailB)  => headB :: tailB }
}
\end{lstlisting}
This code applies \lstinline!f! to each element of the list and accumulates
the resulting $F$-effects of type $F^{B}$. The $F$-effects are
merged using $F$\textsf{'}s \lstinline!zip! function (implemented as an extension
method).

In the code notation, the function \lstinline!trav! looks like this:
\[
\text{trav}_{\text{List}}(f^{:A\rightarrow F^{B}})\triangleq\,\begin{array}{|c||c|}
 & F^{\bbnum 1+B\times\text{List}^{B}}\\
\hline \bbnum 1 & \_\rightarrow\text{pu}_{F}(1+\bbnum 0^{:B\times\text{List}^{B}})\\
A\times\text{List}^{A} & \big(f\boxtimes\overline{\text{trav}}_{\text{List}}(f)\big)\bef\text{zip}_{F}\bef(h^{:B}\times t^{:\text{List}^{B}}\rightarrow\bbnum 0^{:\bbnum 1}+h\times t)^{\uparrow F}
\end{array}\quad.
\]


\subsubsection{Example \label{subsec:Example-traverse-tree}\ref{subsec:Example-traverse-tree}}

Implement a \lstinline!traverse! operation for a binary tree.

\subparagraph{Solution}

We will use the following binary tree data type denoted by \lstinline!T2!:
\begin{lstlisting}
sealed trait T2[A]
final case class Leaf[A](a: A)                 extends T2[A]
final case class Branch[A](l: T2[A], r: T2[A]) extends T2[A]
\end{lstlisting}
We implement \lstinline!trav! similarly to the implementation of
\lstinline!foldLeft! in Section~\ref{subsec:Binary-trees}:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B])(t: T2[A]): F[T2[B]] = t match {
  case Leaf(a)          => f(a).map(b => Leaf(b))            // Reproduce the Leaf structure under F.
  case Branch(t1, t2)   =>
    val (r1, r2) = (trav(f)(t1), trav(f)(t2))    // Traverse the two branches and obtain two results.
    (r1 zip r2).map { case (b1, b2)  => Branch(b1, b2) }   // Reproduce the Branch structure under F.
}
\end{lstlisting}
In the code notation, this function looks like this:
\begin{align*}
 & \text{trav}\,(f^{:A\rightarrow F^{B}})\\
 & \triangleq\,\begin{array}{|c||c|}
 & F^{B+\text{T2}^{B}\times\text{T2}^{B}}\\
\hline A & f\bef(b^{:B}\rightarrow b+\bbnum 0^{:\text{T2}^{B}\times\text{T2}^{B}})^{\uparrow F}\\
\text{T2}^{A}\times\text{T2}^{A} & \big(\overline{\text{trav}}\,(f)\boxtimes\overline{\text{trav}}\,(f)\big)\bef\text{zip}_{F}\bef(l^{:\text{T2}^{B}}\times r^{:\text{T2}^{B}}\rightarrow\bbnum 0^{:B}+l\times r)^{\uparrow F}
\end{array}\quad.
\end{align*}

The code first traverses the two sub-branches (using recursive calls
to \lstinline!trav!) and then combines the resulting values. So,
this implementation represents a depth-first, left-to-right traversal
of the tree.

\subsubsection{Example \label{subsec:Example-traversal-perfect-shaped-tree}\ref{subsec:Example-traversal-perfect-shaped-tree}}

Implement a \lstinline!traverse! operation for a perfect-shaped tree
(\lstinline!PTree!, Section~\ref{subsec:Perfect-shaped-trees}).

\subparagraph{Solution}

We begin writing code as in Example~\ref{subsec:Example-traverse-tree}
by pattern matching:
\begin{lstlisting}[numbers=left]
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B])(t: PTree[A]): F[PTree[B]] = t match {
  case Leaf(a)     => f(a).map(b => Leaf(b))           // Reproduce the Leaf structure under F.
  case Branch(p)   => ???  // Here p has type PTree[(A, A)].
}
\end{lstlisting}
In line 3, we have a pattern variable \lstinline!p! of type \lstinline!PTree[(A, A)]!,
but we need to obtain a value of type \lstinline!F[Branch[B]]! in
that scope. Since a \lstinline!Branch! contains a recursive instance
of the type \lstinline!PTree!, it seems we need to use a recursive
call of \lstinline!trav! here. However, we cannot apply \lstinline!trav(f)!
to \lstinline!p! because the types do not match. We will be able
to apply \lstinline!trav! if instead of \lstinline!f: A => F[B]!
we had a function of type \lstinline!(A, A) => F[(B, B)]!. We can
create such a function out of \lstinline!f! using $F$\textsf{'}s \lstinline!zip!
method:
\begin{lstlisting}
{ case (a1, a2) => f(a1) zip f(a2) }
\end{lstlisting}
 So, we write:
\begin{lstlisting}
  case Branch(p)   => trav { case (a1, a2) => f(a1) zip f(a2) }(p); ??? } // Have F[PTree[(B, B)]].
\end{lstlisting}
We can now use $F$\textsf{'}s \lstinline!map! method to restore the \lstinline!Branch!
structure under $F$. The complete code is:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative : Functor](f: A => F[B])(t: PTree[A]): F[PTree[B]] = t match {
  case Leaf(a)     => f(a).map(b => Leaf(b))           // Reproduce the Leaf structure under F.
  case Branch(p)   => (trav[(A, A), (B, B), F] { case (a1, a2) => f(a1) zip f(a2) }(p)
                      ).map(x => Branch(x))          // Reproduce the Branch structure under F.
}
\end{lstlisting}
Here we assumed that the \lstinline!zip! function is defined on \lstinline!F!
as an extension method. $\square$

These examples show how to implement the \lstinline!traverse! operation
for a container-like data structure $L^{A}$ that stores some values
of type $A$. Each stored value of type $A$ is processed using the
given function $f:A\rightarrow F^{B}$. All of the resulting $F$-effects
need to be collected and merged (using $F$\textsf{'}s \lstinline!zip! and
\lstinline!map!) into a single $F$-effect that wraps a value of
type $L^{B}$. The wrapped value should reproduce the shape of the
data structure that was present in the input value of type $L^{A}$. 

In all examples above, we merged all $F$-effects together by collecting
each $F$-effect exactly once. It would be strange if the \lstinline!traverse!
operation repeated some $F$-effects, as in the following code:
\begin{lstlisting}
def badtrav[A, B, F[_]: Applicative : Functor](f: A => F[B])(la: List[A]): F[List[B]] = la match {
  case Nil            => Applicative[F].pure(Nil)
  case head :: tail   => (f(head) zip f(head) zip badtrav(f)(tail))     // Use the F-effect twice.
                           .map { case (headB, (_, tailB))  => headB :: tailB }
}
\end{lstlisting}
Although this implementation fits the type signature of \lstinline!traverse!,
it contradicts the intuition of traversing the sequence only once.
When $F=$ \lstinline!Future!, the code \lstinline!badtrav(f)(xs)!
will start \emph{two} parallel computations for each element of the
sequence \lstinline!xs! and ignore one of the results. When $F$\textsf{'}s
effect describes parsing (see Section~\ref{subsec:Parsing-with-applicative-and-monadic-parsers})
or other computations that maintain an internal state, invoking such
an $F$-effect twice will likely lead to incorrect results. Below
we will see that the laws of \lstinline!traverse! prohibit implementations
that either repeat some of the $F$-effects or ignore some of the
values.

\subsection{Aggregating tree-like data by folding. Breadth-first traversal\label{subsec:Aggregating-tree-like-data-bfs}}

Although \lstinline!foldMap! (and \lstinline!foldLeft!) are special
cases of \lstinline!traverse!, it is often easier to implement \lstinline!foldMap!
directly. Let us look at some examples of \lstinline!foldMap! that
implements different kinds of tree traversals.

One implementation of \lstinline!foldMap! corresponds to the \lstinline!traverse!
operation defined in Example~\ref{subsec:Example-traverse-tree}.
The code of \lstinline!foldMap! is:
\begin{lstlisting}
def foldMap[A, M: Monoid](f: A => M)(t: T2[A]): M = t match {
  case Leaf(a)          => f(a)
  case Branch(t1, t2)   => foldMap(f)(t1) |+| foldMap(f)(t2)
}
\end{lstlisting}

A special case of a monoid is \lstinline!List[A]!; its \lstinline!combine!
operation is the concatenation of lists. If we use \lstinline!foldMap!
with the monoid \lstinline!M = List[A]! and the function \lstinline!f: A => List[A]!
that creates a single-element list, we obtain a function \lstinline!toList!
that converts a tree \lstinline!T2[A]! into \lstinline!List[A]!,
given a \lstinline!Monoid! instance for \lstinline!List[A]!:
\begin{lstlisting}
def toList[A]: T2[A] => List[A] = foldMap[A, List[A]](List(_))
\end{lstlisting}

In the same way, we may implement \lstinline!toList: L[A] => List[A]!
for any foldable functor \lstinline!L!.

The function \lstinline!toList! captures the requirement that a foldable
functor \lstinline!L! must have a well-defined way of iterating over
the data items stored inside it. So, \lstinline!L! is foldable if
there is a known way of extracting its data items in the form of a
list. We may then use standard aggregation operations on lists (\lstinline!foldLeft!,
\lstinline!reduce!, and so on). Below, we will prove that \lstinline!toList!
is equivalent to \lstinline!foldMap!. So, a foldable functor allows
us to apply any aggregation operation to the data stored in the functor. 

An aggregation\textsf{'}s results may depend on the order of data traversal.
It is important that the function \lstinline!toList! can be implemented
in different ways by choosing a specific order in which the data from
\lstinline!L! is copied to a list. Some data structures have a \textsf{``}natural\textsf{''}
traversal order that is easiest to implement. Let us see some examples
of implementing different traversal orders for lists and trees.

The above code of \lstinline!foldMap! for the binary tree implements
a depth-first traversal of the tree. We can see that by applying \lstinline!toList!
to an example tree (\lstinline!t2!):

\begin{wrapfigure}{l}{0.76\columnwidth}%
\vspace{-0.65\baselineskip}
\begin{lstlisting}
val t2: T2[Int] = Branch(Leaf(8), Branch(Branch(Leaf(3), Leaf(5)), Leaf(4)))

scala> toList(t2)
res0: List[Int] = List(8, 3, 5, 4)
\end{lstlisting}

\vspace{0.5\baselineskip}
\end{wrapfigure}%

\noindent {\footnotesize{}\vspace{-0.3\baselineskip}
}{\footnotesize\par}

\noindent ~~\lstinline!toList(!{\tiny{} \Tree[ 8 [ [ 3 5 ] 4 ] ] }\lstinline!)!{\small{}}\\
{\small{}~~~ $=\left[8,3,5,4\right]$}\\

Let us now implement another version of \lstinline!toList!, called
\lstinline!toList2!, that performs the \emph{breadth-first} traversal
of a binary tree. The idea is to prepare a list of all leaf values
at level $0$ in the tree, then a list of all values at level $1$,
and so on. At each level, values must be collected left to right.
The result will be a list of lists. For example, the tree \lstinline!t2!
shown above has no values at level $0$, value $8$ at level $1$,
value $4$ at level $2$, and values $3$ and $5$ at level $3$.
So, applying \lstinline!toList2! to the tree \lstinline!t2! will
give the nested list $\left[\left[\right],\left[8\right],\left[4\right],\left[3,5\right]\right]$.
Then we can \lstinline!flatten! that list and obtain the list $\left[8,4,3,5\right]$,
which is the breadth-first traversal order of the tree \lstinline!t2!. 

Begin writing code of \lstinline!toList2! by pattern matching on
the tree structure:
\begin{lstlisting}
def toListBFS[A]: T2[A] => List[A] = toList2 andThen (_.flatten)

def toList2[A]: T2[A] => List[List[A]] = {
  case Leaf(a)        => List(List(a))    // Only one value at level 0.
  case Branch(l, r)   => ???         // Have toList2(l) and toList2(r).
}
\end{lstlisting}
After using recursive calls of \lstinline!toList2! on the left and
right sub-trees (\lstinline!l! and \lstinline!r!), we need somehow
to combine the resulting lists. For the tree \lstinline!t2!, applying
\lstinline!toList2! to the sub-trees will return the lists $\left[\left[8\right]\right]$
and $\left[\left[\right],\left[4\right],\left[3,5\right]\right]$.
We need to concatenate the corresponding nested lists and obtain $\left[\left[\right],\left[8\right],\left[4\right],\left[3,5\right]\right]$.
(An initial empty list needs to be added because the sub-trees begin
one level deeper.) The standard \lstinline!zip! operation would not
work correctly here: \lstinline!zip! would truncate the longer list.
Instead, we need to keep the longer list\textsf{'}s elements. So, let us implement
a special helper function (\lstinline!listMerge!) for merging the
nested lists in this way:
\begin{lstlisting}
def listMerge[A](l: List[List[A]], r: List[List[A]]): List[List[A]] = (l, r) match {
  case (Nil, r)               => r  // Keep the elements from the longer list.
  case (l, Nil)               => l
  case (lh :: lt, rh :: rt)   => (lh ++ rh) :: listMerge(lt, rt)
}
\end{lstlisting}
Now we can complete the code of \lstinline!toList2! and run some
tests:
\begin{lstlisting}
def toList2[A]: T2[A] => List[List[A]] = {
  case Leaf(a)        => List(List(a)) // Only one value at level 0.
  case Branch(l, r)   => listMerge(Nil :: toList2(l), Nil :: toList2(r))
}

scala> toList2(t2)
res1: List[List[Int]] = List(List(), List(8), List(4), List(3, 5))

scala> toListBFS(t2)
res2: List[Int] = List(8, 4, 3, 5)
\end{lstlisting}

For lists and other sequences, the easiest traversal orders are either
direct or reverse. A direct traversal is implemented by the standard
\lstinline!foldLeft! operation on \lstinline!Seq!, while a reverse-order
traversal is done simply by applying the \lstinline!reverse! method,
such as \lstinline!la.reverse.foldLeft(...)(...)!.

Similarly to \lstinline!foldLeft! and \lstinline!foldMap!, the \lstinline!traverse!
method may be implemented with different traversal orders. For comparison,
we show the code for the direct-ordered and reverse-ordered \lstinline!List!
traversals:
\begin{lstlisting}
// This code assumes that F has typeclass instances for Applicative and Functor.
def travList[A, B](f: A => F[B])(la: List[A]): F[List[B]] = la match {
  case Nil            => Applicative[F].pure(Nil)
  case head :: tail   => f(head).map2(travList(f)(tail)) { (x, y) => x +: y }
}

def travRevList[A, B](f: A => F[B])(la: List[A]): F[List[B]] = la.reverse match {
  case Nil            => Applicative[F].pure(Nil)
  case head :: tail   => f(head).map2(travRevList(f)(tail)) { (h, t) => t :+ h }
}
\end{lstlisting}
The function \lstinline!travRevList! first reverses the given list,
so that $F$\textsf{'}s effects are merged in the reverse order. Then it swaps
\lstinline!h! and \lstinline!t! using \lstinline!map2!, which restores
the original list structure wrapped under $F$.

Implementing a breadth-first \lstinline!traverse! for a tree is more
difficult. It is not sufficient to convert the tree to a list in the
breadth-first order and collect $F$\textsf{'}s effects. We also need to restore
the original tree structure wrapped under the functor $F$. Section~\ref{subsec:Decorating-a-tree-breadth-first-traversal}
will show an example of implementing a breadth-first \lstinline!traverse!
for binary trees.

\subsection{Decorating a tree. I. Depth-first traversal\label{subsec:Decorating-a-tree1}}

A \lstinline!traverse! method for a given tree-like data structure
may be used to perform various operations on trees, as long as those
operations are described by the effect of some applicative functor
$F$. 

An example of a tree operation is \textsf{``}decorating\textsf{''}, i.e., adding labels
of some sort to each leaf. Since tree-like data types are functors
that have a \lstinline!map! method, a simple decorating operation
replaces each leaf with a function of the data at that leaf. For instance,
we may add the value $20$ to each leaf:
\begin{lstlisting}
val t2 = Branch(Leaf(8), Branch(Branch(Leaf(3), Leaf(5)), Leaf(4)))

scala> t2.map(x => x + 20)  // Assuming a Functor instance for T2.
res0: T2[Int] = Branch(Leaf(28), Branch(Branch(Leaf(23), Leaf(25)), Leaf(24)))
\end{lstlisting}
This transforms the tree {\tiny{}\Tree[ 8 [ [ 3 5 ] 4 ] ] } into
{\tiny{}\Tree[ 28 [ [ 23 25 ] 24 ] ]}. However, the \lstinline!map!
method works separately with each leaf and cannot use any previously
computed values. E.g., we cannot use \lstinline!map! to implement
a \lstinline!zipWithIndex! function for trees, transforming the tree
{\tiny{}\Tree[ 8 [ [ 3 5 ] 4 ] ] } into {\tiny{}\Tree[ (8,0) [ [ (3,1) (5,2) ] (4,3) ] ]}
with added indices ($0$, $1$, $2$, $3$). For such tasks, we need
the additional functionality of the \lstinline!traverse! operation.

Labeling the leaves of a tree with their traversal index requires
us to update an internal state (the number of leaves seen so far)
while traversing the tree. Recall that updating an internal state
is the effect of a \lstinline!State! monad. Since all monads can
also implement the applicative methods (\lstinline!pure! and \lstinline!zip!),
we may use the \lstinline!State! monad as the applicative functor
$F$ for the \lstinline!traverse! function.

In this section, we will implement \lstinline!zipWithIndex! for a
\emph{depth-first} tree traversal. Let us call that function \lstinline!zipWithIndexDF!.
The code of \lstinline!zipWithIndexDF! will have the form:
\begin{lstlisting}
final case class St[A](run: Int => (A, Int))    // A State monad with internal state of type Int.
// Assume that we have defined Applicative and Functor instances for St.
def computeIndex[A]: A => St[(A, Int)] = ???  // Define the "decoration" function.
def zipWithIndexDF[A](tree: T2[A]): T2[(A, Int)] = {
  val result: St[T2[(A, Int)]] = trav[A, (A, Int), St](computeIndex)(tree)
  result.run(0)._1  // Run the State monad and get the result value.
}
\end{lstlisting}
This will be a depth-first traversal if \lstinline!trav! is the function
shown in Example~\ref{subsec:Example-traverse-tree}. It remains
to define the function \lstinline!computeIndex! describing the \textsf{``}decoration
logic\textsf{''}. Begin writing the code as:
\begin{lstlisting}
def computeIndex[A]: A => St[(A, Int)] = a => St { i => ???: ((A, Int), Int) }
\end{lstlisting}
Given a leaf value (\lstinline!a: A!) and a previous internal state
(\lstinline!i: Int!), we must compute the \textsf{``}decorated\textsf{''} leaf value
of type $A\times\text{Int}$ as well as the new internal state. Since
our goal is to label the leaves by the current leaf count, we use
the internal state to store the number of leaves seen so far. So,
we just need to increment the internal state after copying it to a
leaf:
\begin{lstlisting}
def computeIndex[A]: A => St[(A, Int)] = a => St { i => ((a, i), i + 1) }
\end{lstlisting}
This completes the implementation of \lstinline!zipWithIndexDFS!,
which we can now test:
\begin{lstlisting}
scala> zipWithIndexDF(t2)
res0: T2[(Int, Int)] = Branch(Leaf((8,0)), Branch(Branch(Leaf((3,1)), Leaf((5,2))), Leaf((4,3))))
\end{lstlisting}


\subsection{Decorating a tree. II. Breadth-first traversal\label{subsec:Decorating-a-tree-breadth-first-traversal}}

In Section~\ref{subsec:Aggregating-tree-like-data-bfs}, we have
implemented a breadth-first folding for the tree data structure \lstinline!T2[A]!.
What additional code does  \lstinline!traverse! need? Folding over
\lstinline!T2[A]! merely needs to enumerate the values of type \lstinline!A!
in the breadth-first order, but a \lstinline!traverse! function must
return a value of type \lstinline!F[T2[B]]!. This requires us to
\emph{merge the effects} of an arbitrary applicative functor \lstinline!F!
in the breadth-first order, while gathering the values of type \lstinline!B!
into a tree structure (\lstinline!T2[B]!) wrapped under \lstinline!F!.
The function \lstinline!toListBFS! shown in Section~\ref{subsec:Aggregating-tree-like-data-bfs}
is not sufficient for that purpose, because the tree structure cannot
be reproduced if we only have a list of leaf values. Even the nested
list computed by \lstinline!toList2! is not sufficient. We need additional
information about each leaf\textsf{'}s position in the tree.

So, let us begin by adding the required position information to the
nested list of leaf values. For each leaf, we need to store the path
from the root of the tree. We can use the \lstinline!Either! type
constructor and its subtypes \lstinline!Left! and \lstinline!Right!
for marking that path. For example, the position information for the
tree \lstinline!t2 =!{\tiny{}\Tree[ 8 [ [ 3 5 ] 4 ] ]} could be
represented by the following nested list:
\begin{lstlisting}
List(
  List(),                                            // No leaves at level 0.
  List( Left(8) ),                                   // One leaf at level 1.
  List( Right(Right(4)) ),                           // One leaf at level 2.
  List( Right(Left(Left(3))), Right(Left(Right(5))) ), // Two leaves at level 3.
)
\end{lstlisting}
This data structure makes it easy to iterate over the leaf values
in the breadth-first order. At the same time, it keeps enough information
for restoring the original tree since each leaf value comes with the
full path from the root of the tree.

An immediate problem with this data structure is a type mismatch in
the outer \lstinline!List!: its elements are not of the same type
because each subsequent sub-list contains data wrapped under \lstinline!Either!
more deeply. To make the types match, we need to implement a special
list-like data structure whose first element has type \lstinline!List[A]!,
the second element has type \lstinline!List[Either[A, A]]!, the third
element has type \lstinline!List[Either[Either[A, A], Either[A, A]]]!,
and so on. (Exercise~\ref{subsec:Exercise-disjunctive-EvenList-1}
shows a similar data type.) Let us call this data type \lstinline!TD!
(\textsf{``}tree descriptor\textsf{''}):
\begin{lstlisting}
sealed trait TD[A]
final case class Last[A](a: List[A]) extends TD[A]
final case class More[A](a: List[A], tail: TD[Either[A, A]]) extends TD[A]  
\end{lstlisting}
The tree descriptor for the tree \lstinline!t2! shown above will
then look like this:
\begin{lstlisting}
val td2: TD[Int] = More(List(), More(   // No leaves at level 0.
    List( Left(8) ), More(              // One leaf at level 1.
      List( Right(Right(4)) ), Last(    // One leaf at level 2.
        List( Right(Left(Left(3))), Right(Left(Right(5))) ),   // Two leaves at level 3.
))))
\end{lstlisting}
Now the types match, while a wrong number of \lstinline!Left! or
\lstinline!Right! wrappers will be a type error.

This representation of the tree descriptor is certainly not efficient
in terms of memory consumption and processing speed. An \textsf{``}industry-strength\textsf{''}
breadth-first tree traversal will use quite different data structures
while implementing the same logic. For instance, one could use a bit
vector $\left[1,0,1\right]$ instead of the wrapper \lstinline!Right(Left(Right()))!,
avoiding the allocation of many objects in memory. However, our present
goal is not to achieve high efficiency but to obtain correct code
quickly.

The next step is to write functions transforming between trees \lstinline!T2[A]!
and descriptors \lstinline!TD[A]!:
\begin{lstlisting}
def t2ToTD[A]: T2[A] => TD[A] = ???
def tdToT2[A]: TD[A] => T2[A] = ???
\end{lstlisting}
Begin implementing the first function by pattern matching:
\begin{lstlisting}[numbers=left]
def t2ToTD[A]: T2[A] => TD[A] = {
  case Leaf(a)        => Last(List(a))   // One leaf at level 0.
  case Branch(l, r)   => ( t2ToTD(l),  t2ToTD(r) ); ???     // How to combine the subtrees?
}
\end{lstlisting}
It seems reasonable that we need to apply \lstinline!t2ToTD! recursively
to the subtrees \lstinline!l! and \lstinline!r! in line $3$, obtaining
the tree descriptors of type \lstinline!TD[A]! for those subtrees.
We still have to merge these tree descriptors in the correct way.
Inspired by the \lstinline!listMerge! function from Section~\ref{subsec:Aggregating-tree-like-data-bfs},
we implement the descriptor-merging code by concatenating the lists
separately at each depth level:
\begin{lstlisting}
def tdMerge[A](l: TD[A], r: TD[A]): TD[A] = (l, r) match {
  case (Last(la), Last(lb))                 => Last(la ++ lb)
  case (Last(la), More(lb, tail))           => More(la ++ lb, tail)
  case (More(la, tail), Last(lb))           => More(la ++ lb, tail)
  case (More(la, tailA), More(lb, tailB))   => More(la ++ lb, tdMerge(tailA, tailB))
}
\end{lstlisting}
However, writing just \lstinline!tdMerge(t2ToTD(l), t2ToTD(r))! would
not be correct in line $3$ of \lstinline!t2ToTD!. We need somehow
to store the information that the subtrees \lstinline!l! and \lstinline!r!
are located one level deeper at the left and at the right respectively.
To see how that information needs to be stored, let us look at the
tree \lstinline!t2! whose left and right subtrees are \lstinline!l = !{\tiny{}\Tree[ 8  ]}
and \lstinline!r =!{\tiny{}\Tree[  [ 3 5 ] 4  ]}. The descriptors
of these subtrees are computed by recursive calls of \lstinline!t2ToTD!,
which (if implemented correctly) should give this:
\begin{lstlisting}
t2ToTD(l) == Last(List(8))                   // One leaf at level 0.

t2ToTD(r) == More(List(), More(              // No leaves at level 0.
  List( Right(4) ), Last(                    // One leaf at level 1.
    List( Left(Left(3)), Left(Right(5)) )    // Two leaves at level 2.
)))
\end{lstlisting}
The correct merging of these tree descriptors requires wrapping all
data in the subtree \lstinline!l! in an additional \lstinline!Left()!
layer and all data in the subtree \lstinline!r! in an additional
\lstinline!Right()! layer. Let us implement two helper functions
(\lstinline!addLeft! and \lstinline!addRight!) for that transformation:
\begin{lstlisting}
def addLeft[A]: TD[A] => TD[Either[A, A]] = {
  case Last(a)         => Last(a.map(Left(_)))
  case More(a, tail)   => More(a.map(Left(_)), addLeft[Either[A, A]](tail))
}

def addRight[A]: TD[A] => TD[Either[A, A]] = {
  case Last(a)         => Last(a.map(Right(_)))
  case More(a, tail)   => More(a.map(Right(_)), addRight[Either[A, A]](tail))
}
\end{lstlisting}
 We expect that \lstinline!addLeft! and \lstinline!addRight! will
transform the left and right subtree descriptors like this:
\begin{lstlisting}
addLeft(t2ToTD(l)) == Last(List(Left(8)))

addRight(t2ToTD(r)) == More( List(), More(
  List( Right(Right(4)) ), Last(
    List( Right(Left(Left(3))), Right(Left(Right(5))) )
)))
\end{lstlisting}
It remains to mark the subtrees as being one layer deeper. We do this
by inserting an empty list at the beginning of a \lstinline!TD! structure.
The types will then require that the subsequent lists must have one
more layer of \lstinline!Either! wrappers, which they will indeed
have after applying \lstinline!addLeft! and \lstinline!addRight!.
So, the correct merging of the two subtree descriptors is:
\begin{lstlisting}
More(List(), tdMerge( addLeft(t2ToTD(l)), addRight(t2ToTD(r)) ))
\end{lstlisting}
The complete code of \lstinline!t2ToTD! becomes:
\begin{lstlisting}
def t2ToTD[A]: T2[A] => TD[A] = {
  case Leaf(a)        => Last(List(a))
  case Branch(l, r)   => More(List(), tdMerge( addLeft(t2ToTD(l)), addRight(t2ToTD(r)) ))
}
\end{lstlisting}
We can test this code on an example tree:
\begin{lstlisting}
val t2 = Branch(Leaf(8), Branch(Branch(Leaf(3), Leaf(5)), Leaf(4)))

scala> t2ToTD(t2)
res0: TD[Int] = More(List(), More(List(Left(8)), More(List(Right(Right(4))), Last(List(Right(Left(Left(3))), Right(Left(Right(5))))))))
\end{lstlisting}

To implement the inverse function (\lstinline!tdToT2!), we need to
keep in mind that \lstinline!t2ToTD! will only create a certain subset
of possible values of type \lstinline!TD[A]!. Looking at the code
of \lstinline!t2ToTD!, we can see, for instance, that the first list
is always empty when the tree has a branch. The first list is either
empty or contains only one element (the single leaf of the tree).
We will never create values of type \lstinline!TD[A]! where the first
list has more than one element, or where the first list is nonempty
and there are also nonempty subsequent lists. Such values of type
\lstinline!TD[A]! do not correspond to any trees of type \lstinline!T2[A]!.
So, the code of \lstinline!tdToT2! can be a \emph{partial} function
of type \lstinline!TD[A] => T2[A]! that only works with valid tree
descriptors, that is, values of type \lstinline!TD[A]! that were
obtained by applying \lstinline!t2ToTd! to some trees of type \lstinline!T2[A]!.
The composition \lstinline!t2ToTD andThen tdToT2! should be the identity
function.

Begin writing the code of \lstinline!tdToT2! by pattern matching:
\begin{lstlisting}
def tdToT2[A]: TD[A] => T2[A] = {
  case Last(List(a))   => Leaf(a)    // Tree has a single leaf at level 0.
  case _               => ???
}
\end{lstlisting}
Valid tree descriptors can only have a non-empty first list if the
tree has a single leaf at level $0$. So, we may ignore all other
cases and continue with patterns that assume an empty first list:
\begin{lstlisting}[numbers=left]
def tdToT2[A]: TD[A] => T2[A] = {
  case Last(List(a))        => Leaf(a)    // Tree has a single leaf at level 0.
  case More(List(a), _)     => Leaf(a)    // Tree has a single leaf at level 0.         
  case More(List(), tail)   => ???        // Tree has two branches. 
}
\end{lstlisting}
In line $4$, we need to restore two branches of the tree from the
given value \lstinline!tail! of type \lstinline!TD[Either[A, A]]!.
How can we do that? The left branch contains all the leaf data from
\lstinline!tail! where the outermost wrapper is a \lstinline!Left()!.
We can select just that subset of leaf data and remove the outermost
\lstinline!Left()! wrapper by implementing the helper functions \lstinline!removeLeft!
and \lstinline!filterLeft! as shown here:
\begin{lstlisting}
def removeLeft[A]: List[Either[A, A]] => List[A] = _.collect { case Left(x) => x }

def filterLeft[A]: TD[Either[A, A]] => TD[A] = {
  case Last(la)         => Last(removeLeft(la))
  case More(la, tail)   => More(removeLeft(la), filterLeft(tail))
}
\end{lstlisting}
For example, if we apply \lstinline!filterLeft! to the descriptor
\lstinline!t2ToTD(t2).tail! then we will get:
\begin{lstlisting}
filterLeft(t2ToTD(t2).tail) == More(List(8), More(List(), Last(List())))
\end{lstlisting}
This tree descriptor corresponds to the left subtree of \lstinline!t2!.

In a similar way, we implement \lstinline!filterRight! that extracts
the descriptor data for the right subtree:
\begin{lstlisting}
def removeRight[A]: List[Either[A, A]] => List[A] = _.collect { case Right(x) => x }
def filterRight[A]: TD[Either[A, A]] => TD[A] = {
  case Last(la)         => Last(removeRight(la))
  case More(la, tail)   => More(removeRight(la), filterRight(tail))
}

scala> filterRight(t2ToTD(t2).tail)
res1: TD[Int] = More(List(), More(List(Right(4)), Last(List(Left(Left(3)), Left(Right(5))))))
\end{lstlisting}

Now we are ready to complete the implementation of \lstinline!tdToT2!.
We use \lstinline!filterLeft! and \lstinline!filterRight! to obtain
the descriptors for the left and the right subtrees. Applying \lstinline!tdToT2!
recursively to those descriptors will restore the two subtrees:
\begin{lstlisting}
def tdToT2[A]: TD[A] => T2[A] = {
  case Last(List(a))        => Leaf(a)
  case More(List(a), _)     => Leaf(a)
  case More(List(), tail)   => Branch(tdToT2(filterLeft(tail)), tdToT2(filterRight(tail)))
}

scala> tdToT2(t2ToTD(t2)) == t2
res2: Boolean = true
\end{lstlisting}

The next step is to implement \lstinline!traverse! for the type constructor
\lstinline!TD!, which makes \lstinline!TD! into a traversable functor.
We will call its \lstinline!traverse! operation \textsf{``}\lstinline!travTD!\textsf{''}
for clarity. Since \lstinline!TD[A]! is essentially a decorated list
of lists, we will need \lstinline!List!\textsf{'}s \lstinline!traverse! (Example~\ref{subsec:Example-traversable-seq}),
which we here denote by \lstinline!travList!. We will also need a
\lstinline!traverse! function for the functor \lstinline!Either[A, A]!:
\begin{lstlisting}
def travEither[A, B, F[_]: Functor](f: A => F[B])(e: Either[A, A]): F[Either[B, B]] = e match {
  case Left(a)    => f(a).map(Left(_))
  case Right(a)   => f(a).map(Right(_))
}
\end{lstlisting}
Using \lstinline!travList! and \lstinline!travEither!, we can now
implement \lstinline!travTD! like this:
\begin{lstlisting}
def travTD[A, B, F[_]: Applicative : Functor](f: A => F[B])(td: TD[A]): F[TD[B]] = td match {
  case Last(a)         => travList(f)(a).map(Last(_))
  case More(a, tail)   =>
    val headFB: F[List[B]] = travList(f)(a)
    val tailFB: F[TD[Either[B, B]]] = travTD { x: Either[A, A] => travEither(f)(x) }(tail)
    (headFB zip tailFB).map { case (headB, tailB) => More(headB, tailB) }
}
\end{lstlisting}

The final step is to convert \lstinline!TD!\textsf{'}s \lstinline!traverse!
into \lstinline!T2!\textsf{'}s \lstinline!traverse! by using \lstinline!t2ToTD!
and \lstinline!tdToT2!. We first compute the tree descriptor of a
given tree and use \lstinline!travTD! to perform a traversal. The
result is a value of type \lstinline!F[TD[B]]!, which we convert
into a value of type \lstinline!F[T2[B]]! using \lstinline!tdToT2!
lifted to \lstinline!F!:
\begin{lstlisting}
def travBF[A, B, F[_]: Applicative : Functor](f: A => F[B])(tree: T2[A]): F[T2[B]] =
  travTD(f)(t2ToTD(tree)).map(tdToT2) 
\end{lstlisting}

This completes our implementation of a breadth-first traversal for
binary trees. To test the code, we run the example of decorating a
tree with the breadth-first traversal index:
\begin{lstlisting}
def zipWithIndexBF[A](tree: T2[A]): T2[(A, Int)] = {
  val result: St[T2[(A, Int)]] = travBF[A, (A, Int), St](computeIndex)(tree)
  result.run(0)._1  // Run the State monad and get the result value.
}

scala> zipWithIndexBF(t2)
res3: T2[(Int, Int)] = Branch(Leaf((8,0)), Branch(Branch(Leaf((3,2)), Leaf((5,3))), Leaf((4,1))))
\end{lstlisting}

The only difference between \lstinline!zipWithIndexDF! and \lstinline!zipWithIndexBF!
is the choice of the \lstinline!traverse! operation (\lstinline!trav!
or \lstinline!travBF!). The \textsf{``}decoration logic\textsf{''} is described by
the function \lstinline!computeIndex! and does not depend on the
traversal order. As a result, we gain flexibility: an arbitrary traversal
order may be used with an arbitrary \textsf{``}decoration logic\textsf{''}. 

\subsection{The \texttt{Traversable} typeclass. Implementing \texttt{scanLeft}}

We define a \textsf{``}traversable functor\textsf{''} typeclass (called \lstinline!Traversable!)
by specifying a \lstinline!traverse! operation, which is convenient
to provide as an extension method:
\begin{lstlisting}
trait Traversable[L[_]] {
  def trav[A, B, F[_]: Applicative: Functor](f: A => F[B])(la: L[A]): F[L[B]]
}
implicit class TraversableOps[L[_], A](la: L[A])(implicit tr: Traversable[L]) {
  def traverse[B, F[_]: Applicative : Functor](f: A => F[B]): F[L[B]] = tr.trav(f)(la)
}
\end{lstlisting}

As we have mentioned in Section~\ref{subsec:The-traverse-operation},
the \lstinline!foldLeft! operation can be defined via \lstinline!traverse!.
The \lstinline!foldLeft! operation transforms an initial sequence
into a result value by updating some internal state. Another function
similar to \lstinline!foldLeft! is \lstinline!scanLeft!; the difference
is that \lstinline!scanLeft!\textsf{'}s result value is the \emph{sequence}
of all intermediate state values rather than just the last state value.
Let us see how the functionality of \lstinline!scanLeft! can be expressed
using \lstinline!traverse! with a \lstinline!State! monad that handles
the state updates. This will allow us to implement \lstinline!scanLeft!
automatically for every traversable functor.

Assume that \lstinline!L! is a traversable functor whose \lstinline!trav!
function is available:
\begin{lstlisting}
def trav[A, B, F[_]: Applicative: Functor](f: A => F[B])(la: L[A]): F[L[B]] = ...
\end{lstlisting}
We would like to implement \lstinline!scanLeft! with the following
type signature:
\begin{lstlisting}
def scanLeft[A, Z](la: L[A])(init: Z)(f: (A, Z) => Z): L[Z] = ???
\end{lstlisting}
The standard \lstinline!scanLeft! method for lists produces a list
with an extra initial element, which is always equal to the initial
value \lstinline!init!. This initial element is not essential and
may be omitted from \lstinline!scanLeft!. In fact, we \emph{need}
to omit that initial element when we generalize \lstinline!scanLeft!
to data types \lstinline!L[A]! other than \lstinline!List[A]!, since
those data types will not necessarily support adding one more data
item.

In order to update the state of type \lstinline!Z! and also store
the state\textsf{'}s value as the result of the traversal, we will use the
\lstinline!State! monad and compute a value of type \lstinline!State[Z, Z]!.
The code of \lstinline!scanLeft! calls \lstinline!traverse! with
a suitable function \lstinline!accum! that creates the required value
of type \lstinline!State[Z, Z]!:
\begin{lstlisting}
case class State[Z, A](run: Z => (A, Z)) // Assume Applicative and Functor instances for State[Z, *].
def accum[A, Z](a: A, f: (A, Z) => Z): State[Z, Z] = State { z =>
  val newZ = f(a, z)  // Update the internal state using `f`.
  (newZ, newZ)        // Store the internal state, and also return it as a result value.
}
implicit class TraversableScanOps[L[_] : Traversable, A](la: L[A]) {
  def scanLeft[Z](init: Z)(f: (A, Z) => Z): L[Z] = la.traverse(a => accum(a, f)).run(init)._1
}
\end{lstlisting}
In this way, \lstinline!scanLeft! is made available for all traversable
functors.

To test this code, we implement \lstinline!zipWithIndex! via depth-first
traverse for a tree of type \lstinline!T2!:
\begin{lstlisting}
def zipWithIndexDFS[A]: T2[A] => T2[(A, Int)] = 
  _.scanLeft[(A, Int)]((null.asInstanceOf[A], -1)) { case (a, (_, i)) => (a, i + 1) }
val t2 = Branch(Leaf(8), Branch(Branch(Leaf(3), Leaf(5)), Leaf(4)))

scala> zipWithIndexDFS(t2)
res0: T2[(Int, Int)] = Branch(Leaf((8,0)), Branch(Branch(Leaf((3,1)), Leaf((5,2))), Leaf((4,3))))
\end{lstlisting}


\subsection{Tasks that cannot be performed via \texttt{traverse}\label{subsec:Tasks-not-implementable-via-traverse}}

The \lstinline!traverse! function is powerful since it can use an
arbitrary applicative functor \lstinline!F!. However, some computations
are still not expressible via \lstinline!traverse! because they require
information that \lstinline!traverse! cannot have. We will now look
at two examples of this.

The first example is the \textsf{``}depth labeling\textsf{''} of a tree: each leaf
gets a value equal to its depth. For instance, the tree {\tiny{} \Tree[ 8 [ [ 3 5 ] 4 ] ] }
becomes {\tiny{}}{\tiny{} \Tree[ (8,1) [ [ (3,3) (5,3) ] (4,2) ] ] }
after depth labeling. This cannot be implemented via \lstinline!traverse!
because it cannot detect nodes that have the same depth in the tree.
To see this in more detail, recall that the code of \lstinline!traverse!
always collects and merges all the effects of a given functor \lstinline!F!.
If the effect of \lstinline!F! describes incrementing a counter (as
in our examples involving the \lstinline!State! monad), the code
of \lstinline!traverse! will increment that counter at \emph{each}
leaf. We may traverse the tree so that leaves at the same depth are
traversed next to each other (as in the breadth-first traversal).
But the code of \lstinline!traverse! cannot skip the incrementing
when a leaf is at the same depth: \lstinline!traverse! does not receive
any information about the position of values in the tree. So, we cannot
label certain nodes with the same depth value but other nodes with
a different depth value.

Depth labeling can be implemented as a recursive function \lstinline!zipWithDepth!:
\begin{lstlisting}
def zipWithDepth[A](initial: Int = 0): T2[A] => T2[(A, Int)] = {
  case Leaf(a)        => Leaf((a, initial))
  case Branch(l, r)   => Branch(zipWithDepth(initial + 1)(l), zipWithDepth(initial + 1)(r))
}
val t2: T2[Int] = Branch(Leaf(8), Branch(Branch(Leaf(3), Leaf(5)), Leaf(4)))

scala> zipWithDepth()(t2)
res0: T2[(Int, Int)] = Branch(Leaf((8,1)), Branch(Branch(Leaf((3,3)), Leaf((5,3))), Leaf((4,2))))
\end{lstlisting}

More generally, traversals cannot perform computations that depend
on the \emph{position} of data in the tree (e.g., whether the data
is in a left or in a right branch and at what depth). An example of
such a computation is \textsf{``}pretty-printing\textsf{''} that converts trees into
a text form. In the \LaTeX{} format used to typeset this book, the
tree {\tiny{} \Tree[ 8 [ [ 3 5 ] 4 ] ] } is represented by the text
string \lstinline!"\Tree[ 8 [ [ 3 5 ] 4 ] ]"!. The following code
converts trees into the \LaTeX{} format:
\begin{lstlisting}
def printLaTeX[A](t: T2[A])(toString: A => String): String = {
  def printLaTeXSubtree: T2[A] => String = {
    case Leaf(a)        => toString(a)
    case Branch(l, r)   => "[ " + printLaTeXSubtree(l) + " " + printLaTeXSubtree(r) + " ]"
  }
  "\\Tree" + printLaTeXSubtree(t)
} 

scala> printLaTeX(t2)(_.toString)
res1: String = \Tree[ 8 [ [ 3 5 ] 4 ] ]
\end{lstlisting}


\subsection{Recursion schemes. I. Folding operations\label{subsec:Recursion-schemes.-folding}}

The previous section showed two examples of folding and traversing
operations that cannot be expressed through the standard \lstinline!foldMap!
or \lstinline!traverse! methods and are instead implemented via custom
recursive code. If we needed to implement the same operations for
trees of different shapes or for other recursive data types, we would
need to write new custom code for each new data type. That code will
contain a certain common pattern: it will use recursive calls at the
points where the recursive data type refers to itself. It turns out
that we can separate this common pattern from the custom code, reducing
the effort required for implementing the folding and traversing operations
for different recursive data types.

To begin, write the recursive definition of the tree \lstinline!T2!
as:
\[
\text{T2}^{A}\triangleq A+\text{T2}^{A}\times\text{T2}^{A}\quad.
\]
This type refers recursively to itself in two places. To express that,
define a bifunctor $S$ like this:
\[
S^{A,R}\triangleq A+R\times R\quad.
\]
We can now rewrite the definition of \lstinline!T2! as a recursive
type equation: $\text{T2}^{A}\triangleq S^{A,\text{T2}^{A}}$. The
corresponding Scala code is:
\begin{lstlisting}
type S[A, R] = Either[A, (R, R)]
final case class T2[A](run: S[A, T2[A]])
\end{lstlisting}

The bifunctor\index{bifunctor} $S$ is called the \textbf{recursion
scheme}\index{recursion scheme} of the type \lstinline!T2!. The
recursion scheme describes the places where the recursive type refers
to itself in its definition. All the recursive uses correspond to
occurrences of the type parameter $R$ in $S^{A,R}$. 

The folding operation \lstinline!foldMap! takes a function $f$ of
type $A\rightarrow Z$ as a parameter:
\[
\text{foldMap}_{L}(f^{:A\rightarrow Z}):L^{A}\rightarrow Z\quad.
\]
The function $f$ will be applied to all values of type $A$ stored
inside $L^{A}$. As we have seen in the previous section, the function
$f^{:A\rightarrow Z}$ cannot receive any information about the location
of values of type $A$ inside $L^{A}$. In order to access that information
and make the folding operation \textsf{``}location-aware\textsf{''}, we need to change
the type signature of $f$. To figure out the new type signature,
let us look at the code of \lstinline!printLaTeXSubtree! (short notation
\textsf{``}$\text{pls}$\textsf{''}) from the previous section:
\[
\text{pls}\triangleq\,\begin{array}{|c||c|}
 & \text{String}\\
\hline A & \text{toString}\\
\text{T2}^{A}\times\text{T2}^{A} & (\overline{\text{pls}}\boxtimes\overline{\text{pls}})\bef(l\times r\rightarrow\text{"[ "}+l+\text{" "}+r+\text{" ]"})
\end{array}\quad.
\]
We can rewrite this code using the bifunctor $S^{A,R}\triangleq A+R\times R$.
In this example, the result type $Z$ is \lstinline!String!. By calling
\lstinline!printLaTeXSubtree! recursively on the left and the right
subtrees, we obtain two values of type $Z$. We can separate the recursive
calls to \lstinline!printLaTeXSubtree! from the custom string processing
and express \lstinline!printLaTeXSubtree! as the following function
composition:
\[
\text{pls}\triangleq\,\begin{array}{|c||cc|}
 & A & Z\times Z\\
\hline A & \text{id} & \bbnum 0\\
\text{T2}^{A}\times\text{T2}^{A} & \bbnum 0 & \overline{\text{pls}}\boxtimes\overline{\text{pls}}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & Z\\
\hline A & \text{toString}\\
Z\times Z & l\times r\rightarrow\text{"[ "}+l+\text{" "}+r+\text{" ]"}
\end{array}\quad.
\]
The intermediate result is a data structure of type $A+Z\times Z$,
to which we need to apply some string manipulations. We note that
the type $A+Z\times Z$ is the same as $S^{A,Z}$, while the first
matrix in the code above is the lifting $\overline{\text{pls}}^{\uparrow S^{A,\bullet}}\,$with
respect to the type parameter $R$ of $S^{A,R}$. So, we may express
the custom string manipulations specific to \lstinline!printLaTeXSubtree!
via a function \lstinline!toLaTeX! of type $S^{A,Z}\rightarrow Z$:
\[
\text{toLaTeX}:S^{A,Z}\rightarrow Z\quad,\quad\quad\text{toLaTeX}\triangleq\,\begin{array}{|c||c|}
 & Z\\
\hline A & \text{toString}\\
Z\times Z & l\times r\rightarrow\text{"[ "}+l+\text{" "}+r+\text{" ]"}
\end{array}\quad.
\]
Using this function, the implementation of \lstinline!printLaTeXSubtree!
can be rewritten as:
\[
\text{pls}\triangleq\overline{\text{pls}}^{\uparrow S^{A,\bullet}}\bef\text{toLaTeX}\quad.
\]
All custom \textsf{``}location-aware\textsf{''} logic is now encapsulated in the (non-recursive!)
function \lstinline!toLaTeX!. So, we can now generalize the calculation
by defining a new \lstinline!fold! function (denoted by $\text{fold}_{S}$):
\begin{equation}
\text{fold}_{S}:(S^{A,Z}\rightarrow Z)\rightarrow L^{A}\rightarrow Z\quad,\quad\quad\text{fold}_{S}(f)\triangleq\overline{\text{fold}_{S}(f)}^{\uparrow S^{A,\bullet}}\bef f\quad.\label{eq:fold-via-recursion-scheme-1}
\end{equation}
We then obtain $\text{pls}=\text{fold}_{S}(\text{toLaTeX})$. The
corresponding Scala code is:
\begin{lstlisting}
def fmapR[A, R, T](f: R => T): S[A, R] => S[A, T] = _.map { case (r1, r2) => (f(r1), f(r2)) }

def foldS[A, Z](f: S[A, Z] => Z)(tree: T2[A]): Z = f(fmapR(foldS(f))(tree.run))

def toLaTeX[A]: S[A, String] => String = {
  case Left(a)         => a.toString
  case Right((l, r))   => s"[ $l $r ]"
}
def printLaTeX[A](tree: T2[A]): String = "\\Tree" + foldS[A, String](toLaTeX)(tree)

val t2: T2[Int] = T2(Right((T2(Left(8)), T2(Right((T2(Right((T2(Left(3)), T2(Left(5))))), T2(Left(4))))))))

scala> printLaTeX(t2)
res0: String = \Tree[ [ 8 [ 3 5 ] ] 4 ]
\end{lstlisting}

It is important that the function $\text{fold}_{S}$ is parametric
in the recursion scheme $S$ and the result type $Z$ (which is not
required to be a monoid). Different recursion schemes $S$ may be
used to define lists, trees, and other recursive data types. The same
code of $\text{fold}_{S}$ will work for all those data types, as
long as we have the recursion scheme $S$ and the corresponding lifting
function (\lstinline!fmapR! in the Scala code shown above, or $^{\uparrow S^{A,\bullet}}$
in the code notation).

To illustrate the general applicability of $\text{fold}_{S}$ to different
data types, let us implement a \lstinline!printLaTeX! function for
ordinary lists, for non-empty lists, and for rose trees (Section~\ref{subsec:Rose-trees}).

Scala\textsf{'}s standard \lstinline!List! type and the corresponding recursion
scheme $S$ may be defined by:
\[
\text{List}^{A}\triangleq\bbnum 1+A\times\text{List}^{A}\quad,\quad\quad S^{A,R}\triangleq\bbnum 1+A\times R\quad,\quad\quad\text{List}^{A}\triangleq S^{A,\text{List}^{A}}\quad.
\]

A non-empty list type (\lstinline!NEL!) and the corresponding recursion
scheme $S^{A,R}$ may be defined by:
\[
\text{NEL}^{A}\triangleq A+A\times\text{NEL}^{A}\quad,\quad\quad S^{A,R}\triangleq A+A\times R\quad,\quad\quad\text{NEL}^{A}\triangleq S^{A,\text{NEL}^{A}}\quad.
\]

A rose tree (\lstinline!TreeN!) and the corresponding recursion scheme
$S^{A,R}$ may be defined by:
\[
\text{TreeN}^{A}\triangleq A+\text{NEL}^{\text{TreeN}^{A}}\quad,\quad\quad S^{A,R}\triangleq A+\text{NEL}^{R}\quad,\quad\quad\text{TreeN}^{A}\triangleq S^{A,\text{TreeN}^{A}}\quad.
\]
For rose trees, the recursion scheme $S$ is itself a recursively
defined type because it uses the non-empty list (NEL). This is not
a problem: $S$ is still polynomial, which guarantees that any value
of type $S^{A,R}$ contains a finite number of elements of types $A$
and $R$. So, any value of type \lstinline!TreeN[A]! will contain
a finite number of values of type $A$, assuring that the folding
operation will terminate.

To avoid repetitive code, let us define all three data types (\lstinline!List!,
\lstinline!NEL!, and \lstinline!TreeN!) at once through a universal
recursive class \lstinline!Fix! that takes the recursion scheme $S$
as a type parameter:
\begin{lstlisting}
type S1[A, R] = Option[(A, R)]             // For List.
type S2[A, R] = Either[A, (A, R)]          // For NEL.
type S3[A, R] = Either[A, NEL[R]]          // For TreeN.
final case class Fix[S[_, _], A](unfix: S[A, Fix[S, A]])
\end{lstlisting}
Using the class \lstinline!Fix!, we may write the equivalent definitions
\lstinline!List[A]! $\cong$ \lstinline!Fix[S1, A]!, \lstinline!NEL[A]!
$\cong$ \lstinline!Fix[S2, A]!, and \lstinline!TreeN[A]! $\cong$
\lstinline!Fix[S3, A]!. Some example values of these types are:
\begin{lstlisting}
val x1 = Fix[S1, Int](Some((1, Fix[S1, Int](Some((2, Fix[S1, Int](Some((3, Fix[S1, Int](None))))))))))  // Equivalent to List(1, 2, 3) .
val x2 = Fix[S2, Int](Right((1, Fix[S2, Int](Right((2, Fix[S2, Int](Left(3))))))))  // NEL(1, 2, 3) .
val x3 = Fix[S3, Int](Right(NEL(Fix[S3, Int](Right(NEL(Fix[S3, Int](Left(10)), Fix[S3, Int](Right(NEL(Fix[S3, Int](Left(20)), Fix[S3, Int](Left(30)))))))), Fix[S3, Int](Left(40)))))
\end{lstlisting}

In practice, defining data structures via \lstinline!Fix! is both
inconvenient and inefficient. We show these definitions only to clarify
how one can generalize \textsf{``}location-aware\textsf{''} folding operations to
arbitrary recursive data types. (For simplicity, the definition of
\lstinline!S3! uses \lstinline!NEL! rather than \lstinline!Fix[S2, A]!.)

To make the code of $\text{fold}_{S}$ general, we add a functor typeclass
constraint to $S^{A,\bullet}$:
\begin{lstlisting}
def fold[A, Z, S[_,_]](f: S[A, Z] => Z)(t: Fix[S, A])(implicit fs: Functor[S[A, *]]): Z =
  f(t.unfix.map(fold(f)))
\end{lstlisting}
Below we will assume that the appropriate functor instances are defined
for \lstinline!S1!, \lstinline!S2!, and \lstinline!S3!.

It remains to implement functions with type signatures \lstinline!S[A, String] => String!:
\begin{lstlisting}
def toLaTeX1[A]: S1[A, String] => String = {
  case None                 => "Nil"
  case Some((head, tail))   => head.toString + ", " + tail
}
def toLaTeX2[A]: S2[A, String] => String = {
  case Left(a)               => a.toString
  case Right((head, tail))   => head.toString + ", " + tail
}
def toLaTeX3[A]: S3[A, String] => String = {
  case Left(a)      => a.toString
  case Right(nel)   => "[ " + nel.mkString(" ") + " ]" // Assume mkString() is defined for NEL.
}
\end{lstlisting}
We can now use $\text{fold}_{S}$ to convert some data structures
to a \LaTeX{} form:
\begin{lstlisting}
def listToLaTeX[A](t: Fix[S1, A]): String = "[ " + fold[A, String, S1](toLaTeX1)(t) + " ]"
def nelToLaTeX[A](t: Fix[S2, A]): String = "[ " + fold[A, String, S2](toLaTeX2)(t) + " ]"
def treeNToLaTeX[A](t: Fix[S3, A]): String = "\\Tree" + fold[A, String, S3](toLaTeX3)(t)

scala> listToLaTeX(x1)
res1: String = [ 1, 2, 3, Nil ]

scala> nelToLaTeX(x2)
res2: String = [ 1, 2, 3 ]

scala> treeNToLaTeX(x3)
res3: String = \Tree[ [ 10 [ 20 30 ] ] 40 ]
\end{lstlisting}

The definition of $\text{fold}_{S}$ calls itself recursively under
the lifting ($\overline{\text{fold}_{S}(f)}^{\uparrow S^{A,\bullet}}$).
Is it guaranteed that this recursion terminates? The only way it can
terminate is when the lifting $f^{\uparrow S^{A,\bullet}}$ does not
\emph{always} call the function $f$. This will happen if $S^{A,R}$
is a disjunctive type with some parts that do not depend on $R$.
This is indeed the case for lists ($S^{A,R}\triangleq\bbnum 1+A\times R$
or $S^{A,R}\triangleq A+A\times R$) and trees ($S^{A,R}\triangleq A+R\times R$).
Applying $f^{\uparrow S^{A,\bullet}}$ to values of $R$-independent
types will be an identity function; it will not actually call $f$.
This will be the base case of the recursion.

It remains to assure that the recursion reaches the base case with
every value of type $L^{A}$. That is, no values of type $L^{A}$
should cause an infinite loop in $\text{fold}_{S}$. A simple example
where $\text{fold}_{S}$ enters an infinite loop is the recursion
scheme $S^{A,R}\triangleq A+(\bbnum 1\rightarrow R)$. This $S$ is
non-polynomial due to the function type $\bbnum 1\rightarrow R$,
which delays the evaluation of a value of type $R$. This allows us
to implement a well-defined, finite value \lstinline!x: L[A]! which
refers to itself under the delayed evaluation:
\begin{lstlisting}
type S[A, R] = Either[A, Unit => R]
final case class Looping[A](run: S[A, Looping[A]])
lazy val x: Looping[Int] = Looping(Right(_ => x))

scala> x.run.right.get(())                      // No stack overflows.
res0: Looping[Int] = Looping(Right($Lambda$1123/1058984040@753aca85)) 

scala> x.run.right.get(()).run.right.get(())    // This is again the same value:
res1: Looping[Int] = Looping(Right($Lambda$1123/1058984040@753aca85))
\end{lstlisting}
Trying to compute \lstinline!fold(f)(x)! with any \lstinline!f!
will result in an infinite loop.

It seems that we need to restrict recursion schemes $S$ to \emph{polynomial}
bifunctors. Such $S$ will define recursive polynomial data types
$L^{A}$ that are eager data structures (supporting no delayed evaluation
of stored values of type $A$). So, any value \lstinline!x! of type
$L^{A}$ will have to contain a finite number of values of type $A$,
and \lstinline!fold(f)(x)! is guaranteed to terminate for any terminating
function $f:S^{A,Z}\rightarrow Z$.

Instead of working with the general function $\text{fold}_{S}$ and
defining all recursive types via \lstinline!Fix!, it is more convenient
to implement and use specialized versions of $\text{fold}_{S}$ for
already defined recursive types. The general implementation of $\text{fold}_{S}$
in Eq.~(\ref{eq:fold-via-recursion-scheme-1}) can be translated
mechanically (e.g., using macros or code generators) into code specialized
for a given data type and recursion scheme.

For instance, while the type \lstinline!TreeN[A]! is equivalent to
\lstinline!Fix[S3[A, *]]! shown above, it is easier to work with
\lstinline!TreeN!. The specialized version of $\text{fold}_{S}$
for \lstinline!TreeN! has the type signature:
\begin{lstlisting}
def foldTreeN[A, Z](f: S3[A, Z] => Z): TreeN[A] => Z = ???
\end{lstlisting}
The general definition of $\text{fold}_{S}$ in Eq.~(\ref{eq:fold-via-recursion-scheme-1})
shows us how to write the code of \lstinline!foldTreeN!: 
\begin{lstlisting}
def foldTreeN[A, Z](f: Either[A, NEL[Z]] => Z): TreeN[A] => Z = {
  case Leaf(a)      => f(Left(a))
  case Branch(ts)   => f(Right(ts.map(foldTreeN(f))))
}
\end{lstlisting}
Then we can implement \lstinline!printLaTeX! for \lstinline!TreeN!
like this:
\begin{lstlisting}
def printLaTeX[A](tree: TreeN[A]): String = "\\Tree" + foldTreeN[A, String](toLaTeX3)(tree)
\end{lstlisting}

Another simple example of an aggregation operation that cannot be
expressed as a traversal is the task of determining the maximum branching
number of a given rose tree. The function \lstinline!foldTreeN! now
allows us to implement that:
\begin{lstlisting}
def maxBranching[A]: TreeN[A] => Int = foldTreeN[A, Int] {
  case Left(_)      => 0
  case Right(nel)   => math.max(nel.max, nel.length)
// Assuming that we implemented `max` and `length` methods for NEL.
}

scala> maxBranching(x3)
res2: Int = 2
\end{lstlisting}


\subsection{Recursion schemes. II. Unfolding operations}

A folding operation converts a collection to a single value. The opposite
operation is \textsf{``}unfolding\textsf{''}: converting a single value into a collection.
By reversing the direction of certain function arrows in the type
signature of $\text{fold}_{S}$, we can define a general \textsf{``}unfolding\textsf{''}
method that uses an arbitrary recursion scheme $S$ and an arbitrary
function of type $Z\rightarrow S^{A,Z}$:
\begin{equation}
\text{unfold}_{S}:(Z\rightarrow S^{A,Z})\rightarrow Z\rightarrow L^{A}\quad,\quad\text{unfold}_{S}(f)\triangleq f\bef\overline{\text{unfold}_{S}(f)}^{\uparrow S^{A,\bullet}}\quad.\label{eq:unfold-via-recursion-scheme}
\end{equation}
Section~\ref{sec:ch2Converting-a-single} showed an unfolding operation
for sequences: starting from an initial value, a function is applied
repeatedly to compute further elements of the sequence. The \lstinline!unfold!
operation generalizes that computation to an arbitrary recursive type
constructor $L$ whose recursion scheme $S$ is given. 

To get more intuition, we look at some examples using \lstinline!unfold!
with lists and binary trees.

\subsubsection{Example \label{subsec:Example-unfold-list}\ref{subsec:Example-unfold-list}\index{examples (with code)}}

Use \lstinline!unfold! to create a \lstinline!List! of consecutive
powers of $2$ up to a given value $n$: 
\begin{lstlisting}
type S[A, R] = ???
type Z = ???

def unfoldList[A, Z](f: Z => S[A, Z])(init: Z): List[A] = ???

def powersOf2UpTo(n: Long): List[Long] = unfoldList(???)(???)

scala> powersOf2UpTo(1000)
res0: List[Long] = List(1, 2, 4, 8, 16, 32, 64, 128, 256, 512)
\end{lstlisting}


\subparagraph{Solution}

The recursion scheme for \lstinline!List! is $S^{A,R}\triangleq\bbnum 1+A\times R$.
Let us specialize the code of \lstinline!unfold! from Eq.~(\ref{eq:unfold-via-recursion-scheme})
to the type \lstinline!List[A]!:
\begin{lstlisting}
type S[A, R] = Option[(A, R)]
def unfoldList[A, Z](f: Z => S[A, Z])(init: Z): List[A] = f(init) match {
  case None           => Nil
  case Some((a, z))   => a :: unfoldList(f)(z)
}
\end{lstlisting}

We now need to determine a suitable type $Z$ and a suitable function
$f:Z\rightarrow\bbnum 1+A\times Z$ so that the unfolding would produce
the required sequence. If we are in the middle of unfolding, we need
to produce the remaining portion of the list, say, $\left[128,256,512\right]$,
given only a current value of type $Z$. We can do that if the current
value of type $Z$ is $128$ (the smallest remaining power of $2$).
So, let us choose $Z\triangleq$ \lstinline!Long! to represent the
smallest remaining power of $2$. The type $A$ will be also \lstinline!Long!. 

How can we implement the function $f$? It should return \lstinline!None!
(denoted by $1+\bbnum 0$) when the list is finished. When we are
in the middle of generating the list, the call $f(z)$ should return
$\bbnum 0+a\times z^{\prime}$ with some values $a$ and $z^{\prime}$.
The value $a$ must be the new element of the list. The value $z^{\prime}$
will be passed to the next call of $f$. Since the next element must
be twice the previous one, we must have $a=z$ and $z^{\prime}=z*2$.
So, the code of $f$ is:
\begin{lstlisting}
def f(n: Long): Long => Option[(Long, Long)] =
  { z => if (z >= n) None else Some((z, z * 2)) }
\end{lstlisting}
Note that the code of $f$ is \emph{not} recursive, and the value
$n$ is captured inside the nameless function returned by $f(n)$.
We can now complete the solution:
\begin{lstlisting}
def powersOf2UpTo(n: Long): List[Long] = unfoldList(f(n))(1)
\end{lstlisting}


\subsubsection{Example \label{subsec:Example-unfold-tree}\ref{subsec:Example-unfold-tree}}

Implement \lstinline!unfoldT2! for the data type \lstinline!T2[A]!
from Section~\ref{subsec:Recursion-schemes.-folding}. Use it to
create \textsf{``}full\textsf{''} binary trees of given depth, e.g., {\tiny{}\Tree[ [ 0 1 ] [ 2 3 ] ]}
(depth $2$) and {\tiny{}\Tree[ [ [ 0 1 ] [ 2 3 ] ] [ [ 4 5 ] [ 6 7 ] ] ]}
(depth $3$).

\subparagraph{Solution}

We adapt the general code in Eq.~(\ref{eq:unfold-via-recursion-scheme})
to obtain the code of \lstinline!unfoldT2!:
\begin{lstlisting}
type S[A, R] = Either[A, (R, R)]
def unfoldT2[A, Z](f: Z => S[A, Z])(init: Z): T2[A] = f(init) match {
  case Left(a)           => Leaf(a)
  case Right((z1, z2))   => Branch(unfoldT2(f)(z1), unfoldT2(f)(z2))
}
\end{lstlisting}
We plan to implement the function \lstinline!fullBinaryTree! like
this:
\begin{lstlisting}
def fullBinaryTree(n: Int): T2[Int] = {
  type Z = ???
  val init: Z = ???
  val f: Z => Either[Int, (Z, Z)] = ???
  unfoldT2[Int, Z](f)(init)
}
\end{lstlisting}
The next step is to choose a suitable type \lstinline!Z! such that
the unfolding procedure can generate trees of the required shape.
To figure out what \lstinline!Z! must be, we need to consider an
intermediate step that generates a subtree at some point in the middle
of \textsf{``}unfolding\textsf{''}. For the tree of depth $3$ as shown above, an
example of a subtree in the middle is {\tiny{}\Tree[ 4 5 ]}. This
subtree must be computed as \lstinline!unfoldT2(f)(z)! with some
\lstinline!z! of type \lstinline!Z!. The information needed for
this computation is the initial value ($4$) and the total number
($2$) of the subtree\textsf{'}s leaves. So, the type \lstinline!Z! needs
to contain two integers: the value of the first leaf and the size
of the remaining subtree (which will always be a power of $2$).
\begin{lstlisting}[mathescape=true]
type Z = (Int, Int)        // (k, m) where k is the first leaf$\color{dkgreen}\texttt{'}$s value and m is the subtree size.
val init: Z = (0, 1 << n)  // The size of the entire tree is 2 to the power n.
\end{lstlisting}
The initial value for the entire tree shown above will be \lstinline!(0, 8)!.

Next, we figure out the function \lstinline!f!. When \lstinline!f!
is applied to a value \lstinline!(k, m)! of type \lstinline!Z!,
it should generate the corresponding subtree. If the subtree size
\lstinline!m! is $1$, the return value is \lstinline!Left(k)!.
Otherwise, the return value should give two new values of type \lstinline!Z!
corresponding to the two subtrees one level deeper. Those two subtrees
have sizes \lstinline!m / 2! and initial values \lstinline!k! and
\lstinline!m / 2 + k!. So, the code of \lstinline!f! is:
\begin{lstlisting}
val f: Z => Either[Int, (Z, Z)] = {
  case (k, m) if m == 1   => Left(k)
  case (k, m)             => Right(((k, m / 2), (m / 2 + k, m / 2)))
}
\end{lstlisting}

This completes the implementation of \lstinline!fullBinaryTree!.
To test the resulting code, compute a full tree of depth $2$, which
we expect to be~{\tiny{}\Tree[ [ 0 1 ] [ 2 3 ] ]}:
\begin{lstlisting}
scala> fullBinaryTree(2)
res0: T2[Int] = Branch(Branch(Leaf(0), Leaf(1)), Branch(Leaf(2), Leaf(3)))
\end{lstlisting}


\subsubsection{Example \label{subsec:Example-unfold-tree-evenodd}\ref{subsec:Example-unfold-tree-evenodd}}

Use \lstinline!unfoldT2! from Example~\ref{subsec:Example-unfold-tree}
to implement the function \lstinline!evenOdd(n)! that generates binary
trees of type \lstinline!T2[Int]! where the leaves have descending
numbers from \lstinline!n! to \lstinline!0!, but all odd numbers
are on the left and all even numbers on the right. For example, \lstinline!evenOdd(3)!
should generate the tree {\tiny{}\Tree[ 3 [ [ 1 0 ] 2 ] ]} and \lstinline!evenOdd(4)!
should give the tree {\tiny{}\Tree[ [ 3 [ [ 1 0 ] 2 ] ] 4 ]} .

\subparagraph{Solution}

We plan to implement \lstinline!evenOdd! like this:
\begin{lstlisting}
def evenOdd(n: Int): T2[Int] = {
  type Z = ???
  val init: Z = ???
  val f: Z => Either[Int, (Z, Z)] = ???
  unfoldT2[Int, Z](f)(init)
}
\end{lstlisting}
The examples with \lstinline!evenOdd(3)! and \lstinline!evenOdd(4)!
suggest that \lstinline!evenOdd(n)! is a tree containing a leaf with
value \lstinline!n! and a subtree \lstinline!evenOdd(n - 1)!. Can
we use the integer \lstinline!n! as the initial value for unfolding? 

To figure this out, consider an intermediate stage of the unfolding
process where \lstinline!unfoldT2! will apply the function \lstinline!f!
to some value \lstinline!z! of type \lstinline!Z!. If \lstinline!f(z) == Left(k)!,
we will get a \lstinline!Leaf! with value \lstinline!k!. The other
possibility is \lstinline!f(z) == Right((z1, z2))!, where \lstinline!z1!
and \lstinline!z2! are some values of type \lstinline!Z!. This will
generate a \lstinline!Branch! with two subtrees. The function \lstinline!unfoldT2!
will then apply \lstinline!f! to \lstinline!z1! and \lstinline!z2!
in order to create the left and the right subtrees. The difference
between those subtrees must come entirely from the difference between
the values \lstinline!z1! and \lstinline!z2!. 

In our case, we need to make a \lstinline!Leaf! either at the left
or at the right depending on whether the initial leaf value is odd
or even. The function \lstinline!f! must know whether \lstinline!f(z)!
should return a subtree or a \lstinline!Leaf!. This information can
only come from the value \lstinline!z!. So, the type \lstinline!Z!
must contain a \lstinline!Boolean! flag saying whether we need a
\lstinline!Leaf! at the current place. For clarity, let us define
the type \lstinline!Z! as a case class:
\begin{lstlisting}
final case class Z(startAt: Int, makeLeaf: Boolean)
val init = Z(startAt = n, makeLeaf = false)
\end{lstlisting}
When \lstinline!makeLeaf! is \lstinline!true!, we must create a
\lstinline!Leaf!, so \lstinline!f! should return a \lstinline!Left()!.
Otherwise \lstinline!f! should return a \lstinline!Right((z1, z2))!,
where \lstinline!z1! and \lstinline!z2! should set \lstinline!makeLeaf!
depending on \lstinline!startAt! being odd or even. Looking at the
examples, we find that \lstinline!f! must also return a \lstinline!Left()!
when \lstinline!startAt == 0!. So, the code of \lstinline!f! is:
\begin{lstlisting}
val f: Z => Either[Int, (Z, Z)] = {
  case Z(n, false) if n > 0 && n % 2 == 0  =>
    Right((Z(n - 1, false), Z(n, true)))
  case Z(n, false) if n > 0 && n % 2 == 1  =>
    Right((Z(n, true), Z(n - 1, false)))
  case Z(n, _)                             => Left(n) // Make a leaf when n == 0 or makeLeaf == true.
}
\end{lstlisting}
The implementation of \lstinline!evenOdd! is now complete. To test:

\begin{wrapfigure}{l}{0.84\columnwidth}%
\vspace{-0.95\baselineskip}
\begin{lstlisting}
scala> evenOdd(3)
res0: T2[Int] = Branch(Leaf(3), Branch(Branch(Leaf(1), Leaf(0)), Leaf(2)))
\end{lstlisting}

\vspace{0\baselineskip}
\end{wrapfigure}%

\noindent \vspace{0.3\baselineskip}
 {\tiny{}\Tree[ 3 [ [ 1 0 ] 2 ] ]}\\

\noindent $\square$

The kind of reasoning shown in Examples~\ref{subsec:Example-unfold-list}\textendash \ref{subsec:Example-unfold-tree-evenodd}
is known as reasoning by \textbf{co-induction}\index{co-induction}.
It is related to mathematical induction but is significantly different
from the reasoning required to write the code for a folding operation
(which is directly modeled on induction). Note that \lstinline!unfold(f)(z)!
will call itself whenever the value $f(z)$ of type $S^{A,Z}$ contains
additional values of type $Z$. The programmer must carefully choose
a suitable type $Z$ and a suitable function $f$ such that \lstinline!unfold(f)(z)!
stops the recursion at the required places. For instance, if $S^{A,Z}\triangleq A+Z\times Z$,
the function $f$ must sometimes return a value of type $A+\bbnum 0$
to stop the unfolding. One could say that the base cases in co-induction
are not at the beginning of the computation but \textsf{``}in the future\textsf{''}. 

Is the recursion guaranteed to stop while evaluating \lstinline!unfold!?
The type signature $f:Z\rightarrow S^{A,Z}$ itself does not guarantee
that $f(z)$ returns values that will stop the unfolding at the right
places. If $S^{A,Z}\triangleq A+Z\times Z$ and $f(z)$ always returns
values of type $\bbnum 0+Z\times Z$ (for example, $f(z)\triangleq\bbnum 0+z\times z$),
the unfolding operation \lstinline!unfold(f)! will enter an infinite
loop trying to construct a tree of infinite size. This will, of course,
fail since data structures in a computer cannot have infinite size.

Unfolding will always terminate if we use a data type that \emph{delays}
the evaluation of its recursively defined parts. Those parts will
be computed only on demand. To obtain further data, the code needs
to call certain functions. Data types of this kind are sometimes called
\textsf{``}infinite\textsf{''},\index{infinite data types} which is misleading since
only a finite amount of data is ever stored in memory.

As an example, consider the recursion scheme $S^{A,R}\triangleq A+(\bbnum 1\rightarrow R\times R)$.
The corresponding data type $L^{A}\triangleq S^{A,L^{A}}$ is a binary
tree whose branches are evaluated on demand (but leaves are evaluated
eagerly). This data structure supports unfolding with \emph{any} function
$f:Z\rightarrow S^{A,Z}$ because the recursive evaluation of $f$
at the branches is always delayed. For instance, \lstinline!unfold!
can generate a value of type $L^{A}$ whose tree structure has the
form {\tiny{}\Tree[ 1 [ 2  [3 ... ] ] ]} with unbounded size:\vspace{0.6\baselineskip}
\begin{lstlisting}
type S[A, R] = Either[A, Unit => (R, R)]

sealed trait UT[A]      // A tree with on-call evaluation of branches and eager evaluation of leaves.
case class ULeaf[A](a: A)                          extends UT[A]
case class UBranch[A](run: Unit => (UT[A], UT[A])) extends UT[A] // Call run(()) to get the branches.

def unfoldUT[A, Z](f: Z => S[A, Z])(init: Z): UT[A] = f(init) match {
  case Left(a)       => ULeaf(a)
  case Right(func)   => UBranch { _ =>     // It is important to delay the evaluation of func(()).
    val (z1, z2) = func(())                // Force the evaluation of branches at this level.
    (unfoldUT(f)(z1), unfoldUT(f)(z2))     // `unfoldUT` will delay the evaluation of further branches.
  }
}

val tree1toInf = unfoldUT[Int, (Int, Boolean)] { case (z, makeLeaf) =>
  if (makeLeaf) Left(z) else Right(_ => ((z + 1, true), (z + 1, false)))
}((0, false))
\end{lstlisting}
The value \lstinline!tree1toInf! is finite but can compute a tree
of unbounded depth. To visualize \lstinline!tree1toInf!, we write
a function that converts \lstinline!UT[A]! to \lstinline!T2[A]!
by stopping at a given maximum depth. The unevaluated parts of the
tree will be marked with a value called \lstinline!default!:
\begin{lstlisting}
def toT2[A](maxDepth: Int, default: A): UT[A] => T2[A] = {
  case ULeaf(a)        => Leaf(a)e
  case UBranch(func)   => if (maxDepth == 0) Leaf(default) else {
    val (z1, z2) = func(())
    Branch(toT2(maxDepth - 1, default)(z1), toT2(maxDepth - 1, default)(z2))
  }
}
\end{lstlisting}
To test this code, let us truncate the infinite structure \lstinline!tree1toInf!
at depth $4$. The result is a finite tree of type \lstinline!T2[Int]!,
where the unevaluated part of the tree is shown as \textsf{``}\lstinline!-1!\textsf{''}: 

\begin{wrapfigure}{l}{0.82\columnwidth}%
\vspace{-0.95\baselineskip}
\begin{lstlisting}
scala> toT2(maxDepth = 3, default = -1)(tree1toInf)
res0: T2[Int] = Branch(Leaf(1), Branch(Leaf(2), Branch(Leaf(3), Branch(Leaf(4), Leaf(-1)))))
\end{lstlisting}

\vspace{-0.5\baselineskip}
\end{wrapfigure}%

\noindent ~{\tiny{}\Tree[ 1 [ 2  [3 [ 4 -1 ] ] ] ]}

\subsection{Recursion schemes. III. Traversing operations}

Folding with a recursion scheme ($\text{fold}_{S}$) allows us to
implement operations such as \lstinline!printLaTeXSubtree! (Section~\ref{subsec:Recursion-schemes.-folding})
that cannot be expressed via ordinary \lstinline!fold! or \lstinline!traverse!
functions. Another operation not expressible via \lstinline!traverse!
is \lstinline!zipWithDepth!, which we implemented in Section~\ref{subsec:Tasks-not-implementable-via-traverse}
through custom code. We will now implement \lstinline!zipWithDepth!
via a more general traversal operation ($\text{trav}_{S}$) parameterized
by an arbitrary recursion scheme $S$ and an arbitrary functor $F$
(not necessarily applicative!).

To figure out the type signature of $\text{trav}_{S}$, consider the
relationship between \lstinline!foldMap!, $\text{fold}_{S}$, and
the ordinary \lstinline!traverse! (denoted $\text{trav}_{L}$):
\begin{align*}
 & \text{foldMap}_{L}(f^{:A\rightarrow Z}):L^{A}\rightarrow Z\quad,\quad\quad\text{trav}_{L}(f^{:A\rightarrow F^{B}}):L^{A}\rightarrow F^{L^{B}}\quad,\\
 & \text{fold}_{S}(f^{:S^{A,Z}\rightarrow Z}):L^{A}\rightarrow Z\quad,\quad\quad\text{trav}_{S}(f^{:???}):L^{A}\rightarrow F^{L^{B}}\quad.
\end{align*}
We recover \lstinline!foldMap! from \lstinline!traverse! by setting
the applicative functor $F$ as $F^{B}\triangleq Z$. So, we expect
to obtain the type signature of $\text{trav}_{S}$ from the type signature
of $\text{fold}_{S}$ if we replace $Z$ by $F^{L^{B}}$. The first
argument $f$ of $\text{trav}_{S}$ will then have the type $S^{A,F^{L^{B}}}\rightarrow F^{L^{B}}$:
\[
\text{trav}_{S}:(S^{A,F^{L^{B}}}\rightarrow F^{L^{B}})\rightarrow L^{A}\rightarrow F^{L^{B}}\quad.
\]
To implement this method, begin with the type equivalence $L^{A}\cong S^{A,L^{A}}$.
We can apply $\text{trav}_{S}(f)$ recursively to the values of type
$L^{A}$ stored inside $S^{A,L^{A}}$ and obtain a value of type $S^{A,F^{L^{B}}}$:
\[
\big(s^{:S^{A,L^{A}}}\triangleright\,\overline{\text{trav}_{S}(f)}^{\uparrow S^{A,\bullet}}\big):S^{A,F^{L^{B}}}\quad.
\]
It remains to apply $f$ to the last obtained value. This completes
the code of $\text{trav}_{S}$: 
\[
\text{trav}_{S}\big(f^{:S^{A,F^{L^{B}}}\rightarrow F^{L^{B}}}\big)\triangleq\overline{\text{trav}_{S}(f)}^{\uparrow S^{A,\bullet}}\bef f\quad.
\]

The method $\text{trav}_{S}$ works in the same way for all recursion
schemes $S$ and for all type constructors $F$. This makes $\text{trav}_{S}$
powerful but hard to use because we need to work with data structures
defined via the \lstinline!Fix! type constructor. It is easier to
use a specialized version of $\text{trav}_{S}$ for the data structure
at hand, similarly to what we did in the previous sections for folding
and unfolding. 

To illustrate this, let us implement \lstinline!zipWithDepth! via
$\text{trav}_{S}$ with a binary tree recursion scheme. We will use
the \lstinline!State! monad as the functor $F$:
\begin{lstlisting}
final case class St[A](run: Int => (A, Int))  { // A State monad with internal state of type Int.
  import io.chymyst.ch.implement                // Derive these methods automatically from types.
  def flatMap[B](f: A => St[B]): St[B] = implement
  def map[B](f: A => B): St[B] = implement
}
def incrementAndGet: St[Int] = St(s => (s + 1, s + 1))  // Increment the current state value.
def get: St[Int] = St(s => (s, s))             // Fetch the current state value.
def set(s: Int): St[Unit] = St(_ => ((), s))   // Set the state, ignore previous state value.
\end{lstlisting}
Next, define the recursion scheme $S^{A,R}$ and a specialized version
of $\text{trav}_{S}$ for trees of type \lstinline!T2[A]!:
\begin{lstlisting}
type S[A, R] = Either[A, (R, R)]          // Recursion scheme for T2[A].
def travT2[A, B, F[_]](f: S[A, F[T2[B]]] => F[T2[B]]): T2[A] => F[T2[B]] = {
  case Leaf(a)        => f(Left(a))
  case Branch(l, r)   => f(Right((travT2(f)(l), travT2(f)(r))))
}
\end{lstlisting}
It remains to apply \lstinline!travT2! to a suitable function \lstinline!f!.
The value of the internal state will represent the current depth of
the tree element. We need to increment the depth whenever we find
a branch and then traverse the two subtrees starting from the same
depth value. The code is:
\begin{lstlisting}
def zipWithDepth[A](tree: T2[A]): T2[(A, Int)] = travS[A, (A, Int), St] {
  case Left(a) => for { s <- get } yield Leaf((a, s))   // Put the current depth into the Leaf value.
  case Right((l, r))   => for {
    s <- incrementAndGet   // Read the current depth after incrementing it.
    x <- l                 // Traverse the left branch starting from depth `s`.
    _ <- set(s)            // Set the same initial depth `s` for traversing the right branch.       
    y <- r                 // Traverse the right branch.
  } yield Branch(x, y)
}(tree).run(0)._1
\end{lstlisting}
To test the code, apply \lstinline!zipWithDepth! to a sample tree
\lstinline!t2! used earlier in Sections~\ref{subsec:Decorating-a-tree-breadth-first-traversal}
and~\ref{subsec:Tasks-not-implementable-via-traverse}:

\begin{wrapfigure}{l}{0.78\columnwidth}%
\vspace{-0.75\baselineskip}
\begin{lstlisting}
scala> zipWithDepth(t2)
res0: T2[(Int, Int)] = Branch(Leaf((8, 1)), Branch(Branch(Leaf((3, 3)), Leaf((5, 3))), Leaf((4, 2))))
\end{lstlisting}

\vspace{-0.5\baselineskip}
\end{wrapfigure}%

\noindent ~{\tiny{}\Tree[ (8,1) [ [ (3,3) (5,3) ] (4,2) ] ]}

\noindent ~

Here we need to use the \lstinline!State! monad\textsf{'}s method \lstinline!set(s)!
with a value \lstinline!s! obtained from a previous monadic computation.
So, the code involves $\text{trav}_{S}(f)$ with a function $f:S^{A,F^{L^{B}}}\rightarrow F^{L^{B}}$
whose $F$-effect is \emph{not} equivalent to an applicative functor\textsf{'}s
effect (which cannot depend on a previously computed value). We can
see that the recursion scheme-based traversal ($\text{trav}_{S}$)
is more powerful than the plain traversal, $\text{trav}_{L}(f^{:A\rightarrow F^{B}})$,
that may only use $F$\textsf{'}s applicative functor methods.

\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-traversables-7-1}\ref{subsec:Exercise-traversables-7-1}}

Implement \lstinline!foldMap! and \lstinline!traverse! for the following
type constructors:

\textbf{(a)} $F^{A}\triangleq\text{Int}+A+A\times A\quad.$

\textbf{(b)} $F^{A}\triangleq(\text{String}+A)\times(\bbnum 1+A\times A)\quad.$

\textbf{(c)} $F^{A}\triangleq A+A\times F^{A}$ (a recursive definition
equivalent to \lstinline!NEL!, the non-empty list).

\textbf{(d)} $F^{A}\triangleq\bbnum 1+A+A\times A\times F^{A}$ (a
recursive definition).

\textbf{(e)} $F^{A}\triangleq(\bbnum 1+F^{A})\times(\bbnum 1+A\times F^{A})$
(a recursive definition).

\subsubsection{Exercise \label{subsec:Exercise-traversables-7}\ref{subsec:Exercise-traversables-7}}

For the binary tree \lstinline!T2! (Section~\ref{subsec:Recursion-schemes.-folding}),
implement a \lstinline!Traversable! instance for \emph{right-to-left}
depth-first traversal order. Use that \lstinline!Traversable! instance
to implement \lstinline!zipWithIndex! for \lstinline!T2!. Verify
via tests that \lstinline!zipWithIndex! transforms the tree {\tiny{} \Tree[ [ 8 [ 3 5 ] ] 4 ] }
into {\tiny{} \Tree[ [ (8,3) [ (3,2) (5,1) ] ] (4,0) ] } .

\subsubsection{Exercise \label{subsec:Exercise-traversables-8}\ref{subsec:Exercise-traversables-8}}

Implement a \lstinline!Traversable! instance for the data type \lstinline!T3!
defined in Exercise~\ref{subsec:Exercise-applicative-I-1-1}.

\subsubsection{Exercise \label{subsec:Exercise-traversables-8-1}\ref{subsec:Exercise-traversables-8-1}}

Implement a \lstinline!Functor! instance for the data type \lstinline!TD!
defined in Section~\ref{subsec:Decorating-a-tree-breadth-first-traversal}.
Use \lstinline!TD!\textsf{'}s \lstinline!map! method to re-implement the
functions \lstinline!addLeft! and \lstinline!addRight! from that
section. Show that the new implementations are equivalent to the old
ones.

\subsubsection{Exercise \label{subsec:Exercise-traversables-12}\ref{subsec:Exercise-traversables-12}}

Use the specialized version of $\text{fold}_{S}$ for the binary tree
\lstinline!T2! (Section~\ref{subsec:Recursion-schemes.-folding})
to compute the maximum depth of a tree:
\begin{lstlisting}
def maxDepth[A](tree: T2[A]): Int = foldT2(???)(???)

scala> maxDepth(Branch(Leaf(0), Branch(Branch(Leaf(0), Leaf(0)), Leaf(0))))
res0: Int = 3
\end{lstlisting}


\subsubsection{Exercise \label{subsec:Exercise-traversables-11}\ref{subsec:Exercise-traversables-11}}

For the data type \lstinline!T3! defined in Exercise~\ref{subsec:Exercise-applicative-I-1-1},
write a recursion scheme and implement a specialized version of \lstinline!unfold!
as \lstinline!unfoldT3!. Using \lstinline!unfoldT3!, write a function
that generates ternary trees of the form {\tiny{}}{\tiny{} \Tree[.3  0  [.2 0   1   0  ]   0  ] }
starting from the given integer $n$ at the root.

\section{Laws and structure}

To study the laws of the folding and traversing operations, it helps
to choose simpler but equivalent versions of these operations. We
have seen four methods that implement folding operations: \lstinline!foldLeft!,
\lstinline!foldMap!, \lstinline!reduce!, and \lstinline!toList!.
It turns out that all those methods are equivalent when suitable naturality
laws hold.

\subsection{Equivalence of \texttt{reduce}, \texttt{foldLeft}, \texttt{foldMap},
and \texttt{toList}. Monoid morphisms\label{subsec:Equivalence-of-foldLeft,foldMap,reduce,and-toList}}

Scala\textsf{'}s standard \lstinline!reduce! method assumes a non-empty sequence
and will fail otherwise:
\begin{lstlisting}
def reduce[A](la: Seq[A])(red: (A, A) => A): A
\end{lstlisting}
For the purposes of this section, we will modify \lstinline!reduce!
to supply a default value for empty sequences:
\begin{lstlisting}
def reduceE[A](la: Seq[A])(default: A)(red: (A, A) => A): A =
if (la.isEmpty) default else reduce(la)(red)
\end{lstlisting}
Having a default value of type $A$ and a binary operation of type
$A\times A\rightarrow A$ suggest that $A$ may be a monoid (assuming
that the monoid laws hold, see Example~\ref{subsec:tc-Example-Monoids}).
So, we simplify the type signature of \lstinline!reduceE! using a
\lstinline!Monoid! typeclass constraint:
\begin{lstlisting}
def reduceE[M: Monoid](la: Seq[M]): M
\end{lstlisting}

After this change, let us compare the type signatures of the four
methods:
\begin{lstlisting}
def foldLeft[A, B](la: L[A])(init: B)(update: (B, A) => B): B
def foldMap[M: Monoid, A](f: A => M): L[A] => M
def reduceE[M: Monoid]: L[M] => M
def toList[A]: L[A] => List[A]
\end{lstlisting}
\begin{align*}
 & \text{foldLeft}_{L}:L^{A}\rightarrow B\rightarrow(B\times A\rightarrow B)\rightarrow B\quad,\\
 & \text{foldMap}_{L}:(A\rightarrow M)\rightarrow L^{A}\rightarrow M\quad,\\
 & \text{reduceE}_{L}:L^{M}\rightarrow M\quad,\quad\quad\text{toList}_{L}:L^{A}\rightarrow\text{List}^{A}\quad.
\end{align*}
We will now show that these four functions are equivalent, assuming
certain naturality laws.

The formulation of naturality laws is different for functions whose
type parameters have typeclass constraints. For example, consider
\lstinline!reduceE! whose type parameter $M$ is constrained to be
a monoid. The naturality law for functions $\phi$ with type signature
$\forall M.\,L^{M}\rightarrow M$ (but without the typeclass constraint)
is:
\[
\text{for all }f^{:M\rightarrow N}\quad:\quad\quad\phi^{:L^{M}\rightarrow M}\bef f=f^{\uparrow L}\bef\phi^{:L^{N}\rightarrow N}\quad.
\]
Does this law hold with $\phi=$ \lstinline!reduceE!? Since \lstinline!reduceE!
would be used with type parameters $M$ and $N$, those types have
to be monoids for the law to make sense. Also, it turns out that the
law must be used only with functions $f$ that satisfy certain conditions
with respect to the monoids $M$ and $N$. 

To see why, let us set $L=$ \lstinline!List!. We expect that \lstinline!List!\textsf{'}s
\lstinline!reduceE! method will satisfy the naturality law as long
as we formulate that law correctly. We now apply \lstinline!reduceE!
to an empty list and to a list with two elements of type $M$:
\[
\text{reduceE}\left(\left[\right]\right)=e_{M}\quad,\quad\quad\text{reduceE}\left(\left[x,y\right]\right)=x\oplus_{M}y\quad.
\]
Similar values are found when \lstinline!reduceE! is used with the
monoid $N$. Then we have:
\begin{align*}
 & \left[\right]\triangleright\text{reduceE}\triangleright f=f(e_{M})\quad,\quad\quad\left[\right]\triangleright f^{\uparrow L}\triangleright\text{reduceE}=\left[\right]\triangleright\text{reduceE}=e_{N}\quad,\\
 & \left[x,y\right]\triangleright\text{reduceE}\triangleright f=(x\oplus_{M}y)\triangleright f=f(x\oplus_{M}y)\quad,\\
 & \left[x,y\right]\triangleright f^{\uparrow L}\triangleright\text{reduceE}=\left[f(x),f(y)\right]\triangleright\text{reduceE}=f(x)\oplus_{N}f(y)\quad.
\end{align*}
It follows that the law holds for \lstinline!reduceE! only if the
function $f$ has the properties: 
\[
f(e_{M})=e_{N}\quad,\quad\quad f(x\oplus_{M}y)=f(x)\oplus_{N}f(y)\quad.
\]
These properties mean that $f:M\rightarrow N$ preserves the operations
of the monoids $M$ and $N$, in the sense that $f$ maps the empty
value $e_{M}$ to the empty value $e_{N}$ and $M$\textsf{'}s binary operation
to $N$\textsf{'}s. Functions with these properties are called \textsf{``}monoid morphisms\textsf{''}.

\subsubsection{Definition \label{subsec:Definition-monoid-morphism}\ref{subsec:Definition-monoid-morphism}}

For any two monoids $M$ and $N$, a function $f:M\rightarrow N$
is a \textbf{monoid morphism}\index{monoid morphism|textit} if $f$
satisfies the following laws:
\begin{align*}
{\color{greenunder}\text{identity law}:}\quad & f(e_{M})=e_{N}\quad,\\
{\color{greenunder}\text{composition law}:}\quad & f(x^{:M}\oplus_{M}y^{:M})=f(x)\oplus_{N}f(y)\quad.
\end{align*}

The \textbf{monoidal naturality law} \index{monoidal naturality law}of
\lstinline!reduceE! is then formulated as:
\begin{equation}
\text{reduceE}^{M}\bef f^{:M\rightarrow N}=f^{\uparrow L}\bef\text{reduceE}^{N}\quad.\label{eq:monoidal-naturality-law-of-reduceE}
\end{equation}
\[
\xymatrix{\xyScaleY{1.4pc}\xyScaleX{3.5pc}L^{M}\ar[r]\sp(0.5){\ \text{reduceE}^{M}}\ar[d]\sp(0.45){\,f^{\uparrow L}} & M\ar[d]\sp(0.45){\,f}\\
L^{N}\ar[r]\sp(0.5){~\text{reduceE}^{N}} & N
}
\]
Here the types $M$, $N$ are arbitrary monoids and $f:M\rightarrow N$
is an arbitrary monoid morphism between $M$ and $N$. 

The monoidal naturality law expresses a programmer\textsf{'}s expectation that
the code of \lstinline!reduceE[M]! should work in the same way for
every monoid \lstinline!M!. The code of \lstinline!reduceE! should
be fully parametric and may use the monoid \lstinline!M!\textsf{'}s methods
but may not, e.g., inspect the type parameter \lstinline!M! via run-time
reflection and make any decisions based on that. 

With these definitions, we can now give a precise formulation of the
following equivalences:

\subsubsection{Statement \label{subsec:Statement-foldleft-foldmap-equivalence}\ref{subsec:Statement-foldleft-foldmap-equivalence}}

\textbf{(a)} The functions \lstinline!foldMap! and \lstinline!reduceE!
are equivalent as long as \lstinline!foldMap! satisfies the naturality
law with respect to its type parameter $A$. In addition, Exercise~\ref{subsec:Exercise-traversables-laws-1-1}
will show that the monoidal naturality laws of \lstinline!foldMap!
and \lstinline!reduceE! are equivalent.

\textbf{(b)} The function \lstinline!foldLeft! is equivalent to a
new method we call \lstinline!foldFn! (\textsf{``}fold with function\textsf{''}):
\[
\text{foldFn}:L^{B\rightarrow B}\rightarrow B\rightarrow B\quad,
\]
as long as \lstinline!foldLeft! obeys the naturality law with respect
to its type parameter $A$.

\textbf{(c)} The functions \lstinline!foldFn! and \lstinline!reduceE!
are equivalent as long as \lstinline!foldFn! obeys the laws~(\ref{eq:foldFn-first-special-law})
and~(\ref{eq:foldFn-second-special-law}) shown below and \lstinline!reduceE!
obeys the monoidal naturality law. 

We will show in Statement~\ref{subsec:relational-property-for-foldFn}
below that the special laws~(\ref{eq:foldFn-first-special-law})
and~(\ref{eq:foldFn-second-special-law}) follow from parametricity.
So, those laws will hold automatically when the code of \lstinline!foldFn!
is fully parametric. Formulating those special laws allows us to prove
the equivalence of \lstinline!foldFn! and \lstinline!reduceE! without
assuming parametricity.

\subparagraph{Proof}

\textbf{(a)} The equivalence of \lstinline!foldMap! and \lstinline!reduceE!
can be derived from Statement~\ref{subsec:Statement-tr-equivalent-to-ftr}
if we assume that \lstinline!foldMap! satisfies the naturality law
with respect to the type parameter $A$. To prove that equivalence,
we just need to set $F^{A}\triangleq L^{A}$, $G^{B}\triangleq M$,
and $K^{B}\triangleq M$ in Statement~\ref{subsec:Statement-tr-equivalent-to-ftr}.

\textbf{(b)} We first reformulate \lstinline!foldLeft!\textsf{'}s type signature
in a form more similar to that of \lstinline!foldMap!. We change
the order of curried arguments in \lstinline!foldLeft! and also replace
the updater function of type $B\times A\rightarrow B$ by an equivalent
curried function of type $A\rightarrow B\rightarrow B$ (similarly
to what was done in Section~\ref{subsec:From-reduce-and-foldleft-to-foldmap}).
The result is a function we denote by \lstinline!fldl!:
\begin{align*}
 & \text{fldl}:(A\rightarrow B\rightarrow B)\rightarrow L^{A}\rightarrow B\rightarrow B\quad,\\
 & \text{fldl}\,(u^{:A\rightarrow B\rightarrow B})(p^{:L^{A}})(z^{:B})\triangleq\text{foldLeft}\,(p)(z)(\tilde{u})\quad,\\
 & \text{where we defined}:\quad\tilde{u}^{:B\times A\rightarrow B}\triangleq b^{:B}\times a^{:A}\rightarrow u\left(a\right)(b)\quad.
\end{align*}
The function \lstinline!fldl! is equivalent to \lstinline!foldLeft!
since we only replaced some types with equivalent ones. Setting $F^{A}\triangleq L^{A}$,
$G^{A}\triangleq B\rightarrow B$, and $K^{A}\triangleq B\rightarrow B$
in Statement~\ref{subsec:Statement-tr-equivalent-to-ftr}, we obtain
an equivalence between \lstinline!fldl! and \lstinline!foldFn!:
\[
\text{foldFn}:L^{B\rightarrow B}\rightarrow B\rightarrow B\quad,\quad\quad\text{foldFn}\,(p^{:L^{B\rightarrow B}})\triangleq\text{fldl}\,(\text{id}^{:(B\rightarrow B)\rightarrow B\rightarrow B})(p)\quad.
\]

\textbf{(c)} The type signatures of \lstinline!foldFn! and \lstinline!reduceE!
are similar except that \lstinline!foldFn! uses the type $B\rightarrow B$
where \lstinline!reduceE! uses an arbitrary monoid $M$ (having an
empty value $e_{M}$ and a binary operation $\oplus_{M}$). The type
of functions $B\rightarrow B$ is itself a monoid that we here denote
by $\text{MF}^{B}\triangleq B\rightarrow B$. The empty value of that
monoid is the identity function ($e_{_{\text{MF}^{B}}}=\text{id}^{:B\rightarrow B}$),
and the binary operation ($\ensuremath{\oplus}_{_{\text{MF}^{B}}}$)
is the \emph{forward} function composition: 
\[
p^{:B\rightarrow B}\oplus_{_{\text{MF}^{B}}}q^{:B\rightarrow B}\triangleq p\bef q\quad.
\]

To relate \lstinline!foldFn! and \lstinline!reduceE!, we need to
map an arbitrary monoid $M$ to the monoid $\text{MF}^{M}$:
\[
\text{inMF}:M\rightarrow\text{MF}^{M}\quad,\quad\quad\text{inMF}\left(m\right)\triangleq n^{:M}\rightarrow n\oplus_{M}m\quad.
\]
The code of \lstinline!inMF! is just the monoid $M$\textsf{'}s binary operation
($\oplus_{M}$) in a flipped and curried form:
\[
\text{inMF}:M\rightarrow M\rightarrow M\quad,\quad\quad\text{inMF}\triangleq m\rightarrow n\rightarrow n\oplus_{M}m\quad.
\]
In particular, for any value $m^{:M}$ we have, due to the monoid
$M$\textsf{'}s identity law: 
\begin{equation}
\text{inMF}\,(m)(e_{M})=e_{M}\oplus m=m\quad.\label{eq:identity-law-of-inMF}
\end{equation}
The function \lstinline!inMF! is a monoid morphism since the two
laws of Definition~\ref{subsec:Definition-monoid-morphism} hold:
\begin{align*}
 & \text{inMF}\,(e_{M})=n^{:M}\rightarrow n\oplus_{M}e_{M}=n^{:M}\rightarrow n=\text{id}^{:M\rightarrow M}=e_{_{\text{MF}^{M}}}\quad,\\
 & \text{inMF}\,(x\oplus_{M}y)=n^{:M}\rightarrow n\oplus_{M}x\oplus_{M}y\quad,\\
 & \text{inMF}\left(x\right)\oplus_{_{\text{MF}^{M}}}\text{inMF}\left(y\right)=\text{inMF}\left(x\right)\bef\text{inMF}\left(y\right)=(n\rightarrow n\oplus_{M}x)\bef(n\rightarrow n\oplus_{M}y)\\
 & \quad=n\rightarrow n\oplus_{M}x\oplus_{M}y=\text{inMF}\,(x\oplus_{M}y)\quad.
\end{align*}
So, we may use $f\triangleq\text{inMF}$ in the monoidal naturality
law~(\ref{eq:monoidal-naturality-law-of-reduceE}) with monoids $M$
and $N\triangleq\text{MF}^{M}$:
\begin{equation}
\text{reduceE}\bef\text{inMF}=\text{inMF}^{\uparrow L}\bef\text{reduceE}\quad.\label{eq:monoidal-naturality-law-of-reduceE-inMF}
\end{equation}

Now we can express \lstinline!foldFn! and \lstinline!reduceE! through
each other:
\begin{align}
 & \text{foldFn}^{B}=\text{reduceE}^{\text{MF}^{B}}\quad\quad\text{where}\quad\quad\text{MF}^{B}\triangleq B\rightarrow B\quad,\label{eq:foldFn-via-reduceE}\\
 & \text{reduceE}\,(p^{:L^{M}})=\text{foldFn}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{M})\quad.\label{eq:reduceE-via-foldFn}
\end{align}

It remains to demonstrate an isomorphism between \lstinline!foldFn!
and \lstinline!reduceE! in both directions.

\textbf{1)} For a given function \lstinline!foldFn! with the type
signature shown above, we define \lstinline!reduceE! via Eq.~(\ref{eq:reduceE-via-foldFn})
and then define a new \lstinline!foldFn!$^{\prime}$ via Eq.~(\ref{eq:foldFn-via-reduceE}).
We need to show that \lstinline!foldFn!$^{\prime}=$ \lstinline!foldFn!:
\begin{align*}
 & \text{foldFn}^{\prime}(p^{:L^{B\rightarrow B}})=\text{reduceE}\,(p)=\text{foldFn}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{M})\quad.
\end{align*}
Here the function \lstinline!inMF! is used with the type signature
corresponding to the monoid $M\triangleq B\rightarrow B$:
\begin{align*}
 & \text{inMF}:\left(B\rightarrow B\right)\rightarrow\text{MF}^{B\rightarrow B}\cong\left(B\rightarrow B\right)\rightarrow\left(B\rightarrow B\right)\rightarrow B\rightarrow B\quad,\\
 & \text{inMF}\triangleq g^{:B\rightarrow B}\rightarrow h^{:B\rightarrow B}\rightarrow h\bef g\quad,\quad\quad e_{\text{MF}}\triangleq\text{id}^{:B\rightarrow B}\quad.
\end{align*}

We find that $\text{foldFn}^{\prime}(p)=\text{foldFn}\,(p)$ if the
following equation holds:
\begin{equation}
\text{foldFn}^{B}(p^{:L^{B\rightarrow B}})=\text{foldFn}^{B\rightarrow B}(p\triangleright\text{inMF}^{\uparrow L})(\text{id}^{:B\rightarrow B})\quad.\label{eq:foldFn-first-special-law}
\end{equation}

It remains to prove that the law~(\ref{eq:monoidal-naturality-law-of-reduceE})
will hold when \lstinline!reduceE! is defined via Eq.~(\ref{eq:reduceE-via-foldFn}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & p^{:L^{M}}\triangleright\text{reduceE}^{M}\bef f^{:M\rightarrow N}=\big(\text{foldFn}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{M})\big)\triangleright f\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & p^{:L^{M}}\triangleright f^{\uparrow L}\bef\text{reduceE}^{N}=\text{foldFn}\,(p\triangleright f^{\uparrow L}\triangleright\text{inMF}^{\uparrow L})(e_{N})\quad.
\end{align*}
The two sides are equal if the following law holds for any $p^{:L^{M}}$
and any monoid morphism $f^{:M\rightarrow N}$:
\begin{equation}
e_{M}\triangleright\big(p\triangleright\text{inMF}^{\uparrow L}\triangleright\text{foldFn}\big)\triangleright f=e_{N}\triangleright\big(p\triangleright f^{\uparrow L}\triangleright\text{inMF}^{\uparrow L}\triangleright\text{foldFn}\big)\quad.\label{eq:foldFn-second-special-law}
\end{equation}
Since both laws~(\ref{eq:foldFn-first-special-law}) and~(\ref{eq:foldFn-second-special-law})
are assumed to hold, we have demonstrated the equivalence between
\lstinline!reduceE! and \lstinline!foldFn! in one direction.

\textbf{2)} For a given function \lstinline!reduceE! with the type
signature shown above, we define \lstinline!foldFn! via Eq.~(\ref{eq:foldFn-via-reduceE})
and then define a new \lstinline!reduceE!$^{\prime}$ via Eq.~(\ref{eq:reduceE-via-foldFn}).
We need to show that \lstinline!reduceE!$^{\prime}=$ \lstinline!reduceE!:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{reduceE}\left(p\right):}\quad & \text{reduceE}^{\prime}(p^{:L^{M}})=\text{foldFn}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{M})\\
 & =\text{reduceE}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{M})\\
{\color{greenunder}\text{rewrite using }\triangleright\text{-notation}:}\quad & =e_{M}\triangleright\big(p\triangleright\gunderline{\text{inMF}^{\uparrow L}\bef\text{reduceE}}\big)\\
{\color{greenunder}\text{use Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE-inMF})}:}\quad & =e_{M}\triangleright\big(p\triangleright\text{reduceE}\bef\text{inMF}\big)\\
 & =\gunderline{\text{inMF}}\,(p\triangleright\text{reduceE})\gunderline{(e_{M})}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:identity-law-of-inMF})}:}\quad & =p\triangleright\text{reduceE}=\text{reduceE}\,(p)\quad.
\end{align*}

It remains to prove that \lstinline!foldFn! defined via Eq.~(\ref{eq:foldFn-via-reduceE})
will always satisfy the laws~(\ref{eq:foldFn-first-special-law})
and~(\ref{eq:foldFn-second-special-law}). To verify that the law~(\ref{eq:foldFn-first-special-law})
holds:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{foldFn}^{B}(p):}\quad & \text{foldFn}^{B\rightarrow B}(p^{:L^{B\rightarrow B}}\triangleright\text{inMF}^{\uparrow L})(e_{\text{MF}})\\
 & =\text{reduceE}\,(p\triangleright\text{inMF}^{\uparrow L})(e_{\text{MF}})\\
 & =e_{\text{MF}}\triangleright(p\triangleright\gunderline{\text{inMF}^{\uparrow L}\bef\text{reduceE}})\\
{\color{greenunder}\text{use Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE-inMF})}:}\quad & =e_{\text{MF}}\triangleright\big(p\triangleright\text{reduceE}\bef\text{inMF}\big)\\
 & =\gunderline{\text{inMF}}\,(p\triangleright\text{reduceE})\gunderline{(e_{\text{MF}})}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:identity-law-of-inMF})}:}\quad & =p\triangleright\text{reduceE}=\text{reduceE}^{B\rightarrow B}(p)=\text{foldFn}^{B}(p)\quad.
\end{align*}

To verify the law~(\ref{eq:foldFn-second-special-law}), write the
two sides separately and substitute Eq.~(\ref{eq:foldFn-via-reduceE}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & e_{M}\triangleright\big(p\triangleright\gunderline{\text{inMF}^{\uparrow L}\triangleright\text{foldFn}}\big)\triangleright f=e_{M}\triangleright(p\triangleright\gunderline{\text{inMF}^{\uparrow L}\bef\text{reduceE}}\big)\triangleright f\\
{\color{greenunder}\text{use Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE-inMF})}:}\quad & \quad=e_{M}\triangleright\big(p\triangleright\text{reduceE}\bef\text{inMF}\big)\triangleright f=\big(\gunderline{\text{inMF}}\,(p\triangleright\text{reduceE})\gunderline{(e_{M})}\big)\triangleright f\\
{\color{greenunder}\text{use Eq.~(\ref{eq:identity-law-of-inMF})}:}\quad & \quad=p\triangleright\text{reduceE}\triangleright f\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & e_{N}\triangleright\big(p\triangleright\gunderline{f^{\uparrow L}\triangleright\text{inMF}^{\uparrow L}\triangleright\text{foldFn}}\big)=e_{N}\triangleright\big(p\triangleright f^{\uparrow L}\bef\gunderline{\text{inMF}^{\uparrow L}\bef\text{reduceE}}\big)\\
{\color{greenunder}\text{use Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE-inMF})}:}\quad & \quad=e_{N}\triangleright\big(p\triangleright\gunderline{f^{\uparrow L}\bef\text{reduceE}}\bef\text{inMF}\big)\\
{\color{greenunder}\text{use Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE})}:}\quad & \quad=e_{N}\triangleright\big(p\triangleright\text{reduceE}\bef f\bef\text{inMF}\big)=\gunderline{\text{inMF}}\,\big(p\triangleright\text{reduceE}\bef f\big)\gunderline{(e_{N})}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:identity-law-of-inMF})}:}\quad & \quad=p\triangleright\text{reduceE}\triangleright f\quad.
\end{align*}
The two sides are now equal.

This proves that the equivalence between \lstinline!reduceE! and
\lstinline!foldFn! holds also in the other direction. $\square$

Let us comment on the special laws~(\ref{eq:foldFn-first-special-law})
and~(\ref{eq:foldFn-second-special-law}). The next statement will
show that those laws follow from parametricity. If \lstinline!foldFn(p)!
uses any functions of type $B\rightarrow B$ contained in $p:L^{B\rightarrow B}$
in a fully parametric manner, the code of \lstinline!foldFn! cannot
inspect the type $B$ or the code of the functions of type $B\rightarrow B$.
The only way of using those functions is to compose them with each
other in some order, obtaining again a function of type $B\rightarrow B$.
The laws~(\ref{eq:foldFn-first-special-law}) and~(\ref{eq:foldFn-second-special-law})
express this property in different ways by writing the function composition
explicitly as part of the code of the function \lstinline!inMF!.

\subsubsection{Statement \label{subsec:relational-property-for-foldFn}\ref{subsec:relational-property-for-foldFn}}

Denote for brevity $E^{A}\triangleq A\rightarrow A$. Let $L$ be
any \emph{polynomial} functor. (This restriction is acceptable because,
as we will see below, only polynomial functors are traversable.)

\textbf{(a)} Any fully parametric function $\phi:\forall A.\,L^{E^{A}}\rightarrow E^{A}$
satisfies the law~(\ref{eq:foldFn-first-special-law}):
\begin{align}
 & \phi\,(p^{:L^{E^{A}}})=\phi\,(p\triangleright\text{inF}^{\uparrow L})(\text{id}^{:A\rightarrow A})\quad,\label{eq:first-special-law-of-phi}\\
{\color{greenunder}\text{where we defined}:}\quad & \quad\text{inF}:E^{A}\rightarrow E^{A}\rightarrow E^{A}\quad,\quad\text{inF}\triangleq h^{:A\rightarrow A}\rightarrow k^{:A\rightarrow A}\rightarrow k\bef h\quad.\nonumber 
\end{align}

\textbf{(b)} Any fully parametric function $\text{foldFn}:\forall A.\,L^{E^{A}}\rightarrow E^{A}$
satisfies Eq.~(\ref{eq:foldFn-second-special-law}).

\subparagraph{Proof}

\textbf{(a)} Example~\ref{subsec:Example-strong-dinaturality-proof-of-foldFn-law}
(in Appendix~\ref{app:Proofs-of-naturality-parametricity}) shows
that $\phi$ satisfies the strong dinaturality law. The law says that
\begin{comment}
Begin by formulating the relational naturality law of $\phi$: for
all $r^{:A\leftrightarrow B}$,
\[
\text{if }(x^{:L^{A\rightarrow A}},y^{:L^{B\rightarrow B}})\in r^{\updownarrow(L\circ E)}\text{ then }(\phi(x),\phi(y))\in r^{\updownarrow E}\quad.
\]
In order to use this law to derive Eq.~(\ref{eq:first-special-law-of-phi}),
we need to choose a specific relation $r$ and specific types $A$
and $B$. Note that the value $p\triangleright\text{inMF}^{\uparrow L}$
in Eq.~(\ref{eq:first-special-law-of-phi}) has type $L^{E^{A}\rightarrow E^{A}}$.
So, the only hope of using Eq.~(\ref{eq:relational-law-of-g}) is
by choosing the type $B$ as $B\triangleq E^{A}=A\rightarrow A$. 

It remains to choose a suitable relation $r$ between the types $A$
and $B\triangleq A\rightarrow A$. In Eq.~(\ref{eq:relational-law-of-g}),
the relation $r$ is being lifted to the functor $L\circ E$, which
is equivalent to the lifting $(r^{\updownarrow E})^{\updownarrow L}$.
Since $L$ is an unknown functor, we do not have a general formula
for lifting an arbitrary relation $r$ to $L\circ E$. However, we
do have a formula for lifting a relation $r^{:A\leftrightarrow B}$
that comes from a function (either of type $A\rightarrow B$ or of
type $B\rightarrow A$). For instance, given a function $f:B\rightarrow A$,
we define $r\triangleq\text{rev}\left<f\right>$. First we lift $\left<f\right>^{\updownarrow E}$
as:
\[
(g^{:B\rightarrow B},h^{:A\rightarrow A})\in\left<f\right>^{\updownarrow E}\text{ means }g\bef f=f\bef h\quad.
\]
This is a relation of pullback form:
\[
\left<f\right>^{\updownarrow E}=\text{pull}\,\big(g^{:B\rightarrow B}\rightarrow g\bef f,\quad h^{:A\rightarrow A}\rightarrow f\bef h\big)\quad.
\]
A pullback relation can then be lifted to a functor $L$ as shown
in Statement~\ref{subsec:Statement-pullback-as-composition}:
\[
\left<f\right>^{\updownarrow E\updownarrow L}=\text{pull}\,\big((g^{:B\rightarrow B}\rightarrow g\bef f)^{\uparrow L},\quad(h^{:A\rightarrow A}\rightarrow f\bef h)^{\uparrow L}\big)\quad.
\]
We can now rewrite Eq.~ in the form of a chain of pullback relations: 
\end{comment}
for any $f^{:B\rightarrow A}$, $x^{:L^{A\rightarrow A}}$, and $y^{:L^{B\rightarrow B}}$:
\begin{equation}
\text{if }x\triangleright(h^{:A\rightarrow A}\rightarrow f\bef h)^{\uparrow L}=y\triangleright(g^{:B\rightarrow B}\rightarrow g\bef f)^{\uparrow L}\quad\text{then}\quad f\bef\phi(x)=\phi(y)\bef f\quad.\label{eq:strong-dinaturality-law-of-phi-with-f}
\end{equation}
We need to choose $A$, $B$, and $f$ appropriately in order to be
able to derive Eq.~(\ref{eq:first-special-law-of-phi}). After some
guessing and trying, one finds that a good choice is $B\triangleq A\rightarrow A$
and $f^{:B\rightarrow A}$ defined by:
\begin{equation}
f:(A\rightarrow A)\rightarrow A\quad,\quad\quad f\triangleq k^{:A\rightarrow A}\rightarrow k(a_{0})\quad\text{with a fixed }a_{0}:A\quad.\label{eq:f-for-relational-law-of-foldFn-derivation1}
\end{equation}
Each value $a_{0}$ of type $A$ gives us a function $f_{a_{0}}:(A\rightarrow A)\rightarrow A$
for which the law~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
holds. 

We will now show that Eq.~(\ref{eq:first-special-law-of-phi}) follows
from Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f}) with this
choice of $f$. The value $a_{0}$ will be held fixed throughout most
of this proof, so we will write just $f$ instead of $f_{a_{0}}$.

In order to use Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
with this choice of $f$, we need to find suitable values $x$ and
$y$. Note that the conclusion of Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
relates $\phi(x)$ with $\phi(y)$, while Eq.~(\ref{eq:first-special-law-of-phi})
requires us to relate $\phi(p)$ with $\phi(p\triangleright\text{inF}^{\uparrow L})$.
So, we must choose $x=p$ and $y=p\triangleright\text{inF}^{\uparrow L}$.
This choice would work with Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
only if its precondition is satisfied with these $x$ and $y$ and
with $f$ defined by Eq.~(\ref{eq:f-for-relational-law-of-foldFn-derivation1}):
\begin{align*}
 & p\triangleright(h^{:A\rightarrow A}\rightarrow f\bef h)^{\uparrow L}\overset{?}{=}y\triangleright(g^{:B\rightarrow B}\rightarrow g\bef f)^{\uparrow L}=p\triangleright\text{inF}^{\uparrow L}\triangleright(g^{:B\rightarrow B}\rightarrow g\bef f)^{\uparrow L}\\
 & \quad=p\triangleright\big(\text{inF}\bef(g^{:B\rightarrow B}\rightarrow g\bef f)\big)^{\uparrow L}\quad.
\end{align*}
We will show that this equation holds for any $p:L^{A\rightarrow A}$
if we show that:
\[
h^{:A\rightarrow A}\rightarrow f\bef h\overset{?}{=}\text{inF}\bef(g^{:B\rightarrow B}\rightarrow g\bef f)\quad.
\]
We simplify separately each side of this equation and find that they
are equal:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & h^{:A\rightarrow A}\rightarrow f\bef h=h\rightarrow(k\rightarrow\gunderline{k(a_{0}))\bef h}\\
 & \quad=h\rightarrow k\rightarrow h(k(a_{0}))\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \gunderline{\text{inF}}\bef(g^{:B\rightarrow B}\rightarrow g\bef\gunderline f)\\
 & \quad=(h\rightarrow k\rightarrow k\bef h)\bef(g\rightarrow g\bef(k\rightarrow k(a_{0}))\\
{\color{greenunder}\text{compute composition}:}\quad & \quad=h\rightarrow(k\rightarrow k\bef h)\bef(k\rightarrow k(a_{0}))=h\rightarrow k\rightarrow\gunderline{(k\bef h)(a_{0})}\\
 & \quad=h\rightarrow k\rightarrow h(k(a_{0})\quad.
\end{align*}

Since the precondition of Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
is satisfied, its conclusion holds too:
\begin{align*}
 & f\bef\phi(x)\overset{!}{=}\phi(y)\bef f\quad,\\
{\color{greenunder}\text{equivalently}:}\quad & (k^{:A\rightarrow A}\rightarrow k(a_{0}))\bef\phi(p)\overset{!}{=}\phi(p\triangleright\text{inF}^{\uparrow L})\bef(k^{:A\rightarrow A}\rightarrow k(a_{0}))\quad,\\
{\color{greenunder}\text{equivalently}:}\quad & k^{:A\rightarrow A}\rightarrow\phi(p)(k(a_{0}))\overset{!}{=}k^{:A\rightarrow A}\rightarrow\phi(p\triangleright\text{inF}^{\uparrow L})(k)(a_{0})\quad.
\end{align*}
The last equation can be applied to any function $k$ of type $A\rightarrow A$:
\[
\phi(p)(k(a_{0}))\overset{!}{=}\phi(p\triangleright\text{inF}^{\uparrow L})(k)(a_{0})\quad.
\]
In Eq.~(\ref{eq:first-special-law-of-phi}), the function $\phi(p\triangleright\text{inF}^{\uparrow L})$
is applied to the identity function of type $A\rightarrow A$. So,
we set $k\triangleq\text{id}$ in the line above:
\[
\phi(p)(a_{0})\overset{!}{=}\phi(p\triangleright\text{inF}^{\uparrow L})(\text{id})(a_{0})\quad.
\]
Since this holds for any $a_{0}$, we get $\phi(p)\overset{!}{=}\phi(p\triangleright\text{inF}^{\uparrow L})(\text{id})$,
which proves Eq.~(\ref{eq:first-special-law-of-phi}).

\textbf{(b)} We already showed in part \textbf{(a)} that \lstinline!foldFn!
$=\phi$ satisfies Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f}).
We will now show that Eq.~(\ref{eq:foldFn-second-special-law}) follows
if we apply Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
with $A$, $B$, $x$, and $y$ chosen as:
\[
A\triangleq N\quad,\quad\quad B\triangleq M\quad,\quad\quad x^{:L^{N\rightarrow N}}\triangleq p\triangleright f^{\uparrow L}\triangleright\text{inMF}^{\uparrow L}\quad,\quad\quad y^{:L^{M\rightarrow M}}\triangleq p\triangleright\text{inMF}^{\uparrow L}\quad.
\]
In order to use Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f}),
we need to verify that its precondition is satisfied:
\begin{align*}
 & x\triangleright(h\rightarrow f\bef h)^{\uparrow L}=p\triangleright\big(f\bef\text{inMF}\bef(h\rightarrow f\bef h)\big)^{\uparrow L}\\
 & \quad\overset{?}{=}y\triangleright(g\rightarrow g\bef f)^{\uparrow L}=p\triangleright\big(\text{inMF}\bef(g\rightarrow g\bef f)\big)^{\uparrow L}\quad.
\end{align*}
To show that this equation holds, we compare separately the lifted
functions that are applied to $p$:
\begin{align*}
 & f\bef\text{inMF}\bef(h\rightarrow f\bef h)=\big(m^{:M}\rightarrow n^{:N}\rightarrow n\oplus_{N}f(m)\big)\bef(h\rightarrow f\bef h)\\
 & \quad=m^{:M}\rightarrow l^{:M}\rightarrow f(l)\oplus_{N}f(m)\quad,\\
 & \text{inMF}\bef(g\rightarrow g\bef f)=(m^{:M}\rightarrow l^{:M}\rightarrow l\oplus_{M}m)\bef(g\rightarrow g\bef f)\\
 & \quad=m^{:M}\rightarrow l^{:M}\rightarrow f(l\oplus_{M}m)\quad.
\end{align*}
The two functions are equal because of the composition law of the
monoid morphism $f$. So, the precondition of Eq.~(\ref{eq:strong-dinaturality-law-of-phi-with-f})
holds, and we may use its conclusion:
\begin{align*}
 & f\bef\text{foldFn}\,(x)\overset{!}{=}\text{foldFn}(y)\bef f\\
{\color{greenunder}\text{or equivalently}:}\quad & \text{foldFn}\,(p\triangleright\text{inMF}^{\uparrow L})\bef f=f\bef\text{foldFn}(p\triangleright f^{\uparrow L}\bef\text{inMF}^{\uparrow L}).
\end{align*}
This gives the law~(\ref{eq:foldFn-second-special-law}) if we use
$f$\textsf{'}s monoid morphism identity law: $e_{M}\triangleright f=e_{N}$.

\subsubsection{Statement \label{subsec:Statement-reduceE-toList-equivalence}\ref{subsec:Statement-reduceE-toList-equivalence}}

Functions \lstinline!reduceE! and \lstinline!toList! are equivalent
if expressed via each other as:
\begin{align}
 & \text{toList}:L^{A}\rightarrow\text{List}^{A}\quad,\quad\quad\text{toList}=\text{pu}_{\text{List}}^{\uparrow L}\bef\text{reduceE}^{\text{List}^{A}}\quad,\label{eq:toList-via-reduceE}\\
 & \text{reduceE}:L^{M}\rightarrow M\quad,\quad\quad\text{reduceE}=\text{toList}\bef\text{reduceList}\quad,\label{eq:reduceE-via-toList}
\end{align}
assuming the naturality law of \lstinline!toList! and the monoidal
naturality law~(\ref{eq:monoidal-naturality-law-of-reduceE}) of
\lstinline!reduceE!. Here the function $\text{reduceE}^{\text{List}^{A}}$
is \lstinline!reduceE! applied to the monoidal type \lstinline!List[A]!
whose binary operation is the list concatenation. The monoidally natural
function \lstinline!reduceList[M]! is defined for monoids $M$ by:
\begin{lstlisting}
def reduceList[M: Monoid]: List[M] => M = {
  case Nil            => Monoid[M].empty
  case head :: tail   => head |+| reduceList(tail)
}
\end{lstlisting}
\begin{align*}
 & \text{reduceList}^{M}:\text{List}^{M}\rightarrow M\quad,\\
 & \text{reduceList}\triangleq\,\begin{array}{|c||c|}
 & M\\
\hline \bbnum 1 & \_\rightarrow e_{M}\\
M\times\text{List}^{M} & h^{:M}\times t^{:\text{List}^{M}}\rightarrow h\oplus_{M}\overline{\text{reduceList}}\left(t\right)
\end{array}\quad.
\end{align*}


\subparagraph{Proof}

We begin by showing that \lstinline!reduceList! obeys the monoidal
naturality law:
\[
f^{\uparrow\text{List}}\bef\text{reduceList}=\text{reduceList}\bef f\quad.
\]
This law assumes that $f^{:M\rightarrow N}$ is a monoid morphism
between monoids $M$ and $N$. 

Simplify the left-hand side of the law:
\begin{align*}
 & f^{\uparrow\text{List}}\bef\text{reduceList}\\
 & =\,\begin{array}{|c||cc|}
 & \bbnum 1 & N\times\text{List}^{N}\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
M\times\text{List}^{M} & \bbnum 0 & f\boxtimes f^{\uparrow\text{List}}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & N\\
\hline \bbnum 1 & \_\rightarrow e_{N}\\
N\times\text{List}^{N} & h\times t\rightarrow h\oplus_{N}(t\triangleright\overline{\text{reduceList}})
\end{array}\\
 & =\,\,\begin{array}{|c||c|}
 & N\\
\hline \bbnum 1 & \_\rightarrow e_{N}\\
M\times\text{List}^{M} & h\times t\rightarrow f(h)\oplus_{N}(t\triangleright f^{\uparrow\text{List}}\bef\overline{\text{reduceList}})
\end{array}\quad.
\end{align*}
We can use the inductive assumption that recursive calls to \lstinline!reduceList!
already obey the monoidal naturality law. So, we can simplify:
\[
t\triangleright f^{\uparrow\text{List}}\bef\overline{\text{reduceList}}=t\triangleright\overline{\text{reduceList}}\bef f=f(\overline{\text{reduceList}}\left(t\right))\quad.
\]
The bottom-row expression in the last matrix is then rewritten to:
\begin{align*}
 & f(h)\oplus_{N}(t\triangleright f^{\uparrow\text{List}}\bef\overline{\text{reduceList}})\\
 & =f(h)\oplus_{N}f(\overline{\text{reduceList}}\left(t\right))\\
{\color{greenunder}\text{monad morphism law}:}\quad & =f\big(h\oplus_{M}\overline{\text{reduceList}}\left(t\right)\big)\quad.
\end{align*}
Using the monoid morphism identity law ($f(e_{M})=e_{N}$), we find:
\begin{align*}
 & f^{\uparrow\text{List}}\bef\text{reduceList}=\,\begin{array}{|c||c|}
 & N\\
\hline \bbnum 1 & \_\rightarrow f(e_{M})\\
M\times\text{List}^{M} & h\times t\rightarrow f\big(h\oplus_{M}\overline{\text{reduceList}}\left(t\right)\big)
\end{array}\\
 & =\,\begin{array}{|c||c|}
 & N\\
\hline \bbnum 1 & \_\rightarrow e_{M}\\
M\times\text{List}^{M} & h\times t\rightarrow h\oplus_{M}\overline{\text{reduceList}}\left(t\right)
\end{array}\bef f=\text{reduceList}\bef f\quad.
\end{align*}
 This is equal to the right-hand side of the law.

We are now ready to show the isomorphism between \lstinline!reduceE!
and \lstinline!toList! in both directions:

\textbf{(1)} Given a function \lstinline!reduceE! that satisfies
Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE}), define \lstinline!toList!
via Eq.~(\ref{eq:toList-via-reduceE}) and then define a new function
\lstinline!reduceE!$^{\prime}$ via Eq.~(\ref{eq:reduceE-via-toList}).
To show that \lstinline!reduceE!$^{\prime}=$ \lstinline!reduceE!,
we write:
\begin{align*}
 & \text{reduceE}^{\prime}=\text{toList}\bef\text{reduceList}\\
 & =\gunderline{\text{pu}_{\text{List}}^{\uparrow L}\bef\text{reduceE}^{\text{List}^{M}}}\bef\text{reduceList}\\
{\color{greenunder}\text{Eq.~(\ref{eq:monoidal-naturality-law-of-reduceE}) with }N\triangleq\text{List}^{A}:}\quad & =\text{reduceE}^{M}\bef\text{pu}_{\text{List}}\bef\text{reduceList}\quad.
\end{align*}
It remains to show that the composition $\text{pu}_{\text{List}}\bef\text{reduceList}$
is equal to the identity function:
\[
\text{pu}_{\text{List}}\bef\text{reduceList}=\text{id}^{:M\rightarrow M}\quad.
\]
We use the definition of \lstinline!reduceList[M]! and simplify the
matrix composition:
\begin{align*}
 & \text{pu}_{\text{List}}\bef\text{reduceList}=(m^{:M}\rightarrow\bbnum 0+m\times\left[\right])\bef\text{reduceList}\\
 & =\,\begin{array}{|c||cc|}
 & \bbnum 1 & M\times\text{List}^{M}\\
\hline M & \bbnum 0 & m\rightarrow m\times\left[\right]
\end{array}\,\bef\,\begin{array}{|c||c|}
 & M\\
\hline \bbnum 1 & \_\rightarrow e_{M}\\
M\times\text{List}^{M} & h\times t\rightarrow h\oplus_{M}\overline{\text{reduceList}}\left(t\right)
\end{array}\\
 & =\,\,\begin{array}{|c||c|}
 & M\\
\hline M & m\rightarrow m\oplus_{M}\gunderline{\overline{\text{reduceList}}\left(\left[\right]\right)}
\end{array}\,=m\rightarrow\gunderline{m\oplus e_{M}}=m\rightarrow m=\text{id}\quad.
\end{align*}
In the last line, we used the fact that \lstinline!reduceList[M]!
applied to an empty list gives $e_{M}$.

\textbf{(2)} Given a function \lstinline!toList!, define \lstinline!reduceE!
via Eq.~(\ref{eq:reduceE-via-toList}) and then define a new function
\lstinline!toList!$^{\prime}$ via Eq.~(\ref{eq:toList-via-reduceE}).
To show that \lstinline!toList!$^{\prime}=$ \lstinline!toList!,
we write:
\begin{align*}
 & \text{toList}^{\prime}=\text{pu}_{\text{List}}^{\uparrow L}\bef\text{reduceE}^{\text{List}^{A}}\\
 & =\gunderline{\text{pu}_{\text{List}}^{\uparrow L}\bef\text{toList}}\bef\text{reduceList}^{\text{List}^{A}}\\
{\color{greenunder}\text{naturality of }\text{toList}:}\quad & =\text{toList}\bef\text{pu}_{\text{List}}^{\uparrow\text{List}}\bef\text{reduceList}^{\text{List}^{A}}\quad.
\end{align*}
It remains to show that the composition $\text{pu}_{\text{List}}^{\uparrow\text{List}}\bef\text{reduceList}$
is equal to the identity function:
\begin{equation}
\text{pu}_{\text{List}}^{\uparrow\text{List}}\bef\text{reduceList}=\text{id}^{:\text{List}^{M}\rightarrow\text{List}^{M}}\quad.\label{eq:identity-for-reduceList-and-pure}
\end{equation}
We use the definition of \lstinline!reduceList[M]! and set \lstinline!M!
to the monoidal type \lstinline!List[A]!:
\begin{align*}
 & \text{pu}_{\text{List}}^{\uparrow\text{List}}\bef\text{reduceList}^{\text{List}^{A}}=\,\begin{array}{|c||cc|}
 & \bbnum 1 & \text{List}^{M}\times\text{List}^{\text{List}^{M}}\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
M\times\text{List}^{M} & \bbnum 0 & h\times t\rightarrow\text{pu}_{\text{List}}(h)\times(t\triangleright\text{pu}_{\text{List}}^{\uparrow\text{List}})
\end{array}\\
 & \quad\quad\quad\bef\,\begin{array}{|c||c|}
 & \text{List}^{M}\\
\hline \bbnum 1 & \_\rightarrow1+\bbnum 0\\
\text{List}^{M}\times\text{List}^{\text{List}^{M}} & h\times t\rightarrow h\pplus\overline{\text{reduceList}}\left(t\right)
\end{array}\\
 & =\,\,\begin{array}{|c||c|}
 & \text{List}^{M}\\
\hline \bbnum 1 & \_\rightarrow1+\bbnum 0\\
M\times\text{List}^{M} & h\times t\rightarrow\text{pu}_{\text{List}}(h)\pplus(t\triangleright\text{pu}_{\text{List}}^{\uparrow\text{List}}\triangleright\overline{\text{reduceList}})
\end{array}\quad.
\end{align*}
By the inductive assumption, a recursive call to \lstinline!reduceList!
already obeys Eq.~(\ref{eq:identity-for-reduceList-and-pure}):
\[
t\triangleright\gunderline{\text{pu}_{\text{List}}^{\uparrow\text{List}}\triangleright\overline{\text{reduceList}}}=t\triangleright\text{id}=t\quad.
\]
We can now simplify the code and obtain the identity function:
\begin{align*}
 & \text{pu}_{\text{List}}^{\uparrow\text{List}}\bef\text{reduceList}^{\text{List}^{A}}=\,\begin{array}{|c||c|}
 & \text{List}^{M}\\
\hline \bbnum 1 & \_\rightarrow1+\bbnum 0\\
M\times\text{List}^{M} & h^{:M}\times t^{:\text{List}^{M}}\rightarrow\text{pu}_{\text{List}}(h)\pplus t
\end{array}\\
 & =\,\begin{array}{|c||c|}
 & \text{List}^{M}\\
\hline \bbnum 1 & \_\rightarrow1+\bbnum 0\\
M\times\text{List}^{M} & h^{:M}\times t^{:\text{List}^{M}}\rightarrow\bbnum 0+h\times t
\end{array}\,=\,\begin{array}{|c||cc|}
 & \bbnum 1 & M\times\text{List}^{M}\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
M\times\text{List}^{M} & \bbnum 0 & h^{:M}\times t^{:\text{List}^{M}}\rightarrow h\times t
\end{array}\\
 & =\text{id}\quad.
\end{align*}

It remains to prove that \lstinline!reduceE! will satisfy the monoidal
naturality law~(\ref{eq:monoidal-naturality-law-of-reduceE}) when
defined via Eq.~(\ref{eq:reduceE-via-toList}) through any \lstinline!toList!
function. For any monoid morphism $f:M\rightarrow N$ between arbitrary
monoids $M$ and $N$, we write the two sides of the monoidal naturality
law~(\ref{eq:monoidal-naturality-law-of-reduceE}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{reduceE}\bef f=\text{toList}\bef\gunderline{\text{reduceList}\bef f}\\
{\color{greenunder}\text{monoidal naturality of }\text{reduceList}:}\quad & \quad=\gunderline{\text{toList}\bef f^{\uparrow\text{List}}}\bef\text{reduceList}\\
{\color{greenunder}\text{naturality of }\text{toList}:}\quad & \quad=f^{\uparrow L}\bef\text{toList}\bef\text{reduceList}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & f^{\uparrow L}\bef\text{reduceE}=f^{\uparrow L}\bef\text{toList}\bef\text{reduceList}\quad.
\end{align*}
The two sides are now equal. $\square$

\subsection{The missing laws of \texttt{foldMap} and \texttt{reduce}}

The equivalence of \lstinline!foldLeft!, \lstinline!foldMap!, \lstinline!reduce!,
and \lstinline!toList! is ensured by assuming various naturality
laws. However, those naturality laws do not fully describe the programmer\textsf{'}s
expectations about the behavior of folding operations. 

To see why, let us consider \lstinline!toList! with the type signature
$L^{A}\rightarrow\text{List}^{A}$. If \lstinline!toList! is fully
parametric, it will satisfy the naturality law:
\[
(f^{:A\rightarrow B})^{\uparrow L}\bef\text{toList }=\text{toList}\bef f^{\uparrow\text{List}}\quad.
\]
This law describes the property that \lstinline!toList[A]! works
in the same way for all types \lstinline!A!. Certainly, programmers
expect this property to hold. But the main purpose of \lstinline!toList!
is to extract values of type $A$ out of $L^{A}$ and store them in
a list. Naturality laws do not express that purpose.

More precisely, programmers expect that for any \textsf{``}container\textsf{''} value
\lstinline!p! of type \lstinline!L[A]!, the value \lstinline!toList(p)!
should be a list of \emph{all} values of type \lstinline!A! stored
in \lstinline!p!. As an example of unexpected behavior, imagine implementing
the type signature of \lstinline!toList! by a function that always
returns an empty list. For \textsf{``}containers\textsf{''} that store some values
of type \lstinline!A!, this implementation (although it is fully
parametric) would be unacceptable since it loses information: \lstinline!toList(x)!
always ignores  \lstinline!x!.

To prohibit such implementations, we would like to impose a law that
holds only when \lstinline!toList! extracts every value of type \lstinline!A!
stored in \lstinline!p!. Unfortunately, it seems to be impossible
to express this property in the form of an equation satisfied by \lstinline!toList!.
Often, a loss of information is prevented by imposing an identity
law. Can we formulate an identity law for \lstinline!toList! stating
that \lstinline!toList(p)! should extract some known values contained
in \lstinline!p!? If $L$ were a pointed functor\index{pointed functor}
(see Section~\ref{subsec:Pointed-functors-motivation-equivalence}),
we could use its \lstinline!pure! method to inject a known value
\lstinline!x! into the container \lstinline!p! and then require
\lstinline!toList! to extract the same value \lstinline!x!. But
the definition of \lstinline!toList! does not require the functor
$L$ to be pointed. So, in general we cannot inject values into a
data structure of type $L^{A}$ in a way that is guaranteed to preserve
information. This prevents us from formulating an identity law of
\lstinline!toList!.

Another approach to finding laws is to look for type signatures in
the form of a \textsf{``}lifting\textsf{''} that transforms a function of one type
into a function of another type. We have summarized the methods of
several standard typeclasses as \textsf{``}liftings\textsf{''} in Section~\ref{subsec:The-pattern-of-functorial-typeclasses}.
The laws of a \textsf{``}lifting\textsf{''} are the functor laws (identity and composition).
Could we apply that approach to derive laws for folding operations?
The type signature of the \lstinline!foldMap! function is:\index{foldMap function@\texttt{foldMap} function}
\[
\text{foldMap}:\left(A\rightarrow M\right)\rightarrow L^{A}\rightarrow M\quad.
\]
This resembles a \textsf{``}lifting\textsf{''} from functions of type $A\rightarrow M$
to functions of type $L^{A}\rightarrow M$. However, it is not possible
to impose the functor laws on those liftings. For instance, the functor
composition law involves applying \lstinline!foldMap! to a composition
of arguments. But the type signature $A\rightarrow M$ (where $M$
is a fixed type) does not support composition: we cannot compose $A\rightarrow M$
with $B\rightarrow M$.

So, the \textsf{``}lifting\textsf{''} approach fails to give us laws for folding operations.
However, we will see below that lifting-like laws may be imposed on
\lstinline!traverse!, whose type signature is a generalization of
that of \lstinline!foldMap!. We will show that the laws of \lstinline!traverse!
forbid information-losing implementations. Since \lstinline!foldMap!
can be derived from \lstinline!traverse!, we may take the position
that the only acceptable implementations of \lstinline!foldMap! are
those derived from a lawful \lstinline!traverse! function.

\subsection{All polynomial functors are foldable}

It turns out that the lack of available laws does not prevent us from
finding correct implementations of folding operations. The reason
is that folding operations are available only for polynomial functors,
such as \lstinline!Option! and \lstinline!List! (note that \lstinline!List!
is a \emph{recursively} \emph{defined} polynomial functor\index{polynomial functor!recursive}).
Those functors represent data structures that store a finite number
of values. It is clear what it means to \textsf{``}extract all values\textsf{''} from
such data structures. To show that all polynomial functors are foldable,
we will define \lstinline!toList! inductively via structural analysis
of functor types. The definition will ensure that \lstinline!toList!
correctly extracts the values stored by a given data structure.

Let us first show that non-polynomial functors are \emph{not} foldable.
An example of a non-polynomial functor is the \lstinline!Reader!
monad:
\[
\text{Reader}^{R,A}\triangleq R\rightarrow A\quad,
\]
where $R$ is a fixed but arbitrarily chosen type. A value $p$ of
type $R\rightarrow A$ can be viewed as a container that stores one
value of type $A$ for each value of type $R$. We can extract all
values of type $A$ from $p$ only if we can enumerate all possible
values of type $R$. In general, it is not practical to enumerate
all values of a given type (as an example, consider the type $R\triangleq\text{String}\rightarrow\text{String}$).
So, we will not be able to implement \lstinline!toList!, \lstinline!foldMap!,
\lstinline!foldLeft!, or \lstinline!reduce! for the \lstinline!Reader!
type. The only exceptions are types $R$ that have a known finite
set of distinct values, such as $R=$ \lstinline!Boolean!. However,
in those cases the type $R\rightarrow A$ is equivalent to a polynomial
functor:
\[
\text{Reader}^{\text{Boolean},A}=\bbnum 2\rightarrow A\cong A\times A\quad.
\]

We are now ready to show that all polynomial functors are foldable.
It is convenient to use the \lstinline!toList! operation to define
the \textsf{``}\index{foldable functor}foldable functor\textsf{''} typeclass:
\begin{lstlisting}
trait Foldable[L[_]] {
  def toList[A]: L[A] => List[A]
}
\end{lstlisting}
Other folding operations (\lstinline!foldLeft!, \lstinline!foldMap!,
\lstinline!reduce!) can be derived from \lstinline!toList! (Section~\ref{subsec:Equivalence-of-foldLeft,foldMap,reduce,and-toList}).

Polynomial functors are built via five standard type constructions
(Table~\ref{subsec:Type-notation-and-standard-type-constructions}
without the function types). Defining \lstinline!toList! for these
constructions will provide an implementation of folding operations
for all polynomial functors.

\paragraph{Fixed type}

Constant functors $L^{A}\triangleq Z$, where $Z$ is a fixed type,
are viewed as containers that are always empty and cannot store any
values of type $A$. So, \lstinline!toList! for $L$ always returns
an empty list.

\paragraph{Type parameter}

The identity functor $L^{A}\triangleq A$ is viewed as a container
holding a single value of type $A$. So, we simply define \lstinline!toList(x) = List(x)!.

\paragraph{Products}

If $K$ and $L$ are foldable functors that support $\text{toList}_{K}$
and $\text{toList}_{L}$, we define the product $M^{A}\triangleq K^{A}\times L^{A}$
and the corresponding function $\text{toList}_{M}$ as:
\begin{lstlisting}
def toList_M[A]: ((K[A], L[A])) => List[A] = { case (p, q) => toList_K(p) ++ toList_L(q) }
\end{lstlisting}
\[
\text{toList}_{M}:K^{A}\times L^{A}\rightarrow\text{List}^{A}\quad,\quad\text{toList}_{M}\triangleq p^{:K^{A}}\times q^{:L^{A}}\rightarrow\text{toList}_{K}(p)\,\pplus\,\text{toList}_{L}(q)\quad.
\]
This implementation contains an arbitrary choice: the values stored
in $p$ are listed before the values stored in $q$. We could equally
well write $\text{toList}_{L}(q)\,\pplus\,\text{toList}_{K}(p)$ in
the function body. This arbitrary choice corresponds to the arbitrariness
of the order in which a folding operation may traverse the values
stored in a container. Different instances of the \lstinline!Foldable!
typeclass may define different traversal orders, as we have seen in
the code examples earlier in this chapter.

\paragraph{Co-products}

If $K$ and $L$ are foldable functors, we define the co-product $M^{A}\triangleq K^{A}+L^{A}$
and the corresponding function $\text{toList}_{M}:K^{A}+L^{A}\rightarrow\text{List}^{A}$
as:
\begin{lstlisting}
def toList_M[A]: Either[K[A], L[A]] => List[A] = {
  case Left(p)    => toList_K(p)
  case Right(q)   => toList_L(q)
}
\end{lstlisting}
\[
\text{toList}_{M}\triangleq\,\begin{array}{|c||c|}
 & \text{List}^{A}\\
\hline K^{A} & \text{toList}_{K}\\
L^{A} & \text{toList}_{L}
\end{array}\quad.
\]


\paragraph{Recursive types}

We need to implement \lstinline!toList! for a functor $L$ defined
recursively by $L^{A}\triangleq S^{A,L^{A}}$, where the recursion
scheme $S$ is a bifunctor that is itself foldable. A bifunctor $S$
is \textbf{foldable}\index{foldable bifunctor} if there is a \lstinline!toList!
function with this type signature:
\[
\text{toList}_{S}:S^{A,A}\rightarrow\text{List}^{A}\quad.
\]
We define $\text{toList}_{L}$ using $\text{toList}_{S}$ and the
bifunctor $S$\textsf{'}s liftings (denoted by \lstinline!mapS1! and \lstinline!mapS2!):
\begin{lstlisting}
def toList_L[A]: S[A, L[A]] => List[A] = _.mapS2(toList_L).mapS1(List(_)).toList.flatten
\end{lstlisting}
\[
\text{toList}_{L}:S^{A,L^{A}}\rightarrow\text{List}^{A}\quad,\quad\quad\text{toList}_{L}\triangleq\overline{\text{toList}_{L}}^{\uparrow S^{A,\bullet}}\bef\text{pu}_{\text{List}}^{\uparrow S^{\bullet,\text{List}^{A}}}\bef\text{toList}_{S}\bef\text{ftn}_{\text{List}}\quad.
\]
This code first converts values of type $S^{A,L^{A}}$ into type $S^{A,\text{List}^{A}}$
by lifting \lstinline!toList! recursively to the second type parameter
of $S$. We then convert $S^{A,\text{List}^{A}}$ into $S^{\text{List}^{A},\text{List}^{A}}$
by creating one-element lists via $\text{pu}_{\text{List}}$. After
that, all values of type $A$ are extracted from $S^{\text{List}^{A},\text{List}^{A}}$
using $\text{toList}_{S}$, which returns a nested $\text{List}^{\text{List}^{A}}$.
The final \lstinline!flatten! operation reduces that to a $\text{List}^{A}$.

This implements \lstinline!toList! for the five type constructions
that build up polynomial functors. In each case, \lstinline!toList!
extracts all stored values of type $A$. One could write a code generator
to provide a \lstinline!Foldable! instance for any polynomial functor
automatically. When writing code by hand, it is more convenient to
implement folding operations directly, as we did at the beginning
of this chapter.

\subsection{Equivalence of \texttt{traverse} and \texttt{sequence}}

The Scala library contains the methods \lstinline!Future.sequence!
and \lstinline!Future.traverse!. The main use of \lstinline!Future.sequence!
is for transforming a sequence of \lstinline!Future! values that
may run in parallel. The result is a single \lstinline!Future! value
that waits until all parallel computations are finished. Omitting
some details, we may write the type signature of \lstinline!Future.sequence!
as:
\begin{lstlisting}
def sequence[A, L[X] <: TraversableOnce[X]]: L[Future[A]] => Future[L[A]]
\end{lstlisting}

The function \lstinline!Future.sequence! is limited to sequence-like
data types such as \lstinline!List!. To generalize the \lstinline!sequence!
operation to other data types, we replace a sequence-like type by
an arbitrary traversable functor \lstinline!L!. We also replace the
\lstinline!Future! type constructor by an arbitrary applicative functor
(since the \lstinline!traverse! operation accepts one). It turns
out that a function called \lstinline!sequence! can then be defined
via the \lstinline!traverse! operation and vice versa:
\begin{lstlisting}
def sequence[A, L[_]: Traversable, F[_]: Applicative : Functor]: L[F[A]] => F[L[A]] = _.traverse(id)
def traverse[A, B, L[_]: Traversable, F[_]: Applicative : Functor](la: L[A])(f: A => F[B]): F[L[B]] = sequence(la.map(f))
\end{lstlisting}
\begin{align*}
 & \text{seq}_{L}:L^{F^{A}}\rightarrow F^{L^{A}}\quad,\quad\quad\text{seq}_{L}\triangleq\text{trav}_{L}(\text{id})\quad,\\
 & \text{trav}_{L}:(A\rightarrow F^{B})\rightarrow L^{A}\rightarrow F^{L^{B}}\quad,\quad\quad\text{trav}_{L}(f)\triangleq f^{\uparrow L}\bef\text{seq}_{L}\quad.
\end{align*}
An inverse transformation ($F^{L^{A}}\rightarrow L^{F^{A}}$) is not
always possible, as shown in Exercise~\ref{subsec:Exercise-traversables-laws}.
The example with $F=$ \lstinline!Future! and $L=$ \lstinline!List!
may help remember the correct type signature of \lstinline!sequence!.

The equivalence between \lstinline!traverse! and \lstinline!sequence!
holds under the assumption that \lstinline!traverse! obeys a naturality
law with respect to the type parameter $A$. This follows from Statement~\ref{subsec:Statement-tr-equivalent-to-ftr}
where we need to define $\text{tr}\triangleq\text{seq}_{L}$, $\text{ftr}\triangleq\text{trav}_{L}$,
$F\triangleq L$, $G\triangleq F$, and $K\triangleq F\circ L$. 

Because of this equivalence, we may define the \lstinline!Traversable!
typeclass via \lstinline!sequence! instead of via \lstinline!traverse!.
It is easier to study the properties and laws of the \lstinline!sequence!
operation because it has a simpler type signature. It is also easier
to verify the laws of \lstinline!sequence!. So, we will use \lstinline!sequence!
when proving the type constructions for traversable functors.

\subsection{Laws of \texttt{traverse}}

Given a value \lstinline!p: L[A]!, where \lstinline!L! is a traversable
functor, programmers expect that the code \lstinline!p.traverse(f)!
should evaluate the function \lstinline!f: A => F[B]! on each value
of type \lstinline!A! stored within the container \lstinline!p!.
The resulting applicative $F$-effects should be merged into a single
value of type \lstinline!F[L[B]]!. At the same time, the structure
of the initial value \lstinline!p: L[A]! must be preserved as much
as possible in the resulting values of type \lstinline!L[B]! wrapped
under \lstinline!F!. Also, the \lstinline!traverse! function should
work in the same way for all applicative functors \lstinline!F! and
for all types \lstinline!A! and \lstinline!B!. How can we express
these expectations as laws?\footnote{Those laws were given by M.~Jaskelioff\index{Mauro Jaskelioff} and
O.~Rypacek in the paper \textsf{``}An investigation of the laws of traversals\textsf{''},
see \texttt{\href{https://arxiv.org/abs/1202.2919}{https://arxiv.org/abs/1202.2919}}}

A naturality law formalizes the requirement that a function with a
type parameter should work in the same way regardless of the type
chosen for that type parameter. There is one naturality law per type
parameter. So, we impose the naturality laws with respect to type
parameters \lstinline!A! and \lstinline!B!. The form of the naturality
laws follows from the type signature of \lstinline!traverse!:
\[
\text{trav}_{L}^{F,A,B}:(A\rightarrow F^{B})\rightarrow L^{A}\rightarrow F^{L^{B}}\quad.
\]
If we fix \lstinline!B! and let \lstinline!A! vary, the type signature
is of the form $J^{A}\rightarrow K^{A}$, which is a natural transformation
between two contrafunctors. If we fix \lstinline!A! and let \lstinline!B!
vary, this type signature has the form $G^{A}\rightarrow H^{A}$,
which is a natural transformation between two functors. So, we can
formulate the two \index{naturality law!of traverse@of \texttt{traverse}}naturality
laws of \lstinline!traverse! according to the recipes of Section~\ref{subsec:Naturality-laws-and-natural-transformations}.
For any $f^{:X\rightarrow A}$, $g^{:A\rightarrow F^{B}}$, and $h^{:B\rightarrow C}$:
\begin{equation}
\text{trav}_{L}^{F,X,B}(f\bef g)=f^{\uparrow L}\bef\text{trav}_{L}^{F,A,B}(g)\quad,\quad\quad\text{trav}_{L}^{F,A,C}(g\bef h^{\uparrow F})=\text{trav}_{L}^{F,A,B}(g)\bef h^{\uparrow L\uparrow F}\quad.\label{eq:naturality-laws-of-traverse}
\end{equation}
\[
\xymatrix{\xyScaleY{2.0pc}\xyScaleX{5.0pc}L^{X}\ar[rd]\sp(0.6){~~\text{trav}_{L}^{F,X,B}(f\bef g)}\ar[d]\sp(0.45){f^{\uparrow L}} &  & L^{A}\ar[rd]\sb(0.35){\text{trav}_{L}^{F,A,C}(g\bef h)~~}\ar[r]\sp(0.5){\text{trav}_{L}^{F,A,B}(g)} & F^{L^{B}}\ar[d]\sp(0.45){h^{\uparrow L\uparrow F}}\\
L^{A}\ar[r]\sb(0.5){\text{trav}_{L}^{F,A,B}(g)} & F^{L^{B}} &  & F^{L^{C}}
}
\]

We also need a naturality law with respect to the parameter $F$,
which is a type constructor required to be an applicative functor.
This law expresses the requirement that \lstinline!traverse! may
not inspect the type of $F$ directly and make decisions based on
that type. The code of \lstinline!traverse! may only use $F$\textsf{'}s applicative
methods (\lstinline!wu! and \lstinline!zip!). To formulate this
requirement as an \textbf{applicative naturality law}\index{applicative naturality law!of traverse@of \texttt{traverse}},
we write an equation similar to the second naturality law, except
that the function $f$ will now map the applicative functor $F$ to
another \emph{arbitrary} applicative functor $G$:
\noindent \begin{center}
$\xymatrix{\xyScaleY{1.7pc}\xyScaleX{5.0pc}L^{A}\ar[rd]\sb(0.4){\text{trav}_{L}^{G,A,B}(g\bef f)~~}\ar[r]\sp(0.5){\text{trav}_{L}^{F,A,B}(g)} & F^{L^{B}}\ar[d]\sp(0.45){f}\\
 & G^{L^{B}}
}
$
\begin{equation}
\text{trav}_{L}^{G,A,B}(g^{:A\rightarrow F^{B}}\bef f^{:F^{B}\rightarrow G^{B}})=\text{trav}_{L}^{F,A,B}(g)\bef f^{:F^{L^{B}}\rightarrow G^{L^{B}}}\quad.\label{eq:traverse-applicative-naturality-law}
\end{equation}
\par\end{center}

\begin{flushleft}
Here $f$ is a natural transformation with type signature $f:\forall X.\,F^{X}\rightarrow G^{X}$.
So, we are allowed to apply the same code of $f$ to different types:
$f$ has type $F^{B}\rightarrow G^{B}$ in the left-hand side of the
law and $F^{L^{B}}\rightarrow G^{L^{B}}$ in the right-hand side.
\par\end{flushleft}

Another requirement for the function $f$ is that $f$ must map $F$\textsf{'}s
applicative methods (\lstinline!wu! and \lstinline!zip!) to the
corresponding methods of $G$. Otherwise, the code of $\text{trav}_{L}^{F,A,B}$
that uses those methods of $F$ will not be mapped via $f$ to the
same code of $\text{trav}_{L}^{G,A,B}$ that uses $G$\textsf{'}s applicative
methods. 

Natural transformations $f:F^{X}\rightarrow G^{X}$ that preserve
the applicative methods of $F$ and $G$ are called \index{applicative morphism}\textbf{applicative
morphisms} (first defined in Section~\ref{subsec:Applicative-morphisms};
compare to monoid morphisms defined in Section~\ref{subsec:Equivalence-of-foldLeft,foldMap,reduce,and-toList}
and monad morphisms defined in Section~\ref{subsec:Monads-in-category-theory-monad-morphisms}).
The laws of applicative morphisms are:\index{identity laws!of applicative morphisms}\index{composition law!of applicative morphisms}
\begin{align}
{\color{greenunder}\text{identity law }:}\quad & f^{:F^{\bbnum 1}\rightarrow G^{\bbnum 1}}(\text{wu}_{F})=\text{wu}_{G}\quad,\label{eq:identity-law-of-applicative-morphism}\\
{\color{greenunder}\text{composition law}:}\quad & \text{zip}_{G}\big(f(p^{:F^{A}})\times f(q^{:F^{B}})\big)=f(\text{zip}_{F}(p\times q))\quad.\label{eq:composition-law-of-applicative-morphism}
\end{align}
The applicative naturality law is required to hold only when $f$
is an applicative morphism.

Here are some examples of applicative morphisms and applicative naturality.

\subsubsection{Example \label{subsec:Example-some-applicative-morphisms}\ref{subsec:Example-some-applicative-morphisms}\index{examples (with code)}}

Consider applicative functors $\text{Id}^{A}\triangleq A$ and $L^{A}\triangleq A\times A$. 

\textbf{(a)} Show that the standard function $\Delta:\text{Id}^{A}\rightarrow L^{A}$
is an applicative morphism.

\textbf{(b)} Show that the standard function $\pi_{1}:L^{A}\rightarrow\text{Id}^{A}$
is an applicative morphism. 

\subparagraph{Solution}

We need to check that these functions obey the laws of applicative
morphisms.

\textbf{(a)} To verify the identity law, write:
\[
\Delta(\text{wu}_{\text{Id}})=\Delta(1)=1\times1=\text{wu}_{P}\quad.
\]

To verify the composition law, begin with its left-hand side:
\begin{align*}
 & \text{zip}_{L}\big(\Delta(p^{:A})\times\Delta(q^{:A})\big)=\text{zip}_{L}\big((p\times p)\times(q\times q)\big)\\
{\color{greenunder}\text{definition of }\text{zip}_{L}:}\quad & =(p\times q)\times(p\times q)=\Delta(p\times q)=\Delta(\text{zip}_{\text{Id}}(p\times q))\quad.
\end{align*}

\textbf{(b)} To verify the identity law, write:
\[
\pi_{1}(\text{wu}_{L})=\pi_{1}(1\times1)=1=\text{wu}_{\text{Id}}\quad.
\]

To verify the composition law, begin with its right-hand side:
\begin{align*}
 & \pi_{1}\big(\text{zip}_{L}((p_{1}^{:A}\times p_{2}^{:A})\times(q_{1}^{:B}\times q_{2}^{:B}))\big)=\pi_{1}\big((p_{1}\times q_{1})\times(p_{2}\times q_{2})\big)=p_{1}\times q_{1}\\
 & =\text{zip}_{\text{Id}}(p_{1}\times q_{1})=\text{zip}_{\text{Id}}(\pi_{1}(p_{1}\times p_{2})\times\pi_{1}(q_{1}\times q_{2}))\quad.
\end{align*}


\subsubsection{Example \label{subsec:Example-naturality-law-of-traverse}\ref{subsec:Example-naturality-law-of-traverse}}

Verify the naturality law of \lstinline!traverse! for $L^{A}\triangleq A\times A$
using an applicative morphism between $G^{A}\triangleq\bbnum 1+A$
and $H^{A}\triangleq\bbnum 1+A\times A$. 

\subparagraph{Solution}

The \lstinline!traverse! method for $L^{A}\triangleq A\times A$
is defined by:
\[
\text{trav}_{L}(f^{:A\rightarrow F^{B}})\triangleq p^{:A}\times q^{:A}\rightarrow\text{zip}_{F}(f(p)\times f(q))=(f\boxtimes f)\bef\text{zip}_{F}\quad.
\]
The \lstinline!zip! methods of the applicative functors $G$ and
$H$ are:
\begin{align*}
 & \text{zip}_{G}\triangleq\,\begin{array}{|c||cc|}
 & \bbnum 1 & A\times B\\
\hline \bbnum 1\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
A\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
\bbnum 1\times B & \_\rightarrow1 & \bbnum 0\\
A\times B & \bbnum 0 & \text{id}
\end{array}\quad,\\
 & \text{zip}_{H}\triangleq\,\,\begin{array}{|c||cc|}
 & \bbnum 1 & (A\times B)\times(A\times B)\\
\hline \bbnum 1\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
(A\times A)\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
\bbnum 1\times(B\times B) & \_\rightarrow1 & \bbnum 0\\
(A\times A)\times(B\times B) & \bbnum 0 & \text{zip}_{L}
\end{array}\quad.
\end{align*}
The function $\text{zip}_{L}$ was shown in the solution to Example~\ref{subsec:Example-some-applicative-morphisms}(a). 

The applicative morphism $g:G^{A}\rightarrow H^{A}$ is implemented
as:
\[
g:\bbnum 1+A\rightarrow\bbnum 1+A\times A\quad,\quad\quad g\triangleq\,\begin{array}{|c||cc|}
 & \bbnum 1 & A\times A\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
A & \bbnum 0 & \Delta
\end{array}\quad.
\]
The fully parametric implementations of $\text{trav}_{L}$, $\text{zip}_{G}$,
$\text{zip}_{H}$, and $g$ are forced by their type signatures. The
values $\text{wu}_{G}$ and $\text{wu}_{H}$ need to be chosen as
$\text{wu}_{G}\triangleq\bbnum 0^{:\bbnum 1}+1$ and $\text{wu}_{H}\triangleq\bbnum 0^{:\bbnum 1}+1\times1$
in order to obey the identity laws of applicative morphisms.

Next, we verify that $g$ is an applicative morphism. To check the
identity law:
\[
\text{wu}_{G}\triangleright g=\,\begin{array}{|cc|}
\bbnum 0 & 1\end{array}\,\triangleright\,\begin{array}{|c||cc|}
 & \bbnum 1 & A\times A\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
A & \bbnum 0 & \Delta
\end{array}\,=\,\begin{array}{|cc|}
\bbnum 0 & (1\triangleright\Delta)\end{array}\,=\bbnum 0+1\times1=\text{wu}_{H}\quad.
\]
To check the composition law of $g$, first rewrite that law in the
point-free style:\index{composition law!of applicative morphisms!in the point-free style}\index{point-free style}
\begin{equation}
(g\boxtimes g)\bef\text{zip}_{H}=\text{zip}_{G}\bef g\quad.\label{eq:composition-law-applicative-morphism-point-free}
\end{equation}
The pair product ($g\boxtimes g$) can be written in matrix form like
this:
\begin{align*}
g\boxtimes g & =\,\begin{array}{|c||cc|}
 & \bbnum 1 & A\times A\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
A & \bbnum 0 & \Delta
\end{array}\,\boxtimes\,\begin{array}{|c||cc|}
 & \bbnum 1 & B\times B\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
B & \bbnum 0 & \Delta
\end{array}\\
 & =\,\,\begin{array}{|c||cccc|}
 & \bbnum 1 & A\times A\times\bbnum 1 & \bbnum 1\times B\times B & A\times A\times B\times B\\
\hline \bbnum 1\times\bbnum 1 & \text{id} & \bbnum 0 & \bbnum 0 & \bbnum 0\\
A\times\bbnum 1 & \bbnum 0 & \Delta\boxtimes\text{id} & \bbnum 0 & \bbnum 0\\
\bbnum 1\times B & \bbnum 0 & \bbnum 0 & \text{id}\boxtimes\Delta & \bbnum 0\\
A\times B & \bbnum 0 & \bbnum 0 & \bbnum 0 & \Delta\boxtimes\Delta
\end{array}\quad.
\end{align*}
The two sides of the composition law~(\ref{eq:composition-law-applicative-morphism-point-free})
are then simplified to:
\begin{align*}
(g\boxtimes g)\bef\text{zip}_{H} & =\,\begin{array}{||cccc|}
\text{id} & \bbnum 0 & \bbnum 0 & \bbnum 0\\
\bbnum 0 & \Delta\boxtimes\text{id} & \bbnum 0 & \bbnum 0\\
\bbnum 0 & \bbnum 0 & \text{id}\boxtimes\Delta & \bbnum 0\\
\bbnum 0 & \bbnum 0 & \bbnum 0 & \Delta\boxtimes\Delta
\end{array}\,\bef\,\begin{array}{||cc|}
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\bbnum 0 & \text{zip}_{L}
\end{array}\\
 & =\,\,\begin{array}{||cc|}
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\bbnum 0 & (\Delta\boxtimes\Delta)\bef\text{zip}_{L}
\end{array}\quad,\\
\text{zip}_{G}\bef g & =\,\begin{array}{|c||cc|}
 & \bbnum 1 & A\times B\\
\hline \bbnum 1\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
A\times\bbnum 1 & \_\rightarrow1 & \bbnum 0\\
\bbnum 1\times B & \_\rightarrow1 & \bbnum 0\\
A\times B & \bbnum 0 & \text{id}
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & \bbnum 1 & (A\times B)\times(A\times B)\\
\hline \bbnum 1 & \text{id} & \bbnum 0\\
A\times B & \bbnum 0 & \Delta
\end{array}\\
 & =\,\,\begin{array}{||cc|}
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\_\rightarrow1 & \bbnum 0\\
\bbnum 0 & \Delta
\end{array}\quad.
\end{align*}
The remaining difference is:
\[
(\Delta\boxtimes\Delta)\bef\text{zip}_{L}\overset{?}{=}\Delta\quad.
\]
This equation can be verified by applying both sides to some $a^{:A}\times b^{:B}$:
\begin{align*}
 & (a\times b)\triangleright(\Delta\boxtimes\Delta)\bef\text{zip}_{L}=\big((a\times a)\times(b\times b)\big)\triangleright\text{zip}_{L}=(a\times b)\times(a\times b)\quad,\\
 & (a\times b)\triangleright\Delta=(a\times b)\times(a\times b)\quad.
\end{align*}

We are now ready to check that the \lstinline!traverse! method of
$L$ satisfies the naturality law~(\ref{eq:traverse-applicative-naturality-law}),
where we swap $f$ with $g$ and use applicative functors $G$, $H$
instead of $F$, $G$. Write the two sides of the law:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{trav}_{L}^{H,A,B}(f^{:A\rightarrow G^{B}}\bef g^{:G^{B}\rightarrow H^{B}})=\big((f\bef g)\boxtimes(f\bef g)\big)\bef\text{zip}_{H}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{trav}_{L}^{G,A,B}(f^{:A\rightarrow G^{B}})\bef g^{:G^{L^{B}}\rightarrow H^{L^{B}}}=(f\boxtimes f)\bef\gunderline{\text{zip}_{G}\bef g}\\
{\color{greenunder}\text{composition law~(\ref{eq:composition-law-applicative-morphism-point-free})}:}\quad & \quad=\gunderline{(f\boxtimes f)\bef(g\boxtimes g)}\bef\text{zip}_{H}\\
{\color{greenunder}\text{composition law~(\ref{eq:pair-product-composition-law})}:}\quad & \quad=\gunderline{\big((f\bef g)\boxtimes(f\bef g)\big)}\bef\text{zip}_{H}\quad.
\end{align*}
The two sides of the law are now equal. $\square$

In this derivation, we were able to pass from $\text{zip}_{G}$ to
$\text{zip}_{H}$ only because $g$, being an applicative morphism,
maps $G$\textsf{'}s \lstinline!zip! operation into $H$\textsf{'}s \lstinline!zip!
operation according to Eq.~(\ref{eq:composition-law-of-applicative-morphism}).

\subsubsection{Example \label{subsec:Example-pure-is-applicative-morphism}\ref{subsec:Example-pure-is-applicative-morphism}}

For any applicative functor $F$, show that the \lstinline!pure!
method ($\text{pu}_{F}:A\rightarrow F^{A}$) is an applicative morphism
between the identity functor ($\text{Id}^{A}\triangleq A$) and $F$.

\subparagraph{Solution}

The identity functor\textsf{'}s applicative methods are identity functions:
\[
\text{wu}_{\text{Id}}=1\quad,\quad\quad\text{pu}_{\text{Id}}(x)=x\quad,\quad\quad\text{zip}_{\text{Id}}(p\times q)=p\times q\quad.
\]
We need to show that $\text{pu}_{F}$ obeys the laws~(\ref{eq:identity-law-of-applicative-morphism})\textendash (\ref{eq:composition-law-of-applicative-morphism}).
To verify the identity law:
\[
\text{pu}_{F}(\text{wu}_{\text{Id}})\overset{?}{=}\text{wu}_{F}=\text{pu}_{F}(1)=\text{pu}_{F}(\text{wu}_{\text{Id}})\quad.
\]
To verify the composition law:
\begin{align*}
 & \text{zip}_{F}\big(\text{pu}_{F}(p^{:A})\times\text{pu}_{F}(q^{:B})\big)\overset{?}{=}\text{pu}_{F}(\text{zip}_{\text{Id}}(p\times q))\\
 & =\text{pu}_{F}(p\times q)\\
{\color{greenunder}\text{use Exercise~\ref{subsec:Exercise-zip-pure-pure}}:}\quad & =\text{zip}_{F}(\text{pu}_{F}(p)\times\text{pu}_{F}(q))\quad.
\end{align*}
$\square$

We have analyzed the expectation that \lstinline!traverse! should
work in the same way for all types and applicative functors. Another
reasonable expectation is that \lstinline!p.traverse(f)! should visit
\emph{every} value of type \lstinline!A! stored inside \lstinline!p!
(where \lstinline!p! has type $L^{A}$). To formulate that expectation
as a law, consider for simplicity a function $f$ of type $A\rightarrow F^{A}$.
Then the final result of \lstinline!p.traverse(f)! has type $F^{L^{A}}$,
which can be visualized as some values of type $L^{A}$ wrapped under
an $F$-effect. We expect those values to be similar to \lstinline!p!
in their structure.

To formulate this condition in a simple way, we may choose functions
$f$ that always return an \emph{empty} $F$-effect (that is, a result
of applying $F$\textsf{'}s \lstinline!pure! method to some value). So, we
choose $f\triangleq\text{pu}_{F}$. We expect that traversing \lstinline!p!
with this function $f$ should reproduce the same value \lstinline!p!
wrapped in an empty $F$-effect. So, we write the law as \lstinline!p.traverse(pure) == pure(p)!,
or in the point-free style:
\begin{equation}
\text{trav}_{L}^{F,A,A}(\text{pu}_{F})=\text{pu}_{F}\quad.\label{eq:traverse-identity-law-with-pure}
\end{equation}

Alternatively, we may choose $F^{A}\triangleq\text{Id}^{A}=A$ (the
identity functor), $A=B$, and $f^{:A\rightarrow F^{B}}=\text{id}^{:A\rightarrow A}$.
Then the result of \lstinline!p.traverse(f)! will have type $F^{L^{A}}=L^{A}$
and is expected to equal the initial value \lstinline!p!, as the
transformation \lstinline!f! is an identity function. We obtain the
\textbf{identity law}\index{identity laws!of traverse@of \texttt{traverse}}
of \lstinline!traverse!:
\begin{equation}
\text{trav}_{L}^{\text{Id},A,A}(\text{id}^{:A\rightarrow A})=\text{id}^{:L^{A}\rightarrow L^{A}}\quad.\label{eq:traverse-identity-law}
\end{equation}

We can show that this law is equivalent to Eq.~(\ref{eq:traverse-identity-law-with-pure}):

\subsubsection{Statement \label{subsec:Statement-identity-law-traverse-simplified}\ref{subsec:Statement-identity-law-traverse-simplified}}

\textbf{(a)} If \lstinline!traverse! obeys Eq.~(\ref{eq:traverse-identity-law-with-pure})
then it also obeys Eq.~(\ref{eq:traverse-identity-law}).

\textbf{(b)} If \lstinline!traverse! obeys Eq.~(\ref{eq:traverse-identity-law})
and the naturality law~(\ref{eq:traverse-applicative-naturality-law})
then it also obeys Eq.~(\ref{eq:traverse-identity-law-with-pure}).

\subparagraph{Proof}

\textbf{(a)} We may set $F\triangleq\text{Id}$ in the law~(\ref{eq:traverse-identity-law-with-pure}).
Then we get Eq.~(\ref{eq:traverse-identity-law}) since $\text{pu}_{\text{Id}}=\text{id}$.

\textbf{(b)} Example~\ref{subsec:Example-pure-is-applicative-morphism}
shows that $\text{pu}_{F}$ is an applicative morphism between $\text{Id}$
and $F$. So, we may set $F\triangleq\text{Id}$, $G\triangleq F$,
$f\triangleq\text{pu}_{F}$, and $g\triangleq\text{\text{id}}$ in
the applicative naturality law~(\ref{eq:traverse-applicative-naturality-law}).
We then obtain:
\begin{align*}
{\color{greenunder}\text{applicative naturality law~(\ref{eq:traverse-applicative-naturality-law})}:}\quad & \text{trav}_{L}(\text{id}\bef f)\overset{!}{=}\gunderline{\text{trav}_{L}(\text{id})}\bef f\\
{\color{greenunder}\text{identity law~(\ref{eq:traverse-identity-law})}:}\quad & =\text{id}\bef f=f=\text{pu}_{F}\quad.
\end{align*}
At the same time, we have: $\text{trav}_{L}(\text{id}\bef f)=\text{trav}_{L}(f)=\text{trav}_{L}(\text{pu}_{F})$.
So, Eq.~(\ref{eq:traverse-identity-law-with-pure}) holds. $\square$

Apart from visiting every stored value exactly once, we expect that
all $F$-effects returned by $f$ are collected by \lstinline!traverse!
exactly once. To see an example where that expectation fails, consider
a simple traversable functor $L^{A}=A\times A$ and write the \lstinline!traverse!
method like this:
\begin{lstlisting}
def trav2[A, B, F[_]: WuZip : Functor](f: A => F[B])(la: (A, A)): F[(B, B)] = {
  val (f1, f2) = (f(la._1), f(la._2))
  (f1 zip f1 zip f2).map { case ((x, y), z) => (y, z) }
}
\end{lstlisting}
This code applies the function \lstinline!f! to both values in the
given pair. The result is a pair \lstinline!(f1, f2)! of type \lstinline!(F[B], F[B])!.
The code copies the first $F$-effect twice and uses \lstinline!zip!
to combine the three $F$-effects together. The first of the wrapped
values of type \lstinline!B! is discarded.

We expect that the function \lstinline!trav2! should violate some
law of \lstinline!traverse!. It turns out that a suitable law is
found by composing two \lstinline!traverse! operations. Consider
two different applicative functors ($F$ and $G$), two functions
$f:A\rightarrow F^{B}$ and $g:B\rightarrow G^{C}$, and compute the
following composition:
\[
p\triangleright\text{trav}_{L}^{F,A,B}(f)\triangleright\big(\text{trav}_{L}^{G,B,C}(g)\big)^{\uparrow F}:F^{G^{L^{C}}}\quad.
\]

\noindent The second \lstinline!traverse! operation needs to be lifted
to $F$ for the types to match. The result (of type $F^{G^{L^{C}}}$)
looks like a result of a \lstinline!traverse! operation with respect
to the functor $F\circ G$. By Statement~\ref{subsec:Statement-applicative-composition},
the functor $F\circ G$ is applicative. So, we may apply a single
\lstinline!traverse! operation using that functor and obtain:
\noindent \begin{center}
$\xymatrix{\xyScaleY{1.5pc}\xyScaleX{3.4pc}L^{A}\ar[dr]\sb(0.35){\text{trav}_{L}^{F\circ G,A,C}(f\bef g^{\uparrow F})~~}\ar[r]\sp(0.5){\text{trav}_{L}^{F,A,B}(f)} & F^{L^{B}}\ar[d]\sp(0.4){\big(\text{trav}_{L}^{G,B,C}(g)\big)^{\uparrow F}}\\
 & F^{G^{L^{C}}}
}
$
\[
p\triangleright\text{trav}_{L}^{F\circ G,A,C}(f\bef g^{\uparrow F}):F^{G^{L^{C}}}\quad.
\]
\par\end{center}

\noindent The \textbf{composition law}\index{composition law!of traverse@of \texttt{traverse}}
of \lstinline!traverse! says that the two results should be equal:
\begin{equation}
\text{trav}_{L}^{F,A,B}(f)\bef\big(\text{trav}_{L}^{G,B,C}(g)\big)^{\uparrow F}=\text{trav}_{L}^{F\circ G,A,C}(f\bef g^{\uparrow F})\quad.\label{eq:composition-law-of-traverse}
\end{equation}

So far, we have not given a motivation for the law~(\ref{eq:composition-law-of-traverse})
from a programmer\textsf{'}s perspective. We just wrote that law by analogy
with composition laws of other \textsf{``}lifting\textsf{''}-like functions. It is
not obvious that the law~(\ref{eq:composition-law-of-traverse})
indeed prevents \lstinline!traverse! from evaluating some effects
more than once. To get a heuristic explanation, assume that \lstinline!traverse!
evaluates some effect twice. The left-hand side of the law~(\ref{eq:composition-law-of-traverse})
will evaluate some $F$-effect twice and then evaluate some $G$-effect
twice, wrapped under the twice evaluated $F$-effect. The right-hand
side will evaluate an $(F\circ G)$-effect twice. To illustrate the
difference between the resulting effects, we may write heuristically
$(F\times F)\circ(G\times G)$ and $(F\circ G)\times(F\circ G)$.
In general, these results will not be the same when $F$-effects and
$G$-effects do not commute.

To make this heuristic explanation more rigorous, let us show an example
where the law~(\ref{eq:composition-law-of-traverse}) is violated
by the function \lstinline!trav2! shown above. We need to choose
sufficiently complicated applicative functors $F$ and $G$ whose
effects do not commute. A suitable choice is to set both $F$ and
$G$ to the \lstinline!State! monad.\footnote{The paper \texttt{\href{https://arxiv.org/abs/1202.2919}{https://arxiv.org/abs/1202.2919}}
shows another example using the \lstinline!List! monad.} For simplicity, we set all type parameters to \lstinline!Int! and
choose the functions $f$ and $g$ that add their arguments to the
internal state. The complete law-violating code (using the \lstinline!Zippable!
typeclass defined in Section~\ref{subsec:The-Zippable-and-Applicative-typeclass})
is shown below:

\begin{lstlisting}[frame=single,fillcolor={\color{black}},framesep={0.2mm},framexleftmargin=2mm,framexrightmargin=2mm,framextopmargin=2mm,framexbottommargin=2mm]
import io.chymyst.ch.implement // Automatic implementation of some methods.

type L[A] = (A, A) // A very simple but nontrivial traversable functor.

implicit val functorL: Functor[L] = new Functor[L] {
  override def map[A, B](fa: (A, A))(f: A => B): (B, B) = implement
}

def trav2[A, B, F[_] : Zippable : Functor](f: A => F[B])(la: L[A]): F[L[B]] = la.map(f) match {
  case (f1, f2) => f1 zip f1 zip f2
}

final case class S[A](run: Int => (A, Int)) // State monad with internal state of type Int.

implicit val FunctorS: Functor[S] = new Functor[S] {
  override def map[A, B](fa: S[A])(f: A => B): S[B] = implement
}

implicit val ZippableS: Zippable[S] = new Zippable[S] {
  override def zip[A, B](fa: S[A], fb: S[B]): S[(A, B)] = S { i  =>
    val (a, j) = fa.run(i)
    val (b, k) = fb.run(j)
    ((a, b), k)
  }
}

  // Define F[A] as S[S[A]] and implement WuZip and Functor instances.
final case class F[A](run: S[S[A]]) {
  // A runner method to help with testing.
  def eval(i: Int, j: Int): (A, Int, Int) = {
    val (sa, k) = run.run(i)
    val (a, l) = sa.run(j)
    (a, k, l)
  }
}

implicit val FunctorF: Functor[F] = new Functor[F] {
  override def map[A, B](fa: F[A])(f: A => B): F[B] = implement
}

implicit val ZippableF: Zippable[F] = new Zippable[F] {
  override def zip[A, B](fa: F[A], fb: F[B]): F[(A, B)] =
    F((fa.run zip fb.run).map { case (sa, sb) => sa zip sb })
}

val f1: Int => S[Int] = i => S(j => (i + j, i + j))
val f2: Int => S[Int] = f1
val f1f2: Int => F[Int] = i => F(f1(i).map(f2))
val l: L[Int] = (1, 0)

val result1: F[L[Int]] = trav2[Int, Int, F](f1f2)(l)
val result2: F[L[Int]] = {
  val x: S[(Int, Int)] = trav2[Int, Int, S](f1)(l)
  val y: S[S[(Int, Int)]] = x.map(trav2[Int, Int, S](f2))
  F(y)
}

// Show that result1 is not equal to result2:

scala> result1.eval(0, 0)
res0: ((Int, Int), Int, Int) = ((3, 5), 2, 5)

scala> result2.eval(0,0)
res1: ((Int, Int), Int, Int) = ((4, 6), 2, 6) 
\end{lstlisting}

We have found that \emph{some} incorrect implementations of \lstinline!traverse!
are prohibited by its laws. But this is not a proof that all lawful
\lstinline!traverse! functions will collect each $F$-effect exactly
once. We will revisit this question below in Section~\ref{subsec:Laws-of-traverse-and-zipWithIndex}.

\subsection{Laws of \texttt{sequence}}

We now derive the laws of \lstinline!sequence! that correspond to
the laws of \lstinline!traverse! found previously. 

Begin with the naturality laws. Since the type signature of \lstinline!sequence!
($L^{F^{A}}\rightarrow F^{L^{A}}$) has only one type parameter, \lstinline!sequence!
has only one naturality law\index{naturality law!of sequence@of \texttt{sequence}}:
\begin{center}
$\xymatrix{\xyScaleY{1.5pc}\xyScaleX{2.0pc}L^{F^{A}}\ar[d]\sb(0.45){\text{seq}_{L}^{F,A}}\ar[r]\sb(0.5){f^{\uparrow F\uparrow L}} & L^{F^{B}}\ar[d]\sp(0.45){\text{seq}_{L}^{F,B}}\\
F^{L^{A}}\ar[r]\sp(0.5){f^{\uparrow L\uparrow F}} & F^{L^{B}}
}
$
\begin{equation}
(f^{:A\rightarrow B})^{\uparrow F\uparrow L}\bef\text{seq}_{L}^{F,B}=\text{seq}_{L}^{F,A}\bef f^{\uparrow L\uparrow F}\quad.\label{eq:sequence-naturality-law}
\end{equation}
\par\end{center}

It will be shown in Exercise~\ref{subsec:Exercise-traversables-laws-2}
that this naturality law is equivalent to the two naturality laws
of \lstinline!traverse!.

Turning now to the applicative naturality law~(\ref{eq:traverse-applicative-naturality-law})
of \lstinline!traverse!, we derive the corresponding applicative
naturality law of \lstinline!sequence!. Express \lstinline!traverse!
via \lstinline!sequence! and substitute into Eq.~(\ref{eq:traverse-applicative-naturality-law}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{trav}_{L}^{G,A,B}(g\bef f^{B})=(g\bef f^{B})^{\uparrow L}\bef\text{seq}_{L}^{G,B}=g^{\uparrow L}\bef f^{\uparrow L}\bef\text{seq}_{L}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{trav}_{L}^{F,A,B}(g)\bef f^{L^{B}}=g^{\uparrow L}\bef\text{seq}_{L}^{F,B}\bef f^{L^{B}}\quad.
\end{align*}
The function $g^{\uparrow L}$ can be omitted from both sides (see
Exercise~\ref{subsec:Exercise-simplify-law-omit-lifted-function}
for a similar property). Renaming $B$ to $A$, we write the applicative
naturality law\index{applicative naturality law!of sequence@of \texttt{sequence}}
of \lstinline!sequence!:
\begin{center}
$\xymatrix{\xyScaleY{1.5pc}\xyScaleX{2.0pc}L^{F^{A}}\ar[d]\sb(0.45){\text{seq}_{L}^{F,A}}\ar[r]\sb(0.5){(f^{A})^{\uparrow L}} & L^{G^{A}}\ar[d]\sp(0.45){\text{seq}_{L}^{G,A}}\\
F^{L^{A}}\ar[r]\sp(0.5){f^{L^{A}}} & G^{L^{A}}
}
$
\[
(f^{A})^{\uparrow L}\bef\text{seq}_{L}^{G,A}=\text{seq}_{L}^{F,A}\bef f^{L^{A}}\quad.
\]
\par\end{center}

\noindent Here we have written the full type parameters in $f^{X}$
and $\text{seq}_{L}^{F,A}$ for clarity.

Turning now to the identity law, we again express \lstinline!traverse!
via \lstinline!sequence! and substitute into the simpler formulation~(\ref{eq:traverse-identity-law})
of the identity law of \lstinline!traverse!:
\[
\text{id}\overset{!}{=}\text{trav}_{L}^{\text{Id},A,A}(\text{id})=\text{id}^{\uparrow L}\bef\text{seq}_{L}^{\text{Id},A}\overset{!}{=}\text{seq}_{L}^{\text{Id},A}\quad.
\]
So, the \textbf{identity law} of \lstinline!sequence!\index{identity laws!of sequence@of \texttt{sequence}}
is:
\begin{equation}
\text{seq}_{L}^{\text{Id},A}=\text{id}\quad.\label{eq:identity-law-of-sequence}
\end{equation}

The composition law~(\ref{eq:composition-law-of-traverse}) of \lstinline!traverse!
leads to the following composition law of \lstinline!sequence!:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{trav}_{L}^{F,A,B}(f^{:A\rightarrow F^{B}})\bef\big(\text{trav}_{L}^{G,B,C}(g^{:B\rightarrow G^{C}})\big)^{\uparrow F}\\
 & \quad=f^{\uparrow L}\bef\text{seq}_{L}^{F,B}\bef\big(g^{\uparrow L}\bef\text{seq}_{L}^{G,C}\big)^{\uparrow F}\\
 & \quad=f^{\uparrow L}\bef\gunderline{\text{seq}_{L}^{F,B}\bef g^{\uparrow L\uparrow F}}\bef(\text{seq}_{L}^{G,C})^{\uparrow F}\\
{\color{greenunder}\text{naturality law~(\ref{eq:sequence-naturality-law})}:}\quad & \quad=f^{\uparrow L}\bef g^{\uparrow F\uparrow L}\bef\text{seq}_{L}^{F,G^{C}}\bef(\text{seq}_{L}^{G,C})^{\uparrow F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{trav}_{L}^{F\circ G,A,C}(f\bef g^{\uparrow F})=(f\bef g^{\uparrow F})^{\uparrow L}\bef\text{seq}_{L}^{F\circ G,C}\quad.
\end{align*}

\begin{center}
$\xymatrix{\xyScaleY{1.4pc}\xyScaleX{3.0pc}L^{F^{G^{A}}}\ar[r]\sp(0.55){\text{seq}_{L}^{F,G^{A}}}\ar[rd]\sb(0.4){\text{seq}_{L}^{F\circ G,A}} & F^{L^{G^{A}}}\ar[d]\sp(0.4){(\text{seq}_{L}^{G,A})^{\uparrow F}}\\
 & F^{G^{L^{A}}}
}
$
\par\end{center}

Omitting the common function $(f\bef g^{\uparrow F})^{\uparrow L}$
and renaming $C$ to $A$, we obtain the \index{composition law!of sequence@of \texttt{sequence}}\textbf{composition
law} of \lstinline!sequence!:
\begin{equation}
\text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=\text{seq}_{L}^{F\circ G,A}\quad.\label{eq:composition-law-of-sequence}
\end{equation}

Since \lstinline!sequence! has simpler laws and a simpler type signature
than \lstinline!traverse!, we will use \lstinline!sequence! for
verifying the laws of traversable functors. 

\subsection{All polynomial functors are traversable\label{subsec:All-polynomial-functors-are-traversable}}

To show that all polynomial functors are traversable, we implement
a \lstinline!sequence! operation for the five type constructions
that build up polynomial functors. We will verify the laws in every
case.

\paragraph{Fixed type}

If $L^{A}\triangleq Z$ where $Z$ is a fixed type, we define \lstinline!sequence!
via $F$\textsf{'}s \lstinline!pure! method:
\[
\text{seq}_{L}^{F,A}:L^{F^{A}}\rightarrow F^{L^{A}}\cong Z\rightarrow F^{Z}\quad,\quad\quad\text{seq}_{L}^{F,A}\triangleq\text{pu}_{F}^{:Z\rightarrow F^{Z}}\quad.
\]
The identity law~(\ref{eq:identity-law-of-sequence}) holds since
$\text{seq}_{L}^{\text{Id},A}=\text{pu}_{\text{Id}}=\text{id}$. To
verify the composition law~(\ref{eq:composition-law-of-sequence}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=\text{pu}_{F}\bef(\text{pu}_{G})^{\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{pu}_{F}:}\quad & \quad=\text{pu}_{G}\bef\text{pu}_{F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq}_{L}^{F\circ G,A}=\text{pu}_{F\circ G}\\
{\color{greenunder}\text{definition of }\text{pu}_{F\circ G}:}\quad & \quad=\text{pu}_{G}\bef\text{pu}_{F}\quad.
\end{align*}
The two sides of the law are now equal.

\paragraph{Type parameter}

If $L^{A}\triangleq A$ (the identity functor), \lstinline!sequence!
is defined as the identity function. Identity functions always satisfy
identity and composition laws.

The functor composition $L^{A}\triangleq M^{N^{A}}$ (where $M$ and
$N$ are some traversable functors) is another case where the type
parameter is used to define $L$. We assume that $\text{seq}_{M}$
and $\text{seq}_{N}$ are already available and satisfy the laws,
and define $\text{seq}_{L}$ as:
\begin{equation}
\text{seq}_{L}:M^{N^{F^{A}}}\rightarrow F^{M^{N^{A}}}\quad,\quad\quad\text{seq}_{L}\triangleq(\text{seq}_{N}^{F,A})^{\uparrow M}\bef\text{seq}_{M}^{F,N^{A}}\quad.\label{eq:def-sequence-for-functor-composition}
\end{equation}
The laws of $\text{seq}_{L}$ will be proved in Exercise~\ref{subsec:Exercise-traversables-5}.

\paragraph{Products}

Here $L^{A}\triangleq M^{A}\times N^{A}$, with some traversable functors
$M$ and $N$. We assume that $\text{seq}_{M}$ and $\text{seq}_{N}$
are already available and satisfy the laws, and define $\text{seq}_{L}$
as:
\begin{align*}
 & \text{seq}_{L}^{F,A}:M^{F^{A}}\times N^{F^{A}}\rightarrow F^{M^{A}\times N^{A}}\quad,\\
 & \text{seq}_{L}^{F,A}\triangleq m^{:M^{F^{A}}}\times n^{:N^{F^{A}}}\rightarrow\text{zip}_{F}\big((m\triangleright\text{seq}_{M}^{F,A})\times(n\triangleright\text{seq}_{N}^{F,A})\big)\quad.
\end{align*}
In the point-free style, we can write the same code in a shorter formula:
\[
\text{seq}_{L}^{F,A}\triangleq(\text{seq}_{M}^{F,A}\boxtimes\text{seq}_{N}^{F,A})\bef\text{zip}_{F}\quad.
\]

To verify the identity law~(\ref{eq:identity-law-of-sequence}),
use the fact that $\text{zip}_{\text{Id}}=\text{id}$:
\begin{align*}
 & \text{seq}_{L}^{\text{Id},A}=(\gunderline{\text{seq}_{M}^{\text{Id},A}}\boxtimes\gunderline{\text{seq}_{N}^{\text{Id},A}})\bef\text{zip}_{\text{Id}}\\
{\color{greenunder}\text{identity laws of }\text{seq}_{M}\text{ and }\text{seq}_{N}:}\quad & =(\text{id}\boxtimes\text{id})\bef\text{id}=\text{id}\quad.
\end{align*}

To verify the composition law~(\ref{eq:composition-law-of-sequence}),
begin with its left-hand side:
\begin{align*}
 & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=(\text{seq}_{M}^{F,G^{A}}\boxtimes\text{seq}_{N}^{F,G^{A}})\bef\gunderline{\text{zip}_{F}\bef(\text{seq}_{M}^{G,A}\boxtimes\text{seq}_{N}^{G,A})^{\uparrow F}}\bef\text{zip}_{G}^{\uparrow F}\\
 & \quad{\color{greenunder}\text{naturality law of }\text{zip}_{F}:}\quad\\
 & =\gunderline{(\text{seq}_{M}^{F,G^{A}}\boxtimes\text{seq}_{N}^{F,G^{A}})\bef\big((\text{seq}_{M}^{G,A})^{\uparrow F}\boxtimes(\text{seq}_{N}^{G,A})^{\uparrow F}\big)}\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\\
 & \quad{\color{greenunder}\text{composition law~(\ref{eq:pair-product-composition-law})}:}\quad\\
 & =\big((\text{seq}_{M}^{F,G^{A}}\bef(\text{seq}_{M}^{G,A})^{\uparrow F})\boxtimes(\text{seq}_{N}^{F,G^{A}}\bef(\text{seq}_{N}^{G,A})^{\uparrow F})\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\end{align*}
By assumption, $\text{seq}_{M}$ and $\text{seq}_{N}$ satisfy their
composition laws. So, we may rewrite the last line as:
\[
\big(\text{seq}_{M}^{F\circ G,A}\boxtimes\text{seq}_{N}^{F\circ G,A}\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\]
We can now transform the right-hand side of the law to the same expression:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq}_{L}^{F\circ G,A}=\big(\text{seq}_{M}^{F\circ G,A}\boxtimes\text{seq}_{N}^{F\circ G,A}\big)\bef\text{zip}_{F\circ G}\\
{\color{greenunder}\text{definition of }\text{zip}_{F\circ G}:}\quad & =\big(\text{seq}_{M}^{F\circ G,A}\boxtimes\text{seq}_{N}^{F\circ G,A}\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\end{align*}

The given implementation of \lstinline!sequence! will first collect
the $F$-effects stored in $M^{F^{A}}$ and then the $F$-effects
stored in $N^{F^{A}}$. An alternative implementation could first
iterate over the data in $N$ and then in $M$. That implementation
still obeys the laws (see Exercise~\ref{subsec:Exercise-traversables-3-1}
for a proof in case $M=N=\text{Id}$).

\paragraph{Co-products}

Here $L^{A}\triangleq M^{A}+N^{A}$, with some traversable functors
$M$ and $N$. We assume that $\text{seq}_{M}$ and $\text{seq}_{N}$
are already available and satisfy the laws, and define $\text{seq}_{L}$
as:
\[
\text{seq}_{L}:M^{F^{A}}+N^{F^{A}}\rightarrow F^{M^{A}+N^{A}}\quad,\quad\text{seq}_{L}\triangleq\,\begin{array}{|c||c|}
 & F^{M^{A}+N^{A}}\\
\hline M^{F^{A}} & \text{seq}_{M}^{F,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow F}\\
N^{F^{A}} & \text{seq}_{N}^{F,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow F}
\end{array}\quad.
\]

\begin{comment}
{*}{*}{*} do we need this? 

It helps to rewrite $\text{seq}_{L}$ as a composition of two matrices,
separating the functions lifted to $F$:
\begin{align*}
 & \text{seq}_{L}\triangleq\,\begin{array}{|c||cc|}
 & F^{M^{A}} & F^{N^{A}}\\
\hline M^{F^{A}} & \text{seq}_{M}^{F,A} & \bbnum 0\\
N^{F^{A}} & \bbnum 0 & \text{seq}_{N}^{F,A}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & F^{M^{A}+N^{A}}\\
\hline F^{M^{A}} & (m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow F}\\
F^{N^{A}} & (n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow F}
\end{array}\\
 & =\,\begin{array}{|c||cc|}
 & F^{M^{A}} & F^{N^{A}}\\
\hline M^{F^{A}} & \text{seq}_{M}^{F,A} & \bbnum 0\\
N^{F^{A}} & \bbnum 0 & \text{seq}_{N}^{F,A}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & F^{M^{A}+N^{A}}\\
\hline F^{M^{A}} & (m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow F}\\
F^{N^{A}} & (n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow F}
\end{array}\quad.
\end{align*}
\end{comment}
To verify the identity law~(\ref{eq:identity-law-of-sequence}):
\begin{align*}
 & \text{seq}_{L}^{\text{Id},A}=\,\begin{array}{|c||c|}
 & M^{A}+N^{A}\\
\hline M^{\text{Id}^{A}} & \text{seq}_{M}^{\text{Id},A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow\text{Id}}\\
N^{\text{Id}^{A}} & \text{seq}_{N}^{\text{Id},A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow\text{Id}}
\end{array}\\
 & \quad{\color{greenunder}\text{identity laws of }\text{seq}_{M}\text{ and }\text{seq}_{N}:}\quad\\
 & =\,\begin{array}{|c||c|}
 & M^{A}+N^{A}\\
\hline M^{A} & m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}}\\
N^{A} & n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n
\end{array}\,=\,\begin{array}{|c||cc|}
 & M^{A} & N^{A}\\
\hline M^{A} & m\rightarrow m & \bbnum 0\\
N^{A} & \bbnum 0 & n\rightarrow n
\end{array}\,=\text{id}\quad.
\end{align*}

To verify the composition law~(\ref{eq:composition-law-of-sequence}),
write its two sides separately:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}\\
 & \quad=\,\begin{array}{||c|}
\text{seq}_{M}^{F,G^{A}}\bef(m^{:M^{G^{A}}}\rightarrow m+\bbnum 0^{:N^{G^{A}}})^{\uparrow F}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}\\
\text{seq}_{N}^{F,G^{A}}\bef(n^{:N^{G^{A}}}\rightarrow\bbnum 0^{:M^{G^{A}}}+n)^{\uparrow F}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}
\end{array}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq}_{L}^{F\circ G,A}=\,\begin{array}{||c|}
\text{seq}_{M}^{F\circ G,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G\uparrow F}\\
\text{seq}_{N}^{F\circ G,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G\uparrow F}
\end{array}\quad.
\end{align*}
The left-hand side contains some function compositions lifted to $F$.
Write them separately:
\begin{align*}
 & (m^{:M^{G^{A}}}\rightarrow m+\bbnum 0^{:N^{G^{A}}})\bef\text{seq}_{L}^{G,A}\\
 & \quad=\,\begin{array}{|c||cc|}
 & M^{G^{A}} & N^{G^{A}}\\
\hline M^{G^{A}} & \text{id} & \bbnum 0
\end{array}\,\bef\,\begin{array}{|c||c|}
 & G^{M^{A}+N^{A}}\\
\hline M^{G^{A}} & \text{seq}_{M}^{G,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G}\\
N^{G^{A}} & \text{seq}_{N}^{G,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G}
\end{array}\\
 & \quad=\text{seq}_{M}^{G,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G}\quad,\\
 & (n^{:N^{G^{A}}}\rightarrow\bbnum 0^{:M^{G^{A}}}+n)\bef\text{seq}_{L}^{G,A}\\
 & \quad=\,\begin{array}{|c||cc|}
 & M^{G^{A}} & N^{G^{A}}\\
\hline N^{G^{A}} & \bbnum 0 & \text{id}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & G^{M^{A}+N^{A}}\\
\hline M^{G^{A}} & \text{seq}_{M}^{G,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G}\\
N^{G^{A}} & \text{seq}_{N}^{G,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G}
\end{array}\\
 & \quad=\text{seq}_{N}^{G,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G}\quad.
\end{align*}
Then simplify the left-hand side using the composition laws of $\text{seq}_{M}$
and $\text{seq}_{N}$:
\begin{align*}
 & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=\,\begin{array}{||c|}
\gunderline{\text{seq}_{M}^{F,G^{A}}\bef(\text{seq}_{M}^{G,A})^{\uparrow F}}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G\uparrow F}\\
\gunderline{\text{seq}_{N}^{F,G^{A}}\bef(\text{seq}_{N}^{G,A})^{\uparrow F}}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G\uparrow F}
\end{array}\\
 & =\,\begin{array}{||c|}
\text{seq}_{M}^{F\circ G,A}\bef(m^{:M^{A}}\rightarrow m+\bbnum 0^{:N^{A}})^{\uparrow G\uparrow F}\\
\text{seq}_{N}^{F\circ G,A}\bef(n^{:N^{A}}\rightarrow\bbnum 0^{:M^{A}}+n)^{\uparrow G\uparrow F}
\end{array}\quad.
\end{align*}
The two sides are now equal.

\paragraph{Recursive types}

Here $L^{A}\triangleq S^{A,L^{A}}$, with a recursion scheme given
by a bifunctor\index{bifunctor} $S$. In order to obtain the traversable
property of $L$, we will need to assume that $S$ is traversable
with respect to both its type parameters in a special way, which we
call \textsf{``}bitraversable\textsf{''}. It is not sufficient if $S$ is traversable
with respect to each type parameter separately.

A bifunctor $S$ is called \textbf{bitraversable}\index{bitraversable bifunctor}
if it has a \lstinline!bisequence! method (denoted by $\text{seq2}_{S}$):
\[
\text{seq2}_{S}^{F,A,B}:S^{F^{A},F^{B}}\rightarrow F^{S^{A,B}}\quad,
\]
which is parameterized by an arbitrary applicative functor $F$ and
is natural in $F$, $A$, and $B$. The traversable laws of identity\index{identity laws!of bisequence@of \texttt{bisequence}}
and composition\index{composition law!of bisequence@of \texttt{bisequence}}
must also hold for \lstinline!bisequence!:
\begin{align}
{\color{greenunder}\text{identity law of }\text{seq2}_{S}:}\quad & \text{seq2}_{S}^{\text{Id},A,B}=\text{id}^{:S^{A,B}\rightarrow S^{A,B}}\quad,\label{eq:identity-law-of-bisequence}\\
{\color{greenunder}\text{composition law of }\text{seq2}_{S}:}\quad & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef\big(\text{seq2}_{S}^{G,A,B}\big)^{\uparrow F}=\text{seq2}_{S}^{F\circ G,A,B}\quad.\label{eq:composition-law-of-bisequence}
\end{align}

Section~\ref{subsec:All-polynomial-bifunctors-are-bitraversable}
will show that all polynomial bifunctors are bitraversable. So, we
are free to use any polynomial bifunctor $S$ as a recursion scheme.
For now, we assume that a lawful $\text{seq2}_{S}$ is available and
define $\text{seq}_{L}$ as:
\[
\text{seq}_{L}^{F,A}:S^{F^{A},L^{F^{A}}}\rightarrow F^{S^{A,L^{A}}}\quad,\quad\quad\text{seq}_{L}^{F,A}\triangleq\big(\overline{\text{seq}}_{L}^{F,A}\big)^{\uparrow S^{F^{A},\bullet}}\bef\text{seq2}_{S}^{F,A,L^{A}}\quad.
\]
This definition uses a recursive call to $\overline{\text{seq}}_{L}^{F,A}$
lifted with respect to the type parameter $R$ of $S^{A,R}$. The
inductive assumption is that the recursively called $\overline{\text{seq}}_{L}^{F,A}$
already obeys the laws we are proving.

To verify the identity law~(\ref{eq:identity-law-of-sequence}),
we use the assumed law~(\ref{eq:identity-law-of-bisequence}):
\[
\text{seq}_{L}^{\text{Id},A}=\big(\overline{\text{seq}}_{L}^{\text{Id},A}\big)^{\uparrow S^{\text{Id}^{A},\bullet}}\bef\text{seq2}_{S}^{\text{Id},A,L^{A}}=\text{id}^{\uparrow S^{A,\bullet}}\bef\text{id}=\text{id}\quad.
\]

To verify the composition law~(\ref{eq:composition-law-of-sequence}),
write its two sides separately:
\begin{align*}
 & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}\\
 & \quad=\big(\overline{\text{seq}}_{L}^{F,G^{A}}\big)^{\uparrow S^{F^{G^{A}},\bullet}}\bef\text{seq2}_{S}^{F,G^{A},L^{G^{A}}}\bef\big(\overline{\text{seq}}_{L}^{G,A}\big)^{\uparrow S^{G^{A},\bullet}\uparrow F}\bef\big(\text{seq2}_{S}^{G,A,L^{A}}\big)^{\uparrow F}\quad,\\
 & \text{seq}_{L}^{F\circ G,A}=\big(\overline{\text{seq}}_{L}^{F\circ G,A}\big)^{\uparrow S^{F^{G^{A}},\bullet}}\bef\text{seq2}_{S}^{F\circ G,A,L^{A}}\quad.
\end{align*}
We use the naturality law of $\text{seq2}_{S}^{F,A,B}$ with respect
to the parameter $B$:
\[
\text{seq2}_{S}^{F,A,B}\bef(f^{:B\rightarrow C})^{\uparrow S^{A,\bullet}\uparrow F}=f^{\uparrow F\uparrow S^{F^{A},\bullet}}\bef\text{seq2}_{S}^{F,A,C}\quad.
\]
Then the left-hand side of the law~(\ref{eq:composition-law-of-sequence})
becomes:
\begin{align*}
 & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}\\
 & =\gunderline{\big(\overline{\text{seq}}_{L}^{F,G^{A}}\big)^{\uparrow S^{F^{G^{A}},\bullet}}\bef\big(\overline{\text{seq}}_{L}^{G,A}\big)^{\uparrow F\uparrow S^{F^{G^{A}},\bullet}}}\bef\text{seq2}_{S}^{F,G^{A},G^{L^{A}}}\bef\big(\text{seq2}_{S}^{G,A,L^{A}}\big)^{\uparrow F}\\
 & \quad{\color{greenunder}\text{composition law~(\ref{eq:composition-law-of-sequence})}:}\quad\\
 & =(\text{seq}_{L}^{F\circ G,A})^{\uparrow S^{F^{G^{A}},\bullet}}\bef\gunderline{\text{seq2}_{S}^{F,G^{A},G^{L^{A}}}\bef\big(\text{seq2}_{S}^{G,A,L^{A}}\big)^{\uparrow F}}\\
 & \quad{\color{greenunder}\text{composition law~(\ref{eq:composition-law-of-bisequence})}:}\quad\\
 & =(\text{seq}_{L}^{F\circ G,A})^{\uparrow S^{F^{G^{A}},\bullet}}\bef\gunderline{\text{seq2}_{S}^{F\circ G,A,L^{A}}}\quad.
\end{align*}
The two sides of the law~(\ref{eq:composition-law-of-sequence})
are now equal. $\square$

These constructions allow us to implement a \lstinline!Traversable!
instance automatically for any polynomial functor. The corresponding
\lstinline!traverse! function will iterate in a naturally defined
order over all values stored in the functor. To visualize the way
\lstinline!traverse! works, write a polynomial functor $L$ as:
\[
L^{A}=Z_{0}+Z_{1}\times A+Z_{2}\times A\times A+...\quad,
\]
where $Z_{0}$, $Z_{1}$, ..., are fixed types. Any polynomial functor
may be equivalently rewritten in this way, perhaps with an infinite
number of parts in the disjunction. Each part of the disjunction is
written as $Z_{k}\times A\times...\times A$ and contains a finite
number $k$ of values of type $A$ and a value of a fixed type $Z_{k}$.
The computation \lstinline!traverse(f)!, where $f$ is a function
of type $A\rightarrow F^{B}$, iterates over the $k$ values of type
$A$ and merges the $F$-effects in the natural order using $F$\textsf{'}s
\lstinline!zip! operation. This creates a value of type $F^{B\times...\times B}$.
The constant value of type $Z_{k}$ is then lifted into $F$ to get
the final value of type $F^{Z_{k}\times B\times B\times...\times B}$.

If the traversal order needs to be chosen differently, we will need
to provide a custom implementation of a \lstinline!Traversable! instance.

Functors that are not equivalent to polynomial functors are not traversable.\footnote{To prove that any traversable functor \emph{must} be polynomial, one
needs advanced methods beyond the scope of this book. See the paper
by R.~Bird\index{Richard Bird} et al.: \texttt{\href{http://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf}{http://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf}}
and the paper by M.~\index{Mauro Jaskelioff}Jaskelioff and R.~O\textsf{'}Connor:
\texttt{\href{https://arxiv.org/abs/1402.1699}{https://arxiv.org/abs/1402.1699}}} As an example, consider the functors $L^{A}\triangleq E\rightarrow A$
and $F^{A}\triangleq Z+A$, where the types $E$ and $Z$ are arbitrary
but fixed. Then the type signature of \lstinline!sequence! $:L^{F^{B}}\rightarrow F^{L^{B}}$
becomes $(E\rightarrow Z+A)\rightarrow Z+(E\rightarrow A)$, which
is impossible to implement via a fully parametric function.

\subsection{All polynomial bifunctors are bitraversable\label{subsec:All-polynomial-bifunctors-are-bitraversable}}

Bitraversable bifunctors are used in the recursive-type construction
for traversable functors. The properties of bitraversable bifunctors
are similar to the properties of traversable functors. We will now
prove that all polynomial bifunctors are bitraversable. This verifies
that each of the five type constructions of polynomial bifunctors
produces a bitraversable bifunctor satisfying the laws~(\ref{eq:identity-law-of-bisequence})\textendash (\ref{eq:composition-law-of-bisequence}).

\paragraph{Fixed type}

If $S^{A,B}\triangleq Z$ where $Z$ is a fixed type, we define \lstinline!bisequence!
via $F$\textsf{'}s \lstinline!pure! method:
\[
\text{seq2}_{S}^{F,A,B}:S^{F^{A},F^{B}}\rightarrow F^{S^{A,B}}\cong Z\rightarrow F^{Z}\quad,\quad\quad\text{seq2}_{S}^{F,A,B}\triangleq\text{pu}_{F}^{:Z\rightarrow F^{Z}}\quad.
\]
The identity law~(\ref{eq:identity-law-of-bisequence}) holds since
$\text{seq2}_{S}^{\text{Id},A,B}=\text{pu}_{\text{Id}}=\text{id}$.
To verify the composition law~(\ref{eq:composition-law-of-bisequence}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}=\text{pu}_{F}\bef(\text{pu}_{G})^{\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{pu}_{F}:}\quad & \quad=\text{pu}_{G}\bef\text{pu}_{F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq2}_{S}^{F\circ G,A,B}=\text{pu}_{F\circ G}\\
{\color{greenunder}\text{definition of }\text{pu}_{F\circ G}:}\quad & \quad=\text{pu}_{G}\bef\text{pu}_{F}\quad.
\end{align*}
The two sides of the law are now equal.

\paragraph{Type parameter}

If $S^{A,B}\triangleq A$ or $S^{A,B}\triangleq B$, we define \lstinline!bisequence!
as the identity function:
\begin{align*}
{\color{greenunder}\text{if }S^{A,B}\triangleq A:}\quad & \text{seq2}_{S}^{F,A,B}\triangleq\text{id}^{:F^{A}\rightarrow F^{A}}\quad,\\
{\color{greenunder}\text{if }S^{A,B}\triangleq B:}\quad & \text{seq2}_{S}^{F,A,B}\triangleq\text{id}^{:F^{B}\rightarrow F^{B}}\quad.
\end{align*}
Identity functions always satisfy identity and composition laws.

\paragraph{Products}

Here $S^{A,B}\triangleq M^{A,B}\times N^{A,B}$, with some bitraversable
bifunctors $M$ and $N$. We assume that $\text{seq2}_{M}$ and $\text{seq2}_{N}$
are already available and satisfy the laws, and define $\text{seq2}_{S}$
as:
\begin{align*}
 & \text{seq2}_{S}^{F,A,B}:M^{F^{A},F^{B}}\times N^{F^{A},F^{B}}\rightarrow F^{M^{A,B}\times N^{A,B}}\quad,\\
 & \text{seq2}_{S}^{F,A,B}\triangleq(\text{seq2}_{M}^{F,A,B}\boxtimes\text{seq2}_{N}^{F,A,B})\bef\text{zip}_{F}\quad.
\end{align*}

To verify the identity law~(\ref{eq:identity-law-of-bisequence}),
use the fact that $\text{zip}_{\text{Id}}=\text{id}$:
\begin{align*}
 & \text{seq2}_{S}^{\text{Id},A,B}=(\gunderline{\text{seq2}_{M}^{\text{Id},A,B}}\boxtimes\gunderline{\text{seq2}_{N}^{\text{Id},A,B}})\bef\text{zip}_{\text{Id}}\\
{\color{greenunder}\text{identity laws of }\text{seq2}_{M}\text{ and }\text{seq2}_{N}:}\quad & =(\text{id}\boxtimes\text{id})=\text{id}\quad.
\end{align*}

To verify the composition law~(\ref{eq:composition-law-of-bisequence}),
begin with its left-hand side:
\begin{align*}
 & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}\\
 & =(\text{seq2}_{M}^{F,G^{A},G^{B}}\boxtimes\text{seq2}_{N}^{F,G^{A},G^{B}})\bef\gunderline{\text{zip}_{F}\bef(\text{seq2}_{M}^{G,A,B}\boxtimes\text{seq2}_{N}^{G,A,B})^{\uparrow F}}\bef\text{zip}_{G}^{\uparrow F}\\
 & \quad{\color{greenunder}\text{naturality of }\text{zip}_{F}:}\quad\\
 & =\gunderline{(\text{seq2}_{M}^{F,G^{A},G^{B}}\boxtimes\text{seq2}_{N}^{F,G^{A},G^{B}})\bef\big((\text{seq2}_{M}^{G,A,B})^{\uparrow F}\boxtimes(\text{seq2}_{N}^{G,A,B})^{\uparrow F}\big)}\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\\
 & \quad{\color{greenunder}\text{the law~(\ref{eq:pair-product-composition-law})}:}\quad\\
 & =\big((\text{seq2}_{M}^{F,G^{A},G^{B}}\bef(\text{seq2}_{M}^{G,A,B})^{\uparrow F})\boxtimes(\text{seq2}_{N}^{F,G^{A},G^{B}}\bef(\text{seq2}_{N}^{G,A,B})^{\uparrow F})\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\end{align*}
By assumption, $\text{seq2}_{M}$ and $\text{seq2}_{N}$ satisfy their
composition laws. So, we may rewrite the last line as:
\[
\text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}=\big(\text{seq2}_{M}^{F\circ G,A,B}\boxtimes\text{seq2}_{N}^{F\circ G,A,B}\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\]
We can now transform the right-hand side of the law to the same expression:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq2}_{S}^{F\circ G,A,B}=\big(\text{seq2}_{M}^{F\circ G,A,B}\boxtimes\text{seq2}_{N}^{F\circ G,A,B}\big)\bef\text{zip}_{F\circ G}\\
{\color{greenunder}\text{definition of }\text{zip}_{F\circ G}:}\quad & =\big(\text{seq2}_{M}^{F\circ G,A,B}\boxtimes\text{seq2}_{N}^{F\circ G,A,B}\big)\bef\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad.
\end{align*}


\paragraph{Co-products}

Here $S^{A,B}\triangleq M^{A,B}+N^{A,B}$, with some bitraversable
bifunctors $M$ and $N$. We assume that $\text{seq2}_{M}$ and $\text{seq2}_{N}$
are already available and satisfy the laws, and define $\text{seq2}_{S}$
as:
\begin{align*}
 & \text{seq2}_{S}:M^{F^{A},F^{B}}+N^{F^{A},F^{B}}\rightarrow F^{M^{A,B}+N^{A,B}}\quad,\\
 & \text{seq2}_{S}\triangleq\,\begin{array}{|c||c|}
 & F^{M^{A,B}+N^{A,B}}\\
\hline M^{F^{A},F^{B}} & \text{seq2}_{M}^{F,A,B}\bef(m^{:M^{A,B}}\rightarrow m+\bbnum 0)^{\uparrow F}\\
N^{F^{A},F^{B}} & \text{seq2}_{N}^{F,A,B}\bef(n^{:N^{A,B}}\rightarrow\bbnum 0+n)^{\uparrow F}
\end{array}\quad.
\end{align*}

To verify the identity law~(\ref{eq:identity-law-of-bisequence}):
\begin{align*}
 & \text{seq2}_{S}^{\text{Id},A}=\,\begin{array}{|c||c|}
 & M^{A,B}+N^{A,B}\\
\hline M^{\text{Id}^{A},\text{Id}^{B}} & \text{seq2}_{M}^{\text{Id},A,B}\bef(m\rightarrow m+\bbnum 0)^{\uparrow\text{Id}}\\
N^{\text{Id}^{A}} & \text{seq2}_{N}^{\text{Id},A,B}\bef(n\rightarrow\bbnum 0+n)^{\uparrow\text{Id}}
\end{array}\\
 & \quad{\color{greenunder}\text{identity laws of }\text{seq2}_{M}\text{ and }\text{seq2}_{N}:}\quad\\
 & =\,\begin{array}{|c||c|}
 & M^{A,B}+N^{A,B}\\
\hline M^{A,B} & m\rightarrow m+\bbnum 0\\
N^{A,B} & n\rightarrow\bbnum 0+n
\end{array}\,=\,\begin{array}{|c||cc|}
 & M^{A,B} & N^{A,B}\\
\hline M^{A,B} & m\rightarrow m & \bbnum 0\\
N^{A,B} & \bbnum 0 & n\rightarrow n
\end{array}\,=\text{id}\quad.
\end{align*}

To verify the composition law~(\ref{eq:composition-law-of-bisequence}),
write its two sides separately:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}\\
 & \quad=\,\begin{array}{||c|}
\text{seq2}_{M}^{F,G^{A},G^{B}}\bef(m\rightarrow m+\bbnum 0)^{\uparrow F}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}\\
\text{seq2}_{N}^{F,G^{A},G^{B}}\bef(n\rightarrow\bbnum 0+n)^{\uparrow F}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}
\end{array}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq2}_{S}^{F\circ G,A,B}=\,\begin{array}{||c|}
\text{seq2}_{M}^{F\circ G,A,B}\bef(m\rightarrow m+\bbnum 0)^{\uparrow G\uparrow F}\\
\text{seq2}_{N}^{F\circ G,A,B}\bef(n\rightarrow\bbnum 0+n)^{\uparrow G\uparrow F}
\end{array}\quad.
\end{align*}
The left-hand side contains some function compositions lifted to $F$.
Write them separately:
\begin{align*}
 & (m\rightarrow m+\bbnum 0)\bef\text{seq2}_{S}^{G,A,B}\\
 & \quad=\,\begin{array}{|c||cc|}
 & M^{G^{A},G^{B}} & N^{G^{A},G^{B}}\\
\hline M^{G^{A},G^{B}} & \text{id} & \bbnum 0
\end{array}\,\bef\,\begin{array}{|c||c|}
 & G^{M^{A,B}+N^{A,B}}\\
\hline M^{G^{A},G^{B}} & \text{seq2}_{M}^{G,A,B}\bef(m\rightarrow m+\bbnum 0)^{\uparrow G}\\
N^{G^{A},G^{B}} & \text{seq2}_{N}^{G,A,B}\bef(n\rightarrow\bbnum 0+n)^{\uparrow G}
\end{array}\\
 & \quad=\text{seq2}_{M}^{G,A,B}\bef(m^{:M^{A,B}}\rightarrow m+\bbnum 0^{:N^{A,B}})^{\uparrow G}\quad,\\
 & (n\rightarrow\bbnum 0+n)\bef\text{seq2}_{S}^{G,A,B}\\
 & \quad=\,\begin{array}{|c||cc|}
 & M^{G^{A},G^{B}} & N^{G^{A},G^{B}}\\
\hline N^{G^{A},G^{B}} & \bbnum 0 & \text{id}
\end{array}\,\bef\,\begin{array}{|c||c|}
 & G^{M^{A,B}+N^{A,B}}\\
\hline M^{G^{A},G^{B}} & \text{seq2}_{M}^{G,A,B}\bef(m\rightarrow m+\bbnum 0)^{\uparrow G}\\
N^{G^{A},G^{B}} & \text{seq2}_{N}^{G,A,B}\bef(n\rightarrow\bbnum 0+n)^{\uparrow G}
\end{array}\\
 & \quad=\text{seq2}_{N}^{G,A,B}\bef(n^{:N^{A,B}}\rightarrow\bbnum 0^{:M^{A,B}}+n)^{\uparrow G}\quad.
\end{align*}
We then simplify the left-hand side using the composition laws of
$\text{seq2}_{M}$ and $\text{seq2}_{N}$:
\begin{align*}
 & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}=\,\begin{array}{||c|}
\gunderline{\text{seq2}_{M}^{F,G^{A},G^{B}}\bef(\text{seq2}_{M}^{G,A,B})^{\uparrow F}}\bef(m\rightarrow m+\bbnum 0)^{\uparrow G\uparrow F}\\
\gunderline{\text{seq2}_{N}^{F,G^{A},G^{B}}\bef(\text{seq2}_{N}^{G,A,B})^{\uparrow F}}\bef(n\rightarrow\bbnum 0+n)^{\uparrow G\uparrow F}
\end{array}\\
 & =\,\begin{array}{||c|}
\text{seq2}_{M}^{F\circ G,A,B}\bef(m\rightarrow m+\bbnum 0)^{\uparrow G\uparrow F}\\
\text{seq2}_{N}^{F\circ G,A,B}\bef(n\rightarrow\bbnum 0+n)^{\uparrow G\uparrow F}
\end{array}\quad.
\end{align*}
The two sides are now equal.

\paragraph{Recursive types}

Here $S^{A,B}\triangleq T^{A,B,S^{A,B}}$, with a recursion scheme
given by a \textbf{3-functor} \index{3@3-functor}$T^{A,B,R}$ (a
type constructor covariant in each of its three type parameters).
To show that $S$ is bitraversable, we need to assume that $T$ is
traversable with respect to all its type parameters at once, which
we call \textsf{``}3-traversable\textsf{''}. We say that a 3-functor $T^{A,B,R}$
is \textbf{3-traversable}\index{3@3-traversable 3-functor} if it
has a \lstinline!sequence3! method:
\[
\text{seq3}_{T}^{F,A,B,C}:T^{F^{A},F^{B},F^{C}}\rightarrow F^{T^{A,B,C}}\quad,
\]
which is parameterized by an arbitrary applicative functor $F$ and
is natural in $F$, $A$, $B$, and $C$. The traversable laws of
identity\index{identity laws!of sequence3@of \texttt{sequence3}}
and composition\index{composition law!of sequence3@of \texttt{sequence3}}
must also hold for \lstinline!sequence3!:
\begin{align}
{\color{greenunder}\text{identity law of }\text{seq3}_{T}:}\quad & \text{seq3}_{T}^{\text{Id},A,B,C}=\text{id}^{:T^{A,B,C}\rightarrow T^{A,B,C}}\quad,\label{eq:identity-law-of-trisequence}\\
{\color{greenunder}\text{composition law of }\text{seq3}_{T}:}\quad & \text{seq3}_{T}^{F,G^{A},G^{B},G^{C}}\bef\big(\text{seq3}_{T}^{G,A,B,C}\big)^{\uparrow F}=\text{seq3}_{T}^{F\circ G,A,B,C}\quad.\label{eq:composition-law-of-trisequence}
\end{align}

To prove that all polynomial 3-functors are 3-traversable, we would
need to repeat the proofs in this section with more type parameters.
The recursive type construction for 3-traversable 3-functors will
require a 4-traversable 4-functor as a recursion scheme. To prove
that all polynomial 4-functors are 4-traversable, we will need to
use 5-traversable 5-functors, and so on. Note that the proofs in this
section differ from the proofs in Section~\ref{subsec:All-polynomial-functors-are-traversable}
only in having more type parameters in the functions. So, the proofs
for 3-traversable 3-functors, 4-traversable 4-functors, etc., will
remain essentially the same as the proofs for bitraversable bifunctors
except for having more type parameters in each function. For this
reason, we omit those proofs. The conclusion will be that all polynomial
$n$-functors are $n$-traversable for $n=1,2,3,...$

Assume that a lawful $\text{seq3}_{T}$ is available, and define $\text{seq2}_{S}$
as:
\begin{align*}
 & \text{seq2}_{S}^{F,A,B}:T^{F^{A},F^{B},S^{F^{A},F^{B}}}\rightarrow F^{T^{A,B,S^{A,B}}}\quad,\\
 & \text{seq2}_{S}^{F,A,B}\triangleq\big(\overline{\text{seq2}}_{S}^{F,A,B}\big)^{\uparrow T^{F^{A},F^{B},\bullet}}\bef\text{seq3}_{T}^{F,A,B,S^{A,B}}\quad.
\end{align*}
The inductive assumption is that the recursively called $\overline{\text{seq2}}_{S}$
already obeys the laws we are proving.

To verify the identity law~(\ref{eq:identity-law-of-bisequence}),
we use the assumed law~(\ref{eq:identity-law-of-trisequence}):
\[
\text{seq2}_{S}^{\text{Id},A,B}=\big(\overline{\text{seq2}}_{S}^{\text{Id},A,B}\big)^{\uparrow T^{\text{Id}^{A},\text{Id}^{B},\,\bullet}}\bef\text{seq3}_{T}^{\text{Id},A,B,S^{A,B}}=\text{id}^{\uparrow T^{A,B,\bullet}}\bef\text{id}=\text{id}\quad.
\]

To verify the composition law~(\ref{eq:composition-law-of-bisequence}):
\begin{align*}
 & \text{seq2}_{S}^{F,G^{A},G^{B}}\bef(\text{seq2}_{S}^{G,A,B})^{\uparrow F}=\big(\overline{\text{seq2}}_{S}^{F,G^{A},G^{B}}\big)^{\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}\bef\text{seq3}_{T}^{F,G^{A},G^{B},S^{G^{A},G^{B}}}\\
 & \quad\quad\quad\bef\big(\overline{\text{seq2}}_{S}^{G,A,B}\big)^{\uparrow T^{G^{A},G^{B},\bullet}\uparrow F}\bef\big(\text{seq3}_{T}^{G,A,B,S^{A,B}}\big)^{\uparrow F}\quad,\\
 & \text{seq2}_{S}^{F\circ G,A,B}=\big(\overline{\text{seq2}}_{S}^{F\circ G,A,B}\big)^{\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}\bef\text{seq3}_{T}^{F\circ G,A,B,S^{A,B}}\quad.
\end{align*}
We use the naturality law of $\text{seq3}_{T}^{F,A,B,C}$ with respect
to the parameter $C$:
\[
\text{seq3}_{T}^{F,A,B,C}\bef(f^{:C\rightarrow D})^{\uparrow T^{A,B,\bullet}\uparrow F}=f^{\uparrow F\uparrow T^{F^{A},F^{B},\,\bullet}}\bef\text{seq3}_{T}^{F,A,B,D}\quad.
\]
Then the left-hand side of the law~(\ref{eq:composition-law-of-bisequence})
becomes:
\begin{align*}
\text{seq2}_{S}^{F,G^{A},G^{B}}\bef & (\text{seq2}_{S}^{G,A,B})^{\uparrow F}=\gunderline{\big(\overline{\text{seq2}}_{S}^{F,G^{A},G^{B}}\big)^{\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}\bef\big(\overline{\text{seq2}}_{S}^{G,A,B}\big)^{\uparrow F\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}}\\
 & \quad\quad\quad\bef\text{seq3}_{T}^{F,G^{A},G^{B},G^{S^{A,B}}}\bef\big(\text{seq3}_{T}^{G,A,B,S^{A,B}}\big)^{\uparrow F}\\
{\color{greenunder}\text{Eq.~(\ref{eq:identity-law-of-bisequence})}:}\quad & =(\text{seq2}_{S}^{F\circ G,A,B})^{\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}\bef\gunderline{\text{seq3}_{T}^{F,G^{A},G^{B},G^{S^{A,B}}}\bef\big(\text{seq3}_{T}^{G,A,B,S^{A,B}}\big)^{\uparrow F}}\\
{\color{greenunder}\text{Eq.~(\ref{eq:composition-law-of-trisequence})}:}\quad & =(\text{seq2}_{S}^{F\circ G,A,B})^{\uparrow T^{F^{G^{A}},F^{G^{B}},\,\bullet}}\bef\gunderline{\text{seq3}_{T}^{F\circ G,A,B,S^{A,B}}}\quad.
\end{align*}
The two sides of the law~(\ref{eq:identity-law-of-bisequence}) are
now equal.

\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-traversables-1}\ref{subsec:Exercise-traversables-1}}

Show that any traversable functor $L$ admits a method called \lstinline!consume!:
\[
\text{consume}_{L}:(L^{A}\rightarrow B)\rightarrow L^{F^{A}}\rightarrow F^{B}\quad,
\]
defined for any applicative functor $F$. Assuming a suitable naturality
law for \lstinline!consume!, show that the types of \lstinline!sequence!
and \lstinline!consume! are equivalent.

\subsubsection{Exercise \label{subsec:Exercise-traversables-3-1-1}\ref{subsec:Exercise-traversables-3-1-1}}

Consider the following implementation of \lstinline!traverse! for
the functor $L^{A}\triangleq\bbnum 1+A\times A$:
\begin{lstlisting}
def badtrav[A, B, F[_]: Applicative : Functor](f: A => F[B]): Option[(A, A)] => F[Option[(B, B)]] =
  Applicative[F].pure(None)
\end{lstlisting}
Show that \lstinline!badtrav! does \emph{not} satisfy the laws of
\lstinline!traverse!.

\subsubsection{Exercise \label{subsec:Exercise-traversables-3}\ref{subsec:Exercise-traversables-3}}

Show that the laws are \emph{not} satisfied by the implementation
of $\text{seq}:L^{F^{A}}\rightarrow F^{L^{A}}$ for $F^{A}\triangleq\bbnum 1+A$
where \lstinline!seq! always returns an empty option ($1+\bbnum 0$,
or \lstinline!None! in Scala).

\subsubsection{Exercise \label{subsec:Exercise-traversables-3-1}\ref{subsec:Exercise-traversables-3-1}}

For $L^{A}\triangleq A\times A$, consider $\text{seq}_{L}^{F,A}$
which collects the $F$-effects in the opposite order:
\begin{lstlisting}
def seq[F[_]: Applicative : Functor, A]: ((F[A], F[A])) => F[(A, A)] = {
  case (fa1, fa2) => (fa2 zip fa1).map(_.swap) // Use `swap` to restore the order of wrapped values.
}
\end{lstlisting}
In the point-free style, this code is written as:
\[
\text{seq}_{L}^{F,A}:F^{A}\times F^{A}\rightarrow F^{A\times A}\quad,\quad\quad\text{seq}_{L}^{F,A}\triangleq\text{swap}\bef\text{zip}_{F}\bef\text{swap}^{\uparrow F}\quad.
\]
Prove that this implementation of \lstinline!sequence! satisfies
its laws.

\subsubsection{Exercise \label{subsec:Exercise-traversables-laws}\ref{subsec:Exercise-traversables-laws}}

Find an example of an applicative functor $F$ and a traversable functor
$L$ such that one \emph{cannot} implement a natural transformation
with type signature $F^{L^{A}}\rightarrow L^{F^{A}}$ (the inverse
to the type signature of \lstinline!sequence!).

\subsubsection{Exercise \label{subsec:Exercise-traversables-laws-2}\ref{subsec:Exercise-traversables-laws-2}}

Show that the naturality law~(\ref{eq:sequence-naturality-law})
of \lstinline!sequence! is equivalent to the two naturality laws~(\ref{eq:naturality-laws-of-traverse})
of \lstinline!traverse!.

\subsubsection{Exercise \label{subsec:Exercise-traversables-laws-1}\ref{subsec:Exercise-traversables-laws-1}}

Statement~\ref{subsec:Statement-tr-equivalent-to-ftr} shows that
functions \lstinline!tr! and \lstinline!ftr! are equivalent if a
naturality law holds for \lstinline!ftr! with respect to the type
parameter $A$. Under the same assumption, show that the naturality
law of \lstinline!ftr! with respect to the type parameter $B$ is
equivalent to the naturality law of \lstinline!tr!.

\subsubsection{Exercise \label{subsec:Exercise-traversables-laws-1-1}\ref{subsec:Exercise-traversables-laws-1-1}}

Statement~\ref{subsec:Statement-foldleft-foldmap-equivalence}(a)
shows that functions \lstinline!foldMap! and \lstinline!reduceE!
are equivalent. Show that the monoidal naturality law holds for \lstinline!foldMap!
if it holds for \lstinline!reduceE!, and vice versa.

\subsubsection{Exercise \label{subsec:Exercise-traversables-5}\ref{subsec:Exercise-traversables-5}}

Prove that $L^{A}\triangleq M^{N^{A}}$ is a lawful traversable if
$M$ and $N$ are traversable functors.

\subsubsection{Exercise \label{subsec:Exercise-traversables-4}\ref{subsec:Exercise-traversables-4}}

Prove directly that all the bitraversable laws hold for the bifunctor
$S^{A,B}\triangleq A\times B$.

\subsubsection{Exercise \label{subsec:Exercise-traversables-6}\ref{subsec:Exercise-traversables-6}}

For the tree-like type defined as $T^{A}\triangleq\bbnum 1+A\times T^{A}\times T^{A}$,
define a \lstinline!Traversable! instance. Verify that the laws hold
by using a suitable recursion scheme $S^{A,R}$.

\subsubsection{Exercise \label{subsec:Exercise-traversables-9}\ref{subsec:Exercise-traversables-9}}

Is the recursive type constructor $L^{A}\triangleq A+L^{\text{List}^{A}}$
traversable? Explain via examples what sort of data container it is.

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-2}\ref{subsec:Exercise-traversables-10-2}}

Prove that for any two monoid morphisms $\phi:M\rightarrow N$ and
$\psi:N\rightarrow P$ (where $M$, $N$, and $P$ are monoids), the
composition $\phi\bef\psi:M\rightarrow P$ is again a lawful monoid
morphism.

\subsubsection{Exercise \label{subsec:Exercise-traversables-laws-2-1}\ref{subsec:Exercise-traversables-laws-2-1}}

For any applicative functor $F$, Example~\ref{subsec:Example-pure-is-applicative-morphism}
shows that $\text{pu}_{F}:A\rightarrow F^{A}$ is an applicative morphism
between $\text{Id}$ and $F$. Prove that $\text{pu}_{F}$ is the
\emph{only} such morphism. (In terms of category theory, the identity
functor is an initial object in the category of applicative functors\index{category of applicative functors!initial object}.)

\subsubsection{Exercise \label{subsec:Exercise-traversables-10}\ref{subsec:Exercise-traversables-10}}

Given a \emph{monad} $M$ and a monoid morphism $\phi:R\rightarrow S$
between some monoid types $R$ and $S$, prove that $\phi^{\uparrow M}:M^{R}\rightarrow M^{S}$
is also a monoid morphism. (The types $M^{R}$ and $M^{S}$ are monoids
due to Exercise~\ref{subsec:Exercise-monad-of-monoid-is-monoid}).

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-1}\ref{subsec:Exercise-traversables-10-1}}

Given a monoid type $R$ and a \emph{monad} morphism $\phi:M\leadsto N$
between some monads $M$ and $N$, prove that $\phi:M^{R}\rightarrow N^{R}$
is a monoid morphism between $M^{R}$ and $N^{R}$. (The types $M^{R}$
and $N^{R}$ are monoids due to Exercise~\ref{subsec:Exercise-monad-of-monoid-is-monoid}).

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-1-1-1}\ref{subsec:Exercise-traversables-10-1-1-1}}

\textbf{(a)} Each monad has at the same time an applicative functor
instance. Given a monad morphism $\phi:M\leadsto N$ between two monads,
show that $\phi$ is also an applicative morphism between applicative
functors $M$ and $N$.

\textbf{(b)} Show that the converse does not hold: if $\phi:M\leadsto N$
is an applicative morphism between two monads then $\phi$ is \emph{not}
necessarily a monad morphism.

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-3}\ref{subsec:Exercise-traversables-10-3}}

Given an applicative functor $M$ and a monoid morphism $\phi:R\rightarrow S$
between some monoid types $R$ and $S$, prove that $\phi^{\uparrow M}:M^{R}\rightarrow M^{S}$
is also a monoid morphism. (The types $M^{R}$ and $M^{S}$ are monoids
due to Exercise~\ref{subsec:Exercise-applicative-of-monoid-is-monoid}).

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-1-1}\ref{subsec:Exercise-traversables-10-1-1}}

Given a monoid type $R$ and an applicative morphism $\phi:M\leadsto N$
between some applicative functors $M$ and $N$, prove that $\phi:M^{R}\rightarrow N^{R}$
is a monoid morphism between $M^{R}$ and $N^{R}$. (The types $M^{R}$
and $N^{R}$ are monoids due to Exercise~\ref{subsec:Exercise-applicative-of-monoid-is-monoid}).

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-3-1}\ref{subsec:Exercise-traversables-10-3-1}}

Given a monoid $M$, define the functions \lstinline!inMF! and \lstinline!outMF!:
\begin{align*}
 & \text{inMF}:M\rightarrow M\rightarrow M\quad,\quad\quad\text{inMF}\,(x^{:M})\triangleq y^{:M}\rightarrow x\oplus y\quad,\\
 & \text{outMF}:(M\rightarrow M)\rightarrow M\quad,\quad\quad\text{outMF}\,(p^{:M\rightarrow M})\triangleq p(e_{M})\quad.
\end{align*}
This definition of \lstinline!inMF! is similar to that used in the
proof of Statement~\ref{subsec:Statement-foldleft-foldmap-equivalence}(c).

\textbf{(a)} Prove that \lstinline!inMF! is a monoid morphism between
$M$ and the monoid $\text{MF}^{M}$ consisting of all functions of
type $M\rightarrow M$. Define the empty element and the binary operation
of $\text{MF}^{M}$ appropriately.

\textbf{(b)} Prove that $\text{inMF}\bef\text{outMF}=\text{id}$ but
$\text{outMF}\bef\text{inMF}\neq\text{id}$. (Give an example of a
monoid $M$ where the second equation does not hold.)

\textbf{(c)} Prove that \lstinline!outMF! is \emph{not} a monoid
morphism between $\text{MF}^{M}$ and $M$.

\subsubsection{Exercise \label{subsec:Exercise-traversables-10-1-1-2}\ref{subsec:Exercise-traversables-10-1-1-2}}

For any applicative functor $F$ with a known \lstinline!zip! method,
define the \textsf{``}reversed\textsf{''} \lstinline!zip! method as:
\[
\text{zip}_{\text{rev}F}:F^{A}\times F^{B}\rightarrow F^{A\times B}\quad,\quad\quad\text{zip}_{\text{rev}F}\triangleq\text{swap}\bef\text{zip}_{F}\bef\text{swap}^{\uparrow F}\quad.
\]

\textbf{(a)} Show that $\text{zip}_{\text{rev}F}$ also provides a
lawful applicative instance for $F$ (where $F$\textsf{'}s \lstinline!pure!
method remains unchanged). Show that $\text{zip}_{\text{rev}F}=\text{zip}_{F}$
whenever $F$ is commutative.

\textbf{(b)} Show that the \textsf{``}applicative reversal\textsf{''} obeys an applicative
naturality law: for any applicative functors $F$, $G$ and an applicative
morphism $\phi:F^{A}\rightarrow G^{A}$, the following equation holds:
\[
(p^{:F^{A}}\times q^{:F^{B}})\triangleright\text{zip}_{\text{rev}F}\triangleright\phi=(\phi(p)\times\phi(q))\triangleright\text{zip}_{\text{rev}G}\quad.
\]


\section{Discussion and further developments}

\subsection{The missing laws of \texttt{traverse} and \texttt{zipWithIndex}\label{subsec:Laws-of-traverse-and-zipWithIndex}}

The two laws of \lstinline!traverse! shown in this chapter do \emph{not}
in fact guarantee that \lstinline!traverse! behaves as programmers
expect. To see that, we generalize \lstinline!zipWithIndex! to arbitrary
traversable functors and then try proving two intuitively reasonable
properties of \lstinline!zipWithIndex! starting from the two laws
of \lstinline!traverse!. We will find that we \emph{cannot} prove
one of these properties without assuming some new laws of \lstinline!traverse!.
However, another approach based on more advanced techniques makes
it possible to prove all necessary properties. 

Sections~\ref{subsec:Decorating-a-tree1}\textendash \ref{subsec:Decorating-a-tree-breadth-first-traversal}
defined the method \lstinline!zipWithIndex! for certain choices of
traversals over binary trees. How can we define \lstinline!zipWithIndex!
(denoted $\text{zwi}_{L}$) for any traversable functor $L$? We use
$L$\textsf{'}s \lstinline!traverse! method ($\text{trav}_{L}^{F,A,B}$) and
chose the applicative functor $F$ as the \lstinline!State! monad
with the internal state of type \lstinline!Int!:
\[
F^{A}\triangleq\text{State}^{\text{Int},A}\triangleq\text{Int}\rightarrow A\times\text{Int}\quad.
\]
The internal state represents the current value of the index while
we iterate over values of type $A$ stored within $L^{A}$. The code
of \lstinline!zipWithIndex! applies \lstinline!traverse! to a function
of type $A\rightarrow\text{State}^{S,A\times\text{Int}}$ that increments
the index:
\begin{align}
 & \text{zwi}_{L}^{A}:L^{A}\rightarrow L^{A\times\text{Int}}\quad,\nonumber \\
 & \text{zwi}_{L}^{A}\triangleq\text{trav}_{L}^{F,A,A\times\text{Int}}(a^{:A}\rightarrow s^{:\text{Int}}\rightarrow(a\times s)\times(s+1))\bef\text{run}_{\text{State}}(0^{:\text{Int}})\quad.\label{eq:definition-of-zwi}
\end{align}
Here, $\text{run}_{\text{State}}(0^{:\text{Int}})$ is the \lstinline!State!
monad\textsf{'}s runner defined by Eq.~(\ref{eq:definition-of-runState})
and applied to the zero integer value $0^{:\text{Int}}$:
\[
\text{run}_{\text{State}}:S\rightarrow\text{State}^{S,A}\rightarrow A\quad,\quad\quad\text{run}_{\text{State}}(s_{0})\triangleq k^{:S\rightarrow A\times S}\rightarrow s_{0}\triangleright k\triangleright\pi_{1}\quad.
\]

What are a programmer\textsf{'}s intuitive expectations about \lstinline!zipWithIndex!
when applied to arbitrary traversable functors $L$? First, \lstinline!zipWithIndex!
applied to a value $p:L^{A}$ should produce a value of type $L^{A\times\text{Int}}$
that preserves the structure of $p$ and just adds indices at places
where data of type $A$ is stored within $p$. Second, \lstinline!zipWithIndex!
should produce a \emph{different} index for every value of type $A$
stored within $p$. To express these properties rigorously, let us
formulate them as equations.

The first property is that the value $p:L^{A}$ must be recovered
if we drop the index values:
\begin{lstlisting}
p.zipWithIndex.map(_._1) == p
\end{lstlisting}
\[
p\triangleright\text{zwi}_{L}\triangleright\pi_{1}^{\uparrow L}=p\quad,\quad\text{or equivalently}:\quad\text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{id}\quad.
\]

The second property says that each distinct value of type $A$ stored
within $p$ should get a different index. To express this property
via an equation, begin by computing $\text{zwi}_{L}(p)$ and then
discard the values of type $A$, leaving only the indices. The result
is a value $q\triangleq p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}$
of type $L^{\text{Int}}$. We expect $q$ to have the same shape and
structure as $p$ except that $q$ carries integer index values instead
of values of type $A$. The original value ($p$) can be restored
if we replace all index values in $q$ by the corresponding values
of type $A$. So, for any given $p$ there should exist a function
$f_{p}:\text{Int}\rightarrow A$ such that $p$ can be recovered from
$f_{p}$ and $q$ as $p=q\triangleright f_{p}^{\uparrow L}$. In other
words:
\[
\forall p^{:L^{A}}:\quad p=p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}\triangleright f_{p}^{\uparrow L}\quad.
\]

We will now prove the first property of \lstinline!zipWithIndex!,
assuming only that $L$ is a lawful traversable functor. In that proof,
we will use a special subtype of the \lstinline!State! monad.

\subsubsection{Statement \label{subsec:Statement-constant-value-state-monad}\ref{subsec:Statement-constant-value-state-monad}}

Assume that the type $S$ is not void and define a \textsf{``}constant-function
\lstinline!State! monad\textsf{''}, which we denote by $\text{CFState}^{S,A}$,
like this: Rewrite the type of a \lstinline!State! monad  as $\text{State}^{S,A}\triangleq S\rightarrow A\times S\cong(S\rightarrow A)\times(S\rightarrow S)$
and consider values of that type, $p^{:S\rightarrow A}\times q^{:S\rightarrow S}$,
such that $p^{:S\rightarrow A}$ is a\emph{ constant} function (independent
of its argument). In other words, the wrapped value of type $A$ is
independent of the state value of type $S$. This defines a subset
of all possible values of type $\text{State}^{S,A}$. We call that
subset $\text{CFState}^{S,A}$ and write: 
\[
\text{CFState}^{S,A}\triangleq(\_^{:S}\rightarrow A)\times(S\rightarrow S)\quad.
\]
This is a subtype of $\text{State}^{S,A}$ because there is a function
\lstinline!fromCF! of type $\text{CFState}^{S,A}\rightarrow\text{State}^{S,A}$
that is an identity function that merely reassigns types:
\begin{align*}
 & \text{fromCF}:(\_^{:S}\rightarrow A)\times(S\rightarrow S)\rightarrow(S\rightarrow A)\times(S\rightarrow S)\quad,\\
 & \text{fromCF}\triangleq p^{:\_\rightarrow A}\times q^{:S\rightarrow S}\rightarrow p^{:S\rightarrow A}\times q^{:S\rightarrow S}\quad.
\end{align*}
Assuming that $S$ is not a void type, we then have the following
properties:

\textbf{(a)} The type $\text{CFState}^{S,A}$ is a monad with the
same implementation as $\text{State}^{S,A}$.

\textbf{(b)} The function \lstinline!fromCF! is an injective monad
morphism and applicative morphism.

\textbf{(c)} Define a runner for the monad \lstinline!CFState! by:
\[
\text{run}_{\text{CFState}}:\text{CFState}^{S,A}\rightarrow A\quad,\quad\quad\text{run}_{\text{CFState}}\triangleq\text{fromCF}\bef\text{run}_{\text{State}}(s_{0})\quad,
\]
where $s_{0}^{:S}$ is a fixed value. Then the function $\text{run}_{\text{CFState}}$
is the same for all choices of $s_{0}$ and is an applicative morphism
between $\text{CFState}^{S,A}$ and the identity functor. (Note that
$\text{run}_{\text{State}}(s_{0})$ is neither a monad morphism nor
an applicative morphism of type $\text{State}^{S,A}\rightarrow A$.)

\subparagraph{Proof}

\textbf{(a)} Rewrite the code of \lstinline!State! monad\textsf{'}s methods
$\text{pu}_{\text{State}}$ and $\text{ftn}_{\text{State}}$ using
the equivalent type signature $\text{State}^{S,A}\cong(S\rightarrow A)\times(S\rightarrow S)$:
\begin{align*}
 & \text{pu}_{\text{State}}:A\rightarrow(S\rightarrow A)\times(S\rightarrow S)\quad,\quad\quad\text{pu}_{\text{State}}=a^{:A}\rightarrow(\_^{:S}\rightarrow a)\times\text{id}^{:S\rightarrow S}\quad,\\
 & \text{ftn}_{\text{State}}:(S\rightarrow(S\rightarrow A)\times(S\rightarrow S))\times(S\rightarrow S)\rightarrow(S\rightarrow A)\times(S\rightarrow S)\quad,\\
 & \text{ftn}_{\text{State}}=p^{:S\rightarrow(S\rightarrow A)\times(S\rightarrow S)}\times q^{:S\rightarrow S}\\
 & \quad\quad\quad\quad\quad\rightarrow(s^{:S}\rightarrow s\triangleright q\triangleright(s\triangleright p\triangleright\pi_{1}))\times(s^{:S}\rightarrow s\triangleright q\triangleright(s\triangleright p\triangleright\pi_{2}))\quad.
\end{align*}
The code of $\text{pu}_{\text{State}}$ is already of type $A\rightarrow\text{CFState}^{S,A}$
because $\text{pu}_{\text{State}}(a)$ returns a \lstinline!State!
monad containing a constant function ($\_^{:S}\rightarrow A$). It
remains to check that $\text{ftn}_{\text{State}}$ returns values
of the subtype $\text{CFState}^{S,A}$ when applied to a value of
type $\text{CFState}^{S,\text{CFState}^{S,A}}$:
\begin{align*}
 & \text{ftn}_{\text{State}}(p^{:\_^{:S}\rightarrow(\_^{:S}\rightarrow A)\times(S\rightarrow S)}\times q^{:S\rightarrow S})\\
 & =(s^{:S}\rightarrow s\triangleright q\triangleright(\gunderline{s\triangleright p}\triangleright\pi_{1}))\times(s^{:S}\rightarrow...)\\
{\color{greenunder}\text{denote }p_{0}\triangleq s\triangleright p:}\quad & =(s^{:S}\rightarrow\gunderline{s\triangleright q\triangleright\,}(p_{0}\triangleright\pi_{1}))\times(s^{:S}\rightarrow...)\\
{\color{greenunder}\text{denote }a_{0}\triangleq s\triangleright q\triangleright(p_{0}\triangleright\pi_{1}):}\quad & =(s^{:S}\rightarrow a_{0})\times(s^{:S}\rightarrow...)=(\_^{:S}\rightarrow a_{0})\times(s^{:S}\rightarrow...)\quad.
\end{align*}
Here the constants $p_{0}:(\_^{:S}\rightarrow A)\times(S\rightarrow S)$
and $a_{0}:A$ exist due to the assumption that both $p$ and $p_{0}\triangleright\pi_{1}$
are constant functions. It follows that the code of $\text{pu}_{\text{State}}$
and $\text{ftn}_{\text{State}}$ can be reused as $\text{pu}_{\text{CFState}}$
and $\text{ftn}_{\text{CFState}}$ when restricted to the type $\text{CFState}^{S,A}$.
The monad laws hold for $\text{CFState}^{S,A}$ because those laws
hold for the code of $\text{pu}_{\text{State}}$ and $\text{ftn}_{\text{State}}$.

\textbf{(b)} The code of the function \lstinline!fromCF! is an identity
function that only reassigns types. This indicates that \lstinline!CFState!
is a subtype of \lstinline!State! (in the sense of \textsf{``}subtyping\textsf{''}
explained in Section~\ref{subsec:Covariance,-contravariance,-and-subtyping}).

The code of $\text{pu}_{\text{CFState}}$ and $\text{ftn}_{\text{CFState}}$
is the same as the code of $\text{pu}_{\text{State}}$ and $\text{ftn}_{\text{State}}$
respectively. So, \lstinline!fromCF! automatically satisfies the
laws of the monad morphism of type $\text{CFState}^{S,A}\rightarrow\text{State}^{S,A}$. 

The function \lstinline!fromCF! is then also an applicative morphism
(Exercise~\ref{subsec:Exercise-traversables-10-1-1-1}).

To prove that \lstinline!fromCF! is injective, we will show that
it has a left inverse (denoted \lstinline!toCF!):
\[
\text{toCF}:\text{State}^{S,A}\rightarrow\text{CFState}^{S,A}\quad,\quad\quad\text{toCF}\triangleq p^{:S\rightarrow A}\times q^{:S\rightarrow S}\rightarrow(\_^{:S}\rightarrow p(s_{0}))\times q\quad.
\]
Here $s_{0}$ is an arbitrary value of type $S$. That value exists
because $S$ is not a void type.

Now we will show that $\text{fromCF}\bef\text{toCF}=\text{id}$. For
any constant function $p\triangleq\_^{:S}\rightarrow a_{0}$, we have
$p(s_{0})=a_{0}$ and then:
\begin{align*}
\text{fromCF}\bef\text{toCF} & =p^{:\_^{:S}\rightarrow A}\times q^{:S\rightarrow S}\rightarrow(\_^{:S}\rightarrow p(s_{0}))\times q\\
 & =p\times q\rightarrow(\_\rightarrow a_{0})\times q=p\times q\rightarrow p\times q=\text{id}\quad.
\end{align*}

\textbf{(c)} To see that the code of $\text{fromCF}\bef\text{run}_{\text{State}}(s_{0})$
does not depend on $s_{0}$, we first adapt the \lstinline!State!
monad\textsf{'}s runner ($\text{run}_{\text{State}}$) to the type $(S\rightarrow A)\times(S\rightarrow S)$,
which is equivalent to the type $\text{State}^{S,A}$:
\[
\text{run}_{\text{State}}(s_{0})=(p^{:S\rightarrow A}\times q^{:S\rightarrow S})\rightarrow\big(p(s_{0})\times q(s_{0})\big)\triangleright\pi_{1}=p^{:S\rightarrow A}\times q^{:S\rightarrow S}\rightarrow p(s_{0})\quad.
\]
Now we take any value $c$ of type $\text{CFState}^{S,A}$ and write:
\begin{align*}
 & c\triangleright\text{fromCF}\bef\text{run}_{\text{State}}(s_{0})=(p^{:\_^{:S}\rightarrow A}\times q^{:S\rightarrow S})\triangleright\text{fromCF}\triangleright\text{run}_{\text{State}}(s_{0})\\
 & =(p\times q)\triangleright\text{run}_{\text{State}}(s_{0})=p(s_{0})\quad.
\end{align*}
The last value does not depend on $s_{0}$ because $p$ is a constant
function. 

To show that $\text{run}_{\text{CFState}}$ is an applicative morphism
between the applicative functors $\text{CFState}^{S,A}$ and $\text{Id}$,
first note that the \lstinline!pure! method of \lstinline!CFState!
is the same as that of the \lstinline!State! monad. A \textsf{``}pure\textsf{''}
value $\text{pu}_{\text{CFState}}(a^{:A})$ equals the function $s\rightarrow a\times s$.
Applying $\text{run}_{\text{State}}(s_{0})$ to the function $s\rightarrow a\times s$
will always return just the value $a$. This is the same as the identity
functor\textsf{'}s \lstinline!pure! method (which is an identity function)
applied to the value \lstinline!a!. So, $\text{run}_{\text{CFState}}$
will map \lstinline!CFState!\textsf{'}s \lstinline!pure! into the identity
functor\textsf{'}s \lstinline!pure!.

It remains to show that $\text{run}_{\text{CFState}}$ maps \lstinline!CFState!\textsf{'}s
\lstinline!zip! method into the identity functor\textsf{'}s \lstinline!zip!
method (which is again an identity function, $a\times b\rightarrow a\times b$).
The code of \lstinline!CFState!\textsf{'}s \lstinline!zip! is:
\begin{align*}
 & \text{zip}_{\text{CFState}}:(\_^{:S}\rightarrow A)\times(S\rightarrow S)\times(\_^{:S}\rightarrow B)\times(S\rightarrow S)\rightarrow(\_^{:S}\rightarrow A\times B)\times(S\rightarrow S)\quad,\\
 & \text{zip}_{\text{CFState}}:p_{1}^{:\_^{:S}\rightarrow A}\times q_{1}^{:S\rightarrow S}\times p_{2}^{:\_^{:S}\rightarrow B}\times q_{2}^{:S\rightarrow S}\rightarrow(s^{:S}\rightarrow p_{1}(s)\times p_{2}(s))\times(q_{1}\bef q_{2})\quad.
\end{align*}
The function $s^{:S}\rightarrow p(s)\times q(s)$ is a constant function
because $p$ and $q$ are.

Applying $\text{zip}_{\text{CFState}}$ to arbitrary values:
\begin{align*}
 & c_{1}:\text{CFState}^{S,A}\quad,\quad\quad c_{1}=p_{1}^{:\_^{:S}\rightarrow A}\times q_{1}^{:S\rightarrow S}\quad,\\
 & c_{2}:\text{CFState}^{S,B}\quad,\quad\quad c_{2}=p_{2}^{:\_^{:S}\rightarrow B}\times q_{2}^{:S\rightarrow S}\quad,
\end{align*}
 and then applying $\text{run}_{\text{CFState}}$, we get:
\begin{align*}
 & (p_{1}^{:\_^{:S}\rightarrow A}\times q_{1}^{:S\rightarrow S}\times p_{2}^{:\_^{:S}\rightarrow B}\times q_{2}^{:S\rightarrow S})\triangleright\text{zip}_{\text{CFState}}\triangleright\text{run}_{\text{CFState}}\\
 & =(s^{:S}\rightarrow p_{1}(s)\times p_{2}(s))\times(q_{1}\bef q_{2})\triangleright\text{run}_{\text{CFState}}\\
 & =p_{1}(s_{0})\times p_{2}(s_{0})\quad.
\end{align*}
Applying $\text{run}_{\text{CFState}}$ to the initial values, we
find:
\[
(p_{1}^{:\_^{:S}\rightarrow A}\times q_{1}^{:S\rightarrow S})\triangleright\text{run}_{\text{CFState}}=p_{1}(s_{0})\quad,\quad\quad(p_{2}^{:\_^{:S}\rightarrow B}\times q_{2}^{:S\rightarrow S})\triangleright\text{run}_{\text{CFState}}=p_{2}(s_{0})\quad.
\]
So, the composition law of applicative morphisms holds:
\[
(c_{1}\times c_{2})\triangleright\text{zip}_{\text{CFState}}\triangleright\text{run}_{\text{CFState}}=(c_{1}\triangleright\text{run}_{\text{CFState}})\times(c_{2}\triangleright\text{run}_{\text{CFState}})\quad.
\]
$\square$

We are now ready to prove the first property of \lstinline!zipWithIndex!.

\subsubsection{Statement \label{subsec:Statement-properties-of-zipWithIndex}\ref{subsec:Statement-properties-of-zipWithIndex}}

For any traversable functor $L$, define \lstinline!zipWithIndex!
(denoted for brevity by $\text{zwi}_{L}$) via Eq.~(\ref{eq:definition-of-zwi}).
Then \lstinline!zipWithIndex! satisfies the equation:
\[
\text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{id}^{:L^{A}\rightarrow L^{A}}\quad.
\]


\subparagraph{Proof}

To make the definition~(\ref{eq:definition-of-zwi}) easier to work
with, we denote:
\begin{align*}
 & F^{A}\triangleq\text{State}^{\text{Int},A}=(\text{Int}\rightarrow A)\times(\text{Int}\rightarrow\text{Int})\quad,\\
 & g:A\rightarrow\text{State}^{\text{Int},A\times\text{Int}}\quad,\quad\quad g\triangleq a^{:A}\rightarrow(s^{:\text{Int}}\rightarrow a\times s)\times(s^{:\text{Int}}\rightarrow s+1)\quad,\\
 & r:\forall B.\,\text{State}^{\text{Int},B}\rightarrow B\quad,\quad\quad r\triangleq p^{:\text{Int}\rightarrow B}\times q^{:\text{Int}\rightarrow\text{Int}}\rightarrow p(0^{:\text{Int}})\quad.
\end{align*}
and get $\text{zwi}_{L}=\text{trav}_{L}^{F,A,A\times\text{Int}}(g)\bef r$.
Then we write:
\begin{align*}
 & \text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{trav}_{L}^{F,A,A\times\text{Int}}(g)\bef\gunderline{r\bef\pi_{1}^{\uparrow L}}\\
{\color{greenunder}\text{naturality of }r:}\quad & =\gunderline{\text{trav}_{L}^{F,A,A\times\text{Int}}(g)\bef\pi_{1}^{\uparrow L\uparrow\text{State}}}\bef r\\
{\color{greenunder}\text{naturality of }\text{trav}_{L}:}\quad & =\text{trav}_{L}^{F,A,A}(g\bef\pi_{1}^{\uparrow\text{State}})\bef r\quad.
\end{align*}
To compute $g\bef\pi_{1}^{\uparrow\text{State}}$, we first express
the lifting to \lstinline!State! as:
\[
(f^{:A\rightarrow B})^{\uparrow\text{State}^{S,\bullet}}(p^{:S\rightarrow A}\times q^{:S\rightarrow S})=(p\bef f)\times q\quad.
\]
Then we get:
\begin{align*}
 & g\bef\pi_{1}^{\uparrow\text{State}}:A\rightarrow\text{State}^{\text{Int},A}\quad,\\
 & g\bef\pi_{1}^{\uparrow\text{State}}=a^{:A}\rightarrow(s^{:\text{Int}}\rightarrow\pi_{1}(a\times s))\times(s^{:\text{Int}}\rightarrow s+1)\\
 & =a\rightarrow(s^{:\text{Int}}\rightarrow a)\times(s^{:\text{Int}}\rightarrow s+1)=a\rightarrow(\_^{:\text{Int}}\rightarrow a)\times(s^{:\text{Int}}\rightarrow s+1)\quad.
\end{align*}
Now we note that the sub-expression ($\_^{:\text{Int}}\rightarrow a$)
is a constant function. So, $g\bef\pi_{1}^{\uparrow\text{State}}$
can be expressed as a function $h$ of type $A\rightarrow\text{CFState}^{\text{Int},A}$
followed by \lstinline!fromCF!:
\begin{align*}
 & h:A\rightarrow\text{CFState}^{\text{Int},A}=A\rightarrow(\_^{:\text{Int}}\rightarrow A)\times(\text{Int}\rightarrow\text{Int})\quad,\\
 & h\triangleq a\rightarrow(\_^{:\text{Int}}\rightarrow a)\times(s^{:\text{Int}}\rightarrow s+1)\quad,\quad\quad g\bef\pi_{1}^{\uparrow\text{State}}=h\bef\text{fromCF}\quad.
\end{align*}

In this way, we have found that:
\[
\text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{trav}_{L}^{\text{State}^{\text{Int},\bullet},A,A}(h\bef\text{fromCF})\bef r\quad.
\]
Since \lstinline!fromCF! is an applicative morphism, we can use the
applicative naturality law~(\ref{eq:traverse-applicative-naturality-law})
with $B\triangleq A$, $F^{A}\triangleq\text{CFState}^{\text{Int},A}$,
$G^{A}\triangleq\text{State}^{\text{Int},A}$, $f^{:F^{B}\rightarrow G^{B}}=$
\lstinline!fromCF!, and $g^{:A\rightarrow F^{B}}=h$:
\begin{align*}
 & \text{trav}_{L}^{G,A,B}(g\bef f)=\text{trav}_{L}^{F,A,B}(g)\bef f\\
{\color{greenunder}\text{or equivalently}:}\quad & \text{trav}_{L}(h\bef\text{fromCF})=\text{trav}_{L}(h)\bef\text{fromCF}\quad.
\end{align*}
Statement~\ref{subsec:Statement-constant-value-state-monad}(c) gives
the formula $\text{run}_{\text{CFState}}=\text{fromCF}\bef r$, so
we write:
\[
\text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{trav}_{L}(h)\bef\gunderline{\text{fromCF}\bef r}=\text{trav}_{L}(h)\bef\text{run}_{\text{CFState}}\quad.
\]
As $\text{run}_{\text{CFState}}$ is an applicative morphism, we may
again use the applicative naturality law:
\[
\text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{trav}_{L}(h)\bef\text{run}_{\text{CFState}}=\text{trav}_{L}(h\bef\text{run}_{\text{CFState}})\quad.
\]
Now we recall the identity law~(\ref{eq:traverse-identity-law})
and compute:
\begin{align*}
 & h\bef\text{run}_{\text{CFState}}=a\rightarrow(\_^{:\text{Int}}\rightarrow a)\times(s^{:\text{Int}}\rightarrow s+1)\bef\text{run}_{\text{CFState}}=a\rightarrow a=\text{id}^{:A\rightarrow A}\quad,\\
 & \text{zwi}_{L}\bef\pi_{1}^{\uparrow L}=\text{trav}_{L}(h\bef\text{run}_{\text{CFState}})=\text{trav}_{L}(\text{id})=\text{id}\quad.
\end{align*}
 $\square$

The second property of \lstinline!zipWithIndex! says that \lstinline!zipWithIndex!
assigns different indices to each value of type $A$ stored inside
a data structure of type $L^{A}$. This property does \emph{not} seem
to be provable using the two laws of \lstinline!traverse!. One reason
is that an \textsf{``}indexing\textsf{''} function $f_{p}:\text{Int}\rightarrow A$
can be computed only by traversing the entire data structure $p$.
In other words, $f$ is itself a result of a traversal operation.
We want to prove a property of \lstinline!traverse! that combines
$f$ with another traversal (\lstinline!zipWithIndex!) of the same
initial data ($p$). But there are no laws of \lstinline!traverse!
that involve composition of traversals of the same initial container.
A new law of \lstinline!traverse! is necessary. 

Such a law (involving inverse-order traversals) was first proposed
in 2012.\footnote{See \texttt{\href{https://www.cs.ox.ac.uk/jeremy.gibbons/publications/backwards.pdf}{https://www.cs.ox.ac.uk/jeremy.gibbons/publications/backwards.pdf}}}
One year later, R.~Bird\index{Richard Bird} et al.\footnote{\label{fn:uitbaf}See \texttt{\href{https://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf}{https://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf}}}~used
advanced techniques motivated by dependent type theory to prove that
the second property of \lstinline!zipWithIndex! can be derived for
arbitrary polynomial functors $L$ without assuming any new laws of
\lstinline!traverse!. The techniques of Bird et al.~are beyond the
scope of this book, as the required theory is complicated but has
limited practical use. Because all polynomial functors are traversable
(and no other functors are), it is not as important to be able to
characterize \lstinline!traverse! solely via laws. In practice, there
is no uncertainty about how to implement a law-abiding \lstinline!traverse!
correctly, for any given polynomial functor and any traversal order. 

It will suffice for our purposes to use the following statement from
Bird et al.:

\subsubsection{Statement \label{subsec:Statement-Bird-representation-theorem-for-traversal}\ref{subsec:Statement-Bird-representation-theorem-for-traversal}}

Given a traversable functor $L$ and a value $p:L^{A}$, there exists
an integer $n\ge0$ and a function \lstinline!make[A]! with type
signature:
\[
\text{make}:\forall A.\,\underbrace{A\times A\times...\times A}_{n\text{ times}}\rightarrow L^{A}\quad,
\]
such that $p=\text{make}^{A}(a_{1}\times...\times a_{n})$ with suitable
values $a_{1}$, ..., $a_{n}$ of type $A$. The function \lstinline!make[A]!
is natural in the type parameter \lstinline!A!:
\[
\text{for any }f^{:A\rightarrow B}:\quad(a_{1}\times...\times a_{n})\triangleright\text{make}^{A}\triangleright f^{\uparrow L}=(f(a_{1})\times...\times f(a_{n}))\triangleright\text{make}^{B}\quad.
\]
The \lstinline!traverse! method of $L$ is expressed via \lstinline!make!
and the values $a_{1}$, ..., $a_{n}$ as: 
\begin{align*}
 & \quad{\color{greenunder}\text{for any }g^{:A\rightarrow F^{B}}:}\quad\\
 & p\triangleright\text{trav}_{L}(g)=\big(\text{zip}_{L}(g(a_{1})\times\text{zip}_{L}(g(a_{2})\times...\times\text{zip}_{L}(g(a_{n-1})\times g(a_{n}))...)\big)\triangleright\text{restore}^{\uparrow F}\quad,\\
 & \quad{\color{greenunder}\text{where we defined}:}\quad\\
 & \text{restore}\triangleq b_{1}\times(b_{2}\times(...\times(b_{n-1}\times b_{n})...)\rightarrow\text{make}^{B}(b_{1}\times...\times b_{n})\quad.
\end{align*}


\subparagraph{Proof}

This is proved by Bird et al.~as the \textsf{``}representation theorem\textsf{''}.
$\square$

\subsubsection{Statement \label{subsec:Statement-polynomial-functors-Int-A}\ref{subsec:Statement-polynomial-functors-Int-A}}

For any traversable functor $L$, any non-void type $A$, and any
value $p^{:L^{A}}$:

\textbf{(a)} There is a function $t_{p}:\text{Int}\rightarrow A$
such that \lstinline!zipWithIndex! satisfies:
\[
p=p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}\triangleright t_{p}^{\uparrow L}\quad.
\]

\textbf{(b)} There exists an injective function of type $L^{A}\rightarrow(\text{Int}\rightarrow A)\times L^{\text{Int}}$.

\subparagraph{Proof}

\textbf{(a)} By Statement~\ref{subsec:Statement-Bird-representation-theorem-for-traversal},
there exist $a_{1}$, ..., $a_{n}$ of type $A$ such that: 
\[
p=(a_{1}\times...\times a_{n})\triangleright\text{make}^{A}\quad.
\]
Then the definition of \lstinline!zipWithIndex! is evaluated to:
\[
p\triangleright\text{zwi}_{L}=\big((a_{1}\times1^{:\text{Int}})\times(a_{2}\times2^{:\text{Int}})\times...\times(a_{n}\times n^{:\text{Int}})\big)\triangleright\text{make}^{A\times\text{Int}}\quad.
\]
By the naturality law of \lstinline!make!, we get:
\[
p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}=\big(1^{:\text{Int}}\times2^{:\text{Int}}\times...\times n^{:\text{Int}}\big)\triangleright\text{make}^{\text{Int}}\quad.
\]
Now we define $t_{p}$ as a (partial) function of type $\text{Int}\rightarrow A$
such that:
\[
t_{p}(i)\triangleq a_{i}\quad,\quad i=1,2,...,n\quad.
\]
We again use the naturality law of \lstinline!make! and prove the
required property:
\begin{align*}
 & p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}\triangleright t_{p}^{\uparrow L}=\big(t_{p}(1)\times...\times t_{p}(n)\big)\triangleright\text{make}^{\text{Int}}\\
 & \quad=(a_{1}\times...\times a_{n})\triangleright\text{make}^{A}=p\quad.
\end{align*}

\textbf{(b)} Note that $p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L}$
is a value of type $L^{\text{Int}}$. We can combine that function
with $t_{p}$ and define a \textsf{``}tabulating\textsf{''} function (denoted by $\text{tab}_{L}$):
\begin{align*}
 & \text{tab}_{L}^{A}:L^{A}\rightarrow(\text{Int}\rightarrow A)\times L^{\text{Int}}\quad,\\
 & \text{tab}_{L}^{A}\triangleq p^{:L^{A}}\rightarrow t_{p}\times(p\triangleright\text{zwi}_{L}\triangleright\pi_{2}^{\uparrow L})\quad.
\end{align*}
Then the property of $\text{zwi}_{L}$ from \textbf{(a)} is rewritten
as:
\[
\text{tab}_{L}\bef(f^{:\text{Int}\rightarrow A}\times q^{:L^{\text{Int}}}\rightarrow q\triangleright f^{\uparrow L})\overset{!}{=}\text{id}^{:L^{A}\rightarrow L^{A}}\quad.
\]
So, the function $\text{tab}_{L}$ is an injective function of type
$L^{A}\rightarrow(\text{Int}\rightarrow A)\times L^{\text{Int}}$.
$\square$ 

\subsection{Traversable contrafunctors and profunctors are not useful}

In Chapter~\ref{chap:8-Applicative-functors,-contrafunctors}, we
found some uses for applicative contrafunctors and profunctors. We
will now briefly investigate whether contrafunctors or profunctors
may be traversable. To answer that question, we will try implementing
a lawful \lstinline!sequence! function for contrafunctors and profunctors.

Suppose that $L$ is a contrafunctor. A \lstinline!sequence! function
(denoted by $\text{seq}_{L}$) with the type signature:
\[
\text{seq}_{L}^{F,A}:L^{F^{A}}\rightarrow F^{L^{A}}\quad,
\]
is required for $L$ to be traversable. That function must obey the
applicative naturality law, which ensures that $\text{seq}_{L}^{F,A}$
works in the same way for any applicative functor $F$. We note that
$\text{seq}_{L}^{F,A}$ is covariant in $F$ because $L$ is contravariant.
Since $\text{pu}_{F}:\text{Id}^{A}\rightarrow F^{A}$ is an applicative
morphism (Example~\ref{subsec:Example-pure-is-applicative-morphism}),
we may use this morphism to write the applicative naturality law:
\[
\text{seq}_{L}^{F,A}=\text{pu}_{F}^{\downarrow L}\bef\text{seq}_{L}^{\text{Id},A}\bef\text{pu}_{F}\quad.
\]
The identity law gives $\text{seq}_{L}^{\text{Id},A}=\text{id}$.
So, the code of $\text{seq}_{L}^{F,A}$ must be this:
\[
\text{seq}_{L}^{F,A}=\text{pu}_{F}^{\downarrow L}\bef\text{pu}_{F}\quad.
\]
No other implementation of $\text{seq}_{L}$ would obey the applicative
naturality law and the identity law.

We can verify that the composition law~(\ref{eq:composition-law-of-sequence})
holds for this $\text{seq}_{L}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=\text{pu}_{F}^{\downarrow L}\bef\gunderline{\text{pu}_{F}\bef(\text{pu}_{G}^{\downarrow L}\bef\text{pu}_{G})^{\uparrow F}}\\
{\color{greenunder}\text{naturality law of }\text{pu}_{F}:}\quad & \quad=\gunderline{\text{pu}_{F}^{\downarrow L}\bef\text{pu}_{G}^{\downarrow L}}\bef\text{pu}_{G}\bef\text{pu}_{F}=(\text{pu}_{G}\bef\text{pu}_{F})^{\downarrow L}\bef\text{pu}_{G}\bef\text{pu}_{F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{seq}_{L}^{F\circ G,A}=\text{pu}_{F\circ G}^{\downarrow L}\bef\text{pu}_{F\circ G}=(\text{pu}_{G}\bef\text{pu}_{F})^{\downarrow L}\bef\text{pu}_{G}\bef\text{pu}_{F}\quad.
\end{align*}

So, every contrafunctor has a unique \lstinline!sequence! method
that obeys the laws of traversables. However, the code of \lstinline!sequence!
ignores all $F$-effects in its arguments and always produces values
with empty $F$-effects. For example, if $L^{A}\triangleq A\rightarrow Z$
(where $Z$ is a fixed type) then:
\[
\text{seq}_{L}^{F,A}:(F^{A}\rightarrow Z)\rightarrow F^{A\rightarrow Z}\quad,\quad\quad\text{seq}_{L}^{F,A}(f^{:F^{A}\rightarrow Z})=\text{pu}_{F}(a^{:A}\rightarrow f(\text{pu}_{F}(a)))\quad.
\]
The function $f$ will be never applied to nontrivial $F$-effects.
So, $\text{seq}_{L}(f)$ could never use any information that $f$
would return when applied to nontrivial $F$-effects. 

We see that contrafunctors are traversable in a way that is not practically
useful.

Turning now to the case where $L$ is a profunctor, consider a simple
example:
\[
L^{A}\triangleq A\rightarrow A\quad,\quad\quad\text{seq}_{L}:(F^{A}\rightarrow F^{A})\rightarrow F^{A\rightarrow A}\quad.
\]
Since the applicative functor $F$ is unknown and the type signature
of $\text{seq}_{L}$ does not provide any arguments of type $F^{X}$,
we cannot produce values of type $F$ other than via $\text{pu}_{F}$.
So, the only implementation of $\text{seq}_{L}$ is:
\[
\text{seq}_{L}\triangleq p^{:F^{A}\rightarrow F^{A}}\rightarrow\text{pu}_{F}(\text{id}^{:A\rightarrow A})\quad.
\]
This function is not useful because it ignores its argument $p$.

\subsection{Traversals for nested recursive types}

In most of this book, recursive type constructors are defined via
equations of the form $L^{A}\triangleq S^{A,L^{A}}$, where $S$ is
a recursion scheme. We have seen one example of a recursive type constructor
(a perfect-shaped tree, Sections~\ref{subsec:Perfect-shaped-trees}
and~\ref{subsec:Example-traversal-perfect-shaped-tree}) that cannot
be defined in this way. The reason is that the recursive type equation
for a perfect-shaped binary tree\index{perfect-shaped tree} $\text{PT}$
is:
\begin{equation}
\text{PT}^{A}\triangleq A+\text{PT}^{A\times A}\quad.\label{eq:perfect-shaped-binary-tree-type-equation}
\end{equation}
This type equation is not of the form $\text{PT}^{A}\triangleq S^{A,\text{PT}^{A}}$
because the recursive use of $\text{PT}$ contains a nontrivial type
expression ($A\times A$) instead of just $A$. To express the type
equation~(\ref{eq:perfect-shaped-binary-tree-type-equation}) via
a recursion scheme, we introduce an additional functor $P$ and write:
\[
\text{PT}^{A}\triangleq S^{A,\text{PT}^{P^{A}}}\quad,\quad\quad P^{A}\triangleq A\times A\quad.
\]

\textbf{Nested recursive types}\index{nested recursive types} are
defined using type equations of this more general form, for example:
\begin{equation}
T^{A}\triangleq S^{A,T^{P^{A}}}\quad,\quad\text{or}\quad\quad T^{A,B}\triangleq S^{A,B,T^{P^{A},Q^{B}}}\quad,\quad\quad\text{etc.}\label{eq:nested-recursive-type-equations}
\end{equation}
The nested functors $P$, $Q$ replace the type parameters $A$, $B$
of $T^{A,B}$ by different type expressions. 

One could write a nested recursive type of an even more general form
than Eq.~(\ref{eq:nested-recursive-type-equations}), e.g., $T^{A}\triangleq S^{A,T^{T^{A}}}$,
where the nested type expressions may use the type $T$ itself more
than once. We will not consider such type equations in this book,
as it is not clear how much use they have in practice.

Let us see how one can implement a \lstinline!Traversable! instance
for type constructors of the form~(\ref{eq:nested-recursive-type-equations}).

\subsubsection{Statement \label{subsec:Statement-nested-recursive-type-traversable}\ref{subsec:Statement-nested-recursive-type-traversable}}

Given a bitraversable bifunctor $S$ and a traversable functor $P$,
define a nested recursive type constructor $L$ by:
\[
L^{A}\triangleq S^{A,L^{P^{A}}}\quad.
\]
Then $L$ is a lawful traversable functor with the \lstinline!sequence!
function defined as:
\begin{align*}
 & \text{seq}_{L}^{F,A}:S^{F^{A},L^{P^{F^{A}}}}\rightarrow F^{S^{A,L^{P^{A}}}}\quad,\\
 & \text{seq}_{L}^{F,A}\triangleq\big(\text{seq}_{P}^{F,A}\big)^{\uparrow L\uparrow S^{F^{A},\bullet}}\bef\big(\overline{\text{seq}}_{L}^{F,P^{A}}\big)^{\uparrow S^{F^{A},\bullet}}\bef\text{seq2}_{S}^{F,A,L^{P^{A}}}\quad.
\end{align*}
\[
\xymatrix{\xyScaleY{1.4pc}\xyScaleX{6pc}S^{F^{A},L^{P^{F^{A}}}}\ar[r]\sp(0.5){\big(\text{seq}_{P}^{F,A}\big)^{\uparrow L\uparrow S^{F^{A},\bullet}}} & S^{F^{A},L^{F^{P^{A}}}}\ar[r]\sp(0.5){\big(\overline{\text{seq}}_{L}^{F,P^{A}}\big)^{\uparrow S^{F^{A},\bullet}}} & S^{F^{A},F^{L^{P^{A}}}}\ar[r]\sp(0.5){\text{seq2}_{S}^{F,A,L^{P^{A}}}} & F^{S^{A,L^{P^{A}}}}}
\]


\subparagraph{Proof}

We assume that the methods $\text{seq}_{P}$ and $\text{seq2}_{S}$,
as well as the recursively called $\overline{\text{seq}}_{L}$, already
satisfy the laws. To verify the identity law~(\ref{eq:identity-law-of-sequence}):
\[
\text{seq}_{L}^{\text{Id},A}=\big(\text{seq}_{P}^{\text{Id},A}\big)^{\uparrow L\uparrow S^{F^{A},\bullet}}\bef\big(\overline{\text{seq}}_{L}^{\text{Id},P^{A}}\big)^{\uparrow S^{F^{A},\bullet}}\bef\text{seq2}_{S}^{\text{Id},A,L^{P^{A}}}=\text{id}^{\uparrow L\uparrow S}\bef\text{id}^{\uparrow S}\bef\text{id}=\text{id}\quad.
\]
To verify the composition law~(\ref{eq:composition-law-of-sequence}),
write its two sides:
\begin{align*}
 & \text{seq}_{L}^{F,G^{A}}\bef(\text{seq}_{L}^{G,A})^{\uparrow F}=\big(\text{seq}_{P}^{F,G^{A}}\big)^{\uparrow L\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\big)^{\uparrow S}\bef\text{seq2}_{S}^{F,G^{A},L^{P^{G^{A}}}}\\
 & \quad\quad\quad\quad\bef\big((\text{seq}_{P}^{G,A})^{\uparrow L\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{G,P^{A}}\big)^{\uparrow S}\bef\text{seq2}_{S}^{G,A,L^{P^{A}}}\big)^{\uparrow F}\quad,\\
 & \text{seq}_{L}^{F\circ G,A}=\big(\text{seq}_{P}^{F\circ G,A}\big)^{\uparrow L\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{F\circ G,P^{A}}\big)^{\uparrow S}\bef\text{seq2}_{S}^{F\circ G,A,L^{P^{A}}}\\
 & \quad=\big(\text{seq}_{P}^{F,G^{A}}\bef(\text{seq}_{P}^{G,A})^{\uparrow F}\big)^{\uparrow L\uparrow S}\\
 & \quad\quad\quad\quad\bef\big(\overline{\text{seq}}_{L}^{F,G^{P^{A}}}\bef(\overline{\text{seq}}_{L}^{G,P^{A}})^{\uparrow F}\big)^{\uparrow S}\bef\text{seq2}_{S}^{F,G^{A},G^{L^{P^{A}}}}\bef\big(\text{seq2}_{S}^{G,A,L^{P^{A}}}\big)^{\uparrow F}\quad.
\end{align*}
To get the last line, we used the assumed composition laws of $\text{seq}_{P}$,
$\overline{\text{seq}}_{L}$, and $\text{seq2}_{S}$. 

It is now clear that the two sides of the law differ only in the order
of function compositions. The remaining difference is:
\begin{align*}
 & \big(\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\big)^{\uparrow S}\bef\text{seq2}_{S}^{F,G^{A},L^{P^{G^{A}}}}\bef\big((\text{seq}_{P}^{G,A})^{\uparrow L}\bef\overline{\text{seq}}_{L}^{G,P^{A}}\big)^{\uparrow S\uparrow F}\\
 & \overset{?}{=}(\text{seq}_{P}^{G,A})^{\uparrow F\uparrow L\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{F,G^{P^{A}}}\big)^{\uparrow S}\bef(\overline{\text{seq}}_{L}^{G,P^{A}})^{\uparrow F\uparrow S}\bef\text{seq2}_{S}^{F,G^{A},G^{L^{P^{A}}}}\quad.
\end{align*}
We will show that the two sides are equal if we rewrite the left-hand
side so that the various \lstinline!sequence! methods are composed
in the same order as in the right-hand side. This can be done using
naturality laws, which allow us to change the order of composition
of lifted functions:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \big(\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\big)^{\uparrow S}\bef\gunderline{\text{seq2}_{S}^{F,G^{A},L^{P^{G^{A}}}}\bef\big((\text{seq}_{P}^{G,A})^{\uparrow L}\bef\overline{\text{seq}}_{L}^{G,P^{A}}\big)^{\uparrow S\uparrow F}}\\
{\color{greenunder}\text{naturality of }\text{seq2}_{S}:}\quad & =\gunderline{\big(\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\big)^{\uparrow S}\bef\big((\text{seq}_{P}^{G,A})^{\uparrow L}\bef\overline{\text{seq}}_{L}^{G,P^{A}}\big)^{\uparrow F\uparrow S}}\bef\text{seq2}_{S}^{F,G^{A},G^{L^{P^{A}}}}\\
{\color{greenunder}\text{composition under }^{\uparrow S}:}\quad & =\big(\gunderline{\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\bef(\text{seq}_{P}^{G,A})^{\uparrow L\uparrow F}}\big)^{\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{G,P^{A}}\big)^{\uparrow F\uparrow S}\bef\text{seq2}_{S}^{F,G^{A},G^{L^{P^{A}}}}\quad.
\end{align*}
It remains to show that: 
\[
\big(\overline{\text{seq}}_{L}^{F,P^{G^{A}}}\bef(\text{seq}_{P}^{G,A})^{\uparrow L\uparrow F}\big)^{\uparrow S}\overset{?}{=}(\text{seq}_{P}^{G,A})^{\uparrow F\uparrow L\uparrow S}\bef\big(\overline{\text{seq}}_{L}^{F,G^{P^{A}}}\big)^{\uparrow S}\quad.
\]
But this is just the naturality law of $\overline{\text{seq}}_{L}$
under the lifting $(\dots)^{\uparrow S}$. $\square$

As an advanced example of a nested traversable functor, we will derive
a type for square-shaped matrices\footnote{This and other advanced examples of designing and using nested recursive
types are explained in the paper \textsf{``}Manufacturing datatypes\textsf{''} (1999)
by R.~Hinze\index{Ralf Hinze}, see \texttt{\href{https://www.cs.ox.ac.uk/ralf.hinze/publications/WAAAPL99a.ps.gz}{https://www.cs.ox.ac.uk/ralf.hinze/publications/WAAAPL99a.ps.gz}}} with elements of type $A$. For motivation, recall how Example~\ref{subsec:Example-matrix-products}
encoded square matrices via nested lists of type \lstinline!List[List[A]]!.
However, a value of that type is not guaranteed to represent a matrix
of a consistent shape. We would like to define a type \lstinline!Sq[A]!
whose values always contain $n$ nested lists of length $n$ (for
$n=1,2,...$). We need to disallow inconsistent nested lists such
as $\left[\left[1,2\right],\left[\right],\left[3\right]\right]$ that
do not correspond to a square matrix.

Begin by considering an example of a $2\times2$ matrix. The type
of such matrices is not \lstinline!List[List[A]]! but \lstinline!List2[List2[A]]!,
where \lstinline!List2[A]! is a list of \emph{exactly} $2$ elements
of type $A$. We can rewrite the type \lstinline!List2[A]! equivalently
as a pair $A\times A$ and also as a function $\bbnum 2\rightarrow A$
(where $\bbnum 2$ is the type containing exactly $2$ distinct values).
This technique allows us to formulate the types of lists of exactly
$3$, $4$, etc.,~elements as $\text{List}_{3}^{A}\triangleq\bbnum 3\rightarrow A$,
$\text{List}_{4}^{A}\triangleq\bbnum 4\rightarrow A$, and so on.
The type of an $n\times n$ matrix is then written symbolically as
$\bbnum n\rightarrow\bbnum n\rightarrow A$ (where $\bbnum n=\bbnum 1$,
$\bbnum 2$, etc.) or equivalently as $\bbnum n\times\bbnum n\rightarrow A$.

The type \lstinline!Sq[A]! is equivalent to an infinite disjunction
of types representing square matrices of every size ($1\times1$,
$2\times2$, and so on). To define an infinite disjunctive type, we
normally use recursion at type level. In a mathematical sense, this
recursion will be induction on the size of the matrix. So, let us
introduce the size of the matrix as an extra \emph{type parameter}
$N$. It will be convenient to define \lstinline!SqSize[N, A]! as
the type of matrices of size \emph{at least} $N$. We intend $N$
to be equivalent to one of the types $\bbnum 1$, $\bbnum 2$, etc. 

The base case ($N=\bbnum 1$) and the inductive step (from $N$ to
$\bbnum 1+N$) are written as:
\[
\text{Sq}^{A}\triangleq\text{SqSize}^{\bbnum 1,A}\quad,\quad\quad\text{SqSize}^{N,A}\triangleq\left(N\times N\rightarrow A\right)+\text{SqSize}^{\bbnum 1+N,A}\quad.
\]
To implement this in Scala, we first define \lstinline!SqSize[N, A]!
as a disjunctive type and then define \lstinline!Sq[A]!:
\begin{lstlisting}
sealed trait SqSize[N, A]
final case class Matrix[N, A](byIndex: ((N, N)) => A) extends SqSize[N, A]
final case class Next[N, A](next: SqSize[Option[N], A])  extends SqSize[N, A]

type Sq[A] = SqSize[Unit, A]
\end{lstlisting}

As an example, the $2\times2$ matrix $\,\begin{array}{|cc|}
11 & 12\\
21 & 22
\end{array}\,$ is represented by a value of type \lstinline!Sq[Int]! as:\vspace{0.2\baselineskip}

\begin{lstlisting}
val matrix2x2: Sq[Int] = Next(Matrix {
  case (None, None)         => 11
  case (None, Some(_))      => 12
  case (Some(_), None)      => 21
  case (Some(_), Some(_))   => 22
})
\end{lstlisting}

A $3\times3$ matrix will have the form \lstinline!Next(Next(Matrix { ... }))!.
An $n\times n$ matrix has $\left(n-1\right)$ nested \lstinline!Next!
constructors in front of a \lstinline!Matrix! constructor. The size
of a matrix is encoded by its type in this way.

We now implement a \lstinline!sequence! function for \lstinline!Sq!.
Since \lstinline!Sq! is defined by induction using \lstinline!SqSize[N, A]!,
we must first implement \lstinline!sequence! for \lstinline!SqSize!:
\begin{lstlisting}
def sequence[A, N, F[_]: Applicative: Functor]: SqSize[N, F[A]] => F[SqSize[N, A]] = {
  case Matrix(byIndex)   => ???    // Base case.
  case Next(next)        => ???    // Inductive step.
}
\end{lstlisting}
In the base case, we have a function \lstinline!byIndex! of type
$N\times N\rightarrow F^{A}$ that can return $n\times n$ different
values of type $F^{A}$. We need to combine all those values together
by using $F$\textsf{'}s \lstinline!zip! method. The result will be a value
of type $F^{A\times A\times...\times A}$, which we will then need
to convert to the type $F^{N\times N\rightarrow A}$. The only way
of performing these computations is by enumerating all possible values
of type $N$. Note that the type \lstinline!Sq! sets the type parameter
\lstinline!N! in \lstinline!SqSize[N, A]! as \lstinline!N = Unit!.
This forces the type parameters \lstinline!N! in all of the \lstinline!Next()!
constructors to be \lstinline!Unit! wrapped in a number of \lstinline!Option!
constructors. All values of a type of this form can be enumerated
explicitly. However, the type \lstinline!SqSize[N, A]! does not ensure
that \lstinline!N! will be a type with a known finite number of values.
This problem prevents us from implementing the \lstinline!sequence!
method for our current definition of \lstinline!Sq!.

A solution is to add a typeclass constraint (with a typeclass called
\textsf{``}\lstinline!Finite!\textsf{''}) on the type parameter \lstinline!N!. A
suitable typeclass instance of \lstinline!Finite[N]! contains a list
of all values of type \lstinline!N!:
\begin{lstlisting}
type Finite[N] = List[N] // A list of all possible values of type N.
\end{lstlisting}
We can implement functions that create typeclass instances automatically
for all the types we will actually use instead of the type parameter
\lstinline!N!, namely, the types \lstinline!Unit!, \lstinline!Option[Unit]!,
\lstinline!Option[Option[Unit]]! and so on. Suitable typeclass instances
are defined inductively:
\begin{lstlisting}
implicit val finiteUnit: Finite[Unit] = List(())
implicit def finiteOptionN[N: Finite]: Finite[Option[N]] = None +: Finite[N].map(Some(_))
\end{lstlisting}

Using these definitions, we can now extract all values of type $A$
from a value of type $N\times N\rightarrow A$:
\begin{lstlisting}
def access[N: Finite, A](s: SqSize[N, A], i: Int, j: Int): A = s match {
  case Matrix(byIndex) => byIndex((Finite[N].apply(i), Finite[N].apply(j)))
  case Next(next) => access[Option[N], A](next, i, j)
}
\end{lstlisting}
Let us test this code:
\begin{lstlisting}
scala> access(matrix2x2, 0, 1)
res0: Int = 12
\end{lstlisting}

Here is the complete code of \lstinline!sequence! for the type constructor
\lstinline!Sq!:

\begin{lstlisting}[frame=single,fillcolor={\color{black}},framesep={0.2mm},framexleftmargin=2mm,framexrightmargin=2mm,framextopmargin=2mm,framexbottommargin=2mm]
type Finite[N] = List[N] // A list of all possible values of type N.

object Finite { def apply[N: Finite]: Finite[N] = implicitly[Finite[N]] }

sealed abstract class SqSize[N: Finite, A]

final case class Matrix[N: Finite, A](byIndex: ((N, N)) => A) extends SqSize[N, A]

final case class Next[N: Finite, A](next: SqSize[Option[N], A]) extends SqSize[N, A]

type Sq[A] = SqSize[Unit, A]

implicit val finiteUnit: Finite[Unit] = List(())

implicit def finiteOptionN[N: Finite]: Finite[Option[N]] = None +: Finite[N].map(Some(_))

// Access the matrix element at zero-based index (i, j).
def access[N: Finite, A](s: SqSize[N, A], i: Int, j: Int): A = s match {
  case Matrix(byIndex) => byIndex((Finite[N].apply(i), Finite[N].apply(j)))
  case Next(next) => access[Option[N], A](next, i, j)
}

def size[N: Finite, A](s: SqSize[N, A]): Int = s match {      
  case Matrix(_) => Finite[N].length
  case Next(next) => size[Option[N], A](next)
}

def toSeqSeq[N: Finite, A](s: SqSize[N, A]): Seq[Seq[A]] = {
  val length = size(s)
  (0 until length).map(i => (0 until length).map(j => access(s, i, j)))
} 

// Test: visualize matrix2x2 by converting it to nested lists.
scala> toSeqSeq(matrix2x2)
res1: List[List[Int]] = List(List(11, 12), List(21, 22))

def sequenceList[F[_]: Applicative : Functor, A](l: List[F[A]]): F[List[A]] = l match {
  case Nil          => Applicative[F].pure(Nil)
  case head :: tail => (head zip sequenceList(tail)).map { case (x, y) => x +: y }
}

def sequence[N: Finite, F[_]: Applicative : Functor, A](sq: SqSize[N, F[A]]): F[SqSize[N, A]] =
  sq match {
    case Matrix(byIndex)   =>
      val allValuesF: List[F[((N, N), A)]] = for {
        i <- Finite[N]
        j <- Finite[N]
      } yield byIndex((i, j)).map(a => ((i, j), a))

      val fList: F[List[((N, N), A)]] = sequenceList(allValuesF)
      fList.map { values =>
        val valuesMap: ((N, N)) => A = values.toMap.apply
        Matrix[N, A](valuesMap)
      }
    case Next(next)        => sequence[Option[N], F, A](next).map(Next(_))
  }
\end{lstlisting}

To test this code, we define a value \lstinline!matrix2x2List! that
represents a square matrix of lists:
\[
\text{matrix2x2List}:\text{Sq}^{\text{List}^{\text{Int}}}\quad,\quad\quad\text{matrix2x2List}\triangleq\left|\begin{array}{cc}
\left[0,10,100\right] & \left[1,11,101\right]\\
\left[2,12,102\right] & \left[3,13,103\right]
\end{array}\right|\quad.
\]
 Applying \lstinline!sequence! to that value, we obtain a list of
$3$ square matrices:
\[
\text{seq}_{\text{Sq}}\,(\text{matrix2x2List})=\big[\left|\begin{array}{cc}
0 & 1\\
2 & 3
\end{array}\right|,\,\left|\begin{array}{cc}
10 & 11\\
12 & 13
\end{array}\right|,\,\left|\begin{array}{cc}
100 & 101\\
102 & 103
\end{array}\right|\big]\quad.
\]
This represents a kind of transposition operation for tensors of dimension
$2\times2\times3$. The test code is:
\begin{lstlisting}[frame=single,fillcolor={\color{black}},framesep={0.2mm},framexleftmargin=2mm,framexrightmargin=2mm,framextopmargin=2mm,framexbottommargin=2mm]
// Test: use List as an Applicative Functor.
implicit val applicativeList: Applicative[List] = new Applicative[List] {
  override def pure[A](a: A): List[A] = List(a)
  override def zip[A, B](fa: List[A], fb: List[B]): List[(A, B)] = fa zip fb     }

implicit val functorList: Functor[List] = new Functor[List] {
  override def map[A, B](fa: List[A])(f: A => B): List[B] = fa map f
}

// Test value of type Sq[List[Int]]:
val matrix2x2List: Sq[List[Int]] = Next(Matrix {
  case (None, None)         => List(0, 10, 100)
  case (None, Some(_))      => List(1, 11, 101)
  case (Some(_), None)      => List(2, 12, 102)
  case (Some(_), Some(_))   => List(3, 13, 103)
})

// Apply `sequence` to the test value. The result has type List[Sq[Int]].
val list2x2Matrix: List[Sq[Int]] = sequence(matrix2x2List)

// Visualize the result by converting it to nested lists.

scala> toSeqSeq(list2x2Matrix)
res1: List[List[List[Int]]] = List(List(List(0, 1), List(2, 3)), List(List(10, 11), List(12, 13)), List(List(100, 101), List(102, 103))) 
\end{lstlisting}

The type \lstinline!Sq! assures (at compile time) that all matrices
have consistent shapes. However, it is hard to use because of complicated
type parameters and deeply nested type constructors. To achieve good
performance, square matrices and other tensor-like quantities are
usually represented by flat arrays with an interface that recalculates
the indices. When the dimensions of the matrices are known at compile
time, one could use macros or other metaprogramming features to assure
that all matrix operations are consistent, without resorting to complicated
type constructors. Alternatively, one can use dependent types to constrain
matrix dimensions at compile time.\footnote{See, for example, the \texttt{NDScala} library for Scala 3: \texttt{\href{https://github.com/SciScala/NDScala}{https://github.com/SciScala/NDScala}}}
\begin{comment}
this is chapter 9 of the functional programming tutorial traversable
functors to motivate the interrupted introduction of these factors
into practice I always remember the example that you have a list of
some data items and you want to process it by using a function like
this where you have a future as a as a result of this function and
the usual way of doing this in Scala is to use a function called future
dot sequence and I have seen this I have shown this in a previous
tutorial and the idea is that you want to process each element of
this list with this function and you have to wait until the entire
list is done so we have many separate computations for each element
of the list encapsulated by the future for each one of them you want
to wait until the entire list is done and basically this is the type
signature that you want in order to implement this computation you
have a list of a you have a function from a to future B and you want
to get a list of B as a result and you can get it in the future so
you have a future of lists of it as a result and that is the type
signature that the function future that sequence will allow you to
have with some work but in order to understand what this kind of computation
does we want to generalize from the future to an arbitrary type constructor
F and we want to understand what properties these type constructors
must answer so the list we have L and instead the future we have F
and the type signature of the function is like this so this function
is called Traverse I believe there\textsf{'}s also a future of traders with
a type signature like this that works on sequences we want to generalize
to some type constructors F and L and that\textsf{'}s what we will be able
to do once we understand the properties of this operation so this
operation can be implemented for instance if L is this type constructor
then what can we do in order to implement this operation well clearly
we have an LA and we can apply map F so f is this function we can
apply map F and the result will be this now in other words we have
not what we want probably wanders F lb Inc which would be this so
how can we get F of a triple from a triple of F\textsf{'}s or clearly we need
an operation that\textsf{'}s similar to zip zip would be FB times F be going
to F of B times B now we need to apply zip twice and then we get from
here to here so once we have the zip we will be able to implement
this traverse operation that\textsf{'}s the conclusion so far so it seems that
F needs to be a quick ative in order to be able to implement this
type signature for at least for this type constructor so for this
type constructor certainly it is easy to implement if F is if it has
a zip operation then we can implement it like this so that is going
to be a fundamental assumption not for the traverse operation to make
sense the type constructor F must be implicative now the type constructor
L on the other hand doesn't we don't know what that is it could be
applicative or not maybe we'll find out but for now let\textsf{'}s what say
L is traversable if it has this operation now in Scala we have a very
limited version of this Traverse which assumes L to be a sequence
so it\textsf{'}s not based on this idea of being traversable as such it\textsf{'}s just
that one of the properties of a sequence is that you can implement
this operation and we'll see why that is so but for me the example
with lists and futures or sequences of futures is the easy to remember
example that helps me remember the requirements for the traversal
duration so always think that I have a list of some items and the
processing makes me a future so L is a list F the future and then
it is clear that I want this kind of type signature I want to have
a single future and when that future completes I want to have the
entire list of process data that\textsf{'}s the easy to remember example so
not not the other way around for example not a list of future they're
not interested in having a list of futures I want a single future
with the final list of the results and so that\textsf{'}s why this example
helps me remember this somewhat complicated type signature where I
could easily make a mistake they will a to L of B today to have fun
being LF being instead of X well being it\textsf{'}s easy to mix them up but
so remembering this example that I'm starting with that what helps
me remember the time signature of triggers so the questions that I'm
going to answer in this tutorial in this chapter are to find out what
factors L can have this triggers operation to find out if we can simplify
with somewhat complicated type signature can we express it perhaps
through a simpler operation what are the laws that is reasonable to
require for this operation and finally to look at contractors and
pro fungus do they have also some kind of analog of this operation
in previous tutorials are started right away with practical examples
of usage in this chapter I will first do more theory to understand
in more detail and more deeply the properties of this operation that
will be easier to follow the usage examples so to simplify traverse
we notice that traverse is a kind of lifting of sorts it\textsf{'}s the arguments
can be permuted so these are two curried arguments so we can take
this one was the first argument and then we have la to FLV yes the
second argument so it\textsf{'}s a complicated kind of twisted lifting and
we have seen several times already that often you can find a simpler
natural transformation that is computationally equivalent to a lifting
so let\textsf{'}s derive that natural transformation that is equivalent to
traverse to derive it I asked the question so why can't we have F
map to do the work of Traverse f map would have this type signature
it Traverse sorry this is actually yeah so so this is the type signature
investment but Traverse needs F L be here instead of lfb so see this
F needs to be outside and that\textsf{'}s what\textsf{'}s missing so we need to transform
lfb with F inside into F L being with em outside so that\textsf{'}s the transformation
that is a natural transformation we expect to be equivalent the trailer
so what\textsf{'}s called a sequence so this is maybe not a very good name
sequence kind of suggesting that we change the or the order of L and
F in the functor composition not a very good name but that\textsf{'}s a traditional
name and I don't know how else to call it indeed we find that the
functions traversing sequence are computationally equivalent so this
is why well we have defined sequence likeness so then Traverse of
a function f is computed by first doing F map of F like we get here
then we get this and then apply this sequence function that I abbreviated
to seek which performs this lets transposition of the order the type
diagram looks like this so we start with a type LA and we can do Traverse
from it directly with this trap function which takes F and director
gives you from la f OB or you can first do F map so you have a function
f under L you get instead of la l FB and then you change the order
so you reorder the functions composition ever be to FM and the results
must be equal so for any value of this type you go up or horizontal
and the result must be the same value of this type that\textsf{'}s the definition
and as we have seen before this pattern implies that natural transformation
is defined as a composition of F map sub sorry and lifting is defined
as composition of F method natural transformation and then this natural
transformation is equal to lift and if you take identity instead of
F obviously and so then that\textsf{'}s a pattern we've seen time and again
where the result is that traversing sequence are computational global
you can derive one from the other and back and it gets the same function
back so I'm going to spend time through again since it\textsf{'}s exactly the
same proof as we had many times just a different type signature and
notice also F here is an arbitrary placated factor so these functions
don't use the structure of f other than that it is applicative so
that\textsf{'}s that\textsf{'}s an example we have seen just just before we implemented
a traverse function for this type constructor by applying the zip
function of F otherwise we don't you know what F is we just use zip
from it and so we are generic in the function in the functor F as
long as it\textsf{'}s implicit if we don't look at the structure of F we do
look at the structure of L so the Traverse function depends on the
structure of L but it doesn't depend on the structure of F it\textsf{'}s generic
images future that sequence has this type signature and that\textsf{'}s an
example of a sequence natural transformation you note we cannot have
the opposite transformation I'll show the example for that but well
for future analyst you could make an opposite transformation for a
future of a list and you produce a list or individual futures that
are going to be all already copies of this future mapped to select
one copy of the lid and of the element you can do that it\textsf{'}s kind of
useless to transform in the opposite way but what I will show on examples
is that it\textsf{'}s impossible to have this transformation in general and
arbitrary it\textsf{'}s possible for future not for arbitrary yeah so examples
of traversable function functors this example we have already seen
list is another example sequence in general and also finite trees
various shapes there and also traversable an example of an entre versatile
factor is there either Malad and also the lazy list or infinite product
or stream is sometimes called lazy string so let\textsf{'}s see why that is
so let\textsf{'}s implement the sequence for this type constructor first so
I'm going to define this file constructor for convenience and the
seek function is I'm just going to define directly it\textsf{'}s going to have
this type signature and that\textsf{'}s just a zip apply it twice and then
some reordering Oh nested tuple that results from the zip and we know
this is associative because we assume that F is applicative now I'm
using my own typeclass for F which I call the zip for applicative
but you can also use cats applicative just has a slightly different
name for things and it doesn't have the zip syntax so I like the use
of zip syntax so I'm using my own type process here but it\textsf{'}s equal
to standard implicit if that was so the sequence function has this
type signature just as we have seen in the case or filter balls lowlands
and applicatives it\textsf{'}s much easier to reason about this natural transformation
rather than the reason about the lifting it\textsf{'}s also will be the case
of laws or in simpler to formulate so that\textsf{'}s why I will always always
define just a sequence function I will not define a traverse function
the Traverse is easily defined in terms of a sequence - and I'm therefore
in this tutorial concentrating entirely on the sequence function I'll
never implement Traverse directly to save time so let\textsf{'}s have another
example that either as a functor either Z where Z is a constant type
then the sequence must have this type signature so it takes an either
of Z FFA and puts F outside so it\textsf{'}s pulls the function f from the
inside of our plan into the outside that\textsf{'}s the time signature of sequence
so how do we implement that all we need to match so if it\textsf{'}s the left
we have a Z to produce F of Z so the only way to do that is to use
the pure method okay I mean interchange there were there blinds here
the left z does not have any FFA and so in order to produce an F of
something we have to use the pure method from F and then we apply
that to the left of Z and then we get the right type if we have a
right of FA then all we need to do is to put the right inside the
width so we just map that we don't change the value of a we just wrap
it to wrap it into the right type construction so that\textsf{'}s the very
clearly simple implementation so if we actually write this type signature
using a shortcut notation then maybe it\textsf{'}s even easier to understand
how the sequence function works so if we have a Z then we just put
Z inside F using the pure if you cover away then we just put a 0 plus
a into that by mapping with the right type constructor so that we
don't change this value hey let\textsf{'}s see how to implement the sequence
method for the Fortran type so here\textsf{'}s a simple binary tree it has
a value of type a and belief and it has a branch of two trees so how
the hell does sequence work on a tree like we're in the leaf where
we just wrap in the leaf like we did with this either and if we're
in the branch then we apply the sequence method which is the same
sequence we're defining is a recursive function them so we recursively
apply the same since and as a sequence method to the left and to the
right branches of the tree and then we zip them together so then zip
is the operation in the F function wickety function so we can use
that zip them together we get an F of a pair two trees and then we
wrap that fear is a branch under the map so this map is under the
tree sorry under the F function so f is an arbitrary negative function
and we're using its methods map and zip here we used its metal period
but other than that we did not use any knowledge of F so it is in
this way that we are generic in the factor f we are not using any
knowledge about the structure of f other than it has that it has a
map and zip and puree method let us see examples of non traversable
functors so here\textsf{'}s an example it\textsf{'}s a reader mode with a parameter
in its non polynomial and so it will turn now that this is not reversible
so let\textsf{'}s see why what\textsf{'}s takes on applicative function f specifically
like option a and let\textsf{'}s find all implementations of this type signature
which is the pipe signature of sequence now all implementations turn
out to be just one and this implementation always returns none so
it always returns an empty option ignoring its argument so it is not
a very interesting implementation and we will see shortly that this
would not satisfy was overprotective of over a traversable contest
so this is this satisfies the pipe signature but it does not satisfy
the walls we haven't yet seen the laws but it is reasonable to say
that this function completely ignores its argument so it loses information
and typical walls for a lifting would be identity and Composition
laws identity law would tell you that some lifting is identity but
if it\textsf{'}s losing information it cannot be identity so it cannot preserve
the data that you give it but if people lose it will always return
empty option and so that\textsf{'}s reasonable to expect when this note is
not a good implementation and so there are no good implementations
- thank you let\textsf{'}s take another example and you'll see there\textsf{'}s there\textsf{'}s
one implementation for this so we can actually implement this type
signature for this duplicative functor this this code is what you
would expect it\textsf{'}s taking this function so what translate entire signature
let\textsf{'}s just e to the pair and this is pair of eternity doing now if
you have a eat to the pair and you can produce a pair of Italy in
d2 a that\textsf{'}s very easy just duplicate your your function so we do have
implementations from this book we're supposed to produce an implementation
of sequence as a generic in the function so we cannot look at the
structure from the functor F and have a different implementation for
every effort want to be generic and so because we can we cannot implement
it for some F we're stuck in this is not going to prevent reversible
functor let\textsf{'}s take another example where we have a pair so the pair
of some type and a polynomial function and let\textsf{'}s take this as the
implicated and again we find them there is an implementation so that\textsf{'}s
fine well this is actually traversable what\textsf{'}s considered the infinite
list so the infinite list class needs to be defined because we cannot
have the cursive type as a type an alien\textsf{'}s we have to have a class
and it\textsf{'}s a pair of value of type a underlays evaluated tail choosing
again an infinite list of time from all values of tightly so let\textsf{'}s
define a sequence method well we can actually define it it\textsf{'}s quite
easy you you take the head of the list you zip it with the recursive
implication of the same function sequence to the tail from the list
which will sequence were found commute the order of factories and
then you wrap it into the infinite list again so that\textsf{'}s similar to
what our implementation for the sequence operation on a tuple except
that it\textsf{'}s recursive and it turns out this is infinite recursion let\textsf{'}s
check that the even the simplest functor have the identity function
let\textsf{'}s define it like this put some type of class instances for identity
function ages tribunal was defined let\textsf{'}s define an example value of
an infinite list which is if you find like this it\textsf{'}s a recursive definition
we could do a lazy Val instead of def I believe but it\textsf{'}s just cleaning
doing them since it\textsf{'}s a recursive function and you see the tail of
the list will turn again the same list so it\textsf{'}s going to be an infinite
list of 123 on the whole the way to infinity so if we use sequence
on this value then we get a stack overflow interaction because it\textsf{'}s
an infinite recursion so it\textsf{'}s it\textsf{'}s an infinite loop there\textsf{'}s no way
to implement the Traverse of an infinite list because basically what
you would mean is we need to sequence like this we need to have a
list of F values so it\textsf{'}s an infinite list of an infinite list about
four days and that should be mapped into an f of infinite list of
is now how can we do that we need to pull F outside to the outside
of the title which means that we need to evaluate infinitely many
of these f\textsf{'}s in order to pull off outside mean generically that\textsf{'}s
what we need to do we need to evaluate infinitely many elements of
this infinite list in order to put F outside it\textsf{'}s impossible to just
pull F outside magically out of the infinitely many elements here
and so even when F is just an identity factor s won't work it\textsf{'}s impossible
to pull f outside I mean it would be possible for identity factor
of course but we have to be generic in the factor f we cannot use
any methods on F other than zip and map and so we don't know what
F is f could be something that needs to be evaluated in order to pull
a out of it and so because of that it forces us to evaluate infinitely
many elements before we even get a single value of this type and so
that will never end and so that\textsf{'}s impossible so for this reason an
infinite list is not reversible and finally I mentioned that the opposite
type signature isn't is unworkable so why let\textsf{'}s make an example calculation
so let\textsf{'}s say L is an easier which we know is reversible F is this
reader unit which we know is applicative so let\textsf{'}s find all implementations
of this time signature and we find they're not there is no implementation
of this type signature the reason is that this type signature would
have to map this function into this data but that is impossible you
cannot extract Z out of this so you could not possibly return the
Z because you need an integer so which integer are going to give imagine
that this integer is a different data type you don't you don't have
values of it necessarily for integer you could put 0 in that integer
sure but that would not be reasonable from other types so you cannot
possibly pull Z out of this because there aren't any special values
of this type and you can also not get this function because you only
have this function and this function could sometimes fail to return
an 8 could sometimes return a Z so for some integers it could return
is e and you don't know in advance for which integers it will return
Z and for which it will return in a until you run this function on
every possible integer you won't know that and so that\textsf{'}s impossible
to know and so you can't split this into a Z and into it you couldn't
either split it into two functions into Z and into in the same for
the same reason you in order to split it you'd have to run this function
on every possible integer and see what the results are so that\textsf{'}s unworkable
and so that\textsf{'}s why you don't have that\textsf{'}s that\textsf{'}s an informal reason
why you don't have any implementations of this type and a final comment
is that there are several ways of implementing the sequence usually
so let\textsf{'}s consider this type again we have seen we can implement a
sequence by applying zip to x and here\textsf{'}s another implementation we
can arbitrarily select a different order so we instead of zipping
one two three with zip to one I'm sorry this is a mistake two three
say 1 and that\textsf{'}s valid so the type is right and the laws will hold
as well we'll see why so that shows you that there\textsf{'}s more than one
way of implementing the Traverse or in the sequence function which
is equivalent for a given type constructor L different valid ways
of doing it so let\textsf{'}s find out if other polynomial factors are traversable
now one of the central results here is that all polynomial functions
are reversible we will show this quite rigorously later so for now
let\textsf{'}s see how we can implement the Traverse or the sequence function
for an arbitrary polynomial function so we have done it so far for
this and we have also done it for either which is a simple polynomial
factor and the general polynomial factor would have this form it\textsf{'}s
got a polynomial in a with some constant coefficients which I here
denoted as Z Y Q P so we have seen how to implement for a monomial
so let\textsf{'}s first consider monomial like this so one part of this polynomial
then we can apply zip to these so first we look at lfb so RV has this
type so we can apply zip to these we get this and then we can lift
the Z into the functor F by just this standard factor map this function
is always possible for any factor f alternatively we can do F pure
of this Z and then this will become FZ and then we can zip it together
with all others but the result will be exactly the same as using this
function because of the law of identity for applicatives so the result
is going to be this and then that\textsf{'}s a sequence method for a single
monomial and then for each normal you do this and you have a disjunction
of different results of type F of a monomial and then we lift it to
F of the disjunction like we did in the either implementation so we
we have seen therefore that we can perform the traversal equivalently
the sequence operation on monomials and we can also perform them on
disjunctions and therefore we can perform them on arbitrary disjunctions
on monomials and that\textsf{'}s arbitrary polynomial factor also note we could
apply zip here in different orders wicked first zipless and then Z
put Z on the right we can change orders in different ways and so traversal
order could be application-specific it could be necessary to adjust
it for a certain application you can always implement in some order
or automatically say but it might not be necessarily correct for your
application and also we have seen that non polynomial factors at least
some of them are not to her so so this is not reversible because we
cannot have a reasonable implementation of this that does not lose
information and there\textsf{'}s this paper that I'm referencing here it\textsf{'}s
a complicated paper there in theoretical but it has a proof that only
polynomial functions are reversible that that and and also they must
be finite so infinite lists do not fit the conditions of their theorem
only finite containers polynomial factors are essentially containers
with data they can have different shape they can have many items of
data or few or none it could be a disjunction of different shapes
and they also can have extra data of some constant type in addition
to data of the type a but those are the containers that are traversable
and no other containers aren't reversible sorry you have a lazy infinite
stream that\textsf{'}s not reversible they have to be finite and they have
to be polynomial so that is proved in this paper in a complicated
way so I'm not going to try to understand how they did it I believe
that this is so because I have examples that even the simplest non
polynomial factor can't have reasonable implementation of sequence
so even though all polynomial functions are traversable they are usually
traversable in several different ways and so it\textsf{'}s useful to have a
typeclass to declare a specific instance of a typeclass expressing
a specific order of traversal so this order of zipping that we can
choose here corresponds to ordering of traversal in a sense we'll
see that when we look at specific examples of traversing but now let\textsf{'}s
take a look at the laws because we have have been talking about the
laws so far and we need to see more in more detail how they work so
I prefer to look at this type signature of Traverse and to derive
laws using the lifting intuition so it\textsf{'}s a lifting of sorts and every
time we had lifting so far we had laws of identity and Composition
in other words there was some kind of identity here and some kind
of composition of these and this has to translate into identity here
and composition of these I will mention that there is this paper which
is arguing what laws traversals must have from a different perspective
not as formally as I have argued just now because my argument is completely
formal Traverse looks like a lifting therefore it should have laws
like the laws we had before for other liftings it\textsf{'}s purely a formal
argument saying that the form of this function is similar therefore
it should have similar laws but this argument doesn't look at what
Traverse actually does well what it does is that it evaluates some
function on each element this one produces some effect maybe this
F is an applicative factor which could be a monad or it could be known
more and what it encapsulate some kind of effect some kind of computational
context or a result other than B and all these contexts need to be
somehow put together and be outside of the elbe so we need to reconstruct
our container L inside the larger effect described by F so we need
to somehow combine all these effects for individual values of a into
one big effect which will be outside and then we have to combine all
the values of B after somehow pull them out combine them again into
the same shape as the container L inside the F so all that needs to
be done by the traversal function and the authors of that paper argued
that the traversal first of all should visit each element of the container
exactly once it it should evaluate each effect exactly once and then
combine these effects into a larger effect and using this intuition
they formulate some laws that seem to fit this description in some
way well they didn't actually derive these laws from these requirements
but they showed examples where these requirements are violated and
they showed that these examples also violate the laws so this is a
little not very convincing to me that these laws are correct and therefore
I prefer the more formal approach because I have more assurance but
if I find some reasonable identity and Composition laws that\textsf{'}s a correct
set of laws so far in every example we have seen with functors with
contra factors filterable applicative and wounded every single example
had a function with type signature like this which was life like a
lifting in my terminology and in every single example there were identity
law and composition law and these laws were equivalent to all the
other set of laws that were derived from intuition and these laws
also corresponded to some category laws now in this example I don't
know how to formulate this in terms of a category and everything a
way that would be simple enough so I'm satisfied that I find a law
that looks like identity law and the law that looks like a composition
law even though I'm not satisfied that I can find it easy enough category
to reason about so that it\textsf{'}s useful so I'm not going to talk about
the category in this chapter I'm not going to describe this as a lifting
from one category to another because I don't know if that\textsf{'}s really
very useful and I don't know a good formulation of that so let\textsf{'}s look
for these laws so identity law is that we map some special function
here that is identity in some sense into a function here that plays
the role of identity now the type signatures are not a to a they are
a to f of B so what could be playing the role of identity here well
the pure method obviously and F has that method by an assumption so
the identity law is that if we put a pure method here than it should
be lifted to this which is again a pyramid and except it\textsf{'}s applied
to a different type of parameter now another way of formulating identity
is to say that if F is the identity function so then there\textsf{'}s no F
you just be then identity function here is all ordinary a to a and
that should be lifted to identity elite really so f is just identity
function and then this identity should be lifted to this so that is
another way of formulating an identity law let\textsf{'}s find out now to compose
what will be the composition if we have two of these functions like
f and G then we can compose them using F map but the result would
be this because the F G would be nested now we take F be we map G
over it and we get F of G of C now if F and G are in clique are applicative
then the composition F of G is also applicative we know that from
the properties of the platitudes and so it is again a function of
the same type except that it has a different factor instead of F so
f function G factor and the composition of such functions is going
to be of this kind which which is kind of complicated it changes the
functor each time so what should be on the right-hand side what should
be a composition of these traversals now the composition of traversals
obviously works in the same way so you have la 2 FL b and then FL
b 2 f g LC and so that is your final traversal and that should be
equal to applying the traversal right away with this function as a
purgative factor so in other words applying traversal to this if that
is true then traversal of composition is equal to composition of traversals
so that\textsf{'}s with a little twisting where we keep using F map in order
to get composition and keep pulling all the functors F and G all the
implicit is we keep pulling them outside with these twists it looks
like just a composition law traversal of composition of F and G with
some twisting is equal to the composition of the two traversals with
F and with G again with some twisting so these are the laws we're
going to examine and it will turn out these are exactly the same laws
as this paper proposes when I first looked at this it looks like we
have two identity laws are they really independent no they are it
will see that but that\textsf{'}s a question we need to answer also laws for
sequence are probably going to be simpler because in our experience
so far always we found that the laws became simpler if we consider
the natural transformation instead of a lifting so let\textsf{'}s find those
laws for the sequence and finally with the laws in in hand we can
try to answer the question of which functors satisfy these laws we
have found examples where we can implement the type signatures but
are the law is respected by those examples or not so that\textsf{'}s the questions
that we have after this point so let\textsf{'}s look at the first item the
dual so a traverse of pure needs to be Pierce and how fast if it\textsf{'}s
the first law let\textsf{'}s look at the type diagram for this so we started
from away we map it with pure where we map it with traverse of pure
and it should be the same value it was type in favilla so that\textsf{'}s the
first one now the second identity law looks like this and it\textsf{'}s clearly
a consequence of the first identity law if we just put F to be the
identity founder and the first law should hold for every applicative
functors so we really just need this one law when the second one is
a consequence so let\textsf{'}s forget it so we have the identity law which
is this and we have the composition law which we can write like this
and the short notation so for any function of this type and the function
G of this type and for any applicative factors F and G we have so
the composition of F and G is twisted in the sense that G must be
lifted by factor F so this is our twist on the composition other than
that composition of two traversals is equal to Traverse of composition
here will we twist and here we twist let\textsf{'}s look at the type diagram
we start again with LA first with Traverse with function f and we
get F lb well then we want to traverse with G but G works on B so
we have to we have to traverse just on this lb and F needs to stay
outside therefore we use F map of F and we Traverse inside of F using
the Traverse of G which is giving us this value F G LC or we could
directly Traverse on the function H with respect to the factor f of
G which is a functor composition so the factor H could be defined
like this as a functor composition of F and G and we know it\textsf{'}s applicative
so we could just directly Traverse with respect to H so this can be
traversal with respective H of a function H of this type this function
is defined like that so this is the law that we demand now which will
hold let\textsf{'}s derive the corresponding law for laws for its sequence
to do that we just Express Traverse like this your sequence and substitute
that into the laws of chillers so let\textsf{'}s look at identity law Traverse
of pure equals P R so Traverse of pure equals this conduction legal
pure so that\textsf{'}s the law of identity for pure so pure lifted to Bill
and then sequence should be the same as pure applied to the type parameter
Helle I should also mention naturality law that\textsf{'}s always the case
for all of our examples so far such as filterable moon and imitative
but all these natural transformations as well as the liftings they
have natural tea laws for each type parameter that they have there\textsf{'}s
one neutrality law which expresses the fact that the code implementing
that function does not use any information about the type it\textsf{'}s completely
generic in that type and so you can map this type to another type
before the transformation or after the transformation and the results
are going to be the same so here\textsf{'}s what the naturality law looks like
for the sequence now sequence has a more complicated type signature
and its type parameter a is all the way inside therefore in order
to transform in this a to some B we need to have a double F map so
let\textsf{'}s say there\textsf{'}s some function G that transforms a to be in order
to transform this a or transform that a we need to lift this G twice
and that\textsf{'}s how I would write it down so first we can sequence this
LF a to FL a and then we can lift G twice in this order or we can
first lift G twice in this order so that instead of LF a we get LF
B and then we sequence on that so that should be the same function
that\textsf{'}s a natural t law and the code for sequence will automatically
respect that law if it is a code that is generic type parameter a
will never use any information about the type of a so usually naturality
laws are satisfied automatically by generic code and so it\textsf{'}s not important
to check them but it\textsf{'}s important to check the other laws so now let\textsf{'}s
look at the composition law we need a bit more work about it traversal
of F followed by the traversal of G lifted so let\textsf{'}s substitute the
definition of traversal and then we get this formula now we can decompose
a lifting of composition going to factor law so we can get like this
now naturality law means we can interchange this and we get that and
finally the right hand side is written like that so again we can do
that because of the thumb tree law so in other words the composition
law for traverse says that this is equal to this they have a common
prefix that we can omit because these are arbitrary functions F and
G for which this must hold and so it\textsf{'}s sufficient that these things
are equal so that\textsf{'}s the law for sequence much simpler and here\textsf{'}s the
type diagram because the type types here are actually complicated
so we start with an L of F G C C is some type parameter I could use
a here but I just can't see now first we sequence with with respect
to F and then we just pull F out and we get F LGC then we sequence
with respect to G but we lift it to be operating inside of F so that
means we interchange this L and this G the result is f G LLC alternatively
we could sequence with respect to the factor f of G this is maybe
not a very nice known notation not very consistent but this is the
composition of functions F and G so it\textsf{'}s the page I mentioned here
I should probably for clarity I should use page instead of F G and
so if we sequence with respect to F G that means we pull out F G at
the same time to the outside and we get F G of LC so the result of
these two computations must be the same having formulated the laws
now can look at constructions and we can check that the laws hold
we can also check whether some factors can be traversable when they
are polynomial or when they recursive or they're not polynomial so
we have so far seen some examples but now let\textsf{'}s be more systematic
about it here are the constructions that I found for traversable factors
and I will go through each of them one by one now now before I do
that let me explain what is a by traversable and by functor so by
factor is a type constructor with two type parameters which is a functor
in both of them so it has a map with respect to each of them separately
and it also has a map with respect to both of them at once of course
because you can just you can transform a to some C and B to some D
separately or at the same time if you wish it doesn't doesn't it so
that\textsf{'}s a bi functor and by factors are by traversable in the following
sense they have this natural transformation where we have an F wrapping
each of the type parameters of s and this F can be pulled out by this
sequence by sequence natural transformation and it should be natural
both an A and B separately so that\textsf{'}s the assumption and so you see
this a and B remains and we have pulled F out to the outside so if
statute transformation exists which is natural in both a and B and
it has the same laws the laws of identity and the laws of composition
now the law of identity is this and it needs to be adapted obviously
to this by seek so that you have pure here and pure here and the law
of composition obviously needs to be adapted as well because we have
F G a F G B and so on but other than that it\textsf{'}s a direct generalization
so analogous laws must hold so let me now begin by deriving the laws
that must hold for these constructions rather deriving the fact that
the laws hold for these constructions first construction is constant
functor and identity factor so both of them are traversable till you
find that we need to define the sequence method and to show that the
laws hold for it that\textsf{'}s very simple so for convenience I'm just going
to put a type parameter up front the constant factor is a function
that doesn't end on its type parameter so L of F of a is the same
as I've elevated Simon is this constant times e and FMLA is f of Z
so sequence is this type signature and clearly this is f dot P R is
just a pure of F functor can't in any other way produce a type signature
like this but is generic in if we're not allowed to know what F is
except that it has a pyramid zip and map so let\textsf{'}s just define that
as a pure let\textsf{'}s check Louis so the identity law is that F map of pure
followed by sequence must be equal to F dot P R now I'm putting here
this underscore ell notation for clarity I could have said for example
instead of F dot P R I could have said pure underscore F what I mean
by this is that it\textsf{'}s the method that is defined for the type instructor
F such this method is not generic and F it is defined for each F separately
in the typeclass instance similarly F map is not generic and L it\textsf{'}s
defined for each L separately so I could I could have written like
this as well it\textsf{'}s not Scala notation necessarily but it will do for
now just for clarity to remind myself where these methods come from
now sequence is defined for L so here I could write it like this to
remind myself that the sequence is defined separately for each ill
but it is generic and F so I could write it like that so sequence
has F as a type parameter but L as type constructor or or functor
for which it is defined in the Thai class now so let\textsf{'}s see if we can
verify this law F map is identity for the constant factor so this
is identity and so this is just F here sequence is sorry F map of
F of F here is identity function f map does not transform anything
because there is nothing to transform we only have this Z cannot transform
so this is just at the energy function you can cross it out and then
we have the law that sequence equals pure at sequence equals pure
by definition so the law is satisfied let\textsf{'}s consider now the composition
law which is the composition of sequence of F and the lifted sequence
of G so now I'm using this notation with more sense in here I didn't
have to say it\textsf{'}s sequence Hamilton if I could just say sequence but
here it is important now to say which for which type parameter is
because the factor is a type parameter in sequence function so we
have the sequence applied with respect to applicative factor f and
this with respect to G and F G is the factor which is the composition
of F and G we just {[}Music{]} denote it like that for gravity so
this is now sequence is defined as pure F map is {[}Music{]} defined
in with respect to F so this is f naught F naught F naught L F map
l was identity but F map F is not and we actually don't know what
it does because it f is an arbitrary let\textsf{'}s look at your family so
you need to keep it symbolic so we have f dot P R which is this sequence
and then because the Scala operation and then responds to my composition
symbol and then we have F map F of G period because this is that the
pure in G and that will be equal to FG pure but actually what is the
definition of pure for composition of functions that\textsf{'}s exactly this
it\textsf{'}s a pure of F followed by lifted pyrimidine so it\textsf{'}s the definition
of what pure use for the function f G and so therefore the law holds
you now consider the identity function you know the identity function
is like this or sometimes can it can be denoted by it with a capital
I so this is identity factor I believe the cat\textsf{'}s library does this
maybe not maybe Scalzi so how do we define a sequence for it well
this is just an identity function because it doesn't do anything because
we we have f of a it goes to F of a because it is just wrapping it
doesn't do anything so this is identity function we could even write
this sequence function differently like this in order to underscore
that factor that\textsf{'}s just identity function now let\textsf{'}s check the laws
the identity law which is this now if F map is the identity function
sequences identity function f map L is just this F map L is the identity
function of its argument and because of this identity function and
so this is equal to f dot P so this is f dot pure followed by identity
and that should be equal to F dot penis clearly so composition is
that we have a sequence followed by F map of sequence now sequence
of anything is identity so it\textsf{'}s just composing one identity and F
map of another identity but we know that lifting F map of identity
is again identity so this is just about composing one identity or
another that\textsf{'}s always going to be identity so that is also identity
by definition and so this law holds let\textsf{'}s move on to the next construction
which is the product for any traversable factors G and H their product
is reversible now I'm going to introduce my own typeclass which reversible
just for convenience but actually the cat\textsf{'}s library has a traverse
that class but I just want to show that this is easy and I prefer
to define seek and they prefer to define Traverse in their tag class
so that\textsf{'}s why I have my own tie plus but basically this is very simple
that concept has just one method and this method is Elif a to FLI
traversal defined that method now this red is a problem with IntelliJ
and install a plug-in cannot resolve the types it compares this entire
code comparison works also we have a convenience method to get the
Traverse into instance this drove in the syntax to use seek as a as
a method rather than user so let me then show how I can define an
instance of track of travel this traversable type was for this type
constructor which is a product of l1 and l2 given that both l1 and
l2 are traversable factors so first time to mend the functor instance
well that is standard it\textsf{'}s just a map this map that just a functor
instance for product and then i implement the traverse instance using
C so what am I supposed to do I have this a1 F a times l2 FA and I
need to give F of L 1 a times Delta a so first I can say please so
sorry first I can sequence each of these separately because l1 is
reversible and all to escape most of my assumptions for a sequence
separately both of them then I can zip the results and I get what
I need so that\textsf{'}s my idea about how to implement so I just say sequence
the first one sequence the second one and zip now I could also write
the code like this but then my IntelliJ doesn't understand where these
things are coming from but the code actually still compiles if I do
that so this is just much more your boss telling me where the sequence
comes from from which factor and in principle I expect a scholar to
be able to derive this automatically and it can this is much simpler
to write about intelligent isn't like it so let\textsf{'}s check the laws here
so there is the identity law which is that we can map the pure and
apply seek and that\textsf{'}s the same as the pure so how do we verify this
law well we substitute the code of F naught which is up here and we
apply both sides of this law to some arbitrary value of this type
and one until two so we do that looking at the laws actually we have
to start from LA and the law will give me this so the value on which
I apply both sides of the law must be of type le so that\textsf{'}s why as
eleiza is a pair of L 1 and L 2 and then that\textsf{'}s why I take some arbitrary
pair and i apply the law to it so the left-hand side first i apply
this and that\textsf{'}s going to be that there\textsf{'}s a half map this is L 1 this
is no true and then we apply the seek function to this now the code
of seek is I'm going to write it like this in a shorter form and the
result will be that I have this seek zip this so now we need to assume
obviously that the law already holds for a 1 and L 2 separately and
therefore for example we have this which is the formulation of the
law for L 1 and so we can substitute that in here and then we get
that now because of the identity law of applicative zipping of two
peers is a pure of the peer so this equals F pure of the peer and
that\textsf{'}s exactly the same as applying F pure which is on the right hand
side directly to the pier so we have verified the identity law let\textsf{'}s
verify the composition law this is how we could write it I have written
out sequence seek of L seek of the hell everywhere it\textsf{'}s just for clarity
so in my notation that means a sequence made up defined for the tag
constructor L applied with respect to the type parameter F so now
let\textsf{'}s apply again both sides of this law to some suitable value let\textsf{'}s
see what type that value should have which you have the type L F G
C as the initial point in the time diagram so let\textsf{'}s have an arbitrary
value of type L F G C so sort of C values a here so L F GA naar betray
value of that is a pair of l1 f g l2 f g and applying the code for
seek to this we get a 1 FG a sikh with respect to f zip f l2 mg is
equal respective so that applying just the first step according to
the type diagram that would lead us in here which is f of LG F of
L 1 G 2 G so now we apply F naught F of this to that now what is f
naught F of CBG know seeing G is with respect to L sorry with respect
to G over L so it acts on some value of this type and it will give
this according to the definition but we need to lift this function
was F naught s the difficulty here is that f is an arbitrary function
we don't know what ethnic F does we don't have code for it we need
to keep it symbolic so clearly we need to use a definition of zip
F G somehow to find out what F naught F will do so the definition
of zip of F G is this there\textsf{'}s some FG x and f gy with zip them in
the factor F G so so have G X is a type F of G of X G Y is of type
F of G the definition of the zip for the composition is one of the
constructions for applicative it\textsf{'}s it\textsf{'}s a zip in the F factor followed
by F map F of zip in the G factor is this what we have in our formula
not quite because we have not just something zip F something after
which we we do F method but we have these things transformed we seek
so let\textsf{'}s transform using some function so let\textsf{'}s use a natural G of
zip so that we can transform like this so we transform under the factor
f and so the result is the same as if we transformed FG x and f gy
and then zip so that\textsf{'}s a natural T you can transform first and then
zip or you can first zip and then transform that\textsf{'}s up to up to us
so finally if we use this formula we can see that F map f of seek
L of G is like this it\textsf{'}s the F map F of this so this is seek L of
genii because seek L of G gives us this kind of expression which is
similar to this P of GX being sequence of this zip G and Q of G Y
is sequence of this and then we apply this to those things and so
the result is that F map of seek L applied to that is it\textsf{'}s like that
it\textsf{'}s sick F map sick G and then seek F maps engine so that\textsf{'}s what
we find this really apply natural TMZ so that\textsf{'}s the left hand side
of the composition and the right hand side is that so these must be
equal now the definition of seek F G is the code of seek that is just
applied to this type parameter F G instead of that F so the same code
and it would have been the same code if we could have this equal to
that now these are six with respect to factor l1 so these are defined
for l1 and these are defined for how to now we assume that for l1
l2 the composition law already holds so that means for example this
so this is the composition law for a 1 and similarly for L 2 so therefore
this is the same as this and that is the same as now and so the composition
law holds for hell the third construction is disjunction so for any
traversable factors G and H the disjunction is again a traversable
factor so we have seen an example of implementing this in either and
in polynomial factors but this is a general construction so let\textsf{'}s
see how to again we assume that factors l1 and l2 reversible and we
defined well as the disjunction of l1 and l2 a standard factor instance
but is Junction so now let\textsf{'}s implement the founder instance traversable
instance for the disjunction so here\textsf{'}s how it will work we have this
so we apply sequence to each of these separately we get this disjunction
and finally we depending on which one we have we lift it into the
F of disjunction using just constructors of the disjunction that left
or right so that\textsf{'}s how it works first we do a sequence on each of
them so if we are on the left and we sequence it again this and then
we map it with left apply which means that we put a left on top of
this and left it\textsf{'}s just a wrapper in a disjunction so it doesn't change
the value and the same we do for the right now the redness here is
again due to limitations of IntelliJ it\textsf{'}s unlabeled transferred types
directly but this code runs it compiles and runs let\textsf{'}s check the laws
at the entity law we do the same thing as before we just substitute
the code of the flap in here and then we apply to some LA we'll know
that both sides of the identity law applied to an arbitrary element
of this type now since the code is symmetric with respect 1 or L 2
it\textsf{'}s sufficient to apply this to some left of l1e and check the law
for that so if we apply it to the left and we are in this case we
have any left applied to to this which is well we are actually in
this case and we're looking at F Nobel first so we are you want to
lift like this now you notice we can't really simplify this because
these are map and pyramids of factors that we don't know these are
a one factor and if I look at your factor so we don't really know
what these methods do we cannot substitute any more code so that\textsf{'}s
our symbolic computation right now we apply sync to this now we're
on the left so we apply seek to to this which is I'm going to denote
that a seek l1 to that and {[}Music{]} followed by map of left block
now nothing more to simplify unless we use the fact that l1 already
has this law and so this law for l1 looks like this therefore we can
simplify here substitute that we get F P were of l1 a map left apply
which by naturally T appear and we have pure of left over one a and
that\textsf{'}s exactly what the right hand side will do when applied to left
of l1 e so identity law holds let\textsf{'}s consider the composition law the
composition law needs to be applied to a value of this type and again
it\textsf{'}s sufficient to consider the left and applied to that so first
we apply the seek and that gives us in this now we apply f map F to
belt and we notice we have F map F which is this one and another if
map F so we can combine them and the result is that we have this two
which we apply map with respect to F of this kind of function so we
first do we left apply to that and then we further apply this function
so now this is not quite Scala code because I have this map underscore
F for convenience and clarity so that\textsf{'}s just keep in mind I'm not
actually using your Scala code here but I could adjust that the types
will be less clear so now what does this do now this c g you know
seek g has to act on the left and we need to substitute the code of
seek when acting on the left it gives us this expression my definition
of our seek up here so let\textsf{'}s substitute into this and we get then
this expression so now we can use the composition law that by assumption
already holds forever so then we have this one Perelman which we can
rewrite like this now this law is not quite giving us the expression
that we want which is this expression because we have this function
here but here we have another map so we're not we have another left
apply to map and also on the right hand side of the law would have
another left apply so in order to put another map inside the seek
we use a net reality of seek we add this on the right-hand side which
gives us this seek so seekers natural so we can apply seek to a transformed
argument or we can apply transformation to the result of seek and
so then if we define what lab FG is that is f map of F map and we
put that onto the left-hand summit so that we have CK one of this
map F of this map F both of that you know we can combine the map F
this one with this one and now we have a single map F with a bigger
function but that\textsf{'}s exactly what we had for the left hand side and
kill him before and so therefore the right hand side response is equal
to the left hand side his code so the code is equal and the same would
be if we replaced a woman so this shows what laws hold for injunction
now let\textsf{'}s consider there is construction for which is a recursive
function defined as la equals some s of a and la so as s is an arbitrary
by functor so this could be any type function of two type parameters
and this equation this is a type equation it defines a type la recursively
so examples of this are lists and trees different factors s can encode
very easily different kinds of wastes and trees now this also describes
infinite lists and infinite data structures and we have seen an example
where the infinite data structure does not actually work you can implement
the function without the recursive calls never stopped so that\textsf{'}s the
problem the laws will appear to hold in our proof but actually it
will not work in practice so I will comment on this when I use the
recursive assumption in the proof so let\textsf{'}s see how it works so I introduced
the biofilter as a type parameter up front so that I don't worry about
it so much it\textsf{'}s easier in Scala to do this now I cannot just introduce
a type like I did here because reclusive type illnesses are not allowed
in school so I need to introduce your class so once that actually
makes the code a little clunkier because now and you need a name for
the data value inside and I need to wrap and unwrap but that\textsf{'}s a small
inconvenience so the class contains a value of type s of a end of
the same 11 so this is a type function that I'm considering as a parameter
and now I can implement a factor instance for this now this obviously
is going to be recursive so in order to map this l 8lb i map a to
be here and I need to map this la to L be recursively by using the
same map so I need to do this under yes so I need to map at the same
time a to b and le to l be under the type constructor s so that\textsf{'}s
by map so I'm mapping the two type parameters of s at the same time
and then my map is a method that takes two functions with two type
parameters so it goes a to C and in this case we have the by factor
with type parameters a and L of a and so it takes two functions a
to C and elevate to D gives us FCD so this is combining map with respect
to the first router and map with respect to the second parameter in
a single call which is completely equivalent to doing first the first
type parameter with a map and then the second parameter with another
map but it\textsf{'}s just easier to do it with one method probably two and
this is the recursive call to the same function so this is how we
do it the functor instance now let\textsf{'}s look at the Traverse instance
how would that work so L of a is the same as recursively defined as
s of a and elevate so now we need to transform this into this if we
want to implement seek how do we do that well we can obviously seek
here recursively so the second argument of s is a recursive instance
of the same type and so we can assume that for that recursive intense
instance we already have the implementation so that would be the recursive
call to the same function so that would be transferring into Fla so
now we have so we just do a by map where the first one doesn't do
anything it\textsf{'}s identity and the second function is a sequence recursively
calling the same and now we do by Traverse or by sequence actually
by sequence which is as I indicated before it\textsf{'}s transforming s fafb
into F si be pulling out the F at the same time out of both type parameters
to the outside so we use that and transform s of F a Fla until F of
s of any LA so this is exactly what we need so in other words seek
is just a composition of by map and by seek but on that one factor
now I can use cats library it has by functors by traversable and it
has this voice sequence so I'm using white functor and white reverse
which is canceling berry typeclasses just I could have defined them
just as easily as the trove that class and the redness again is a
some problem with types although I indicated all types explicitly
but still it doesn't like it so therefore the Seekers just by map
with identity and seek which is a recursive call followed by the by
sequence call on the by factor so that\textsf{'}s exactly what we plan to do
first we buy map so that we seek under yes with respect to the second
type parameter and then we do by sequence yes so that works and let\textsf{'}s
check the laws so the identity will need to substitute the code now
for for clarity I still write f9l here and so on but it is easier
now to distinguish sequence with respect to L and s because s is a
by function so it has basic and violent and L has seek so I'm going
to write seek L I could have done it like this but I'm not going to
write it for quality for brevity so okay substitute the code of f9
that gives us this code which we defined here the lab instance the
defunct our instance for help which maps using by map under F and
the recursive on the second so let\textsf{'}s write it down so this is a by
map of this function and the recursive FF L as a second argument of
why map so the same function as we're defining here is here so now
we need to apply secrets the result is this followed by by map actually
followed by bicycles remind myself what is my definition of sequence
for you yes just take the S out of the case class which I'm not going
to write here because it\textsf{'}s just wrapping and unwrapping we want to
pretend that this case class is just a type so first it\textsf{'}s apply map
with identity and seek and then it\textsf{'}s a buy seek on the result I'm
going to ignore this because this is just wrapping right so seek is
by map followed by by sick this is sick L so we have this expression
now we can combine the by maps just like we can combine maps because
they're by factors it\textsf{'}s just first we have a map in the first type
driver so we combine these two which gives us this and then we have
a map in the second type planner which combines these two and that
gives us that so f map L and then seek is the same law that we're
trying to prove not F not LPR and then seek is f pure so it\textsf{'}s a recursive
invocation of the same law and we're trying to prove in the second
function of Y map so we can use an inductive assumption that we already
proved that by recursion and therefore we just substitute into this
expression we substitute F P R inside of this because that\textsf{'}s a look
so then we get by map of pure pure by seek and the identity law for
s means that s by map pure pure basic is the same as s is just pure
and so that\textsf{'}s wasting holds for Escalades by traversable function
by function and so this becomes the right-hand side of the identity
law and that\textsf{'}s the proof of the identity law so now let me mark about
using the recursive calls and it is that inductive assumptions corresponds
to recursive calls in code so in mathematics the inductive assumption
is that on the previous step things were already proved in code it
means that we are going to call the same function recursively assuming
that it will return the correct results then our step also returns
the correct results but that assumes that the recursive call terminates
and actually returns the results and we have seen an example that
on the infinite list it does not return it has anything to do and
so that\textsf{'}s where it is going to break so actually the inductive assumption
can be used as long as all these functions actually terminate their
evaluation and return their results and if so then it\textsf{'}s off it\textsf{'}s all
fine but so at this level we will not see any problem with infinite
lists we are using the inductive assumption and everything appears
to be correct however we have not established that these functions
will actually return at all and for the infinite list they don't and
so for some factors defined using this construction using some by
factor s some factors will have infinite loops and others won't so
that is a separate thing we need to establish in order to check that
they are actually reversible this is usually not a problem for factors
because this map is going to be cold maybe later and this is a lazy
call and so there is no infinite loop but we have seen we do a bye
map and then we do a buy sequence and so that call will evaluate everything
and that will break for the infinite list now all I'm saying is that
this proof of laws and the same will apply to the proof of the competition
law that I'm going to talk about shortly this proof is only as good
as the fact that all the functions return and if you have an infinite
loop in one of the implementations then as proof oh well great because
the inductive assumption cannot be used because the use of inductive
assumptions is is translated into recursive calls in the code and
if those goals never terminate them you can't call them so I'm not
going to present an analysis here as to what possible functors s are
admissible because I don't know how to do that analysis in general
so and that\textsf{'}s a much more difficult topic of recursive types what
are the reclusive types for which certain methods would terminate
and that\textsf{'}s for another chapter so from now we will assume that we
check separately that all the methods will terminate and if that is
so then this proof is correct we are allowed to use inductive asymmetries
let\textsf{'}s look at the composition law so this is the composition law and
the law is an equation both sides of which need to be applied to an
arbitrary value of this type let\textsf{'}s check so L F G C so use a instead
of C I'll probably check rank it in the slides that it is a and not
C I'm using a in the code consistency so s of F G a L F G so that\textsf{'}s
lfg so let\textsf{'}s apply both sides of this law to some value of this type
and we get first sequence which is by map of identity and sequence
followed by by seek and then we do a map of seek which is a function
that takes this and does a by map and and by seek but now with respect
to G so I'm just writing it out what the code is for seek and this
should be equal to by map for the advicing but with respect to FG
so that\textsf{'}s our law let\textsf{'}s check that the schools and we certainly assume
that it already holds for for the s by function which means that this
equation holds and by seek map is by seek now this is just a law this
twisted composition of seek can seek is seek so how do we use this
in order to prove this now clearly to use this we need a value of
this type which is not the same as what we have here now this is s
FG x FG y so is very homogeneous it must be the same functors FG and
we don't have the same factors here we have F G and L F G so this
L is outside if L were inside of all of this here then we could just
say this is why this is eleve is why a is X and then we are of this
shape so that means we need to permute L over there so in order to
do this we need to sequence this with respect to F G and then L will
get inside but we need to use the sequence inside the type constructor
s which requires a buy map now this is going to be a buy map with
respect to the second type parameter of S which is the recursive invocation
and so that by map and sequence is a recursive call to sequence therefore
we can use the law of composition for that as if it\textsf{'}s already proved
and we will do that so here\textsf{'}s what we do so in this law that we are
going to use now we're substituting this value s FJ XY which is defined
like a by map or a sequence with respect to L and respect to F and
then G and then that\textsf{'}s of the right shape with X equal to a and y
equal to L of a so that\textsf{'}s what we wanted and the result is this expression
which is a by map followed by by seek followed by map now we can certainly
use natural reality and exchange map sorry no we we here we're having
basic F map by CG that\textsf{'}s what we're using here it\textsf{'}s a basic FG that\textsf{'}s
that\textsf{'}s the law we're just writing out the law we're substituting s
F G X Y into both sides of the law services here and this is here
so this holds this is an equation that holds so now by inductive assumption
a composition law for sig L already holds when we use it here and
so therefore we can rewrite this right-hand side like this this is
just sick L of F G now this right-hand side is the same as the equation
that we need to check which I noted as a start and just marked labeled
that equation by start for convenience so the right-hand side of that
equation is now the same as this therefore it\textsf{'}s also equal to that
so it remains to show that that the left-hand side is equal to the
left hand side of star let\textsf{'}s write it down so like this yeah it\textsf{'}s
really easier to compare so by map now we have this instead of that
and we have a by seek and we have a map of by seek instead of this
now if we look carefully we have a map f of by seek and here we have
a map f of something that\textsf{'}s followed by by 6 so that can be pulled
out with a map F and omitted so this is a training lab F by sake of
G in both of these so as we just a minute forget this so now the problem
that they are not equal because we have a different order of by seek
and map so have a by cyghfer by map here and here we have a map followed
by by seek when we need to interchange a map and the natural transformation
that\textsf{'}s naturality law and it\textsf{'}s the same way for by factories just
by secant by map can be interchanged let\textsf{'}s write down the naturality
law for clarity it looks like this so we have a basic follower by
map f of by milk of something and then we have a by map of map f of
that function let other functions we have two functions because by
map takes two functions both of them need to be mapped and then we
have bicycling so we have interchange the order by seek and map f
so if we use this law in this equation then we get that the first
line is equal to again we can interchange by c combining up and we
get by C cadine so now we have by map followed by by map followed
by by seek by map can be combined and then we have this by map fold
but by seek and this is exactly the left hand side which is this emitted
by C kanji sorry this one so this concludes the proof of the composition
law and therefore we find that this construction was valid now the
question is which by factors are by traversal and the answer is the
same all polynomial by factors are by traversable now we see without
recursion these constructions 1 2 3 our constructions that allow us
to get any polynomial factor with arbitrary types in it doesn't have
to be monoid like in applicative can be any any constant type any
polynomial function these constructions cover all these cases now
construction to even has two different implementations we can zip
in one order or in the opposite order it still will work and therefore
all polynomial functions at reversible now exactly the same constructions
worked for by traversable constant and a and B are quite reversible
in the same way as constant an identity function traversable and products
and disjunctions are by traversable and so-called by traversable polynomial
by factors can be used in this construction I'm not going to go through
proofs for these by functor constructions they're pretty much the
same as the proofs I went through except you have more type parameters
to worry about what implementations are exactly the same and so the
conclusion is that all polynomial factors including recursive polynomial
factors as long as the Traverse and seek methods return in finite
time and they not go into infinite loops as long as that is the case
all polynomial factors at reversible all polynomial by factors also
traversable and you can go on you can define a by factor by recursion
using a try factor in the same way you can say sa B is equal to some
T of a be s a B or T X Y Z is a try factor and as long as that try
factor is tried reversible exactly the same proof would show that
the by functor is by traversable so you can have recursion at any
level as long as it\textsf{'}s a finite level of recursion obviously at some
point you would have some n factor that is not recursive recursive
and or maybe several of them and then as long as that\textsf{'}s polynomial
is going to be 2n traversable and then you go back and have your n
minus-1 traversable recursive thumpers and so on and so it\textsf{'}s clear
that all polynomial factors with arbitrary recursion so it\textsf{'}s it\textsf{'}s
a tree for example whose branches can be themselves lists or you know
you can have a list of branches or anything like that all of that
is traversable so that\textsf{'}s a major result of this consideration so now
let me consider foldable functors now we we know the fold operation
in a standard library it\textsf{'}s a full left turns out that the fold operation
is a consequence of having a traverse operation and later we'll also
see that the scan operation the scan left is also a consequence of
traverse operation so how do we derive the fold from jurors the main
idea is that we should take a specific applicative factor which is
a constant factor fa equals a constant Z or is a Z is a monoid type
and so the zip operation on this factor is just a monoid operation
which I will denote like this and we have seen in Chapter 8 that these
are applicative now the type signatures are much simplified now in
type signature of triggers becomes this and this method is called
fold map well the does is that it takes a container of type of values
of type II and takes a function that map\textsf{'}s each value to a mono it
and then it traverses the container and combines all these monoid
values into one big monoid value and that\textsf{'}s for example aggregation
and any kind of aggregation Sun Oven integer list or some pointer
so the general method with an arbitrary monoid which is generic in
the monoid is called faulkner the type signature of seek becomes simple
like this so and that is just to concatenate all monoid values in
the container into one using the monoidal operation so this is called
M concat now there aren't any more laws because the laws are about
combining oh the identity law will be automatically satisfied and
the composition law is trivial because you can't compose these things
because there is it takes two more nodes compose them you get the
second one the first one is just just going and so all these laws
are trivially satisfied there aren't any laws anymore for this foldable
operation for the fold map there aren't any laws and all traversable
functors have these operations now nevertheless it\textsf{'}s convenient to
define the foldable typeclass that has these operations like fold
map and M concat and fold left because you could traverse the containers
in different order and that would be different implementations even
though there are no laws and it\textsf{'}s basically a consequence of traversable
so there aren't any factors that are foldable but not reversible so
all polynomial factors and only the polynomial functions are both
foldable and traverses and and so nevertheless it\textsf{'}s convenient to
define this typeclass because you can have different implementations
of fold for different order of the traverses and finally let me show
how to define the fold method and that\textsf{'}s a trick where you take this
as your manual type now this is a mono eight where B is an arbitrary
fixed type and these functions are just concatenated by composition
and the identity value is the identity function now if you substitute
into the following up into this substitute B to B you get this type
signature it\textsf{'}s a curried function with one two three arguments so
if you just rearrange these arguments you see this is exactly the
type signature fold left it has your container it has initial value
it has the update function and it returns the final accumulated value
so monoid is gone we have a specific one right here the arbitrament
already that is is gone until we have an arbitrary type B so if we
put B before this argument and it\textsf{'}s not obvious where the money went
but it is just a consequence of a signature of Traverse where we first
put them on oil in it and then you specify to this node and so for
this reason every foldable has a foolproof method for map and M concat
and every traversable also has them and so now we know which factors
are foldable and reversible so let\textsf{'}s ask our contra functors useful
or profanity was useful in the same capacity because you could imagine
that you want a conscious factor that you Traverse with respect to
a function or my servers and my answer to that is after several analysis
is that they're not very useful here\textsf{'}s why let\textsf{'}s take a contra fantasy
now let\textsf{'}s try to do a seek on it so that would be this kind of time
signature now if I have a CFA I can control map with the pure method
the pure F which is a to FA I can come up with that to get CFA to
see a so I can get down and then I can put that into the F using pure
F so I get F CA and in fact there is no other way of doing this generically
now it seems that wall control factors are automatically reversible
and I could even say F doesn't have to be a factor it\textsf{'}s not using
map well if I'm just using pure life and so it\textsf{'}s it can be arbitrary
Pro functor as long as it has pure it\textsf{'}s actually applicative proof
functor or even just appointed with a pure so it can be just just
like that so is that useful well in my view it\textsf{'}s not useful because
I completely ignore all effects of EV I'm using pure of F so whatever
F ahead here I'm ignoring its effects I'm never going to have any
effect full value here either it\textsf{'}s going to be always a pure so it\textsf{'}s
not very useful and in the other direction you can't do it f seiei
to CFA because you can't extract out of F necessary now if you have
a proof factor see anything things are not workable here\textsf{'}s why consider
this simple example a simplest growth factor just neither a functor
nor control factor because it has a in both covariant and contravariant
positions now we need a sequence function of this type signature but
that\textsf{'}s it\textsf{'}s it\textsf{'}s impossible we cannot get an F of a to a unless we
somehow get an a to a first but we can't there\textsf{'}s no way to extract
a to a out of here so I can't get an A you can't get an F of a and
so there\textsf{'}s no way to do this so so the only way to implement this
type signature is to return pure F of identity here but that ignores
its argument and functions that ignore its argument will not respect
the identity law they will not preserve information and finally let\textsf{'}s
try to try to traverse profile too with respect to proof hunters and
we find again that the only way to do that is to ignore all effects
here are two examples so consider this contra factor and this contra
factor then now if it\textsf{'}s applicative it means that s is a mono it and
so this type the only way to implement is to return an empty value
of F on the right ignoring all of Fame and the second example is you
take this contra function and this function and again you can show
that the only way to implement sequence of this type signature is
to always return empty option and again that would ignore its argument
and so it\textsf{'}s most likely you're not going to be useful because we ignore
all the effects will never return anything that\textsf{'}s not empty and so
on so we we are able to implement these type signatures let\textsf{'}s see
I have I have this test code here for a check that we can actually
implement only one type signature and that it returns an empty option
in both of these counter examples so take a look at this in more detail
if you feel like but I'm of the opinion right now that since all of
these examples show that I have to ignore all effects I have to return
a value that ignores all the input data or ignores all the effects
in the factor or per factor f that\textsf{'}s probably not a very useful implementation
but there isn't any other also note that the laws of jurors suppose
the notes actually say that effects of f cannot be ignored they don't
actually say that so you can traverse each element once and that\textsf{'}s
guaranteed but then you can just ignore the effects maybe well for
some functors that\textsf{'}s possible as we have just seen so the conclusion
is that traversable contractors and pro founders aren't really so
great and aren't very useful let\textsf{'}s look at some examples where we
use traversable factors the first example is we can convert any traversable
factor data structure to a list to do that we will actually have a
trick and that will all define a list as a monoid not as a type constructor
is a constant type but as a monoid so it will be a constant factor
so to express this i define the type constructor z which has a type
parameter b but it\textsf{'}s equal to list of c or c is a fixed type parameter
up here so it\textsf{'}s not depending on B so this will be a constant factor
which are defined like this so it doesn't change anything the type
parameter is not used and it\textsf{'}s applicative as well as a constant for
a factor that is a monoid so I define the evocative instance using
the rajab and this is an eel and a monoid composition or concatenation
of lists and just for fun I want to define the monoid composition
in the opposite order {[}Music{]} so having defined this and now have
Z as a applicative factor and I can use chillers which is this function
that I defined for convenience on the reversible value and so now
I Traverse F of a which is this L of C probably better cold now this
L of C is of type unknown type constructor hell so we Traverse it
with a function that takes a value of type C and returns a list containing
a single element C the result of traversing is that for each element
from the data container L will have a list of a single element and
then we will concatenate all these lists because that\textsf{'}s the effect
located factor Z and we need to combine all these effects for it all
values in the container health and the combining is done using the
monoid composition and so then the result will be a list of all elements
from the container l so let\textsf{'}s see how that works we define L like
this we have seen the implementation already and now we define a value
of this type the value is triple 1 2 3 and then we did recall to listing
it and the result is a list of 3 2 1 because we have defined the opposite
order here so if we define a straightforward order we would have a
list of 1 2 3 otherwise we have a list of 3 2 1 so in this way we
see that any traversable factor is a data container that can be converted
to lists and the order of list elements depends on the order of traversal
which is specified by the traversal hibiclens instance the second
example is to aggregate data from a tree by using a fold so in fact
since fold is an operation of traverse where the negative factor is
a constant factor here we have just used fold map essentially we don't
need traversable here we it\textsf{'}s efficient to have foldable we could
have defined this function by folding with with a list but let\textsf{'}s continue
using traverse just so that we understand these are very much related
to each other so here\textsf{'}s a definition of a simple tree tag it has a
leaf of type A and it has a branch with two trees now we define the
function instance in the usual way and then we define the traversal
industry which is defined also in the usual way this is a recursive
construction construction for and so I'm just writing it out this
follows from the recursive construction if we have a leaf and we just
put the leaf constructor under F and if we have a branch and we run
sequence on both branches this is a recursive call to the sequence
function that we're defining right now and then we zip the results
and put the result in to pull under the branch here are some example
values of this type so this is a tree of integers and this is a tree
of strings and I driven this picture to visualize this tree t2 has
a more complicated structure T 1 is just 1 2 \& 3 it\textsf{'}s easy to see
what T 2 is not so easy to see anymore so let\textsf{'}s fold over T 1 so T
1 is 1 2 3 in order to fold let\textsf{'}s define a 1:08 for integers which
will be a multiplication and what\textsf{'}s the fold map with a function that
squares each element so the result of fold map will be the product
of squares of all and of all the elements stored in the tree which
will be 1 times 4 times 9 which is 36 the third example is to decorate
a tree with order labels obtained from a depth-first traversal so
for instance this tree if we Traverse a depth first then first we'll
reach a and we'll reach B then C and then D so then the order of traversal
is 1 2 3 4 and so we expect these labels to be attached so how do
we attach labels like this way it\textsf{'}s not just a map map would we can
certainly attach constant or a function of each of the label each
of the element but that\textsf{'}s not we won't want to attach a value that
depends on the traversal so that\textsf{'}s the Traverse and since it depends
on the traversal we need to compute it as we go and so that\textsf{'}s I mean
we need to maintain state as we go as we traverse the tree and maintain
in the state can be done with the state monad so we use that statement
others duplicative function and that expresses the effect that we
are traversing with so to visualize this traversal I could say that
we are first so let\textsf{'}s Traverse t2 since we have it on the screen so
we first Traverse t2 by visiting a and we have some function that
takes a and returns a monad value or a purgative function value in
general now we done this being and we combine the effects so we have
four different effects in these four different a purgative functionaries
we need to combine them using zip and so the result is an F of tuple
of a B here and then it\textsf{'}s going to be this nested tuple so it\textsf{'}s f
of domestic tuple and then we have to recreate the three structure
out of that nested tuple by remembering where these a B and C and
D were in it in the tree and we have to reconstruct a tree under F
so this and I'm not going to write much more detail with basic analysis
so it\textsf{'}s to be reconstructed as the value of this type after after
zip and you would just have an F of some tuple and you could have
converted it to a list first but then you completely lose the structure
of the tree so the traverse operation must somehow keep track of the
structure of the continue and you recreate it after zipping so it
zips the effects so the effects are combined linearly in the traversal
order using the zip and then after the effects are combined effects
are mono it always of some kind so they are combined linearly there
is no tree structure only effects but there is tree structure on the
values ABCD or whatever values you get after you transform them with
the function f and so those values have to be arranged so that could
be of type B those values have to be arranged into the same tree structure
as before so now we are going to use a state monad as the positive
effect and statement others applicative and you will use if statement
values together what happens is that you update the state each state
mullet value could update the state in some way and so this is a linear
sequence of effects that you need to combine by updating the state
first using this updater than using this updater and so on in the
linear order of traversal and that\textsf{'}s what well we will use now we'll
have a monadic value make label so we define first of all a state
monad I'm using the Katz library with integer state so for convenience
I'll call this s right now just in this test so I'll I define a make
label value which is a monadic value that updates the state and returns
also this value that is the previous value before I'm dating I could
return the next value it doesn't really matter and that\textsf{'}s going to
be melodic value that we are going to produce for each of the elements
of the tree now we actually need to represent this as an applicative
factor so let me just quickly convert to my zip typeclass which is
my my version of applicative of course the cats library already has
a positive instance for this but it\textsf{'}s easier to use my my typeclasses
because they're very small they don't have a lot of methods and it\textsf{'}s
easy to see what they do so we need to define the wrapped unit which
is just a pure of state factor and we need to define zip which is
defined in a regular way for anyone that this is just combined the
two effects in this order we could have changed the order here so
this is this is a freedom but let\textsf{'}s just keep this order and this
function will allow us to produce her with zip tie plus instance from
a statement well you know what\textsf{'}s right a tree traversal so a tree
traversal we already implemented up here so I just copied this code
in I don't think I needed to duplicate nice code we can just remove
this code so how do we use that so we have t2 which was defined before
that we Traverse using the effect type yes so this is the apocatip
factor that we have now a wizard instance for we have this was a probe
s and the result type is going to be a tuple of string int so we're
adding an integer label to a string type of the tree which which t2
has 32 think and the type is tray of string so the function with which
which reverse takes a label and prefer this make label operation which
is a monadic value and then we map now this magnetic value returns
the integer and then we map that integer into a tuple with the previous
tree leaf is really called leaf action to be more clear so we map
a tree leaf to a tuple of leaf and label and we get out of the Monad
so now this is a value of type s of tree of string int so this is
a result of traversal and now we need to run it so we run the statement
out on the initial value one and we extract the value out of it so
this is that cats library API two is to extract values from statement
and the result is as expected so it\textsf{'}s a labeled with 1v we still in
soon so in this way we can decorate the tree with labels that are
computed as we go on in a traversal the next example is templum and
skin functions scan map and scan left so we have seen that fold can
be implemented if you have it reversible and scan also follows from
properties of traversable so scan map is something I made up it\textsf{'}s
a method analogous to fold map where you have a monoid and you map
your data to a monoid type and then the result is that unlike fold
map it\textsf{'}s not just one monoid value that aggregates everything you
don't just accumulate the final value you keep all the intermediate
accumulated values and put them back into your container so this is
like the scan function so how does it work we use a statement again
it\textsf{'}s state is the MU node so initially it will be empty and then we
will accumulate a monoid state so this is how we accumulate we get
the previous state and we set the new accumulated state and then we
get again in new states so that we have it on hand and then we Traverse
with a function that is f which is this given F followed by accumulate
and so the result is s of Z which is a state that accumulates Z and
also gives it as a value so after traversing we get a value of type
s of L of Z\textsf{'}s and then we run it on an initially empty value and that\textsf{'}s
what we return extracting out of the state monad after we have run
it so here\textsf{'}s the test let\textsf{'}s use a string as monoid so we imported
a cat\textsf{'}s instance just for standard string 108 and let\textsf{'}s do a scan
map on t2 with no transformation so then we're just going to accumulate
the string as we go so we get a a b a b c a b c d so that\textsf{'}s the result
of scan map and we can also implement the standard scan left function
which is very similar except we don't accumulate a monoid now we just
accumulate a value of some arbitrary type z and we do the same thing
as we did before very similar code and here\textsf{'}s a test so we let\textsf{'}s accumulate
the length of strings in the in the tree so then we'll get one two
three four because all all strings have length 1 and the last example
is the traverse a rigid tree or non monadic tree as I don't think
vegetation well it is a widely used word to describe this kind of
tree but basically imagine a tree that must have all branches fall
so it cannot be unbalanced then it must be full it\textsf{'}s a binary tree
that must have one two four eight 16 32 and so on elements and no
other number of elements can be in the tree that\textsf{'}s very rigid in its
shape this recursive type equation he finds it and you can easily
see why so it\textsf{'}s either a or it\textsf{'}s the same T of any times a so then
it\textsf{'}s either a times a or the same tree of a times a times a times
a so then it\textsf{'}s for A\textsf{'}s or eight A\textsf{'}s or sixteen is consumed so it\textsf{'}s
it\textsf{'}s at this infinite disjunction with 1 a or 2 is 4 4 8 or 8 8 and
so on so that\textsf{'}s that\textsf{'}s why this equation works not we don't have a
construction where I did not write down the construction that can
generalize this tree this kind of construction could be generalized
like this where you have a recursive equation with some buy furniture
yes and the second type 300 - 2 s is not a la as it was previously
not just LA but it\textsf{'}s L of some RA where R is another function so in
this case it will be a pair but in general it could be some other
function and that function must be both applicative and reversible
and if so then I believe if this construction will work as well so
this is a more general recursive construction let\textsf{'}s see the code for
this so the definition of the tree type is this it has a single element
in the branch which is a tree of the pair parameter instead so we
substitute to the type parameter which is kind of non-trivial so we
can define the functor instance which is a bit of work because now
we need to map this branch of type 3 of a a we need to map it recursively
with a function that map\textsf{'}s a a to be me but we don't have that function
we have a to be so we need to construct a function that map\textsf{'}s a a
to be B out of F and the traversal is again a depth-first traversal
where the leaf is handled the same way as before but the branch is
handled differently because we actually have this kind of type and
we need to get this so we need to do we need to use zip for F in order
to pull F out of the tuple and so we first zip the FS together like
this and this needs to be done under the B so it\textsf{'}s a b fa fa map of
the zip so that\textsf{'}s going to have B of F of praa and then we sequence
that so we get F of of PNA and then we wrap it into the branch so
that\textsf{'}s how traversal works and certainly we tested you know this is
how we would create this data structure so it\textsf{'}s always branch branch
branch branch and finally leaf with a very large may be nested tuple
so this is why this tree is so rigid he does not have cannot have
unbalanced shape so that works we use in mono omoide instance for
integer and do a fold map with identity to get the sum of all these
numbers so these examples indicate that we can do a lot of things
with Traverse but there are there are actually things we cannot do
using each others because the Traverse separation we remember it is
an operation that needs to be generic in the effect against which
we Traverse in this applicative factor so two things that cannot be
implemented the traversal which you find interesting but this is so
or the breadth-first traversal for a tree so imagine a tree of this
shape and you want to do breadth-first traversal so you traverse first
at this level you get one and you traverse at this level you get to
an elite reverse at this level now that can cannot be expressed as
a traversal with some effect because as I indicated here to visualize
a traversal we need to imagine that we are computing a large linear
sequence of effects into one combining a large linear sequence of
effects into one big effect and now in order to express this you would
have to have to know for example here that this two must be two and
not one now this effect cannot come from just recursive traversal
you cannot just take a usual recursive depth first traversal and run
some more nuts and clever Monat with it and get this other chamber
so now we certainly probably can implement it in some other way but
I don't see how you can simply do a recursive function on a tree that
traverses it against an applicative factor and get this effect and
especially when you call recursively on this subtree you need to know
how many nodes will be in some other levels in some other subtree
so it\textsf{'}s not clear how you could possibly get that information from
any kind of monitor or {[}Music{]} applicative function now certainly
you can implement breadth-first traversal but what I'm saying is that
you cannot take some special monad and some recursive function calls
and implement it so easily I hope you can still implement it in some
way that is not too bad as a traversal certainly the standard way
of implementing breadth-first traversal to use a queue or you in queue
each node and then you in queue it to children and then you DQ and
as you dqu in queue more so that can be certainly done in a statement
that that computation but the problem is you need the traversal which
recreates the original structure so if if all you want is twofold
then you can implement fold in a breadth-first while using a queue
like that but what you need is to recreate the original tree structure
and that\textsf{'}s very cursive structure that has a depth-first logic to
it and so that\textsf{'}s the difficulty and certainly you can do it with a
lot of extra work it\textsf{'}s not just a couple of lines of code like it
was for depth-first traversal so at this point I don't have code for
this maybe this can be done in some clever way but I haven't found
it and the second thing that I don't think you can do is to label
depth of the tree using a traversal so again it\textsf{'}s a similar problem
that effects so these aren't going to be effects and obviously you
need some kind of state to hold this one and then update it to two
and two three but effects are concatenated linearly and so it cannot
be that this three is the same as this you cannot avoid concatenating
this effect with this one when you do a traversal because of this
linear nature of combining effects and you cannot avoid combining
effects during traversal and so there\textsf{'}s no way to skip incrementing
the counter when you do choose ourselves so I don't think you can
easily express this kind of labeling and this is trivial to do ad
hoc as a function on a tree but what you want is to have a generic
traversal which is generic in the factor f and just uses the zip and
combines the effects and that\textsf{'}s why traversals are not so flexible
you cannot avoid incrementing your counter you cannot have logic that
says oh here I don't increment because they're at the same level can't
have that logic you must combine the only factors together so I keep
talking about being generic in the function f so let me talk about
it in a more rigorous manner so what we're looking for is to be generic
in the sense that the code for traverse as well as the code for the
sequence in it should not depend on a specific function f it should
only use the methods P R and zip or maybe map as well from F and {[}Music{]}
recall recall how we dealt with a similar problem when we discovered
founders so a factor is a data container that is generic in type of
the data that it holds and the map function or F map is a function
that cannot use specific type information about A or B it must manipulate
this data blindly with no change you take a container you take each
element a very replacement may be and put it back at the same place
but you don't know what types those are and so we expressed the law
of natural T for various functions by saying that F map should come
should commute with those functions so you can transform first or
you can transform later and that\textsf{'}s what it means that it\textsf{'}s generic
in the type of data so similarly generic in the function f means that
if we map a functor F to some other function G then a traverse with
respect to F will be mapped to the Traverse with respect to G so we
need this mapping somehow so this cannot be this can be formulated
mathematically I don't want to spend too much time on this but I just
want to mention this is an important mathematical development perhaps
but this is the first time where we have seen this kind of thing and
we don't have I don't have good notation right now for this natural
transformations between applicative functors and its need to be considered
so here\textsf{'}s the mathematical formulation we consider - Lickety functors
F and G and we consider a natural transformation between them such
that it Maps period of F into pure of G and it maps zip of F into
the project and then we demand that it also should map the Traverse
of F into Traverse of G so if we do that well all we do is we just
say we have a traverse with respect to F let\textsf{'}s apply it and then map
F to G in the result because the result of Traverse is F of something
F of some L of a or L of B so we can map that into G of not using
the natural transformation or we can first map f to G and then apply
Traverse and that should be the same and so this is a natural T with
respect to the functor as a type parameter so it\textsf{'}s a higher-order
type parameter itself otherwise it\textsf{'}s quite similar to not two naturality
and it\textsf{'}s just more complicated because it\textsf{'}s not just any natural transformation
natural transformation by itself has laws already and in addition
to being a natural transformation it needs to preserve the implicative
properties of f so that pure is mapped to pure zip is mapped to zip
and and the laws of course must hold for both implicated factors so
if you look at this notation which is slightly made shorter on purpose
by omitting various type parameters it really looks like lifting from
a function f to G into function of traversing you have to traverse
G but F and G are not types they're type constructors so and this
is not a function this is a natural transformation which is more complicated
object so when we used category theory to describe such liftings we
would usually say this is a morphism in one category and this is a
morphism in another category but a morphism as we define it was always
between two types now we don't have types we have type constructors
so we need basically a morphism between type constructors so that\textsf{'}s
more more precisely between implicative factors so a morphism between
applicative functions that\textsf{'}s what we need to lift into a morphism
of traversable faculties and so that requires a more general definition
of category than what I have given so far we're in them in the definition
I've given so far morphisms were just twisted function types but now
we need morphisms between type constructors or even more restricted
ones morphisms between applicative functions with extra properties
so category theory prescribes this in a general definition of category
can be given which I'm not going to give right now because I don't
see in the use for it other than to indicate the property of natural
T as being similar to previous properties I encourage you to look
at category theory at some point but I will only talk about what I
see it as being useful and so right now it is useful to think about
this as a lifting and the wall is of course need to hold such as identity
and Composition so if we map F to F then we don't change anything
that should be lifted into identity and composition F to G to H mapped
it to composition of those mappings and so on so those are the rigorous
four forms for the naturality war with respect to applicative function
I'm not writing it down in detail because just as naturality with
respect to ordinary type parameter this is this morality with respectively
funter is going to be satisfied automatically by any code that nearly
uses peer and zip and why is that it\textsf{'}s because when we perform a natural
transformation that maps FPR to GD P R and F zip to gzip the code
doesn't actually change at all the code has f as a type parameter
and it just uses pure and zip in the code to cause them as we have
been doing when we implemented the traverse instances for example
so let\textsf{'}s see here is for example we are using zip the zip is from
the function f but F is a type parameter so if we now apply a natural
transformation from F to G for some other negative function G this
code doesn't change at all it still uses the zip and we have the condition
that there\textsf{'}s natural transformation that maps F to G it Maps zip to
zip and peer to peer and and map to map of course that\textsf{'}s the natural
T of natural transformation so our code won't change and our code
therefore is invariant under this natural transformation and so it\textsf{'}s
automatically going to satisfy naturality with respect to the type
parameter F and it also will automatically satisfy not reality with
respect to time over a and that\textsf{'}s why we never need to check these
laws explicitly or even spend so much time writing them explicitly
because they are they are useful sometimes when your reason about
code and prove some other law was then learned are useful but for
coding for implementing you always have natural T automatically so
this concludes chapter 9 and here are some exercises for you to implement
let me just comment and this exercise so I don't want you to prove
laws by hand for this factor T it\textsf{'}s cumbersome but instead use constructions
so we have proved general constructions so if you express T as a recursive
equation using some by factor s and you show that the wife on dress
is by traversable again using constructions of course then you don't
need to and to do the proof yourself if the constructions already
proved once you have found them and similarly for for this construction
you don't need to prove anything by hand this exercise is a little
different but you can try to prove this extra construction that I
mentioned this one but it\textsf{'}s very similar to other constructions and
so it\textsf{'}s not necessarily part of this exercise this concludes chapter
9 
\end{comment}

