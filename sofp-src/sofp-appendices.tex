
\appendix

\chapter{Notations\label{chap:Appendix-Notations}}

\global\long\def\gunderline#1{\mathunderline{greenunder}{#1}}%
\global\long\def\bef{\forwardcompose}%
\global\long\def\bbnum#1{\custombb{#1}}%
Certain notations and terms were chosen in this book differently from
what the functional programming community currently uses. The proposed
notation is well adapted to reasoning about types and code, and especially
for designing data types and proving the laws of various typeclasses.

\section{Summary of notations for types and code}
\begin{description}
\item [{$F^{A}$}] type constructor $F$ with type argument $A$. In Scala,
\lstinline!F[A]!
\item [{$x^{:A}$}] value $x$ has type $A$; in Scala, \lstinline!x:A!
\item [{$\bbnum 1,\,1$}] the unit type and its value; in Scala, \lstinline!Unit!
and \lstinline!()!
\item [{$\bbnum 0$}] the void type. In Scala, \lstinline!Nothing!
\item [{$A+B$}] a disjunctive type. In Scala, this type is \lstinline!Either[A, B]! 
\item [{$x^{:A}+\bbnum 0^{:B}$}] a value of a disjunctive type $A+B$.
In Scala, \lstinline!Left(x)!
\item [{$A\times B$}] a product (tuple) type. In Scala, this type is \lstinline!(A,B)!
\item [{$a^{:A}\times b^{:B}$}] value of a tuple type $A\times B$. In
Scala, \lstinline!(a, b)!
\item [{$A\rightarrow B$}] the function type, mapping from $A$ to $B$
\item [{$x^{:A}\rightarrow f$}] a nameless function (as a value). In Scala,
\lstinline!{ x:A => f }!
\item [{$\text{id}$}] the identity function; in Scala, \lstinline!identity[A]!
\item [{$\triangleq$}] \textsf{``}is defined to be\textsf{''} or \textsf{``}is equal by definition\textsf{''}
\item [{$\overset{!}{=}$}] \textsf{``}must be equal according to what we know\textsf{''}
\item [{$\overset{?}{=}$}] \textsf{``}we ask \textemdash{} is it equal? \textemdash{}
because we still need to prove that\textsf{''}
\item [{$\square$}] \textsf{``}this proof or this derivation is finished\textsf{''}
\item [{$\cong$}] for types, a natural isomorphism between types; for
values, \textsf{``}equivalent\textsf{''} values according to an already established
isomorphism
\item [{$A^{:F^{B}}$}] special type annotation, used for defining unfunctors
(GADTs)
\item [{$\wedge$}] logical conjunction; $\alpha\wedge\beta$ means \textsf{``}both
$\alpha$ and $\beta$ are true\textsf{''}
\item [{$\vee$}] logical disjunction; $\alpha\vee\beta$ means \textsf{``}either
$\alpha$ or $\beta$ or both are true\textsf{''}
\item [{$\Rightarrow$}] logical implication; $\alpha\Rightarrow\beta$
means \textsf{``}if $\alpha$ is true then $\beta$ is true\textsf{''}
\item [{$\text{fmap}_{F}$}] the standard method \lstinline!fmap! of a
functor $F$. In Scala, \lstinline!Functor[F].fmap!
\item [{$\text{flm}_{F},\text{ftn}_{F},\text{pu}_{F}$}] the standard methods
\lstinline!flatMap!, \lstinline!flatten!, and \lstinline!pure!
of a monad $F$
\item [{$F^{\bullet}$}] the type constructor $F$ understood as a type-level
function. In Scala, \lstinline!F[_]! 
\item [{$F^{\bullet}\leadsto G^{\bullet}$}] or $F\leadsto G$ a natural
transformation between functors $F$ and $G$. In Scala, \lstinline!F ~> G!
\item [{$\forall A.P^{A}$}] a universally quantified type expression.
In Scala 3, \lstinline![A] => P[A]!
\item [{$\exists A.P^{A}$}] an existentially quantified type expression.
In Scala, \lstinline!{ type A; val x: P[A] }! 
\item [{$\bef$}] the forward composition of functions: $f\bef g$ is $x\rightarrow g(f(x))$.
In Scala, \lstinline!f andThen g!
\item [{$\circ$}] the backward composition of functions: $f\circ g$ is
$x\rightarrow f(g(x))$. In Scala, \lstinline!f compose g!
\item [{$\circ$}] the backward composition of type constructors: $F\circ G$
is $F^{G^{\bullet}}$. In Scala, \lstinline!F[G[A]]! 
\item [{$\triangleright$}] use a value as the argument of a function:
$x\triangleright f$ is $f(x)$. In Scala, \lstinline!x.pipe(f)!
\item [{$f^{\uparrow G}$}] a function $f$ raised to a functor $G$; same
as $\text{fmap}_{G}f$
\item [{$f^{\uparrow G\uparrow H}$}] a function raised first to $G$ and
then to $H$. In Scala, \lstinline!h.map(_.map(f))! 
\item [{$f^{\downarrow H}$}] a function $f$ raised to a contrafunctor 
\item [{$\diamond_{M}$}] the Kleisli product operation for the monad $M$
\item [{$L\varangle M$}] or equivalently $T_{L}^{M}$ \textemdash{} the
monad transformer of a monad $L$ applied to a monad $M$
\item [{$\oplus$}] the binary operation of a monoid. In Scala, \lstinline!x |+| y!
\item [{$\Delta$}] the \textsf{``}diagonal\textsf{''} function of type $\forall A.\,A\rightarrow A\times A$
\item [{$\pi_{1},\pi_{2},...$}] the projections from a tuple to its first,
second, ..., parts
\item [{$\boxtimes$}] pair product of functions, $(f\boxtimes g)(a\times b)\triangleq f(a)\times g(b)$
\item [{$\left[a,b,c\right]$}] an ordered sequence of values. In Scala,
\lstinline!Seq(a, b, c)!
\item [{$\begin{array}{||cc|}
x\rightarrow x & \bbnum 0\\
\bbnum 0 & a\rightarrow a\times a
\end{array}$}] ~a function that works with disjunctive types (a \textsf{``}\index{disjunctive functions}disjunctive
function\textsf{''})
\end{description}

\section{Detailed explanations}

$F^{A}$ means a type constructor $F$ with a type parameter $A$.
In Scala, this is \lstinline!F[A]!. Type constructors with multiple
type parameters are denoted by $F^{A,B,C}$. Nested type parameters
such as Scala\textsf{'}s \lstinline!F[G[A]]! are denoted by $F^{G^{A}}$,
meaning $F^{(G^{A})}$.

$x^{:A}$ means a value $x$ that has type $A$; this is a \textbf{\index{type annotation}type
annotation}. In Scala, a type annotation is \lstinline!x:A!. The
colon symbol, $:$, in the superscript shows that $A$ is not a type
argument (as it would be in a type constructor, $F^{A}$). The notation
$x:A$ can be used as well, but $x^{:A}$ is easier to read when $x$
is inside a larger code expression. 

$\bbnum 1$ means the unit type\index{unit type}, and $1$ means
the value of the unit type. In Scala, the unit type is \lstinline!Unit!,
and its value is \lstinline!()!. Example of using the unit type is
$\bbnum 1+A$, which corresponds to \lstinline!Option[A]! in Scala.

$\bbnum 0$ means the void\index{void type} type (the type with no
values). In Scala, this is the type \lstinline!Nothing!. Example
of using the void type is to denote the empty part of a disjunction.
For example, in the disjunction $\bbnum 1+A$ the non-empty part is
$\bbnum 0+A$, which in Scala corresponds to \lstinline!Some[A]!.
The empty part $\bbnum 1+\bbnum 0$ corresponds to \lstinline!None!.
Similarly, $A+\bbnum 0$ denotes the left part of the type $A+B$
(in Scala, \lstinline!Left[A]!), while $\bbnum 0+B$ denotes its
right part (in Scala, \lstinline!Right[A]!). Values of disjunctive
types are denoted similarly. For instance, $x^{:A}+\bbnum 0^{:B}$
denotes a value of the left part of the type $A+B$; in Scala, this
value is written as \lstinline!Left[A,B](x)!.

$A+B$ means the disjunctive type made from types $A$ and $B$ (or,
a disjunction of $A$ and $B$). In Scala, this is the type \texttt{}\lstinline!Either[A, B]!.

$x^{:A}+\bbnum 0^{:B}$ denotes a value of a disjunctive type $A+B$,
where $x$ is the value of type $A$, which is the chosen case, and
$\bbnum 0$ stands for other possible cases. For example, $x^{:A}+\bbnum 0^{B}$
is \lstinline!Left[A,B](x)! in Scala. Type annotations $^{:A}$ and
$^{:B}$ may be omitted if the types are unambiguous from the context.

$A\times B$ means the product type made from types $A$ and $B$.
In Scala, this is the tuple type \lstinline!(A,B)!.

$a^{:A}\times b^{:B}$ means a value of a tuple type $A\times B$;
in Scala, this is the tuple value \lstinline!(a, b)!. Type annotations
$^{:A}$ and $^{:B}$ may be omitted if the types are unambiguous
from the context.

$A\rightarrow B$ means a function type from $A$ to $B$. In Scala,
this is the function type \lstinline!A => B!.

$x^{:A}\rightarrow y$ means a nameless function with argument $x$
of type $A$ and function body $y$. (Usually, the body $y$ will
be an expression that uses $x$. In Scala, this is \lstinline!{ x: A => y }!.
Type annotation $^{:A}$ may be omitted if the type is unambiguous
from the context.

$\text{id}$ means the identity function. The type of its argument
should be either specified as $\text{id}^{A}$ or $\text{id}^{:A\rightarrow A}$,
or else should be unambiguous from the context. In Scala,  \lstinline!identity[A]!
corresponds to $\text{id}^{A}$.

$\triangleq$ means \textsf{``}equal by definition\textsf{''}. A definition of a function
$f$ is written as $f\triangleq(x^{:\text{Int}}\rightarrow x+10)$;
in Scala, this is \lstinline!val f = { x: Int => x + 10 }!. A definition
of a type constructor $F$ is written as $F^{A}\triangleq\bbnum 1+A$;
in Scala, this is \lstinline!type F[A] = Option[A]!.

$\cong$ for types means an equivalence (an isomorphism) of types.
For example, $A+A\times B\cong A\times\left(\bbnum 1+B\right)$. The
same symbol $\cong$ for \emph{values} means \textsf{``}equivalent\textsf{''} according
to an equivalence relation that needs to be established in the text.
For example, if we have established an equivalence that allows nested
tuples to be reordered whenever needed, we can write $\left(a\times b\right)\times c\cong a\times\left(b\times c\right)$,
meaning that these values are mapped to each other by the established
isomorphism functions. 

$A^{:F^{B}}$ in type expressions means that the type constructor
$F^{\bullet}$ assigns the type $F^{B}$ to the type expression $A$.
This notation is used for defining unfunctors (GADTs). For example,
the Scala code

\begin{lstlisting}
sealed trait F[A]
case class F1() extends F[Int]
case class F2[A](a: A) extends F[(A, String)]
\end{lstlisting}
defines an unfunctor\index{unfunctor}, which is denoted by $F^{A}\triangleq\bbnum 1^{:F^{\text{Int}}}+A^{:F^{A\times\text{String}}}$.

$\wedge$ (conjunction), $\vee$ (disjunction), and $\Rightarrow$
(implication) are used in formulas of Boolean as well as constructive
logic in Chapter~\ref{chap:5-Curry-Howard}, e.g., $\alpha\wedge\beta$,
where Greek letters stand for logical propositions.

$\text{fmap}_{F}$ means the standard method $\text{fmap}$ of the
\lstinline!Functor! typeclass, implemented for the functor $F$.
In Scala, this may be written as \texttt{}\lstinline!Functor[F].fmap!.
Since each functor $F$ has its own specific implementation of $\text{fmap}_{F}$,
the subscript \textsf{``}$F$\textsf{''} is not a type parameter of $\text{fmap}_{F}$.
The method $\text{fmap}_{F}$ actually has \emph{two} type parameters,
which can be written out as $\text{fmap}_{F}^{A,B}$. Then the type
signature of $\text{fmap}$ is written in full as $\text{fmap}_{F}^{A,B}:\left(A\rightarrow B\right)\rightarrow F^{A}\rightarrow F^{B}$.
For clarity, we may sometimes write out the type parameters $A,B$
in the expression $\text{fmap}_{F}^{A,B}$, but in most cases these
type parameters $A$, $B$ can be omitted without loss of clarity.

$\text{pu}_{F}$ denotes a monad $F$\textsf{'}s method \lstinline!pure!.
This function has type signature $A\rightarrow F^{A}$ that contains
a type parameter $A$. In the code notation, the type parameter may
be either omitted or denoted as $\text{pu}_{F}^{A}$. If we are using
the \lstinline!pure! method with a complicated type, e.g., $\bbnum 1+P^{A}$,
instead of the type parameter $A$, we might want to write this type
parameter for clarity and write $\text{pu}_{F}^{\bbnum 1+P^{A}}$.
The type signature of that function is then 
\[
\text{pu}_{F}^{1+P^{A}}:\bbnum 1+P^{A}\rightarrow F^{\bbnum 1+P^{A}}\quad.
\]
But in most cases we will not need to write out the type parameters.

$\text{flm}_{F}$ denotes the curried version of a monad $F$\textsf{'}s method
\lstinline!flatMap!. The type signature of $\text{flm}_{F}$ is $\text{flm}_{F}:(A\rightarrow F^{B})\rightarrow F^{A}\rightarrow F^{B}$.
Note that Scala\textsf{'}s standard \lstinline!flatMap! type signature is
not curried. The curried method $\text{flm}_{F}$ is easier to use
in calculations involving the monad\textsf{'}s laws.

$\text{ftn}_{F}$ denotes a monad $F$\textsf{'}s method \lstinline!flatten!
with the type signature $\text{ftn}_{F}:F^{F^{A}}\rightarrow F^{A}$.

$F^{\bullet}$ means the type constructor $F$ understood as a type-level
function, \textemdash{} that is, with a type parameter unspecified.
In Scala, this is \lstinline!F[_]!. The bullet symbol, $\bullet$,
is used as a placeholder for the missing type parameter. When no type
parameter is needed, $F$ means the same as $F^{\bullet}$. (For example,
\textsf{``}a functor $F$\textsf{''} and \textsf{``}a functor $F^{\bullet}$\textsf{''} mean the same
thing.) However, it is useful for clarity to be able to indicate the
place where the type parameter would appear. For instance, functor
composition is denoted as $F^{G^{\bullet}}$; in Scala 2, this is
\texttt{}\lstinline!Lambda[X => F[G[X]]]! when using the \textsf{``}kind
projector\textsf{''}\index{kind@\textsf{``}kind projector\textsf{''} plugin} plugin.\footnote{\texttt{\href{https://github.com/typelevel/kind-projector}{https://github.com/typelevel/kind-projector}}}
When the first type parameter of a bifunctor $P^{A,B}$ is fixed to
$Z$, the resulting functor may be denoted by $P^{Z,\bullet}$. As
another example, $T_{L}^{M,\bullet}$ denotes a monad transformer
for the base monad $L$ and the foreign monad $M$. The foreign monad
$M$ is a type parameter in $T_{L}^{M,\bullet}$, and so is the missing
type parameter denoted by the placeholder symbol $\bullet$. (However,
the base monad $L$ is not a type parameter in $T_{L}^{M,\bullet}$
because the construction of the monad transformer depends sensitively
on the internal details of $L$.)

$F^{\bullet}\leadsto G^{\bullet}$ or $F\leadsto G$ means a natural
transformation between two functors $F$ and $G$. In some Scala libraries,
this is denoted by \lstinline!F ~> G!.

$\forall A.P^{A}$ is a universally quantified type expression, in
which $A$ is a bound type parameter.

$\exists A.P^{A}$ is an existentially quantified type expression,
in which $A$ is a bound type parameter.

$\bef$ means the forward composition\index{forward composition}
of functions: $f\bef g$ (reads \textsf{``}$f$ before $g$\textsf{''}) is the function
defined as $x\rightarrow g(f(x))$.

$\circ$ means the backward composition\index{backward composition}
of functions: $f\circ g$ (reads \textsf{``}$f$ after $g$\textsf{''}) is the function
defined as $x\rightarrow f(g(x))$.

$\circ$ with type constructors means their (backward) composition,
for example $F\circ G$ denotes the type constructor $F^{G^{\bullet}}$.
In Scala, this is \lstinline!F[G[A]]!. 

$x\triangleright f$ (the \textbf{pipe notation})\index{pipe notation}\index{\$@$\triangleright$-notation!see \textsf{``}pipe notation\textsf{''}}
is a different syntax for $f(x)$. The value $x$ is passed as the
argument to the function $f$. In Scala, the expression $x\triangleright f$
is written as \lstinline!x.pipe(f)! or, if \lstinline!f! is a method,
\lstinline!x.f!. This syntax is used with many standard methods such
as \lstinline!size! or \lstinline!toSeq!. Because the argument $x$
is to the left of the function $f$ in this notation, forward compositions
of functions such as $x\triangleright f\triangleright g$ are naturally
grouped to the left as it is done in Scala code, for example \lstinline!x.toSeq.sorted!.
The operation $\triangleright$ (pronounced \textsf{``}pipe\textsf{''}) groups weaker
than the forward composition ($\bef$), and so we have $x\triangleright f\bef g=x\triangleright f\triangleright g$
in this notation. Reasoning about code in the pipe notation uses the
identities
\begin{align*}
x\triangleright f=f(x)\quad,\quad\quad & \left(x\triangleright f\right)\triangleright g=x\triangleright f\triangleright g\quad,\\
x\triangleright f\bef g=x\triangleright\left(f\bef g\right)\quad,\quad\quad & x\triangleright f\triangleright g=x\triangleright f\bef g\quad.
\end{align*}
The pipe symbol groups stronger than the function arrow, so $x\rightarrow y\triangleright f=x\rightarrow(y\triangleright f)$.
Here are some examples of reasoning with functions in the pipe notation:
\begin{align*}
 & \left(a\rightarrow a\triangleright f\right)=\left(a\rightarrow f(a)\right)=f\quad,\\
 & f\triangleright\left(y\rightarrow a\triangleright y\right)=a\triangleright f=f(a)\quad,\\
 & f(y(x))=x\triangleright y\triangleright f\neq x\triangleright\left(y\triangleright f\right)=f(y)(x)\quad.
\end{align*}
The correspondence between the forward composition and the backward
composition:
\begin{align*}
 & f\bef g=g\circ f\quad,\\
 & x\triangleright(f\bef g)=x\triangleright f\bef g=x\triangleright f\triangleright g=g(f(x))=(g\circ f)(x)\quad.
\end{align*}

$f^{\uparrow G}$ means a function $f$ lifted to a functor $G$.
For a function $f^{:A\rightarrow B}$, the application of $f^{\uparrow G}$
to a value $g^{:G^{A}}$ is written as $f^{\uparrow G}(g)$ or as
$g\triangleright f^{\uparrow G}$. In Scala, this is \lstinline!g.map(f)!.
Nested lifting (i.e., lifting to the functor composition $H\circ G$)
can be written as $f^{\uparrow G\uparrow H}$, which means $\left(f^{\uparrow G}\right)^{\uparrow H}$
and produces a function of type $H^{G^{A}}\rightarrow H^{G^{B}}$.
Applying a nested lifting to a value $h$ of type $H^{G^{A}}$ is
written as $f^{\uparrow G\uparrow H}h$ or $h\triangleright f^{\uparrow G\uparrow H}$.
In Scala, this is \lstinline!h.map(_.map(f))!. The functor composition
law is written as
\[
p^{\uparrow G}\bef q^{\uparrow G}=\left(p\bef q\right)^{\uparrow G}\quad.
\]
Note the similarity between Scala code \lstinline!x.map(p).map(q)!
and the notation $x\triangleright p^{\uparrow G}\triangleright q^{\uparrow G}$.

$f^{\downarrow H}$ means a function $f$ lifted to a contrafunctor
$H$. For a function $f^{:A\rightarrow B}$, the application of $f^{\downarrow H}$
to a value $h:H^{B}$ is written as $f^{\downarrow H}h$ or $h\triangleright f^{\downarrow H}$,
and yields a value of type $H^{A}$. In Scala, this is \lstinline!h.contramap(f)!.
Nested lifting is denoted as $f^{\downarrow H\uparrow G}\triangleq(f^{\downarrow H})^{\uparrow G}$.

$\diamond_{M}$ means the Kleisli product operation for a given monad
$M$. This is a binary operation working on two Kleisli functions
of types $A\rightarrow M^{B}$ and $B\rightarrow M^{C}$ and yields
a new function of type $A\rightarrow M^{C}$.

$\oplus$ means the binary operation of a monoid, for example $x\oplus y$.
The specific monoid type should be defined for this expression to
make sense. For example, in Scala the monoidal operation is usually
denoted by \lstinline!x |+| y!.

$\Delta$ means the standard \textsf{``}diagonal\textsf{''} function of type $\forall A.\,A\rightarrow A\times A$,
i.e., $\Delta\triangleq a^{:A}\rightarrow a\times a$ or, in Scala,
\begin{lstlisting}
def delta[A](a: A): (A, A) = (a, a)
\end{lstlisting}

$\pi_{1},\pi_{2},...$ denote the functions extracting the first,
second, ..., parts in a tuple. In Scala, $\pi_{1}$ is \lstinline!(_._1)!.

$\boxtimes$ means the pair product\index{pair product of functions}
of functions, where the result is a pair of the values of the two
functions: $(f\boxtimes g)(a\times b)=f(a)\times g(b)$. In Scala,
this operation can be defined by
\begin{lstlisting}
def pair_product[A,B,P,Q](f: A => P, g: B => Q): ((A, B)) => (P, Q) = {
  case (a, b) => (f(a), g(b))
}
\end{lstlisting}
The operations $\Delta$, $\pi_{i}$ (where $i=1,2,...$), and $\boxtimes$
allow us to express any function operating on tuples. Useful properties
for reasoning about code of such functions: 
\begin{align*}
{\color{greenunder}\text{identity law}:}\quad & \Delta\bef\pi_{i}=\text{id}\quad,\\
{\color{greenunder}\text{naturality law}:}\quad & f\bef\Delta=\Delta\bef(f\boxtimes f)\quad,\\
{\color{greenunder}\text{left projection law}:}\quad & (f\boxtimes g)\bef\pi_{1}=\pi_{1}\bef f\quad,\\
{\color{greenunder}\text{right projection law}:}\quad & (f\boxtimes g)\bef\pi_{2}=\pi_{2}\bef g\quad,\\
{\color{greenunder}\text{composition law}:}\quad & (f\boxtimes g)\bef(p\boxtimes q)=(f\bef p)\boxtimes(g\bef q)\quad,
\end{align*}
as well as the functor lifting laws for $\Delta$ and $\pi_{i}$:
\begin{align*}
 & f^{\uparrow F}\bef\Delta=\Delta\bef f^{\uparrow(F\times F)}=\Delta\bef(f^{\uparrow F}\boxtimes f^{\uparrow F})\quad,\\
 & (f^{\uparrow F}\boxtimes f^{\uparrow G})\bef\pi_{1}=f^{\uparrow(F\times G)}\bef\pi_{1}=\pi_{1}\bef f^{\uparrow F}\quad.
\end{align*}

$\left[a,b,c\right]$ means an ordered sequence of values, such as
a list or an array. In Scala, this can be \lstinline!List(a, b, c)!,
\lstinline!Vector(a, b, c)!, \lstinline!Array(a, b, c)!, or another
collection type.

$f^{:Z+A\rightarrow Z+A\times A}\triangleq\begin{array}{||cc|}
z\rightarrow z & \bbnum 0\\
\bbnum 0 & a\rightarrow a\times a
\end{array}\,$ is the \textbf{matrix notation}\index{matrix notation} for a function
whose input and/or output type is a disjunctive type (\index{disjunctive functions}a
\textbf{disjunctive function}). In Scala, the function $f$ is implemented
as
\begin{lstlisting}
def f[Z, A]: Either[Z, A] => Either[Z, (A, A)] = {
  case Left(z)   => Left(z) // Identity function on Z.
  case Right(a)  => Right((a, a)) // Delta on A.
}
\end{lstlisting}
The rows of the matrix indicate the different \lstinline!case!s in
the function\textsf{'}s code, corresponding to the different parts of the input
disjunctive type. If the input type is not disjunctive, there will
be only one row. The columns of the matrix indicate the parts of the
output disjunctive type. If the output type is not disjunctive, there
will be only one column.

A matrix may show all parts of the disjunctive types in separate \textsf{``}type
row\textsf{''} and \textsf{``}type column\textsf{''}:
\begin{equation}
f^{:Z+A\Rightarrow Z+A\times A}\triangleq\begin{array}{|c||cc|}
 & Z & A\times A\\
\hline Z & \text{id} & \bbnum 0\\
A & \bbnum 0 & a\rightarrow a\times a
\end{array}\quad.
\end{equation}
This notation clearly indicates the input and the output types of
the function and is useful at some stages of reasoning about the code.
The vertical double line separates input types from the function code.
In the code above, the \textsf{``}type column\textsf{''} shows the parts of the input
disjunctive type $Z+A$. The \textsf{``}type row\textsf{''} shows the parts of the
output disjunctive type $Z+A\times A$.

The matrix notation is adapted to \emph{forward} function composition
$f\bef g$. Assume that $A$ is a monoid type, and consider the composition
of the function $f$ shown above and the function $g$ defined as
\begin{lstlisting}
def g[Z, A: Monoid]: Either[Z, (A, A)] => A = {
  case Left(_)          => Monoid[A].empty
  case Right((a1, a2))  => a1 |+| a2
}
\end{lstlisting}
In the matrix notation, the function $g$ is written (with and without
types) as
\[
g\triangleq\begin{array}{|c||c|}
 & A\\
\hline Z & \_\rightarrow e^{:A}\\
A\times A & a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}
\end{array}\quad,\quad\quad g\triangleq\begin{array}{||c|}
\_\rightarrow e^{:A}\\
a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}
\end{array}\quad.
\]
The forward composition $f\bef g$ is computed by forward-composing
the matrix elements using the rules of the ordinary matrix multiplication,
omitting any terms containing $\bbnum 0$:
\begin{align*}
f\bef g & =\begin{array}{||cc|}
\text{id} & \bbnum 0\\
\bbnum 0 & a\rightarrow a\times a
\end{array}\,\bef\,\begin{array}{||c|}
\_\rightarrow e^{:A}\\
a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}
\end{array}\\
 & =\,\begin{array}{||c|}
\text{id}\bef(\_\rightarrow e^{:A})\\
\left(a\rightarrow a\times a\right)\bef\left(a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}\right)
\end{array}=\begin{array}{||c|}
\_\rightarrow e^{:A}\\
a\rightarrow a\oplus a
\end{array}\quad.
\end{align*}
Applying a function to a disjunctive value such as $x^{:Z+A}$ is
computed by writing $x$ as a row vector:
\[
x=z^{:Z}+\bbnum 0^{:A}=\begin{array}{|cc|}
z^{:Z} & \bbnum 0\end{array}\quad,
\]
and the computation $x\triangleright f\bef g$ again follows the rules
of matrix multiplication:
\[
x\triangleright f\bef g=\begin{array}{|cc|}
z^{:Z} & \bbnum 0\end{array}\,\triangleright\,\begin{array}{||c|}
\_\rightarrow e^{:A}\\
a\rightarrow a\oplus a
\end{array}=z\triangleright(\_\rightarrow e)=e\quad.
\]
Since the standard rules of matrix multiplication are associative,
the properties of the $\triangleright$-notation such as $x\triangleright(f\bef g)=(x\triangleright f)\triangleright g$
are guaranteed to hold.

To use the matrix notation with \emph{backward} compositions ($f\circ g$),
all function matrices need to be transposed. (A standard identity
of matrix calculus is that the transposition reverses the order of
composition, $\left(AB\right)^{T}=B^{T}A^{T}$.) The argument types
will then appear in the top row and the result types in the left column;
the double line is above the matrix since that is where the function
inputs come from. The above calculations are then rewritten as
\begin{align*}
g\circ f & =\begin{array}{|c|cc|}
 & Z & A\times A\\
\hline\hline A & \_\rightarrow e^{:A} & a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}
\end{array}\,\circ\,\begin{array}{|c|cc|}
 & Z & A\\
\hline\hline Z & \text{id} & \bbnum 0\\
A\times A & \bbnum 0 & a\rightarrow a\times a
\end{array}\\
 & =\,\begin{array}{|cc|}
\hline\hline \text{id}\bef(\_\rightarrow e^{:A}) & \left(a\rightarrow a\times a\right)\bef\left(a_{1}\times a_{2}\rightarrow a_{1}\oplus a_{2}\right)\end{array}=\begin{array}{|cc|}
\hline\hline \_\rightarrow e^{:A} & a\rightarrow a\oplus a\end{array}\quad.\\
(g\circ f)(x) & =\begin{array}{|cc|}
\hline\hline \_\rightarrow e^{:A} & a\rightarrow a\oplus a\end{array}\,\begin{array}{|c|}
z^{:Z}\\
\bbnum 0
\end{array}=(\_\rightarrow e^{:A})(z)=e\quad.
\end{align*}
The \emph{forward} composition seems to be easier to read and to reason
about in the matrix notation.

\chapter{Glossary of terms\label{chap:Appendix-Glossary-of-terms}}
\begin{description}
\item [{Code~notation}] \index{code notation}A mathematical notation
developed in this book for deriving properties of code in functional
programs. Variables have optional type annotations, such as $x^{:A}$
or $f^{:A\rightarrow B}$. Nameless functions are denoted by$x^{:A}\rightarrow f$,
products by $a\times b$, and values of a disjunctive type $A+B$
are written as $x^{:A}+\bbnum 0^{:B}$ or $\bbnum 0^{:A}+y^{:B}$.
Functions working with disjunctive types are denoted by matrices.
Lifting of functions to functors, such as $\text{fmap}_{L}(f)$, is
denoted by $f^{\uparrow L}$; function compositions are denoted by
$f\bef g$ (forward composition) and $f\circ g$ (backward composition);
and function applications by $f(x)$ or equivalently $x\triangleright f$.
See Appendix~\ref{chap:Appendix-Notations} for details.
\item [{\index{contrafunctor}Contrafunctor}] A type constructor having
the properties of a contravariant functor\index{contrafunctor} with
respect to a type parameter. Instead of \textsf{``}contravariant functor\textsf{''},
this book uses the shorter name \textsf{``}contrafunctor\textsf{''}.
\item [{Disjunctive~type}] \index{disjunctive type}A type representing
one of several distinct possibilities. In Scala, this is usually implemented
as a sealed trait extended by several case classes. The standard Scala
disjunction types are \lstinline!Option[A]! and \lstinline!Either[A, B]!.
Also known as \index{sum type!see \textsf{``}disjunctive type\textsf{''}}\textbf{sum
}type, \textbf{tagged union}\index{tagged union type!see \textsf{``}disjunctive type\textsf{''}}
type, \textbf{co-product}\index{co-product type!see \textsf{``}disjunctive type\textsf{''}}
type, and variant type (in Object Pascal and in OCaml). The shortest
name is \textsf{``}sum type,\textsf{''} but the English word \textsf{``}disjunctive\textsf{''} is
less ambiguous to the ear than \textsf{``}sum\textsf{''}.
\item [{Exponential-polynomial~type}] \index{exponential-polynomial type}A
type constructor built using products, disjunctions (sums or co-products),
and function types (\textsf{``}exponentials\textsf{''}), as well as type parameters
and fixed types. For example,  \lstinline!type F[A] = Either[(A,A), Int=>A]!
is an exponential-polynomial type constructor. Such type constructors
are always profunctors and can also be functors or contrafunctors.
\item [{\index{functor block}Functor~block}] A short syntax for composing
several \lstinline!map!, \lstinline!flatMap!, and \lstinline!filter!
operations applied to a functor-typed value. The type constructor
corresponding to that value must be a functor and is fixed throughout
the entire functor block. For example, the Scala code
\begin{lstlisting}
for { x <- List(1,2,3); y <- List(10, x); if y > 2 }
  yield 2 * y
\end{lstlisting}
is equivalent to the code
\begin{lstlisting}
List(1, 2, 3).flatMap(x => List(10, x))
  .filter(y => y > 1).map(y => 2 * y)
\end{lstlisting}
and computes the value \lstinline!List(20, 20, 20, 6)!. This is a
functor block that \textsf{``}raises\textsf{''} computations to the \lstinline!List!
functor. Similar syntax exists in a number of languages and is called
a \textbf{\textsf{``}for-comprehension\textsf{''}}\index{for-comprehensions (Python)@\texttt{for}-comprehensions (Python)}
or a \textsf{``}list comprehension\textsf{''} in Python, \textbf{\textsf{``}do-notation\textsf{''}}\index{do-notation (Haskell)@\texttt{do}-notation (Haskell)}
in Haskell, and \textbf{\textsf{``}computation expressions\textsf{''}}\index{computation expressions (F#)@computation expressions (F\#)}
in F\#. I use the name \textsf{``}functor block\textsf{''} in this book because it
is shorter and more descriptive. (The type constructor used in a functor
block needs to be at least a functor but does not have to be a monad.)
\item [{Kleisli~function}] \index{Kleisli!functions} A function with
type signature $A\rightarrow M^{B}$ for some fixed monad $M$. Also
called a Kleisli morphism\index{Kleisli!morphisms} (a morphism in
the Kleisli category corresponding to the monad $M$). The standard
monadic method $\text{pure}_{M}:A\rightarrow M^{A}$ has the type
signature of a Kleisli function. The Kleisli composition operation,
$\diamond_{M}$, is a binary operation that combines two Kleisli functions
(of types $A\rightarrow M^{B}$ and $B\rightarrow M^{C}$) into a
new Kleisli function (of type $A\rightarrow M^{C}$).
\item [{\index{method}Method}] This word is used in two ways: 1) A method$_{1}$
is a Scala function defined as a member of a typeclass. For example,
\lstinline!flatMap! is a method defined in the \lstinline!Monad!
typeclass. 2) A method$_{2}$ is a Scala function defined as a member
of a data type declared as a Java-compatible \lstinline!class! or
\lstinline!trait!. Trait methods$_{2}$ are necessary in Scala when
implementing functions whose arguments have type parameters (because
Scala function values defined via \lstinline!val! cannot have type
parameters). So, many typeclasses such as \lstinline!Functor! or
\lstinline!Monad!, whose methods$_{1}$ require type parameters,
will use Scala \lstinline!traits! with methods$_{2}$ for their implementation.
The same applies to type constructions with quantified types, such
as the Church encoding. 
\item [{Nameless~function}] \index{nameless function}An expression of
function type, representing a function. For example, \lstinline!(x: Int) => x * 2!.
Also known as function expression, function literal, anonymous function,\index{anonymous function!see \textsf{``}nameless functions\textsf{''}}
closure, \index{lambda-function!see \textsf{``}nameless function\textsf{''}}lambda-function,
lambda-expression, or simply a \textsf{``}lambda\textsf{''}.
\item [{Partial~type-to-value~function~(PTVF)}] A function with a type
parameter but defined only for a certain subset of types.\index{partial type-to-value function}
In Scala, PTVFs are implemented via a typeclass constraint:
\begin{lstlisting}
def double[T: Semigroup](t: T): T = implicitly[Semigroup[T]].combine(t, t)
\end{lstlisting}
This PTVF is defined only for types \lstinline!T! for which a \lstinline!Semigroup!
typeclass instance is available.
\item [{Polynomial~functor}] \index{polynomial functor}A type constructor
built using disjunctions (sums), products (tuples), type parameters
and fixed types. For example, in Scala, \lstinline!type F[A] = Either[(Int, A), A]!
is a polynomial functor with respect to the type parameter \lstinline!A!,
while \lstinline!Int! is a fixed type (not a type parameter). Polynomial
functors are also known as \textbf{algebraic data types}\index{algebraic data type}.
\item [{Product~type}] \index{product type}A type representing several
values given at once. In Scala, product types are the tuple types,
for example \lstinline!(Int, String)!, and case classes. Also known
as \index{tuples}\textbf{tuple} type, \textbf{struct} (in C and C++),
and \textbf{record}.
\item [{\index{profunctor}Profunctor}] A type constructor whose type parameter
occurs in both covariant and contravariant positions and satisfying
the appropriate laws; see Section~\ref{subsec:f-Profunctors}.
\item [{Type~notation}] \index{type notation}A mathematical notation
for type expressions developed in this book for easier reasoning about
types in functional programs. Disjunctive types are denoted by $+$,
product types by $\times$, and function types by $\rightarrow$.
The unit type is denoted by $\bbnum 1$, and the void type by $\bbnum 0$.
The function arrow $\rightarrow$ groups weaker than $+$, which in
turn groups weaker than $\times$. This means
\[
Z+A\rightarrow Z+A\times A\quad\text{is the same as}\quad\left(Z+A\right)\rightarrow\left(Z+\left(A\times A\right)\right)\quad.
\]
 Type parameters are denoted by superscripts. As an example, the Scala
definition\texttt{}
\begin{lstlisting}
type F[A] = Either[(A, A => Option[Int]), String => List[A]]
\end{lstlisting}
is written in the type notation as 
\[
F^{A}\triangleq A\times\left(A\rightarrow\bbnum 1+\text{Int}\right)+(\text{String}\rightarrow\text{List}^{A})\quad.
\]
\item [{\index{unfunctor}Unfunctor}] A type constructor that cannot possibly
be a functor, nor a contrafunctor, nor a profunctor. An example is
a type constructor with explicitly indexed type parameters, such as
$F^{A}\triangleq\left(A\times A\right)^{:F^{\text{Int}}}+\left(\text{Int}\times A\right)^{:F^{\bbnum 1}}$.
The Scala code for this type constructor is
\begin{lstlisting}
sealed trait F[A]
final case class F1[A](x: A, y: A)   extends F[Int]
final case class F2[A](s: Int, t: A) extends F[Unit]
\end{lstlisting}
This can be seen as a \index{GADT!see \textsf{``}unfunctor\textsf{''}}\textbf{GADT}
(generalized algebraic data type) that uses specific values of type
parameters instead of the type parameter $A$ in at least some of
its case classes.
\end{description}

\section{How the term \textsf{``}algebra\textsf{''} is used}

This book avoids using the terms \textsf{``}algebra\index{algebra}\textsf{''} or
\textsf{``}algebraic\index{algebraic}\textsf{''} because these terms are too ambiguous.
The functional programming community uses the word \textsf{``}algebra\textsf{''} in
at least \emph{four} incompatible ways.

\paragraph{Definition 0.}

In mathematics, an \textquotedblleft algebra\textquotedblright{} is
a vector space with multiplication and certain standard properties.
For example, we need $1*x=x$, the addition must be commutative, the
multiplication must be distributive over addition, etc. The set of
all $10\times10$ matrices with real coefficients is an \textquotedblleft algebra\textquotedblright{}
in this sense. These matrices form a $100$-dimensional vector space,
and they can be multiplied and added. This definition of \textsf{``}algebra\textsf{''}
is not used in functional programming.

\paragraph{Definition 1.}

For a given functor $F$, an \textsf{``}$F$-algebra\textsf{''}\index{$F$-algebra}
is a type $A$ and a function with type signature $F^{A}\rightarrow A$.
(This definition comes from category theory.) The name \textsf{``}$F$-algebra\textsf{''}
emphasizes the dependence on a chosen functor $F$. There is no direct
connection between this \textsf{``}algebra\textsf{''} and Definition~0, except when
the functor $F$ is defined by $F^{A}\triangleq A\times A$: a function
of type $A\times A\rightarrow A$ may then be interpreted as a \textsf{``}multiplication\textsf{''}
operation. However, $A$ is a type and not a vector space, and there
are no distributivity or commutativity laws. This book does not use
$F$-algebras because knowing their category-theoretical properties
does not help in practical programming.

\paragraph{Definition 2.}

Polynomial functors are often called \textquotedblleft algebraic data
types\textquotedblright . However, they are not \textquotedblleft algebraic\textquotedblright{}
in the sense of Definitions~0 or 1. For example, \lstinline!Either[Option[A], Int]!
is considered an \textquotedblleft algebraic data type\textquotedblright ,
denoted by $F^{A}\triangleq1+A+\text{Int}$ in the type notation.
However, values of the type $F^{A}$ do not support the addition and
multiplication operations required by Definition~0 of \textsf{``}algebra\textsf{''}.
The type $F^{A}$ may admit some binary or unary operations (e.g.,
that of a monoid), but these operations will likely not be commutative
or distributive. Also, there is not necessarily a function with type
$F^{A}\rightarrow A$, as required for Definition~1. Rather, the
word \textsf{``}algebra\textsf{''} refers to \textsf{``}school-level algebra\textsf{''} dealing with
polynomials, since these data types are built from sums and products
of types. If the data contains a function type, e.g., \lstinline!Option[Int => A]!,
the type is no longer polynomial. This book uses more precise terms
\textquotedblleft polynomial types\textquotedblright{} and \textquotedblleft exponential-polynomial
types\textquotedblright .

\paragraph{Definition 3.}

People talk about the \textquotedblleft algebra\textquotedblright{}
of properties of functions such as \lstinline!map! or \lstinline!flatMap!,
meaning that these functions satisfy certain \textsf{``}algebraic\textsf{''} laws
(e.g., the composition, naturality, or associativity laws). But these
laws do not make functions \lstinline!map! or \lstinline!flatMap!
into an algebra in the sense of Definition~0 or in the sense of Definition~1.
There is also no relation to the algebraic data types of Definition~2.
So, this is a different usage of the word \textsf{``}algebra\textsf{''}. However,
there is no general \textsf{``}algebra of laws\textsf{''} that we can use; every derivation
proceeds in a different way, specific to the laws being proved. In
mathematics, \textsf{``}algebraic\textsf{''} equations are distinguished from differential
or integral equations. In that sense, the laws used in functional
programming are \emph{always} algebraic: they are just equations with
compositions and applications of functions. So, it brings no particular
advantage to call laws \textsf{``}algebraic\textsf{''}. This book calls them \textsf{``}equations\textsf{''}
or just \textsf{``}laws\textsf{''}.

\paragraph{Definition 4.}

The Church encoding of a free monad (also known as the \textsf{``}tagless
final\index{tagless final} interpreter\textsf{''}) is the type $\forall E^{\bullet}.\,(S^{E^{\bullet}}\leadsto E^{\bullet})\rightarrow E^{A}$.
It uses a higher-order type constructor $S^{E^{\bullet}}$ parameterized
by a \emph{type constructor} parameter $E^{\bullet}$. In this context,
one calls an \textsf{``}algebra\textsf{''} a value of type $S^{E^{\bullet}}\leadsto E^{\bullet}$,
which is an $S$-algebra in the category of \textsf{``}type constructors\textsf{''}
(functors in a category without any non-identity morphisms). So, Definition~4
is related to Definition~1, with a specific choice of a functor.
However, knowing that $S^{E^{\bullet}}\leadsto E^{\bullet}$ is an
$S$-algebra in the category of type constructors does not provide
any help or additional insights for practical work with the Church
encoding of a free monad.

The higher-order type constructor $S$ is used to parameterize the
effects described by a Church-encoded free monad, so this book calls
it the \textsf{``}effect constructor\textsf{''}.

So, it appears that the current usage of the word \textsf{``}algebra\textsf{''} in
functional programming is both inconsistent and unhelpful to practitioners.
In this book, the word \textsf{``}algebra\textsf{''} always means a branch of mathematics,
as in \textsf{``}high-school algebra\textsf{''}. Instead of \textsf{``}algebra\textsf{''} as in Definitions~1
to~4, this book talks about \textsf{``}polynomial types\textsf{''} or \textsf{``}polynomial
functors\textsf{''} or \textsf{``}exponential-polynomial functors\textsf{''} etc.; \textsf{``}equations\textsf{''}
or \textsf{``}laws\textsf{''}; and an \textsf{``}effect constructor\textsf{''} $S$.

\chapter{Inferring code from types with the LJT algorithm\label{app:CH-correspondence-LJT-algorithm}}

The \href{http://apt13.unibe.ch/slides/Dyckhoff.pdf}{Gentzen-Vorobieff-Hudelmaier algorithm}
and its generalizations

See also the \href{https://github.com/Chymyst/curryhoward}{curryhoward}
project

\section{Slides}

In formal logic, this statement is written in the syntax
\[
X,Y,...,Z\vdash T
\]
and is called a \textbf{sequent}\index{sequent (in logic)} having
the premises $X$, $Y$, ..., $Z$ and the goal $T$.

A sequent in formal logic can be proved if proof task;

The elementary proof task is represented by a \textbf{sequent}

Notation: $A,B,C\vdash G$; the \textbf{premises} are $A,B,C$ and
the \textbf{goal} is G

Proofs are achieved via axioms and derivation rules

Axioms: such and such sequents are already true

Derivation rules: this sequent is true if such and such sequents are
true

To make connection with logic, represent code fragments as \textbf{sequents}

\textcolor{blue}{$A,B\vdash C$} represents an \emph{expression} of
type \texttt{\textcolor{blue}{\footnotesize{}C}} that uses \texttt{\textcolor{blue}{\footnotesize{}x:\ A}}
and \texttt{\textcolor{blue}{\footnotesize{}y:\ B}}{\footnotesize\par}

Examples in Scala:

\texttt{\textcolor{blue}{\footnotesize{}(x:\ Int).toString + \textquotedbl abc\textquotedbl}}
is an expression of type \texttt{\textcolor{blue}{\footnotesize{}String}}
that uses an \texttt{\textcolor{blue}{\footnotesize{}x:\ Int}} and
is represented by the sequent $\text{Int}\vdash\text{String}$

\texttt{\textcolor{blue}{\footnotesize{}(x:\ Int) $\rightarrow$
x.toString + \textquotedbl abc\textquotedbl}} is an expression
of type \texttt{\textcolor{blue}{\footnotesize{}Int $\rightarrow$
String}} and is represented by the sequent $\emptyset\vdash\text{Int}\rightarrow\text{String}$

Sequents only describe the \emph{types} of expressions and their parts

Translating language constructions into the logic II

What are the derivation rules for the logic of types?

Write all the constructions in FP languages as sequents

This will give all the derivation rules for the logic of types

Each type construction has an expression for creating it and an expression
for using it

Tuple type $A\times B$

Create: $A,B\vdash A\times B$ 

Use: $A\times B\vdash A$ and also $A\times B\vdash B$

Function type $A\rightarrow B$

Create: if we have $A\vdash B$ then we will have $\emptyset\vdash A\rightarrow B$ 

Use: $A\rightarrow B,A\vdash B$

Disjunction type $A+B$

Create: $A\vdash A+B$ and also $B\vdash A+B$

Use: $A+B,A\rightarrow C,B\rightarrow C\vdash C$

Unit type $1$

Create: $\emptyset\vdash1$

Translating language constructions into the logic III

Additional rules for the logic of types

In addition to constructions that use types, we have \textsf{``}trivial\textsf{''}
constructions:

a single, unmodified value of type $A$ is a valid expression of type
$A$

For any $A$ we have the sequent $A\vdash A$

if a value can be computed using some given data, it can also be computed
if given\,\emph{additional} data

If we have $A,...,C\vdash G$ then also $A,...,C,D\vdash G$ for any
$D$

For brevity, we denote by $\Gamma$ a sequence of arbitrary premises

the order in which data is given does not matter, we can still compute
all the same things given the same premises in different order

If we have $\Gamma,A,B\vdash G$ then we also have $\Gamma,B,A\vdash G$

Syntax conventions:

the implication operation associates \emph{to the right}

$A\rightarrow B\rightarrow C$ means $A\rightarrow\left(B\rightarrow C\right)$

precedence order: implication, disjunction, conjunction

$A+B\times C\rightarrow D$ means $\left(A+\left(B\times C\right)\right)\rightarrow D$

Quantifiers: implicitly, all our type variables are universally quantified

When we write $A\rightarrow B\rightarrow A$, we mean $\forall A:\forall B:A\rightarrow B\rightarrow A$

The logic of types I

Now we have all the axioms and the derivation rules of the logic of
types.

What theorems can we derive in this logic?

Example: $A\rightarrow B\rightarrow A$

Start with an axiom $A\vdash A$; add an unused extra premise $B$:
$A,B\vdash A$

Use the \textsf{``}create function\textsf{''} rule with $B$ and $A$, get $A\vdash B\rightarrow A$

Use the \textsf{``}create function\textsf{''} rule with $A$ and $B\rightarrow A$,
get the final sequent $\emptyset\vdash A\rightarrow B\rightarrow A$
showing that $A\rightarrow B\rightarrow A$ is a \textbf{theorem}
since it is derived from no premises

What code does this describe?

The axiom $A\vdash A$ represents the expression $x^{A}$ where $x$
is of type $A$

The unused premise $B$ corresponds to unused variable $y^{B}$ of
type $B$

The \textsf{``}create function\textsf{''} rule gives the function $y^{B}\rightarrow x^{A}$

The second \textsf{``}create function\textsf{''} rule gives $x^{A}\rightarrow\left(y^{B}\rightarrow x\right)$

Scala code: \texttt{\textcolor{blue}{\footnotesize{}def f{[}A, B{]}:\ A
$\rightarrow$ B $\rightarrow$ A = (x:\ A) $\rightarrow$ (y:\ B)
$\rightarrow$ x}}{\footnotesize\par}

Any code expression\textsf{'}s type can be translated into a sequent

A proof of a theorem directly guides us in writing code for that type

Correspondence between programs and proofs

By construction, any theorem can be implemented in code
\begin{center}
\begin{tabular}{|c|c|}
\hline 
\textbf{Proposition} & \textbf{Code}\tabularnewline
\hline 
\hline 
$\forall A:A\rightarrow A$ & \texttt{\textcolor{blue}{\footnotesize{}def identity{[}A{]}(x:\ A):\ A
= x}}\tabularnewline
\hline 
$\forall A:A\rightarrow1$ & \texttt{\textcolor{blue}{\footnotesize{}def toUnit{[}A{]}(x:\ A): Unit
= ()}}\tabularnewline
\hline 
$\forall A\forall B:A\rightarrow A+B$ & \texttt{\textcolor{blue}{\footnotesize{}def inLeft{[}A,B{]}(x:A):\ Either{[}A,B{]}
= Left(x)}}\tabularnewline
\hline 
$\forall A\forall B:A\times B\rightarrow A$ & \texttt{\textcolor{blue}{\footnotesize{}def first{[}A,B{]}(p:\ (A,B)):\ A
= p.\_1}}\tabularnewline
\hline 
$\forall A\forall B:A\rightarrow B\rightarrow A$ & \texttt{\textcolor{blue}{\footnotesize{}def const{[}A,B{]}(x:\ A):\ B$\rightarrow$A
= (y:B)$\rightarrow$x}}\tabularnewline
\hline 
\end{tabular}
\par\end{center}

Also, non-theorems \emph{cannot be implemented} in code 

Examples of non-theorems:\\
 $\forall A:1\rightarrow A$; \  \  $\quad\forall A\forall B:A+B\rightarrow A$;
\\
$\forall A\forall B:A\rightarrow A\times B$; \  $\quad\forall A\forall B:(A\rightarrow B)\rightarrow A$

Given a type\textsf{'}s formula, can we implement it in code? Not obvious.

Example: $\forall A\forall B:((((A\rightarrow B)\rightarrow A)\rightarrow A)\rightarrow B)\rightarrow B$

Can we write a function with this type? Can we prove this formula?

The logic of types II

What kind of logic is this? What do mathematicians call this logic?

This is called \textsf{``}intuitionistic propositional logic\textsf{''}, IPL (also
\textsf{``}constructive\textsf{''})

This is a \textsf{``}nonclassical\textsf{''} logic because it is different from Boolean
logic

Disjunction works very differently from Boolean logic

Example: $A\rightarrow B+C\vdash(A\rightarrow B)+(A\rightarrow C)$
does not hold in IPL

This is counter-intuitive!

We cannot implement a function with this type:

\texttt{\textcolor{blue}{\footnotesize{}def q{[}A,B,C{]}(f: A $\rightarrow$
Either{[}B, C{]}): Either{[}A $\rightarrow$ B, A $\rightarrow$ C{]}}}{\footnotesize\par}

Disjunction is \textsf{``}constructive\textsf{''}: need to supply one of the parts

But \texttt{\textcolor{blue}{\footnotesize{}Either{[}A $\rightarrow$
B, A $\rightarrow$ C{]}}} is not a function of \texttt{\textcolor{blue}{\footnotesize{}A}} 

Implication works somewhat differently

Example: $\left(\left(A\rightarrow B\right)\rightarrow A\right)\rightarrow A$
holds in Boolean logic but not in IPL

Cannot compute an \texttt{\textcolor{blue}{\footnotesize{}x:\ A}}
because of insufficient data

Conjunction works the same as in Boolean logic

Example: 
\[
A\rightarrow B\times C\vdash\left(A\rightarrow B\right)\times\left(A\rightarrow C\right)
\]
 

The logic of types III

How to determine whether a given IPL formula is a theorem?

The IPL cannot have a truth table with a fixed number of truth values

This was shown by G\"odel in 1932 (see \href{https://en.wikipedia.org/wiki/Many-valued_logic}{Wikipedia page})

The IPL has a decision procedure (algorithm) that either finds a proof
for a given IPL formula, or determines that there is no proof

There may be several inequivalent proofs of an IPL theorem

Each proof can be \emph{automatically translated} into code

The \href{https://github.com/Chymyst/curryhoward}{curryhoward} library
implements an IPL prover as a Scala macro, and generates Scala code
from types

The \href{https://hackage.haskell.org/package/djinn-ghc}{djinn-ghc}
compiler plugin and the \href{https://github.com/nomeata/ghc-justdoit}{JustDoIt plugin}
implement an IPL prover in Haskell, and generate Haskell code from
types

All these IPL provers use the same basic algorithm called LJT 

and all cite the same paper {\footnotesize{}\href{https://rd.host.cs.st-andrews.ac.uk/publications/jsl57.pdf}{[Dyckhoff 1992]}}{\footnotesize\par}

because most other papers on this subject are incomprehensible to
non-specialists, or describe algorithms that are too complicated

Proof search I: looking for an algorithm

Why our initial presentation of IPL does not give a proof search algorithm

The FP type constructions give nine axioms and three derivation rules:

\begin{minipage}[t]{0.49\columnwidth}%
\begin{itemize}
\item $\Gamma,A,B\vdash A\times B$ 
\item $\Gamma,A\times B\vdash A$ 
\item $\Gamma,A\times B\vdash B$
\item $\Gamma,A\rightarrow B,A\vdash B$
\item $\Gamma,A\vdash A+B$ 
\item $\Gamma,B\vdash A+B$
\item $\Gamma,A+B,A\rightarrow C,B\rightarrow C\vdash C$
\item $\Gamma\vdash1$
\item $\Gamma,A\vdash A$
\end{itemize}
%
\end{minipage}%
\begin{minipage}[t]{0.49\columnwidth}%
\[
\frac{\Gamma,A\vdash B}{\Gamma\vdash A\rightarrow B}
\]
\[
\frac{\Gamma\vdash G}{\Gamma,D\vdash G}
\]
\[
\frac{\Gamma,A,B\vdash G}{\Gamma,B,A\vdash G}
\]
%
\end{minipage}

\medskip{}
Can we use these rules to obtain a finite and complete search tree?
No.

Try proving $A,B+C\vdash A\times B+C$: cannot find matching rules

Need a better formulation of the logic

Proof search II: Gentzen\textsf{'}s calculus LJ (1935)

A \textsf{``}complete and sound calculus\textsf{''} is a set of axioms and derivation
rules that will yield all (and only!) theorems of the logic
\begin{align*}
\text{(}X\text{ is atomic)\,}\frac{}{\Gamma,{\color{blue}X}\vdash X}\:Id & \qquad\frac{}{\Gamma\vdash{\color{blue}\top}}\,\top\\
\frac{\Gamma,A\rightarrow B\vdash A\quad\;\Gamma,B\vdash C}{\Gamma,{\color{blue}A\rightarrow B}\vdash C}\:L\rightarrow & \qquad\frac{\Gamma,A\vdash B}{\Gamma\vdash{\color{blue}A\rightarrow B}}\,R\rightarrow\\
\frac{\Gamma,A\vdash C\quad\;\Gamma,B\vdash C}{\Gamma,{\color{blue}A+B}\vdash C}\:L+ & \qquad\frac{\Gamma\vdash A_{i}}{\Gamma\vdash{\color{blue}A_{1}+A_{2}}}\,R+_{i}\\
\frac{\Gamma,A_{i}\vdash C}{\Gamma,{\color{blue}A_{1}\times A_{2}}\vdash C}\:L\times_{i} & \qquad\frac{\Gamma\vdash A\quad\;\Gamma\vdash B}{\Gamma\vdash{\color{blue}A\times B}}\,R\times
\end{align*}

Two axioms and eight derivation rules

Each derivation rule says: The sequent at bottom will be proved if
proofs are given for sequent(s) at top

Use these rules \textsf{``}bottom-up\textsf{''} to perform a proof search

Sequents are nodes and proofs are edges in the proof search tree

Proof search example I

Example: to prove $\left(\left(R\rightarrow R\right)\rightarrow Q\right)\rightarrow Q$

Root sequent $S_{0}:\emptyset\vdash\left(\left(R\rightarrow R\right)\rightarrow Q\right)\rightarrow Q$

$S_{0}$ with rule $R\rightarrow$ yields $S_{1}:\left(R\rightarrow R\right)\rightarrow Q\vdash Q$

$S_{1}$ with rule $L\rightarrow$ yields $S_{2}:\left(R\rightarrow R\right)\rightarrow Q\vdash R\rightarrow R$
and $S_{3}:Q\vdash Q$

Sequent $S_{3}$ follows from the $Id$ axiom; it remains to prove
$S_{2}$

$S_{2}$ with rule $L\rightarrow$ yields $S_{4}:\left(R\rightarrow R\right)\rightarrow Q\vdash R\rightarrow R$
and $S_{5}:Q\vdash R\rightarrow R$

We are stuck here because $S_{4}=S_{2}$ (we are in a loop)

We can prove $S_{5}$, but that will not help

So we backtrack (erase $S_{4}$, $S_{5}$) and apply another rule
to $S_{2}$

$S_{2}$ with rule $R\rightarrow$ yields $S_{6}:\left(R\rightarrow R\right)\rightarrow Q;R\vdash R$

Sequent $S_{6}$ follows from the $Id$ axiom

Therefore we have proved $S_{0}$

Since $\left(\left(R\rightarrow R\right)\rightarrow Q\right)\rightarrow Q$
is derived from no premises, it is a theorem

$Q.E.D.$

Proof search III: The calculus LJT

Vorobieff-Hudelmaier-Dyckhoff, 1950-1990

The Gentzen calculus LJ will loop if rule $L\rightarrow$ is applied
$\geq2$ times

The calculus LJT keeps all rules of LJ except rule $L\rightarrow$

Replace rule $L\rightarrow$ by pattern-matching on $A$ in the premise
$A\rightarrow B$:
\begin{align*}
\text{(}X\text{ is atomic)\,}\frac{\Gamma,X,B\vdash D}{\Gamma,X,{\color{blue}X\rightarrow B}\vdash D}\:L\rightarrow_{1}\\
\frac{\Gamma,A\rightarrow B\rightarrow C\vdash D}{\Gamma,{\color{blue}(A\times B)\rightarrow C}\vdash D}\:L\rightarrow_{2}\\
\frac{\Gamma,A\rightarrow C,B\rightarrow C\vdash D}{\Gamma,{\color{blue}(A+B)\rightarrow C}\vdash D}\:L\rightarrow_{3}\\
\frac{\Gamma,B\rightarrow C\vdash A\rightarrow B\quad\quad\Gamma,C\vdash D}{\Gamma,{\color{blue}(A\rightarrow B)\rightarrow C}\vdash D}\:L\rightarrow_{4}
\end{align*}

When using LJT rules, the proof tree has no loops and terminates

See \href{http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2618}{this paper}
for an explicit decreasing measure on the proof tree

Proof search IV: The calculus LJT

\textsf{``}\emph{It is obvious that it is obvious}\textsf{''} \textemdash{} a mathematician
after thinking for a half-hour

Rule $L\rightarrow_{4}$ is based on the key theorem: {\footnotesize{}
\[
\left(\left(A\rightarrow B\right)\rightarrow C\right)\rightarrow\left(A\rightarrow B\right)\,\Longleftrightarrow\,\left(B\rightarrow C\right)\rightarrow\left(A\rightarrow B\right)
\]
}{\footnotesize\par}

The key theorem for rule $L\rightarrow_{4}$ is attributed to Vorobieff
(1958)

A stepping stone to this theorem:{\footnotesize{}
\[
\left(\left(A\rightarrow B\right)\rightarrow C\right)\rightarrow B\rightarrow C
\]
}Proof: $f^{\left(A\rightarrow B\right)\rightarrow C}\rightarrow b^{B}\rightarrow f\:(x^{A}\rightarrow b)$

Proof search V: From deduction rules to code

The new rules are equivalent to the old rules, therefore...

Proof of a sequent $A,B,C\vdash G$ $\Leftrightarrow$ code/expression
$t(a,b,c):G$

Also can be seen as a function $t$ from $A,B,C$ to $G$

Sequent in a proof follows from an axiom or from a transforming rule

The two axioms are fixed expressions, $x^{A}\rightarrow x$ and $1$

Each rule has a \emph{proof transformer} function: $\text{PT}_{R\rightarrow}$
, $\text{PT}_{L+}$ , etc.

Examples of proof transformer functions:
\begin{align*}
\frac{\Gamma,A\vdash C\quad\;\Gamma,B\vdash C}{\Gamma,{\color{blue}A+B}\vdash C}\:L+\\
PT_{L+}(t_{1}^{A\rightarrow C},t_{2}^{B\rightarrow C})=x^{A+B}\rightarrow & \ x\ \text{match}\begin{cases}
a^{A}\rightarrow t_{1}(a)\\
b^{B}\rightarrow t_{2}(b)
\end{cases}
\end{align*}
\begin{align*}
\frac{\Gamma,A\rightarrow B\rightarrow C\vdash D}{\Gamma,{\color{blue}(A\times B)\rightarrow C}\vdash D}\:L\rightarrow_{2}\\
PT_{L\rightarrow_{2}}(f^{\left(A\rightarrow B\rightarrow C\right)\rightarrow D})=g^{A\times B\rightarrow C}\rightarrow & f\,(x^{A}\rightarrow y^{B}\rightarrow g(x,y))
\end{align*}

Verify that we can indeed produce PTs for every rule of LJT

Proof search example II: deriving code

Once a proof tree is found, start from leaves and apply PTs

For each sequent $S_{i}$, this will derive a \textbf{proof expression}
$t_{i}$

Example: to prove $S_{0}$, start from $S_{6}$ backwards:{\footnotesize{}
\begin{align*}
S_{6}:\left(R\rightarrow R\right)\rightarrow Q;R\vdash R\quad(\text{axiom }Id)\quad & t_{6}(rrq,r)=r\\
S_{2}:\left(R\rightarrow R\right)\rightarrow Q\vdash\left(R\rightarrow R\right)\quad\text{PT}_{R\rightarrow}(t_{6})\quad & t_{2}(rrq)=\left(r\rightarrow t_{6}(rrq,r)\right)\\
S_{3}:Q\vdash Q\quad(\text{axiom }Id)\quad & t_{3}(q)=q\\
S_{1}:\left(R\rightarrow R\right)\rightarrow Q\vdash Q\quad\text{PT}_{L\rightarrow}(t_{2},t_{3})\quad & t_{1}(rrq)=t_{3}(rrq(t_{2}(rrq)))\\
S_{0}:\emptyset\vdash\left(\left(R\rightarrow R\right)\rightarrow Q\right)\rightarrow Q\quad\text{PT}_{R\rightarrow}(t_{1})\quad & t_{0}=\left(rrq\rightarrow t_{1}(rrq)\right)
\end{align*}
}{\footnotesize\par}

The proof expression for $S_{0}$ is then obtained as
\begin{align*}
t_{0} & =rrq\rightarrow t_{3}\left(rrq\left(t_{2}\left(rrq\right)\right)\right)=rrq\rightarrow rrq(r\rightarrow t_{6}\left(rrq,r\right)\\
 & =rrq\rightarrow rrq\left(r\rightarrow r\right)
\end{align*}
Simplified final code having the required type: 
\[
t_{0}:\left(\left(R\rightarrow R\right)\rightarrow Q\right)\rightarrow Q=\left(rrq\rightarrow rrq\left(r\rightarrow r\right)\right)
\]

To \emph{prove} that there is no proof, one needs to use methods of
proof theory that are beyond the scope of this book. A good introduction
to the required technique is the book \textsf{``}\emph{Proof and Disproof
in Formal Logic}\textsf{''} by R.~Bornat.\footnote{~\href{https://www.amazon.com/Proof-Disproof-Formal-Logic-Introduction/dp/0198530277}{R. Bornat, "Proof and Disproof in Formal Logic", Oxford, 2005 - link to Amazon.com}} 

\begin{comment}
so suppose we we know we need this type how do we get this code turns
out mathematicians have studied this question for a long time mathematicians
were only studying the logical domain of course not the program in
domain since about 1930 or so many mathematicians have studied this
including church tarski girdle lots of people in Poland and Germany
in England and in the United States it took a very long time about
50 years between 1913 and 1980 between the beginning of this activity
when this was first formulated as logic with these particular rules
and the time when the Curie Howard respondents was realized that or
was it wasn't was discovered at that time it became clear that these
things have a direct bearing on functional programming actually helping
people to write code because if you know how to prove things here
you just directly write the code

~

showing here this is equivalent in Scala just different syntax importantly
non theorems cannot be implemented in code some on theorems are statements
in logic that cannot be derived statements that are false or undereye
verbal examples of these statements are these for all a from one follows
a now this is certainly suspicious in terms of logic what if a were
false then we would have it from true false false that\textsf{'}s very obviously
wrong and we cannot implement a function of this type to implement
it we would have to take a unit argument and produce a value of type
a where a is arbitrary type but how can we produce a value of type
a of the type that we don't even know what it is and there is no data
for us to produce that value so it is impossible another example of
an impossible type is this type so from a plus B follows a if you
wanted to implement this function you would have to take a value of
disjunction type a plus B and return a value of type a but how can
you do that what exodus Junction type happens to contain B and no
a just B it cannot contain a if it contains a B it\textsf{'}s a disjunction
so then we don't have an A and then we again cannot produce any and
having a B which is a completely different arbitrary type doesn't
help us to produce me exactly the same reason shows why we cannot
produce an A a and B given a because that requires a B we cannot produce
and also this is not implementable because we are required to produce
an A but all we have is a function from A to B this function will
consume an A if given only this function cannot possibly produce an
A for us but we are required to produce an A as a result so we cannot
and also there is no proof of this formula in the logic so these examples
actually lead us to a natural question how can we decide given a certain
formula whether it is a theorem in logic and therefore whether it
can be implemented in code it is not obvious consider this example
can we write a function with this type in Scala it is not obvious
can we prove this formula it is not clear not quite obvious right
now suppose I were of the opinion that this cannot be proved but how
do I show that this cannot be proved I certainly cannot just try all
possible proofs that would be infinitely many possible proofs that
would give me all kinds of other formulas and that would give me nothing
that I can stand oh how to answer these questions so it is really
a very hard question we are not going to try to answer it on our own
we were going to use the results of mathematicians they have studied
these questions for many many years for centuries logic has been studied
since ancient Greece more than 2,000 years of study all we need to
do is to find out by what name mathematicians call this logic they
are probably already studied it what kind of logic is this that we
are using that follows from the type constructions remember and the
very beginning of our consideration we started with the type constructions
that our programming languages have so that\textsf{'}s set of type constructions
specifies the set of rules of derivation of the logic mathematicians
call this logic intuitionistic propositional logic or IPL also they
call it constructive propositional logic but it is less frequently
used most frequently used name is this and mathematicians also call
this a non classical logic because this logic is actually different
from the boolean logic that we are familiar with the logic of the
values true and false and their truth tables I assume that you are
familiar with those computations using truth tables and operations
and or not in the boolean logic so actually this logic the logic of
types as I call it or intuitionistic propositional logic is very different
from boolean logic in certain ways it\textsf{'}s similar in other ways disjunction
for instance works very differently here\textsf{'}s an example consider this
sequence if it has given that from a follows B plus C then either
from a follows B or from a follows C it sounds right from the common-sense
point of it if if B plus C Falls a B or C if I was I'm using plus
as a logical or so if B or C follows then it kind of makes sense either
B follows or C Falls indeed this is correct in the boolean logic which
we can find out by writing the truth table so we enumerate all the
possibilities for a B and C to be true or false or eight such possibilities
and for each of those possibilities we write the truth value of this
the truth value of this and we see from the table that whenever this
is true then this is also true in the boolean logic but this does
not hold in the intuitionistic logic for the logic of types well why
does it not hold that\textsf{'}s counterintuitive well in fact there is very
little that\textsf{'}s intuitive about this so-called intuitionistic logic
actually we need to think differently about this logic we need to
think can we implement an expression of this sequent so implementing
it would mean if we're given this expression we can build an expression
of this type so we're given an expression of type A to B plus C let\textsf{'}s
say some F of this type can we build an expression of this type we
can this differently by asking can we implement a function that takes
this as an argument and returns this well we know that this is equivalent
one of our derivation rules is that if you have this sequence then
you can also have a sequence that is a function type from this to
this so for the programmer it is easier to reason about a function
taking this as an argument and returning this so how can we implement
this function this function takes F and needs to return a value of
this type so the body of this function if we could implement it and
have to construct a value of type either of something there are only
two ways of constructing a value of type either one is to construct
the left value second is to construct the right value how do we decide
whether to construct the left value or the right value we have to
decide it somehow on the basis of what information can we decide it
we don't actually have any such information what we have here is a
function from a to either BC so given some value of a of type a we
could compute f of that value and then we would have either B or C
we could decide them whether to we could take them that B or that
C but that\textsf{'}s not what we need to return we don't need to return either
of BC we need to return either of this function or that function and
that function is not yet applied to any a it is it is too late for
us to ask what is the a we already have to return the left of this
or a right of that in other words this type either of something-something
is not itself a function of a it contains functions away but itself
it cannot be decided on the basis of any assets too late so we need
to supply a left or right so here right away immediately we have to
decide whether this will return a left or a right and we cannot really
decide that if we decide we return the left we must then return a
function from A to B so there\textsf{'}s no way for us to construct this function
if we're given this function because this function could sometimes
return C instead of B and then we'll be stuck we cannot do this and
we can also return we cannot also return the right either so it is
impossible to implement a function of this type implication also works
a little differently in the intuitionistic logic here\textsf{'}s an example
this holds in boolean logic but not in intuitionistic logic again
let\textsf{'}s see why how can we compute this given this this function will
give us an e only when given an argument of this type but how can
we produce a value of this type we cannot we don't have information
that will allow us to produce a value of this type a and B are some
arbitrary types remember there is universal quantifier outside of
all this for all a and for all B we're supposed to produce this and
that is impossible we don't have enough data to produce some values
type a and so we cannot implement this function conjunction works
kind of the same as in boolean logic so here\textsf{'}s an example this implemented
and this is also in boolean logic a true theorem now in boolean logic
the usual way of deciding whether something is true or something is
a theorem is to write a truth table unfortunately the intuitionistic
logic cannot have a truth table it cannot have a fixed number of truth
values even if you allow more than two truth values such that the
validity of formulas the truth of theorems can be decided on the basis
of the truth table this was shown by noodle and this means we should
not actually try to reason about this logic using truth values it
is not very useful even an infinite infinite number of truth values
will not help instead however it turns out that this logic has a decision
procedure or an algorithm and this algorithm is guaranteed either
to find the proof for any given formula of the internation intuitionistic
logic or to determine that there is no proof for that formula the
algorithm can also find several in equivalent proofs if there is a
theorem so a theorem could have several in equivalent proofs and since
each proof could be automatically translated into code of that type
it means we could generate several in equivalent expressions of some
type sometimes so that is the situation with this logic which we discover
if we write if we read papers about intuitionistic propositional logic
that are available in the literature and their open source projects
on the web such as the gen GHC which is a compiler plugin for haskell
this is another project doing the same thing and for Scala are implemented
occurred the Clary Howard library both of these Scala and Haskell
all of these color and Haskell projects do the same thing they take
a type of some expression for function and generate code for it automatic
by translating the type into sequence finding a proof in this logic
using the algorithm and translating that proof back into code in the
way that we have seen in an example it is interesting that all these
provers and there\textsf{'}s a few others there\textsf{'}s one more for the idris language
I did not mention here they all used the same decision procedure or
the same basic algorithm which is called ljt which was explained in
a paper by dick off here they all side the same paper and I believe
this is so because most other papers on this subject are unreadable
to non-specialists they are written in a very complicated way or they
describe algorithms that are too complicated so I will show how this
works in the rest of this tutorial in order to find out how to get
an algorithm we need to ask well first of all do we have the rules
of derivation that allow us to create an algorithm already here is
a summary of the axioms and the rules of derivation that we have found
so far these are direct translations of the cold expressions that
we held in the programming language in the notation of sequence now
there\textsf{'}s one other notation for derivation rules which looks like a
fraction like this the numerator is one or more sequins and the denominator
is a sequence and this notation means in order to derive what is in
the denominator you have to present proofs for what is in the numerator
so this is the convention in the literature this fraction like syntax
or notation now we keep in mind that proofs of sequence are actually
just called expressions that have these types as some variables and
this type is the entire expression so these are directly responding
to proofs of this sequence and to the proofs of these derivation rules
and so if we have a proof that operates by combining some of these
axioms and some of these generation rules which directly translate
that back into code now the question is do these rules give us an
algorithm for finding a proof the answer is no how can we use these
rules to obtain an algorithm well suppose we need to prove some sequence
like this in order to prove it we could first see if the sequence
is one of the axioms if so then we have already proved if we know
what expression to write now in this case none of the axioms match
this so much means maybe a is a times B so B here is C and then on
the Left we must have C or you must have a times B now we don't you
don't have C on the left as we have because even that\textsf{'}s not the same
we also don't have a times B at the premise we have a but we don't
have a times B so these rules don't match the other rules don't match
the premises and the goal either but also these rules so how can we
use them well when the writer must be an implication we don't have
an application on the right here we could try to delete some of the
premises because it\textsf{'}s unused well actually it doesn't look like a
good idea could you read a for example and we end up with an really
hopeless sequence from B plus C we cannot get an A ever and so but
sounds hopeless so this doesn't seem to help and changing the order
doesn't seem to help much either and so we cannot find matching rules
but actually this sequence is provable just a clever combination of
what axiom to start with and what role to use and then again some
axiom and so on it will give us that time sure because I know how
to write code for this this is not difficult you have a function with
two arguments one of them is a the other is B plus C so disjunction
of either B C and we are supposed to produce a disjunction of tuple
a B or C that\textsf{'}s easy look at this disjunction if we have a B in this
disjunction then we can produce a left of the tuple a B because we
always have an A anyway if we have a see in this disjunction then
we could return this part of the disjunction in the right of C and
we're done but unfortunately we see that the rules here do not give
us an algorithm for deciding this we need a better formulation of
the logic again mathematicians need to save us from the situation
and they have done so mathematicians have studied this logic for a
long time starting from the early 20th of the last century the first
algorithmic formulation of the logic that was found is due to Jensen
who published what he called the calculus just ignore the word calculus
it means not very much complete and sound calculus means that he came
up with some rules of derivation which are summarized here such that
they are equivalent to these they derive all the same theorems and
only the same theorems so they derive all the stuff that is right
and only that stuff they don't derive any wrong statements it\textsf{'}s very
hard to come up with such a system of axioms and derivation rules
that are equivalent to another one in this sense also it\textsf{'}s very hard
to prove that these are actually the rules that will give you all
the theorems that could be right in this logic that you can actually
derive all the theorems that are right yet work is already done by
mathematicians so we're not going to try to do it ourselves we're
just going to understand how these rules work now the syntax here
is slightly enhanced compared with this the enhancement is that their
names pretty cool now these are just labels they don't really do anything
in terms of sequence these help us identify which we all have has
been applied to which sequence and that\textsf{'}s all we do so other than
that it is the same notation so the fraction such as this one means
that there is a sequence in the denominator which we will prove if
there are proofs given for sequence in the numerator in this rule
there are two sequence of them in the numerator other rules may have
one sequence in the numerator or no sequence in the numerator so these
rules that will have no previous sequence required those are axioms
this axiom means if you have an atomic X in other words it\textsf{'}s a variable
it\textsf{'}s a type variables not not a complicated expression just attack
variable and you can derive that same variable this is our accion
right here now why is it important that this is atomic that this is
type variable and not a more complicated expression actually not important
but it\textsf{'}s the simplest rule that you can come up with and mathematicians
always like the most minimal set of rules so that\textsf{'}s why they say let\textsf{'}s
only consider this rule for the type variables X not for more complicated
expressions but we can consider this rule for any expression of course
the identity axiom well here is a truth truth axiom net which derives
the truth which is the ste symbol which I denote it by one the format
in logical notation this is the T symbol well let\textsf{'}s just call this
one for clarity so that can be derived from any premises with no previous
sequence necessary none of these other rules now what do these other
rules do they do an interesting thing actually each of these rules
is either about something in the sequence on the left to the trans
time or something in the sequence to the right of the transplant which
I here shown in blue so these are the interesting parts of the sequence
that are being worked on or transformed by the rule so here\textsf{'}s an example
this rule is actually two rules the eyes the index so I is one or
two another two rules just written for gravity like this with index
I and each of them says you will prove this if you prove one of if
you prove this so for example you will prove C given if you're given
a a one A two if you will prove C given just a one which makes sense
because if you can prove C given a one you don't need a two we can
ignore this a T we can already proved C from anyone so in this way
it would be proved and so all these rules work in this way you can
prove what\textsf{'}s on the bottom of the seat of the of the fraction if you're
given proofs for what\textsf{'}s on the top so these are eight derivation rules
and two axioms we can use this now to make a proof search how do we
do that I start with a sequence we see which rule matches that sequence
so the sequence must have something on the left and something on the
right well at least one of these it cannot be empty so it must be
something somewhere and there are only four kinds of expressions in
our logic type variables conjunctions implications and disjunctions
now notice I'm using this arithmetic arithmetic all notation for logic
just because I like it better and I will show that it has advantages
later so we take a sequence we see which rule matches one of them
won't match because either in the premise we have one of these expressions
were in the goal we have one of these expressions and then we find
the rule of match that matches we apply that rule so we now have new
sequence one or more that we will need to be proved and if they're
true then we fork the tree and now we have to prove both of them son-in
we continue doing that for each of the sequence until we hit axioms
so the tree will and this leaf or we hit a sequence to which no rule
applies in which case we cannot prove it and the entire thing is unprovable
so in the search tree there will be sequence at the nodes of the tree
and proofs will be at the edges of the tree so each node sends its
proof to the root of the tree this calculus is guaranteed by mathematicians
to be such that indeed if you cannot find a rule that applies that
means the sequence cannot be proved which was not the case here the
sequence can be proved and yet we cannot find a rule that applies
so in this calculus we can use bottom-up approach to make a proof
search as a tree here we cannot that is the advantage capitalizing
on the mathematicians results let us look at an example suppose we
want to prove this formula this theorem so first step we need to write
a sequence and this needs to be proved from no premises so we write
a sequence s0 which has an empty set of premises this is a single
now what rule applies to this sequence with your bottom up so in other
words we look at these rules and they refine which denominator matches
our sequential and our cylinders empty set on the left so all the
rules on the left cannot be applied but on the right we have an expression
which is an implication at the top level of this expression there
is this implies that so this is of the form a implies B so this rule
applies we have a sequence of the form something in our case this
is an empty set and then a implies B so we apply this rule which is
the right implication and we get a new sequence which is that what
was here before the implication is now put on the left to the trans
of the to the left of the trans time and it means that this expression
needs to be now to the left of the turnstile so now this is the sequence
s1 now we need to prove s1 well we see what rule applies to us one
well on the right there is just Q so nothing can be done of these
rules and Q is not truth so we cannot use the axiom either so let\textsf{'}s
look at their left rules on the Left we have now an implication so
this is let\textsf{'}s say a and this is B so we have a rule which has a implication
B on the left this is the row left implication let\textsf{'}s apply it that
law will give us two new sequence so these two new sequence are s2
and s3 no these ones as you can check if you match a location B against
this implication Q so this is a this is B so then you get these two
sequence now we have to prove these two sequence as 2 and s 3 s 3
is easy it is just the axiom of identity it is this now as 2 again
has an implication on the left let\textsf{'}s again apply the rule left implication
to that we get two more sequence as foreign s5 as for is this because
5 is this so now actually we are in trouble because as 2 and s 4 is
are the same sequence as 5 actually we could prove with some more
work but that won't help because we are in a situation when to prove
as two we need to prove again s 2 so that\textsf{'}s it that\textsf{'}s a loop that
will never give us anything it means we applied the wrong rule so
we need to backtrack this step when we apply the rule left implication
to s 2 we erase is 4 in this 5 and try a different rule to apply to
s 2 which rule can apply to s 2 well as to is this it actually has
implication on the right so we can use the right implication rule
and if we do that we get a sequence s 6 which is this and this sequence
immediately follows from the identity axiom because it has promise
are on the left and premise are and goal are on the right and that
is this axiom whatever other premises and the premise X on the left
premise X on the right and that is a type variable so that\textsf{'}s perfect
we have done the proof as 6 follows from the axiom and therefore we
have proved s0 no more sequins need to be proved and because sequence
s0 shows this to be derived from no premises than this formula is
the theorem that\textsf{'}s what the theorem means in the logic so that is
how we use this calculus to do proof search now we notice that we
were a bit stuck at some point we had a loop now if we are in the
loop we don't know what to do maybe we need to continue applying the
same rule maybe some new sequence come up or maybe we should not continue
it is not clear what to do and just looking at the rule left implication
shows us that it\textsf{'}s copying this premise a implication B it is copied
into the premises of the new sequence and so it will generate a loop
assuredly after the second time you apply it however this sequence
might be new so we might need to apply it second time we don't know
that so that is a problem it will do now there have been a lot of
work trying to fix this problem and literally decades from research
by mathematicians the main ones I found were what are the off we published
in the Soviet Union who de Meyer and dick Hoff who published in the
United States over this time discovered gradually a new set of rules
which is called ljt or the calculus ljt which cures this problem of
looping the way it clears this problem is by replacing this rule left
implication through four new rules which are listed here all other
rules are kept the same from this calculus except the rule left implication
which is replaced in what way so left implication was applying it
applied to a sequence when the sequin had an implication among the
premises or on the left to the left of the turnstile the new rules
look in more detail at what is that implication so that implication
could have one of the four expressions as the argument of the implication
it could have an atomic expression as the argument it would have a
conjunction as the argument could have a disjunction as the argument
or it could have an implication as the argument in our logic there
are no more expressions except these four atomic variables conjunctions
disjunction and implications and so we have here enumerated all the
possibilities for what could be to the left of the implication in
this premise which I have here shown in the blue in blue and so for
each of these we do certain things replacing this sequence with one
or more other sequence again it\textsf{'}s quite a lot of work to prove that
these rules are equivalent to these and also that the new rules are
somehow better they are not giving loops a lot of work which I am
NOT going to go through because that\textsf{'}s far too complicated for the
scope so what we need suffice it to say that we have very smart people
who published on this and it is reasonably sure that this is correct
so the T in the name lgt starts stands for terminating so if we use
these rules in the same way by by creating a proof tree the proof
tree will have no loops and will terminate after a finite number of
steps and there is actually this paper that is also helpful for understanding
how to implement this algorithm and this paper shows explicitly how
to construct an integer function from sequence to integers which is
a measure of the complexity of the sequence and this measure decreases
every time you apply a rule so it strictly decreases and since this
is a strictly decreasing measure on the proof tree it means that all
the next nodes in the proof tree will have a smaller value of this
measure so eventually it will hit zero and the proof tree will terminate
at that leaf either that or you have no more rules to apply and if
you have no more laws to apply then again mathematicians have proved
it means our sequence cannot be proved so this is an important result
that we are going to use and note that this this rule is quite complicated
it does a very interesting thing it takes this expression which has
implication inside an implication and it transforms this expression
in a weird way namely the B here is separated from the C by parenthesis
but here it is not separated so this transformation is highly non-trivial
and unexpected and its validity is based on this theorem that this
in the intuitionistic logic is equivalent to this equivalent means
they're both following from the other so from this promos that and
from there follows this so this key theorem was attributed to rob
you off my dick off in this paper and this is this lemma 2 which says
that if this sorry that the this derivation is if and only if that
derivations will have these two equivalences and the proof is trivial
and the 34 is a reference to to borrow be off now when a mathematician
says that something is trivial doesn't mean that a statement is easy
to understand it doesn't mean that the proof is easy to find or that
it has trees easy to understand it means none of these things it just
means that right now for this mathematician it is not interesting
to talk about how it is done that\textsf{'}s all it means could be for any
number of reasons for example mathematicians could just be lazy or
have no time to again explain this and so they say it\textsf{'}s trivial don't
be don't be deceived when you see somebody says that something is
trivial in a mathematical text so to prove this one stepping stone
could be to prove this first this is an easier theorem and if you
prove this then clearly from here you can get B to C B to C you can
substitute in here you can get a to B and then you have here a to
B so in this way you can show this equivalence in one direction now
the proof of this statement is obviously trivial in order to show
the expression of this type I will use my short notation so this is
F which has this type the first argument of the function the second
is B which is at this type then we need to produce a see how do we
produce a C we apply F to an argument of this type the argument of
this type is a function that takes a and returns a B so we take some
X of type a and we return a B which was this B so we ignore this X
we just returned that B and that\textsf{'}s the argument of F so this expression
is the proof of this sequence in other words this is the code that
has this type and therefore the proof must be available somehow so
the details of proving this theorem are left as an exercise for the
reader again when you see in a mathematical text that something is
left as an exercise for the reader it does not mean that it is easy
to do it does not mean that for you it would be a useful exercise
to do it also does not mean that the author knows how to do it it
means none of these things it just means the author doesn't feel like
doing it right now and showing it to you for whatever reason could
be because they are lazy it could be because I don't know how to do
it could be because they feel that they should know how to do it but
they don't really do know how to do it could be any of these reasons
don't be deceived when you see something like this but of course I
had to actually produce an expression function of this type in order
to implement my curry forward language because as I will show in a
moment we need to be able to implement all these has code in order
to help approver so why is that we believe the mathematicians that
the new rules are equivalent to the old rules which means that if
you find a proof using these rules somehow you should be able to find
the proof also using our initial rules which means that if you found
that proof it would easily translate that to code because each step
here is directly corresponding to a certain code expression as we
have seen at the beginning of this tutorial these cold expressions
from each of these operations so in order to do this with new rules
in other words in order to create code from proof using new rules
we need to show equivalence or we need to show how to get code out
of each of the new rules now proof of a sequence means that we have
some expression let\textsf{'}s say T what uses variables a B and C of these
types and expression itself has type G and also as I have shown this
could be conveniently seen as a function the T as a function from
a B and C from these three arguments to the type G so for each sequencing
a proof we should be able to show either that it follows from an axiom
one of these or that it show it follows from a derivation rule and
the derivations all transforms one proof into another the axioms are
just fixed expressions as we had before the axiom that actually didn't
change between our initial formulation of the logic and the new calculus
lgt they actually did not change the derivation rules changed each
new derivation rule means that you're given expressions that prove
the sequence in the numerator one or more and you are out of these
expressions somehow you have to construct an expression that proves
this sequence now when I say an expression proves the sequence what
it means is that expression has the type that is described by the
sequence it\textsf{'}s the same thing because we described types of expressions
through sequence and only those sequence that correspond to valid
and existing expressions in the programming language only those sequence
can be proved by the logic this is by construction so now we need
to just find what are these expressions that corresponds to each of
the derivation rules in each rule has a proof transformer function
as I call it and the proof transfer function is explicitly a function
that takes one or more expressions that are in the numerator and converts
that to the expression in the denominator that has this type so it
has an expression as it has an explicit function we need to write
down for each of the derivation rules so let\textsf{'}s see how this is done
for these two examples of derivation laws first example have a rule
that says if you want to derive this sequence we need to derive these
two sequence now this sequence represents an expression of type C
which uses an expression of type A plus B so let\textsf{'}s represent this
as a function from a plus B to C now we will be able to just ignore
these other premises which are common arguments and all these functions
we just pass them and we don't write them out what is the proof transformer
for this derivation rule the proof transformer for it is a function
that has two arguments t1 which is the proof of this must be a function
of type II to see and t2 which is a proof of this sequence which must
be a function of type B to see now earlier I said that sequence represent
expressions that use certain variables but equivalently we can say
these are functions that take these variables and return these expressions
that\textsf{'}s more convenient when you implement this in code so what we
need is a function that takes a to C and B to C and returns a function
from a plus B to C and this is the code that does it we take an argument
of type a plus B and we return a match expression if it\textsf{'}s in the left
we applied t1 to that value and we get to see if it\textsf{'}s in the right
we apply t2 to that value and we get a C so in any case we get a syllabus
so this is a function from a plus B to C as required another example
is the proof transformer for this rule this rule has one sequence
going to one sequence so in order to transform is proof into this
we need a function that takes argument of type A to B to C to D and
returns a function of type tuple a B going to C to D so here\textsf{'}s the
code we take a function f of type A to B to C to D we return a function
that takes a G of this type shown here in blue and return we need
to return a D so how do we get a deal we apply F to a function of
type A to B to C so we create that function out of G X of type a going
to Y of type B going to G of x1 so this is a function of type A to
B to C which is the argument of F as required and the result is of
type D so that is what we write so this kind of code is the proof
transformer for this derivation arrow and we need to produce this
proof transformers for every rule of the calculus lgt and I have done
it because I have implemented the Korea Howard library that uses LG
T so I'll must have done it for each flow this is a bit tedious because
there are many of those rules and you need to implement all this machinery
of passing arguments no matter how many in this gamma which are emitted
from this notation for brevity but in of course in the real code you
have to deal with all that too so let\textsf{'}s see how this works on an example
because once the proof tree is found we need to start backwards from
the leaves of the tree back to the root on each step we take the proof
expression apply the proof transformer to ative according to the rule
that was used on that step we get a new proof expression and so on
so for each sequence we will get a proof expression and at the end
we'll have a proof expression for the root sequence and that will
be the answer so I will denote denote by T I the proof expressions
for the sequence s hi so starting from s6 s6 was this sequence in
our proof so I mean yes just just going through the proof example
it was here backwards from a 6 back to a 0 s-six was this it followed
from axiom identity it\textsf{'}s proof expression t6 is a function of two
variables these two variables of these two types and this function
just returns the second variable so it\textsf{'}s a function of RR q and r
and just denote this by our argued and Garibaldi\textsf{'}s types r RQ variable
of this type is hard here so this function is very simple just ignores
the first argument and returns or so that is what the axiom does the
next sequence was as to as to was obtained by rule our implication
or right implication from s 6 so the proof transformer for right implication
let\textsf{'}s look at the right implication and see what the proof transformer
must be so we are given this sequence for this expression which is
the function body the function body that uses a variable of type a
somehow out of this we need to produce a function expression that
takes an argument of type a and returns that functional body so this
is the code which is just writing a new argument returning the function
body that was our proof transformer we need to convert function body
into a function so we just write that argument and arrow in the function
body so in our case we need this as a function body and so our t2
is a function of our Q and this function is this the sequence s 3
followed from the axiom and so it was just this function this is just
the identity function then we used the left implication so this was
actually still done in the calculus algae but the same thing works
in the calculus lgt I'm just using algae because it\textsf{'}s simpler for
example here proof transformer for the left implication is a little
more complicated and so if you look at it what what does it have to
be it takes these two expressions and returns this expression so it
takes a function from A to B to a and from B to C and it returns a
function from A to B to see how does it do it given a function a to
b you use this to derive a from it then you substitute that a into
the function into B you get a B when you use this to derive see from
that B and that\textsf{'}s your C so you use this function a to be twice you
put it in here once and then you get an A and substitute back into
the same function when you get a B then you use that and that\textsf{'}s exactly
what the proof transformer does it takes this rrq and it uses it twice
substitutes into it something that was obtained from one of the terms
and then uses the second term on the result so then this is the proof
transformer for the rule left implication the result of the proof
transformation is the proof for the sequence s1 finally we use the
right implication again which is just this function construction and
we get the proof expression for the sequence s0 now this proof expression
is written through these t1 t2 t3 we have to substitute all this back
in order to get the final expression so if we substitute first of
all we find this is our our cubone going to tea one of our cutie one
of our queue is this so we have to put it here now t3 is just identity
so we can just remove that so that gets you riq going to our Q of
T 2 T 2 is less if I have to put it in T 6 is just identity on R so
this is our going to our and so finally you have this expression so
that is the final code that has the required type notice that we have
derived this code completely algorithmic to it there was no guessing
we found which rules applied to the sequence with transformed sequence
according to the rules once we found the proof which was if we use
the calculus ljt the proof will be just a finite tree with no loops
it will terminate you can get an exhaustive depth-first search for
it for example and you find all the possible proofs if you want as
well well you will find many in any case in some for some expressions
and then we use the proof transformers which are fixed functions that
you can upfront compute for each these expressions are proof transformers
applied to the previous proofs so these are completely fixed algorithmically
fixed so we have derived this code completely algorithmically given
this expression this type so it is in this way that the career Howard
correspondence allows us to derive the code of functions from there
type signatures another important application of the correspondence
is to analyze type by some morphisms or type equivalences and I was
led to this by asking the question so in this logic or in the types
are these operations plus and times as I denoted them more like logic
more like the disjunction and conjunction or are they more like arithmetic
plus and times because this is kind of not so clear right away our
logic is this intuitionistic logic it in any case this is different
from boolean logic so what are the properties of these types really
so are the properties such that it is better to think about these
operations as plus and times rather than logical conjunction and disjunction
can answer this question I looked at identities that we have in the
water these are some identities from simple ones obvious ones to less
obvious identities like this the equal sign here stands for implication
in both directions so both this implies that and vice versa because
of this each of the implications means a function so since these are
all identities in logic it means that for example the implication
from here to here is a theorem of logic and so it can be implemented
as we know all our identities in logic can be implemented in code
and we even have an algorithm now that can automatically produce proofs
and automatically produce code so that means for any of these identities
that has some ik some expression X on the left and some Y on the right
so some kind of X equals y we have X implies Y and y implies X if
we convert that to code we will have a pair of functions function
from X to one and the function from Y to X what do these functions
do well they convert values in some ways from type X to type Y and
back so do these functions Express the equivalence of the types x
and y so that any value of type X can be converted to some equivalent
value type while and back without any loss of information is that
so that was the question I asked I looked at some examples well first
what does it mean more rigorously that types are equivalent for as
mathematicians say isomorphic the types are isomorphic and we will
use this notation for that if there is a one-to-one correspondence
between the sets of values of these types and in order to demonstrate
that we need a pair of functions one going from A to B the other going
from B to a such that the composition of these functions in both directions
is equal to identity function so F compose G or F value G will give
you from A to B and then from B to a is back so that would be identity
of a to a this will be identity of B to B if this is true if the composition
is identity it means we indeed did not lose any information let\textsf{'}s
consider an example this is an identity in the logic a conjunction
with one is equal to a in Scala the types responding to the left and
the right hand sides of this conjunction all of this are equivalent
are the conjunction of a and unit and a itself now we need functions
with these types indeed we can write functions is having these types
a pair of a and unit we need to produce an a out of that we'll just
take the first element of the pair you are done take an X of type
a will produce tuple of a and unit very easy just put a unit value
in the tuple in here done and it\textsf{'}s easy to verify that composition
of these functions will not change any values so it will be identity
in both directions another example this is an identity in logic if
this is understood as a disjunction one or a or true or a is true
that is an identity in logic for theorem in the logic are the types
equivalent though the type for 1 plus a is the option in Scala it
is option in Haskell at is called maybe this type is standard library
type in pretty much every functional programming language now option
of a is a disjunction of one or unit and a it is certainly not equivalent
to just unit because this type could contain a value of a in it but
this could not so there is no way that you could transform this type
to this and then back without losing information you could transform
so since this is a theorem you have functions from this type to this
type and back some functions you have them but these functions do
not compose to identity they cannot because what if you had a here
you must map it into unit from this unit back you must map into this
unit you cannot get an a out of unit and so that will erase this information
and that cannot become isomorphism so we see that some logic identities
do yield isomorphism types but others do not why is that let\textsf{'}s look
at some more examples to figure out why in all these examples we can
implement functions F 1 and F 2 between the two sets to two types
in both directions and then we can check we certainly can implement
them because these are logical identities but then we can check if
the compositions are identity functions and if so the types are isomorphic
but we find that in the first three examples we can do it but in this
last example we can note now I have written the logical identities
logical theorems with the arithmetic notation I call this arithmetical
notation because this suggests arithmetic operations plus and times
and if you look at these identities this looks like a well-known algebraic
identity from the school algebra in this too but this certainly seen
your own as an arithmetic as an as an arithmetic identity this is
certainly not true in arithmetic it is true in logical if you replace
this with disjunction and this with conjunction this is an identity
in logic so this suggests an interesting thing if you replace disjunction
by plus and conjunction by x and the result is an identity in arithmetic
then it is an isomorphism of types otherwise it is not let\textsf{'}s see why
this is so indeed this is so I call this the arithmetic arithmetic
oh very hard correspondence to see how it works let\textsf{'}s consider only
the types without loss of generation of generality that have a finite
set of possible values for example a boolean type has only two possible
true and false integer let\textsf{'}s say in the computers all the integers
are fine nights ago so those types have a finite set of possible values
and this does not limit our generality because in the computer everything
is finite all types have a finite set of possible values now let\textsf{'}s
consider how many values a given type has so that would be the size
of the type or using the mathematical terminology it\textsf{'}s called a cardinality
of the type so let\textsf{'}s see what is the cardinality of various type constructions
the sum type for example if the cardinality of types a and B is known
and the cardinality of a plus B the sum type the disjunction of a
and B is the sum of the two cardinalities or sizes this is because
a value of the disjunction type is constructed as either a value of
the first part or a value of the second part and so you cannot have
both together and so obviously the different number of values is just
the sum of the two sizes that the number of different values of the
sum type is just the sum of the numbers of different values of types
a and B for the product type again we have an interesting thing it\textsf{'}s
the arithmetic product of the sizes of a and B because for every a
value you could have an arbitrary B value so this is a direct product
or transient product of sets and we have school level identities about
the operations plus and times such as these identities or these all
of these identities are valid for arithmetic and they show if you
translate that into statements about the sizes of types they show
that the size of the type on the left is equal to the size of the
type on the right and that is very suggestive in other words if you
take a identity like this and you compute the size of the type on
the left and the size of the type on the right you get an arithmetic
identity of the sizes but you don't get that identity here because
the earth medical formula is not right this is very suggestive if
the sizes are equal and maybe the types are equivalent or isomorphic
when the sizes are not equal then certainly they cannot be equivalent
the function type very interestingly also is described in the same
way it provides the set of all maps between the two sets of values
so for example from integer to boolean that would be all the functions
that take some integer and return some boolean so that\textsf{'}s and a number
of boolean values \textasciicircum{} the number of integer values
that\textsf{'}s how many different functions you can have as a combinatorial
number so it\textsf{'}s an exponential and so the size of the type of function
a to be is the size of the type of B \textasciicircum{} the size of
type of a and again we have all the school identities about powers
and how to multiply powers and so on and they are directly translated
into these three identities if you take the sizes of the types on
the left and on the right the sizes will be equal due to these three
identities since the sizes are equal it\textsf{'}s very likely that the type
our actual equivalent so far haven't seen any counter examples to
this in these constructions so this gives us a meaning of the Curie
Howard correspondence so far we have seen three facets of the curly
Howard correspondence one is the correspondence between types and
logical formulas two is the correspondence between code and proofs
and three the correspondence between the cardinality of a type or
the set size of the type and the arithmetic identities that we have
in the school algebra about these types so arithmetical identities
signify type equivalence or isomorphism while logic identities only
talk about how you create some value of this type out of value of
another type so that does not guarantee that it preserves information
it just guarantees that you can implement some function of that type
it doesn't tell you that the function will be an isomorphism so if
one type is logically equivalent to another it means are equally implementable
if one is implementable another is also implementable but no more
than that whereas arithmetical identities actually tell you about
isomorphism of types therefore if you look at types and write them
using my preferred notation which is using the arithmetic all symbols
instead of logical symbols instead of these I'll use these symbols
if I do that this is very suggestive of a possible isomorphism of
types then it becomes very easy for me to reason about types I can
see right away that these two are isomorphic types or that these two
are isomorphic types because I am used to looking at school algebra
it\textsf{'}s very obvious then that this is not an isomorphism of types because
this doesn't make sense in the school algebra so reasoning about isomorphic
types is basically school level algebra involving polynomials and
powers so if you are familiar with all these identities as you should
be it will be very easy for you the reason about what types are equivalent
as long as all these types are made up of constants or primitive types
disjunctions tuples or conjunctions and functions which will then
directly be translated into exponential polynomial expressions constants
sums products and expand powers or Exponential\textsf{'}s so I call these exponential
polynomial types that is types built up from these type constructions
so all we have been talking about in this tutorial is what I call
exponential polynomial types these are the basic type constructions
that I started with tuple product function exponential disjunction
some unit constant or 1 now just one comment that in the functional
programming community today there is a terminology algebraic types
so people usually call algebraic types the types that are made from
constant types sums and products excluding Exponential\textsf{'}s I do not
find this terminology it\textsf{'}s very helpful I find it confusing because
what is particularly an algebraic about these identities these are
identities of school algebra the properties of the function type are
described by algebraic identities like this so it would be strange
to call the function type not algebraic whereas these types are algebraic
they are very similar to each other in terms of their properties being
described by identity is known from school algebra so instead of algebraic
types I would prefer to say polynomial types this is much more descriptive
and precise and if you want to talk about function types as well then
you just can you can just say exponential polynomial types or exfoli
types for short so by way of summarizing what we have done so far
what are the practical implications of the career Howard correspondence
so one set of implications is actually for writing code and reason
and eternal code one thing we can do now is if we're given a function
with some type and usually this will be typed with type parameters
all type trainers fully parametric types such as the function we have
been considering here all these functions do not have any types that
are specific like integer or string all the types are fully parametric
and then there are some constructions some type expressions made out
of these types so these are what I call fully parametric functions
for these functions we have a decision procedure an algorithm that
based on the ljt calculus which decides whether this function can
be implemented in code and computer scientists a type is inhabited
if you can produce a value of this type in your program so CH of T
is this proposition which they call type is inhabited and I prefer
to call it just that you can compute a value of this type or code
has the type O code can create a value of this type and so we have
a algorithm that can also generate the code from type when it is possible
if it is not possible the algorithm will tell you so often not always
but often this algorithm can be used actually to generate the code
you want we can also use what I call the arithmetic of glory Harvard
correspondence to reason about type isomorphisms and to transform
types isomorphic we simplify type expressions just like we simplify
expressions in school level algebra by expanding brackets by permuting
the order of terms like a plus B is equal to B plus a or associativity
a times B all times C can be expanded and so on so this allows us
once we have written types in the short notation in the notation that
I prefer which resembles school algebra because it uses the plus and
times symbols instead of the logic symbols so once we rewrite our
types and this notation which I have been doing consistently in this
tutorial it enables us the reason very easily but which types are
equal or isomorphic because we are all familiar with the school level
algebra what are the problems that we cannot solve using this knowledge
one thing we cannot do is to generate code automatically such that
it will be an isomorphism so for instance in an example here we are
able to generate automatically the code of these functions but it
will not be an isomorphism and the lgt algorithm cannot check that
this is nice a morphism that\textsf{'}s the important thing this algorithm
does not know about equations or isomorphisms it only knows that it
found some code that has the type you wanted whether this code is
useful to you or not we don't know the algorithm doesn't know this
also if the algorithm finds several such several proofs of a sequence
it will generate several not in equivalent versions of your code it
doesn't know which one is is useful maybe some of them are useless
maybe not the algorithm cannot automatically decide that in general
another thing we cannot do is to express complicated conditions via
types such as that array is sorted the type system is not powerful
enough in all the languages I listed you need a much more powerful
type system such as that in the programming language interests or
add them or cook those are much more powerful type systems that can
express such complicated conditions but for those type systems there
is no algorithm that will generate code another thing we cannot do
is to generate code that has type constructors such as the map function
here\textsf{'}s an example in Scala this is a map function on a list so there\textsf{'}s
the list of a a is a type parameter and then we say dot map and map
has another type frame to be it takes a function from A to B for any
B so a is fixed but now from any B we can take a function from A to
B and generate a list of B so if we wrote this formula in the short
type notation this would look something like this I'm writing subscript
a because this is a type parameter so this is like an argument or
a type parameter I'm writing it like this and then from this this
is the first argument of the function and then there is a second argument
which is this F and that is another quantifier for B inside parentheses
so this formula has a quantifier inside so far we have been dealing
with formulas that have all quantifiers outside and so we never write
quantifiers explicitly but here we have to write them inside this
is a more powerful logic which is called first-order logic in other
words this is a logic where you have quantifiers anywhere in the formula
including inside the formula unfortunately this logic is undecidable
so there is no algorithm that we can use either to find the proof
and therefore code freedom type or to show that there is no proof
no code so we're kind of stuck in all these directions some more remarks
about the curry Harvard correspondence first is that only with parameterize
types we can get some interesting information out of it if we take
concrete types like integer then the proposition CH event meaning
that our code can have a value of type int it that\textsf{'}s always true can
always write any some integer value we don't need any previous data
for it so for all specific types all these propositions are always
choice completely void of information the only interesting part comes
when we start considering type variables if we start asking can we
make a type which is either of a B going to a going to B in soon for
all a B once we start doing this with type parameters a B and so on
then we get interesting information as we have seen in this tutorial
another remark is that functions like this one are not sufficiently
described by their type so that this is the type of integer going
to integer now looking at this type we can put this into a sequence
but we'll never get enough information to actually get this function
so only certain class of functions which are fully typed biometric
their type signature is informative enough so that we can derive code
automatically only in much more powerful type systems you can have
type information that is enough to specify fully a code like this
another caveat is that I don't know the proof that arithmetic identity
guarantees the type equivalence it is certainly a necessary condition
because if two types have different cardinality or different size
of their sets of values that they cannot be equivalent or they cannot
be isomorphic so this is a necessary condition but it\textsf{'}s not a sufficient
condition it looks like I don't know if this is sufficient I haven't
seen any counter examples so far final remarks about type correspondence
the logical constant false did not appear in any of my slides so far
this was on purpose it has extremely limited practical use in programming
languages because actually we have types corresponding to false Scala
has type called nothing Haskell has type usually called void that
corresponds to the logical constant false what does it mean CH of
nothing is false it means your code can never have a value of type
nothing or in Haskell void you can never compute a value of this type
so clearly it has a very limited practical significance you will never
be able to compute any values of this type ever in any program it\textsf{'}s
identically falseness this constant so if you want to add it to the
logic it\textsf{'}s very easy you just have one rule and you're not done you
can derive things with it if you want but they will have almost never
any use in practical code also we did not talk about negation none
of the calculus calculate that I should have in logical negation as
in operation again for the same reason we do not have a programming
language construction that represents logical negation negation by
definition is like this is an application from 8 to 4 so that\textsf{'}s not
a not a means from a follows falsehood now since you cannot ever get
false in a programming language you cannot really implement this function
in any useful sense and so i have seen some haskell library that used
this type void as a type parameter in some way but certainly it\textsf{'}s
a very limited and rare use and so it is not really lumen 18 to include
negation it could probably find some very esoteric uses of it but
almost never useful and finally there is another set of important
implications from the Kurihara correspondence these are implications
for people who want to design new programming languages as we have
seen the Karaka with correspondence maps the type system of a programming
language into a certain logical system where prepositions follow from
each other or can be proved from each other and this enables us to
reason about programmed to see what kind of code can be written if
some other kind of code can be written and logical reasoning is very
powerful it\textsf{'}s simpler than trying to write code and it gives you algorithms
and all kinds of mathematical results that have been found over the
centuries so languages like those listed here have all the five type
constructions that I wasted in the beginning of this tutorial and
mapping them into logic gives a full constructive logic or full intuitionistic
logic with all logical operations and or so conjunction disjunction
implication and the truth constant whereas languages such as C C++
Java and c-sharp and so on they're mapped to incomplete logics because
they do not have some of these operations for instance they do not
have type constructions of correspond to disjunction we also do not
have the true constant or the false constant so they are mapped to
a logic that lacks some of the foundational logical operation so it
can be only fewer theorems can be proved in that logic and so your
reasoning about theory types is hampered languages called scripting
languages sometimes such as Python or JavaScript will be and so on
also our belongs there in that line those languages only have one
type they actually don't check types at compile time and so they're
mapped to logics with only one proposition those logics are extremely
small in terms of what kind of things you can reason about and so
if you write a program in these languages you are completely unable
to reason at the level of types whereas in these languages you are
able to reason but in a limited way you're not having a complete logic
so this suggests a principle for designing the type system in a new
programming language the first step would be to choose a good and
complete logic that is free of inconsistency mathematicians have studied
all kinds of logics and they are always interested in questions such
as is this logic consistent consistent means you cannot derive false
from true is this logic complete can you derive all things that are
true are there enough axioms and rules of derivation or maybe there
are too many axioms and rules of derivation you can delete some of
them and have fewer mathematicians have always been interested in
such questions they found all kinds of interesting logics where you
can derive a lot of interesting theorems non trivial theorems and
they found the minimum sets of axioms and rules of derivations for
these logics use their results take one of the logics that they do
them and develop such as intuitionistic logic model logic temporal
logic linear logic and so on take one of these logics for each of
the basic operations of this logic provide type constructions in your
programming language that are easy to use for instance your logic
has disjunction implication or something else provide a type constructor
for each of them that\textsf{'}s easy to use easy to write down such as provided
by the languages we have seen then every type will be mapped to a
logical form of the OPF logical formula for every type and there will
be a type for every logical formula and then for each rule of the
new logic for each derivation rule there should be a construct in
the code that corresponds to it so that you could transform proofs
in logic into code and code into proofs if you do that your language
will be faithful to the scorecard correspondence you will be able
to use logic to reason about your language and one important result
at this level while we have seen that you can sometimes generate code
that is maybe nice but a very important result is that if your logic
is free of inconsistency it means that no program will ever be able
to derive an inconsistent an inconsistent type means that you had
a function that requires some type a but it was called with a different
type beam which is incompatible and that basically crashes so in languages
like C and C++ we have all kinds of crashes like a segmentation fault
in Java the exceptions nullpointerexception or class cast exception
which happens when you call a function on the wrong type of argument
and that happens if your logic is inconsistent if your logic can derive
incorrect statements from correct premises then if you translate that
derivation into code and the that code will derive incompatible type
at the wrong place and it will crash the crash will happen at runtime
the compiler will not catch this inconsistency because the compiler
only checks the logic of types and the logic checks out you have followed
the rules of derivation of the logic the compiler can check out all
these logical rules but the compiler does not know that your logic
is inconsistent maybe and then it will deep have derived an inconsistent
result falsehood from truth for example and that will crash at runtime
now we know that crashing at runtime is not a good outcome so in fact
languages like Oh camel have been studied and for other languages
some subsets of Haskell I believe called safe Haskell have been studied
and it has been shown that they cannot crash and they're the way to
show it mathematically is to use the fact that they are based on a
complete and consistent logic and then all you need to show is that
your compiler does not have some critical bugs that allow it to oversee
that you have not followed the derivation rules of the logic that
is an extremely valuable feature of functional programming languages
that are based on the Curie habit correspondence you can prove their
safety at compile time or at least exclude a large number of possible
bugs and errors certainly these languages are quite large and they
include features that are not covered by the Carey Hart correspondence
type constructors that I have not considered in this tutorial and
those might may not be safe but at least the foundation of these languages
the foundation of the type system will be safe so that is the final
lesson from the great Howard correspondence this concludes the tutorial 
\end{comment}


\chapter{Parametricity theorem and naturality laws\label{app:Proofs-of-naturality-parametricity}}

Functional programming (FP) focuses on a small set of language features
\textemdash{} the six type constructions and the nine code constructions\index{nine code constructions},
introduced in Sections~\ref{subsec:Type-notation-and-standard-type-constructions}
and~\ref{subsec:The-rules-of-proof}; see also Tables~\ref{tab:Mathematical-notation-for-basic-code-constructions}
and \ref{tab:six-pure-type-constructions}\textendash \ref{tab:nine-pure-code-constructions}.
These constructions create \textbf{fully parametric} programs\index{fully parametric!code}
and are sufficient to implement all design patterns of FP. At the
same time, restricting programs to be fully parametric will enable
several ways of mathematical reasoning about code. Examples of such
reasoning are treating programs as mathematical values (referential
transparency); deriving the most general type from code (type inference);
and deriving code from type (code inference).

Another property of fully parametric code is automatic adherence to
naturality laws. By the \textbf{parametricity theorem}\index{parametricity theorem|textit},
any fully parametric function with a type parameter will satisfy a
certain naturality law. Not having to derive and verify naturality
laws by hand saves a lot of time.

\begin{wraptable}{l}{0.58\columnwidth}%
\begin{centering}
\vspace{-0.2\baselineskip}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\small{}Type construction} & \textbf{\small{}Scala example} & \textbf{\small{}Type notation}\tabularnewline
\hline 
\hline 
{\small{}unit or a \textsf{``}named unit\textsf{''}} & {\small{}}\lstinline!Unit!{\small{} or }\lstinline!None!{\small{} } & {\small{}$\bbnum 1$}\tabularnewline
\hline 
{\small{}type parameter} & {\small{}}\lstinline!A! & {\small{}$A$}\tabularnewline
\hline 
{\small{}product type} & {\small{}}\lstinline!(A, B)! & {\small{}$A\times B$}\tabularnewline
\hline 
{\small{}co-product type} & {\small{}}\lstinline!Either[A, B]! & {\small{}$A+B$}\tabularnewline
\hline 
{\small{}function type} & {\small{}}\lstinline!A => B! & {\small{}$A\rightarrow B$}\tabularnewline
\hline 
{\small{}recursive type} & {\small{}}\lstinline!Fix[F[_]]! & {\small{}$\text{Fix}^{F^{\bullet}}$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\index{fully parametric!type constructions}The six type constructions
of fully parametric programs.\label{tab:six-pure-type-constructions}}
\vspace{-0.2\baselineskip}
\end{wraptable}%

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\small{}Code construction} & \textbf{\small{}Scala example} & \textbf{\small{}Code notation}\tabularnewline
\hline 
\hline 
{\small{}use unit value} & {\small{}}\lstinline!()! & {\small{}$1$}\tabularnewline
\hline 
{\small{}use argument (bound variable)} & {\small{}}\lstinline!x! & {\small{}$x$}\tabularnewline
\hline 
{\small{}create function} & {\small{}}\lstinline!x => expression! & {\small{}$x\rightarrow expression$}\tabularnewline
\hline 
{\small{}use function} & {\small{}}\lstinline!f(x)! & $f(x)$ ~or~ $x\triangleright f$\tabularnewline
\hline 
{\small{}create tuple} & {\small{}}\lstinline!(a, b)! & {\small{}$a\times b$}\tabularnewline
\hline 
{\small{}use tuple} & {\small{}}\lstinline!{ case (a, b) => ... }!{\small{} or }\lstinline!p._1!{\small{}
or }\lstinline!p._2!{\small{} } & {\small{}$a\times b\rightarrow...$ ~or~ $p\triangleright\pi_{1}$
~or~ $p\triangleright\pi_{2}$}\tabularnewline
\hline 
{\small{}create disjunctive value} & {\small{}}\lstinline!Left[A, B](a)! & {\small{}}%
\begin{minipage}[c]{0.19\columnwidth}%
{\small{}\vspace{0.2\baselineskip}
$a^{:A}+\bbnum 0^{:B}$ ~or~~ $\begin{array}{|cc|}
a & \bbnum 0\end{array}$\vspace{0.2\baselineskip}
}%
\end{minipage}\tabularnewline
\hline 
{\small{}use disjunctive value} & {\small{}}%
\begin{minipage}[c]{0.33\columnwidth}%
{\small{}}\lstinline!p match { case Left(a)   => f(a)            case Right(b)  => g(b)          }!%
\end{minipage} & {\small{}}%
\begin{minipage}[c]{0.23\columnwidth}%
{\small{}\vspace{0.2\baselineskip}
$p\triangleright\,\begin{array}{|c||c|}
 & C\\
\hline A & f\\
B & g
\end{array}\,~\text{ or }~p\triangleright\,\begin{array}{||c|}
f\\
g
\end{array}$\vspace{0.2\baselineskip}
}%
\end{minipage}\tabularnewline
\hline 
{\small{}use recursive call} & {\small{}}\lstinline!def f(x) = { ... f(y) ... }! & {\small{}$f(x)\triangleq...~\overline{f}(y)~...$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\index{fully parametric!code constructions}The nine code constructions
of fully parametric programs.\label{tab:nine-pure-code-constructions}}
\end{table}

\noindent \index{Philip Wadler}The paper \textsf{``}\emph{Theorems for free}\textsf{''}
by Philip Wadler\footnote{See \texttt{\href{https://homepages.inf.ed.ac.uk/wadler/papers/free/free.ps}{https://homepages.inf.ed.ac.uk/wadler/papers/free/free.ps}}
and some further explanations in the blog posts \texttt{\href{https://reasonablypolymorphic.com/blog/theorems-for-free/}{https://reasonablypolymorphic.com/blog/theorems-for-free/}}
and \texttt{\href{https://bartoszmilewski.com/2014/09/22/}{https://bartoszmilewski.com/2014/09/22/}}} derives the parametricity theorem from J.~Reynolds\textsf{'}s \textsf{``}abstraction
theorem\textsf{''}.\index{John Reynolds}\footnote{See the \href{https://people.mpi-sws.org/~dreyer/tor/papers/reynolds.pdf}{paper}
by J.~Reynolds, \textsf{``}\emph{Types, abstraction, and parametric polymorphism}\textsf{''},
Information Processing~83, p.~513 (1983).} Finding naturality laws via the Reynolds-Wadler approach is hard:
their technique is based on replacing functions by \textsf{``}relations\textsf{''}
(both between types and between values). One first needs to learn
special new rules for working with relations.\footnote{Tutorials on that subject are rare; one is the \href{https://www.researchgate.net/publication/262348393_On_a_Relation_on_Functions}{paper}
by \index{Ronald Backhouse}R.~Backhouse, \textsf{``}On a relation on functions\textsf{''}
(1990). See also the \href{https://themattchan.com/docs/algprog.pdf}{book}
by \index{Richard Bird}R.~Bird and O.~de Moor, \textsf{``}The algebra of
programming\textsf{''} (1997).} Then the parametricity theorem gives a statement about certain properties
of relations. To derive a naturality law for functions, we must somehow
replace each relation by a function. This replacement is done by \emph{guessing}:
the parametricity theorem provides no guidance at that point.

Adapting the parametricity theorem to the needs of FP practitioners,
this Appendix derives all naturality laws for fully parametric code
without using relations and without guessing.%
\begin{comment}
Additional literature:

{[}1{]}: Girard, J.-Y.; Scedrov, A. \& Scott, P. J. Normal Forms and
Cut-Free Proofs as Natural Transformations. Logic From Computer Science,
Mathematical Science Research Institute Publications 21, Springer-Verlag,
1992, 217-241. http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.811

{[}2{]}: Bainbridge, E. S.; Freyd, P. J.; Scedrov, A. \& Scott, P.
J. Functorial polymorphism. Theoretical computer science, Elsevier,
1990, 70, 35-64. https://core.ac.uk/display/82270459

{[}3{]}: De Lataillade, J. Dinatural Terms in System F. Logic in Computer
Science, 24th Annual IEEE Symposium, 267-276, 2009. https://www.irif.fr/\textasciitilde delatail/dinat.pdf

{[}4{]}: Pistone, P. On completeness and parametricity in the realizability
semantics of System F. https://arxiv.org/abs/1802.05143

See discussion here: https://cstheory.stackexchange.com/questions/42256/is-case-analysis-on-normal-forms-of-lambda-terms-sufficient-to-prove-parametrici
\end{comment}


\section{Commutativity laws for profunctors and bifunctors\label{sec:Commutativity-laws-for-type-constructors}}

The first result concerns type constructors with two type parameters,
such as bifunctors. The bifunctor commutativity law~(\ref{eq:f-fmap-fmap-bifunctor-commutativity})
was introduced in Section~\ref{subsec:Bifunctors}. We will now prove
the commutativity law by induction on the type structure of a given
bifunctor.\footnote{In the blog post \texttt{\href{https://byorgey.wordpress.com/2018/03/30/}{https://byorgey.wordpress.com/2018/03/30/}},
Brent Yorgey\index{Brent Yorgey} gave a proof of the commutativity
law for bifunctors by using the Reynolds-Wadler parametricity theorem. } It turns out that the proof also requires the commutativity laws
for type constructors with \emph{contravariant} type parameters. All
those proofs are completely analogous, so we will first prove the
commutativity law for profunctors, which are type constructors with
one contravariant and one covariant type parameter. The profunctor
commutativity law will be important for the proof of the main parametricity
theorem (Section~\ref{sec:Naturality-laws-for-fully-parametric-functions}). 

\subsection{Proof of the profunctor commutativity law\label{subsec:Proof-of-the-profunctor-commutativity-law}}

Given an arbitrary type constructor $P^{A,B}$ contravariant with
respect to $A$ and covariant with respect to $B$, we formulate the
profunctor \textbf{commutativity law}\index{commutativity law!of profunctors}
by analogy with the bifunctor law~(\ref{eq:f-fmap-fmap-bifunctor-commutativity}):
\begin{align}
{\color{greenunder}\text{commutativity law of }P:}\quad & \text{cmap}_{P^{\bullet,B}}(f^{:A\rightarrow C})\bef\text{fmap}_{P^{A,\bullet}}(g^{:B\rightarrow D})=\text{fmap}_{P^{C,\bullet}}(g)\bef\text{cmap}_{P^{\bullet,D}}(f)\quad,\nonumber \\
{\color{greenunder}\text{in a shorter notation}:}\quad & f^{\downarrow P^{\bullet,B}}\bef g^{\uparrow P^{A,\bullet}}=g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}\quad.\label{eq:profunctor-commutativity-law}
\end{align}

\begin{wrapfigure}{l}{0.38\columnwidth}%
\vspace{-1.7\baselineskip}
\[
\xymatrix{\xyScaleY{2.5pc}\xyScaleX{6.0pc}P^{C,B}\ar[r]\sp(0.55){\text{cmap}_{P^{\bullet,B}}(f^{:A\rightarrow C})~~~}\ar[d]\sb(0.45){\text{fmap}_{P^{C,\bullet}}(g)} & P^{A,B}\ar[d]\sb(0.45){\text{fmap}_{P^{A,\bullet}}(g^{:B\rightarrow D})}\\
P^{C,D}\ar[r]\sb(0.45){~~~~\text{cmap}_{P^{\bullet,D}}(f)} & P^{A,D}
}
\]

\vspace{-1.7\baselineskip}
\end{wrapfigure}%

We will now prove that Eq.~(\ref{eq:profunctor-commutativity-law})
holds for any \textbf{fully parametric profunctor}\index{fully parametric!profunctor}
$P^{A,B}$, i.e., a profunctor defined via a combination of the six
type constructions from Table~\ref{tab:six-pure-type-constructions}.
We will assume that all contrafunctor liftings such as $f^{\downarrow P^{\bullet,B}}$
and all functor liftings such as $g^{\uparrow P^{A,\bullet}}$ are
implemented as shown in Chapter~\ref{chap:Functors,-contrafunctors,-and}
for functors and contrafunctors corresponding to each of the type
constructions. (Section~\ref{sec:Uniqueness-of-functor-and-contrafunctor}
will show that the code from Chapter~\ref{chap:Functors,-contrafunctors,-and}
is the only possible lawful implementation of the liftings.)

The proof goes by induction on the type structure of $P^{A,B}$. The
base case contains the first two constructions (the unit type and
the type parameter), which gives two possibilities: $P^{A,B}\triangleq\bbnum 1$
(constant profunctor) and $P^{A,B}\triangleq B$ (identity profunctor).
The other possibility of using a type parameter, $P^{A,B}\triangleq A$,
does not give a profunctor since $P^{A,B}$ must be contravariant
in $A$.

The commutativity law holds for $P^{A,B}\triangleq\bbnum 1$ because
all liftings are identity functions: $\text{cmap}_{P^{\bullet,B}}(f)=\text{id}$
and $\text{fmap}_{P^{A,\bullet}}(g)=\text{id}$. For the same reason,
the law will also hold for the constant profunctor $P^{A,B}\triangleq Z$
where $Z$ is a fixed type (or a type parameter other than $A$ or
$B$).

For the profunctor $P^{A,B}\triangleq B$, the law holds because $\text{cmap}_{P^{\bullet,B}}(f)=\text{id}$.
So, the base case is proved.

The inductive step looks at the outer construction in the type expression
of $P^{A,B}$, which must be one of product, co-product, function,
or recursion. These constructions create larger type expressions out
of smaller ones; for example, $P^{A,B}\triangleq\left(A\rightarrow B\right)\times B$
uses the product construction with sub-expressions $A\rightarrow B$
and $B$, which can be viewed as profunctors $R^{A,B}\triangleq A\rightarrow B$
and $S^{A,B}\triangleq B$. The inductive assumption is that the commutativity
law is already proved for all sub-expression types (such as $R^{A,B}$
or $S^{A,B}$). We will then need to prove the law for the entire
type expression $P^{A,B}$.

\paragraph{Product type}

If the outer level of $P^{A,B}$ is a product, we must have $P^{A,B}=R^{A,B}\times S^{A,B}$
where $R^{A,B}$ and $S^{A,B}$ are some profunctors. The code for
liftings follows from Statement~\ref{subsec:functor-Statement-functor-product}
and Exercise~\ref{subsec:functor-Exercise-functor-laws}:
\[
f^{\downarrow P^{\bullet,B}}\triangleq f^{\downarrow R^{\bullet,B}}\boxtimes f^{\downarrow S^{\bullet,B}}\quad,\quad\quad g^{\uparrow P^{A,\bullet}}\triangleq g^{\uparrow R^{A,\bullet}}\boxtimes g^{\uparrow S^{A,\bullet}}\quad.
\]
By inductive assumption, $R$ and $S$ already satisfy the commutativity
law~(\ref{eq:profunctor-commutativity-law}). So, we compute
\begin{align*}
{\color{greenunder}\text{expect to equal }g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}:}\quad & \gunderline{f^{\downarrow P^{\bullet,B}}}\bef\gunderline{g^{\uparrow P^{A,\bullet}}}=\big(f^{\downarrow R^{\bullet,B}}\boxtimes f^{\downarrow S^{\bullet,B}}\big)\bef\big(g^{\uparrow R^{A,\bullet}}\boxtimes g^{\uparrow S^{A,\bullet}}\big)\\
{\color{greenunder}\text{composition law~(\ref{eq:pair-product-composition-law})}:}\quad & =\big(\gunderline{f^{\downarrow R^{\bullet,B}}\bef g^{\uparrow R^{A,\bullet}}}\big)\boxtimes\big(\gunderline{f^{\downarrow S^{\bullet,B}}\bef g^{\uparrow S^{A,\bullet}}}\big)\\
{\color{greenunder}\text{inductive assumption}:}\quad & =\big(g^{\uparrow R^{C,\bullet}}\bef f^{\downarrow R^{\bullet,D}}\big)\boxtimes\big(g^{\uparrow S^{C,\bullet}}\bef f^{\downarrow S^{\bullet,D}}\big)\\
{\color{greenunder}\text{composition law~(\ref{eq:pair-product-composition-law})}:}\quad & =\big(\gunderline{g^{\uparrow R^{C,\bullet}}\boxtimes g^{\uparrow S^{C,\bullet}}}\big)\bef\big(\gunderline{f^{\downarrow R^{\bullet,D}}\boxtimes f^{\downarrow S^{\bullet,D}}}\big)=g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}\quad.
\end{align*}


\paragraph{Co-product type}

In this case, we must have $P^{A,B}=R^{A,B}+S^{A,B}$ where $R^{A,B}$
and $S^{A,B}$ are some profunctors. The code for liftings follows
from Statement~\ref{subsec:functor-Statement-functor-coproduct}
and Exercise~\ref{subsec:functor-Exercise-functor-laws}:
\[
f^{\downarrow P^{\bullet,B}}\triangleq\,\begin{array}{||cc|}
f^{\downarrow R^{\bullet,B}} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S^{\bullet,B}}
\end{array}\quad,\quad\quad g^{\uparrow P^{A,\bullet}}\triangleq\,\begin{array}{||cc|}
g^{\uparrow R^{A,\bullet}} & \bbnum 0\\
\bbnum 0 & g^{\uparrow S^{A,\bullet}}
\end{array}\quad.
\]
By inductive assumption, $R$ and $S$ already satisfy the commutativity
law~(\ref{eq:profunctor-commutativity-law}). So, we compute
\begin{align*}
{\color{greenunder}\text{expect to equal }g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}:}\quad & f^{\downarrow P^{\bullet,B}}\bef g^{\uparrow P^{A,\bullet}}=\,\begin{array}{||cc|}
f^{\downarrow R^{\bullet,B}} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S^{\bullet,B}}
\end{array}\,\bef\,\begin{array}{||cc|}
g^{\uparrow R^{A,\bullet}} & \bbnum 0\\
\bbnum 0 & g^{\uparrow S^{A,\bullet}}
\end{array}\\
{\color{greenunder}\text{matrix composition}:}\quad & =\,\begin{array}{||cc|}
\gunderline{f^{\downarrow R^{\bullet,B}}\bef g^{\uparrow R^{A,\bullet}}} & \bbnum 0\\
\bbnum 0 & \gunderline{f^{\downarrow S^{\bullet,B}}\bef g^{\uparrow S^{A,\bullet}}}
\end{array}\\
{\color{greenunder}\text{inductive assumption}:}\quad & =\,\,\begin{array}{||cc|}
g^{\uparrow R^{C,\bullet}}\bef f^{\downarrow R^{\bullet,D}} & \bbnum 0\\
\bbnum 0 & g^{\uparrow S^{C,\bullet}}\bef f^{\downarrow S^{\bullet,D}}
\end{array}\quad.
\end{align*}
The right-hand side of the law, $g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}$,
can be rewritten as
\begin{align*}
 & g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}=\,\begin{array}{||cc|}
g^{\uparrow R^{C,\bullet}} & \bbnum 0\\
\bbnum 0 & g^{\uparrow S^{C,\bullet}}
\end{array}\,\bef\,\begin{array}{||cc|}
f^{\downarrow R^{\bullet,D}} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S^{\bullet,D}}
\end{array}\\
{\color{greenunder}\text{matrix composition}:}\quad & =\,\begin{array}{||cc|}
g^{\uparrow R^{C,\bullet}}\bef f^{\downarrow R^{\bullet,D}} & \bbnum 0\\
\bbnum 0 & g^{\uparrow S^{C,\bullet}}\bef f^{\downarrow S^{\bullet,D}}
\end{array}\quad.
\end{align*}
This now coincides with the left-hand side of the law.

\paragraph{Function type}

The type expression $P^{A,B}\triangleq R^{B,A}\rightarrow S^{A,B}$
(note the swapped type parameters in $R^{B,A}$) is contravariant
in $A$ and covariant in $B$ when $R^{A,B}$ and $S^{A,B}$ are any
profunctors. The lifting code follows from Statement~\ref{subsec:functor-Statement-functor-exponential}
and Exercise~\ref{subsec:functor-Exercise-contrafunctor-exponential}:
\[
(f^{:A\rightarrow C})^{\downarrow P^{\bullet,B}}\triangleq p^{:P^{C,B}}\rightarrow f^{\uparrow R^{B,\bullet}}\bef p\bef f^{\downarrow S^{\bullet,B}}\quad,\quad\quad(g^{:B\rightarrow D})^{\uparrow P^{A,\bullet}}\triangleq p^{:P^{A,B}}\rightarrow g^{\downarrow R^{\bullet,A}}\bef p\bef g^{\uparrow S^{A,\bullet}}\quad.
\]
By inductive assumption, $R$ and $S$ satisfy the commutativity law~(\ref{eq:profunctor-commutativity-law}).
Omitting types, we compute
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & f^{\downarrow P}\bef g^{\uparrow P}=\big(p\rightarrow f^{\uparrow R}\bef p\bef f^{\downarrow S}\big)\bef\big(p\rightarrow g^{\downarrow R}\bef p\bef g^{\uparrow S}\big)\\
{\color{greenunder}\text{compute composition}:}\quad & =p\rightarrow\gunderline{g^{\downarrow R}\bef f^{\uparrow R}}\bef p\bef\gunderline{f^{\downarrow S}\bef g^{\uparrow S}}\\
{\color{greenunder}\text{inductive assumption}:}\quad & =p\rightarrow f^{\uparrow R}\bef g^{\downarrow R}\bef p\bef g^{\uparrow S}\bef f^{\downarrow S}\quad.
\end{align*}
The right-hand side of the law is then transformed to the same expression:
\begin{align*}
 & g^{\uparrow P}\bef f^{\downarrow P}=\big(p\rightarrow g^{\downarrow R}\bef p\bef g^{\uparrow S}\big)\bef\big(p\rightarrow f^{\uparrow R}\bef p\bef f^{\downarrow S}\big)\\
{\color{greenunder}\text{compute composition}:}\quad & =p\rightarrow f^{\uparrow R}\bef g^{\downarrow R}\bef p\bef g^{\uparrow S}\bef f^{\downarrow S}\quad.
\end{align*}


\paragraph{Recursive type}

A recursive profunctor $P$ is defined using a type equation of the
form
\[
P^{A,B}\triangleq S^{A,B,P^{A,B}}\quad,\quad\text{equivalently denoted as}\quad P^{A,B}\triangleq\text{Fix}^{S^{A,B,\bullet}}\quad,
\]
where $S^{A,B,R}$ must be contravariant in $A$ but covariant in
$B$ and $R$. The liftings are defined by
\[
(f^{:A\rightarrow C})^{\downarrow P^{\bullet,B}}\triangleq f^{\downarrow S^{\bullet,B,P^{C,B}}}\bef\big(f^{\overline{\downarrow P^{\bullet,B}}}\big)^{\uparrow S^{A,B,\bullet}}\quad,\quad\quad(g^{:B\rightarrow D})^{\uparrow P^{A,\bullet}}\triangleq g^{\uparrow S^{A,\bullet,P^{A,B}}}\bef\big(g^{\overline{\uparrow P^{A,\bullet}}}\big)^{\uparrow S^{A,D,\bullet}}\quad.
\]
The inductive assumption is two-fold: first, that the recursively
used lifting to $P^{A,B}$ already satisfies the commutativity law.
Second, that the profunctor $S^{A,B,R}$ satisfies the commutativity
law with respect to \emph{any} two parameters (we will prove that
law for covariant type parameters in Section~\ref{subsec:Proofs-of-commutativity-for-bifunctor}).

Both sides of the commutativity law are functions of type $P^{C,B}\rightarrow P^{A,D}$
or equivalently (if we expand the type recursion) as functions of
type $S^{C,B,P^{C,B}}\rightarrow S^{A,D,P^{A,D}}$. To avoid mistakes,
we write out the type parameters in this computation:
\begin{align*}
{\color{greenunder}\text{expect to equal }g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}:}\quad & f^{\downarrow P^{\bullet,B}}\bef g^{\uparrow P^{A,\bullet}}=f^{\downarrow S^{\bullet,B,P^{C,B}}}\bef\gunderline{\big(f^{\overline{\downarrow P^{\bullet,B}}}\big)^{\uparrow S^{A,B,\bullet}}\bef g^{\uparrow S^{A,\bullet,P^{A,B}}}}\bef\big(g^{\overline{\uparrow P^{A,\bullet}}}\big)^{\uparrow S^{A,D,\bullet}}\\
{\color{greenunder}^{B,R}\text{-commutativity of }S^{A,B,R}:}\quad & =\gunderline{f^{\downarrow S^{\bullet,B,P^{C,B}}}\bef g^{\uparrow S^{A,\bullet,P^{C,B}}}}\bef\big(f^{\overline{\downarrow P^{\bullet,B}}}\big)^{\uparrow S^{A,D,\bullet}}\bef\big(g^{\overline{\uparrow P^{A,\bullet}}}\big)^{\uparrow S^{A,D,\bullet}}\\
{\color{greenunder}^{A,B}\text{-commutativity of }S^{A,B,R}:}\quad & =g^{\uparrow S^{C,\bullet,P^{C,B}}}\bef f^{\downarrow S^{\bullet,D,P^{C,B}}}\bef\big(\gunderline{f^{\overline{\downarrow P^{\bullet,B}}}\bef g^{\overline{\uparrow P^{A,\bullet}}}}\big)^{\uparrow S^{A,D,\bullet}}\\
{\color{greenunder}\text{inductive assumption}:}\quad & =g^{\uparrow S^{C,\bullet,P^{C,B}}}\bef f^{\downarrow S^{\bullet,D,P^{C,B}}}\bef\big(g^{\overline{\uparrow P^{C,\bullet}}}\bef f^{\overline{\downarrow P^{\bullet,D}}}\gunderline{\big)^{\uparrow S^{A,D,\bullet}}}\\
{\color{greenunder}\text{composition under }^{\uparrow S^{A,D,\bullet}}:}\quad & =g^{\uparrow S^{C,\bullet,P^{C,B}}}\bef\gunderline{f^{\downarrow S^{\bullet,D,P^{C,B}}}\bef\big(g^{\overline{\uparrow P^{C,\bullet}}}\big)^{\uparrow S^{A,D,\bullet}}}\bef\big(f^{\overline{\downarrow P^{\bullet,D}}}\big)^{\uparrow S^{A,D,\bullet}}\\
{\color{greenunder}^{A,R}\text{-commutativity of }S^{A,B,R}:}\quad & =g^{\uparrow S^{C,\bullet,P^{C,B}}}\bef\big(g^{\overline{\uparrow P^{C,\bullet}}}\big)^{\uparrow S^{C,D,\bullet}}\bef f^{\downarrow S^{\bullet,D,P^{C,D}}}\bef\big(f^{\overline{\downarrow P^{\bullet,D}}}\big)^{\uparrow S^{A,D,\bullet}}\\
{\color{greenunder}\text{definitions of liftings}:}\quad & =g^{\uparrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,D}}\quad.
\end{align*}

This concludes the proof of the profunctor commutativity law.

\subsection{Commutativity laws for bifunctors and bi-contrafunctors\label{subsec:Proofs-of-commutativity-for-bifunctor}}

A bi-contrafunctor $P^{A,B}$ is a type constructor contravariant
with respect to both $A$ and $B$. The commutativity law for bi-contrafunctors
is formulated as 
\begin{align}
{\color{greenunder}\text{commutativity law of }P:}\quad & \text{cmap}_{P^{\bullet,D}}(f^{:A\rightarrow C})\bef\text{cmap}_{P^{A,\bullet}}(g^{:B\rightarrow D})=\text{cmap}_{P^{C,\bullet}}(g)\bef\text{cmap}_{P^{\bullet,B}}(f)\quad.\nonumber \\
{\color{greenunder}\text{shorter notation}:}\quad & f^{\downarrow P^{\bullet,D}}\bef g^{\downarrow P^{A,\bullet}}=g^{\downarrow P^{C,\bullet}}\bef f^{\downarrow P^{\bullet,B}}\quad.\label{eq:bi-contrafunctor-commutativity-law}
\end{align}

\begin{wrapfigure}{l}{0.43\columnwidth}%
\vspace{-1.7\baselineskip}
\[
\xymatrix{\xyScaleY{2.5pc}\xyScaleX{6.0pc}P^{C,D}\ar[r]\sp(0.55){\text{cmap}_{P^{\bullet,D}}(f^{:A\rightarrow C})~~~}\ar[d]\sb(0.45){\text{cmap}_{P^{C,\bullet}}(g^{:B\rightarrow D})} & P^{A,D}\ar[d]\sb(0.45){\text{cmap}_{P^{A,\bullet}}(g)}\\
P^{C,B}\ar[r]\sb(0.45){~~~~\text{cmap}_{P^{\bullet,B}}(f^{:A\rightarrow C})} & P^{A,B}
}
\]

\vspace{-1.7\baselineskip}
\end{wrapfigure}%

Any bifunctor or bi-contrafunctor whose type expression is built up
using the six type constructions (Table~\ref{tab:six-pure-type-constructions})
will satisfy its commutativity law. To prove that, we need to repeat
the same calculations as for profunctors in Section~\ref{subsec:Proof-of-the-profunctor-commutativity-law}
except for changing \lstinline!cmap! into \lstinline!fmap! or back
when needed. We only need to check that the proof will still work
after such changes. Looking over the proof in Section~\ref{subsec:Proof-of-the-profunctor-commutativity-law},
we find that we never used the functor or contrafunctor \emph{composition
law} for $P$. The only usage of the composition law was with respect
to the type parameter $R$ in the structure functor $S^{A,B,R}$ of
the recursive type construction. However, in all cases $S^{A,B,R}$
needs to be \emph{covariant} in $R$ because that is required by the
recursive type equation. So, we are assured that the same proof with
minor changes will work for bifunctors and bi-contrafunctors.

The same techniques and proofs apply to type constructors with more
than two type parameters.

\section{Naturality laws for fully parametric functions\label{sec:Naturality-laws-for-fully-parametric-functions}}

The goal of this section is to derive and prove a naturality law for
any function implemented via fully parametric code. Simple examples
of such functions are the \lstinline!map!, \lstinline!filter!, and
\lstinline!fold! methods for the \lstinline!Option! functor, whose
type signatures can be written as
\begin{align*}
 & \text{fmap}_{\text{Opt}}^{A,B}:\left(A\rightarrow B\right)\rightarrow\text{Opt}^{A}\rightarrow\text{Opt}^{B}\quad,\\
 & \text{filt}_{\text{Opt}}^{A}:\left(A\rightarrow\bbnum 2\right)\rightarrow\text{Opt}^{A}\rightarrow\text{Opt}^{A}\quad,\\
 & \text{fold}_{\text{Opt}}^{A,B}:B\times\left(A\times B\rightarrow B\right)\rightarrow\text{Opt}^{A}\rightarrow B\quad.
\end{align*}
These methods satisfy appropriate naturality laws \textemdash{} one
law per type parameter. When a method\textsf{'}s type signature is that of
a natural transformation between functors (or between contrafunctors),
the naturality laws have the form derived in Section~\ref{subsec:Naturality-laws-and-natural-transformations}.
For instance, fixing the type parameter $A$ in the \lstinline!fmap!
method, we obtain a type signature of the form $F^{B}\rightarrow G^{B}$
where $F^{\bullet}$ and $G^{\bullet}$ are functors:
\[
\text{fmap}_{\text{Opt}}^{B}:F^{B}\rightarrow G^{B}\quad,\quad\quad F^{B}\triangleq A\rightarrow B\quad,\quad\quad G^{B}\triangleq\text{Opt}^{A}\rightarrow\text{Opt}^{B}\quad.
\]
The corresponding naturality law (which is equivalent to the functor
composition law) is
\[
f^{\uparrow F}\bef\text{fmap}_{\text{Opt}}=\text{fmap}_{\text{Opt}}\bef f^{\uparrow G}\quad.
\]
However, the type signature of \lstinline!fold! is not of the form
$P^{B}\rightarrow Q^{B}$ with any functors or contrafunctors $P$,
$Q$. In general, it is not obvious how to write the naturality law
for transformations with such type signatures. The parametricity theorem
provides a recipe for writing naturality laws and also gives a proof
that fully parametric functions always satisfy their naturality laws.

The key insight for deriving that theorem is that type signatures
of a fully parametric function must always contain type parameters
in either covariant or contravariant positions. So, any such type
signature may be written as $P^{A,A}\rightarrow Q^{A,A}$ using some
\emph{profunctors} $P$ and $Q$. For instance, the type signature
of \lstinline!fold! with respect to the type parameter $B$ (with
the parameter $A$ fixed) is written as
\begin{equation}
\text{fold}_{\text{Opt}}^{B}:P^{B,B}\rightarrow Q^{B,B}\quad,\quad\text{where}\quad P^{X,Y}\triangleq Y\times\left(A\times X\rightarrow Y\right)\quad,\quad\quad Q^{X,Y}\triangleq\text{Opt}^{A}\rightarrow Y\quad.\label{eq:fold-type-signature-via-profunctors-derivation1}
\end{equation}

The next task is to motivate the naturality law for transformations
having type $\forall A.\,P^{A,A}\rightarrow Q^{A,A}$, where the type
parameters of the profunctors are set to be the same. Such transformations
are called \textsf{``}dinatural\textsf{''}. (\emph{Natural} transformations between
profunctors $P$ and $Q$ have a different type signature, namely
$\forall(A,B).\,P^{A,B}\rightarrow Q^{A,B}$, where the type parameters
$A$, $B$ must be independent).

\subsection{Dinatural transformations between profunctors}

\index{dinatural transformation}A \textbf{dinatural transformation}
is a function $t^{A}:P^{A,A}\rightarrow Q^{A,A}$, where $P^{X,Y}$
and $Q^{X,Y}$ are profunctors contravariant in $X$ and covariant
in $Y$. A dinatural transformation $t$ must satisfy the naturality
law
\begin{equation}
(f^{:A\rightarrow B})^{\downarrow P^{\bullet,A}}\bef t^{A}\bef f^{\uparrow Q^{A,\bullet}}=f^{\uparrow P^{B,\bullet}}\bef t^{B}\bef f^{\downarrow Q^{\bullet,B}}\quad.\label{eq:naturality-law-for-dinatural-transformations-of-profunctors}
\end{equation}

\begin{wrapfigure}{l}{0.43\columnwidth}%
\vspace{-2\baselineskip}
\[
\xymatrix{\xyScaleY{1.8pc}\xyScaleX{2.5pc} & P^{A,A}\ar[r]\sp(0.5){t} & Q^{A,A}\ar[rd]\sb(0.45){\text{xmap}_{Q}(\text{id})(f)}\\
P^{B,A}\ar[rd]\sp(0.55){\text{xmap}_{P}(\text{id})(f)}\ar[ru]\sb(0.55){\text{xmap}_{P}(f)(\text{id})} &  &  & Q^{A,B}\\
 & P^{B,B}\ar[r]\sp(0.5){t} & Q^{B,B}\ar[ru]\sp(0.45){\text{xmap}_{Q}(f)(\text{id})}
}
\]

\vspace{-0.7\baselineskip}
\end{wrapfigure}%

To build up intuition for that law (see diagram at left), compare
the laws of natural transformations $t^{A}:F^{A}\rightarrow G^{A}$
when $F^{\bullet}$ and $G^{\bullet}$ are functors,
\begin{equation}
t^{A}\bef(f^{:A\rightarrow B})^{\uparrow G}=(f^{:A\rightarrow B})^{\uparrow F}\bef t^{B}\quad,\label{eq:naturality-law-for-case-of-functors}
\end{equation}
and when $F^{\bullet}$ and $G^{\bullet}$ are contrafunctors,
\begin{equation}
(f^{:A\rightarrow B})^{\downarrow F}\bef t^{A}=t^{B}\bef(f^{:A\rightarrow B})^{\downarrow G}\quad.\label{eq:naturality-law-for-case-of-contrafunctors}
\end{equation}
We would obtain naturality laws of that form if we could fix the type
parameter $A$ in the profunctors $P^{A,B}$ and $Q^{A,B}$ and consider
then as functors with respect to the type parameter $B$. However,
this requires us to have a transformation $\tilde{t}:P^{A,B}\rightarrow Q^{A,B}$
defined for arbitrary (not necessarily equal) type parameters $A$,
$B$. We do not have such a function: we are only given a transformation
with the type signature $t:P^{A,A}\rightarrow Q^{A,A}$, defined only
in the \textsf{``}diagonal\textsf{''} case\footnote{As a memory aid, we may consider the word \textsf{``}dinaturality\textsf{''} to be
a shorthand for \textsf{``}diagonal naturality\textsf{''}.} $A=B$. As a rule, we cannot extend the code of $t$ to some $\tilde{t}$
that works with arbitrary type parameters $A$, $B$. A law for $t$
must be an equation that somehow transforms both type parameters of
$t$ by using a lifted function $f^{:A\rightarrow B}$. 

The naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
combines the laws~(\ref{eq:naturality-law-for-case-of-functors})
and~(\ref{eq:naturality-law-for-case-of-contrafunctors}) in the
way required for all types to match. On the other hand, the laws~(\ref{eq:naturality-law-for-case-of-functors})
and~(\ref{eq:naturality-law-for-case-of-contrafunctors}) will follow
from Eq.~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
when $P^{A,A}$ and $Q^{A,A}$ are both functors or both contrafunctors
in $A$. To further motivate the law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}),
we will now derive the known forms of naturality laws for \lstinline!filter!
and \lstinline!fold! for arbitrary filterable or foldable functors
$F$.

\subsubsection{Example \label{subsec:Example-derive-naturality-of-filter-from-dinaturality}\ref{subsec:Example-derive-naturality-of-filter-from-dinaturality}
(naturality law of \lstinline!filter!)\index{solved examples}}

To derive the naturality law of \lstinline!filter!, express \lstinline!filter!\textsf{'}s
type signature through profunctors $P$ and $Q$ as
\[
\text{filt}_{F}^{A}:P^{A,A}\rightarrow Q^{A,A}\quad,\quad\quad P^{X,Y}\triangleq(X\rightarrow\bbnum 2)\quad,\quad\quad Q^{X,Y}\triangleq F^{X}\rightarrow F^{Y}\quad,
\]
and then write the law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}),
\begin{equation}
f^{\downarrow P^{\bullet,A}}\bef\text{filt}_{F}^{A}\bef f^{\uparrow Q^{A,\bullet}}\overset{?}{=}f^{\uparrow P^{B,\bullet}}\bef\text{filt}_{F}^{B}\bef f^{\downarrow Q^{\bullet,B}}\quad.\label{eq:filter-law-via-dinatural-transformation-derivation1}
\end{equation}
It remains to substitute the code for the liftings using the specific
types of $P$ and $Q$:
\begin{align*}
(f^{:A\rightarrow B})^{\downarrow P^{\bullet,A}}=p^{:B\rightarrow\bbnum 2}\rightarrow f\bef p\quad, & \quad\quad f^{\uparrow P^{B,\bullet}}=\text{id}\quad,\\
(f^{:A\rightarrow B})^{\downarrow Q^{\bullet,B}}=q^{:F^{B}\rightarrow F^{B}}\rightarrow f^{\uparrow F}\bef q\quad, & \quad\quad f^{\uparrow Q^{A,\bullet}}=q^{:F^{A}\rightarrow F^{A}}\rightarrow q\bef f^{\uparrow F}\quad.
\end{align*}
Then we rewrite Eq.~(\ref{eq:filter-law-via-dinatural-transformation-derivation1})
as
\[
(p\rightarrow f\bef p)\bef\text{filt}_{F}\bef(q\rightarrow q\bef f^{\uparrow F})\overset{?}{=}\text{id}\bef\text{filt}_{F}\bef(q\rightarrow f^{\uparrow F}\bef q)\quad.
\]
To simplify the form of the naturality law, apply both sides to an
arbitrary $p^{:P^{B,A}}=p^{:B\rightarrow\bbnum 2}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & p\triangleright(p\rightarrow f\bef p)\bef\text{filt}_{F}\bef(q\rightarrow q\bef f^{\uparrow F})\\
{\color{greenunder}\triangleright\text{-notation}:}\quad & \quad=\gunderline{p\triangleright(p}\rightarrow f\bef p)\triangleright\text{filt}_{F}\triangleright(q\rightarrow q\bef f^{\uparrow F})\\
{\color{greenunder}\text{apply functions}:}\quad & \quad=\gunderline{(f\bef p)\triangleright\text{filt}_{F}}\triangleright(q\rightarrow q\bef f^{\uparrow F})=\gunderline{\text{filt}_{F}(f\bef p)\triangleright(q}\rightarrow q\bef f^{\uparrow F})=\text{filt}_{F}(f\bef p)\bef f^{\uparrow F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & p\triangleright\gunderline{\text{id}\bef}\text{filt}_{F}\bef(q\rightarrow f^{\uparrow F}\bef q)=p\triangleright\text{filt}_{F}\triangleright(q\rightarrow f^{\uparrow F}\bef q)\\
 & \quad=\gunderline{\text{filt}_{F}(p)\triangleright(q}\rightarrow f^{\uparrow F}\bef q)=f^{\uparrow F}\bef\text{filt}_{F}(p)\quad.
\end{align*}
So, we obtained the naturality law~(\ref{eq:naturality-law-of-filter})
of \lstinline!filter!,
\[
\text{filt}_{F}(f\bef p)\bef f^{\uparrow F}=f^{\uparrow F}\bef\text{filt}_{F}(p)\quad.
\]


\subsubsection{Example \label{subsec:Example-derive-naturality-of-fold-from-dinaturality}\ref{subsec:Example-derive-naturality-of-fold-from-dinaturality}
(naturality law of \lstinline!fold!)}

To derive the naturality law of \lstinline!fold! with respect to
the type parameter $B$, we begin with Eq.~(\ref{eq:fold-type-signature-via-profunctors-derivation1})
that shows the type signature of \lstinline!fold! as a transformation
of type $P^{B,B}\rightarrow Q^{B,B}$ between profunctors $P$, $Q$.
Since the type parameter $A$ is fixed, the naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
now involves an arbitrary function $f^{:B\rightarrow C}$,
\begin{equation}
(f^{:B\rightarrow C})^{\downarrow P^{\bullet,B}}\bef\text{fold}_{F}^{B}\bef f^{\uparrow Q^{B,\bullet}}=f^{\uparrow P^{C,\bullet}}\bef\text{fold}_{F}^{C}\bef f^{\downarrow Q^{\bullet,C}}\quad.\label{eq:fold-naturality-from-profunctor-derivation2}
\end{equation}
The lifting code required for the profunctors $P^{X,Y}\triangleq Y\times\left(A\times X\rightarrow Y\right)$
and $Q^{X,Y}\triangleq F^{A}\rightarrow Y$ is
\begin{align*}
(f^{:B\rightarrow C})^{\downarrow P^{\bullet,B}}=\text{id}^{B}\boxtimes(h^{:A\times C\rightarrow B}\rightarrow a^{:A}\times b^{:B}\rightarrow h(a\times f(b)))\quad, & \quad\quad f^{\uparrow P^{C,\bullet}}=f\boxtimes(h^{:A\times C\rightarrow B}\rightarrow h\bef f)\quad,\\
(f^{:B\rightarrow C})^{\downarrow Q^{\bullet,C}}=\text{id}\quad, & \quad\quad f^{\uparrow Q^{B,\bullet}}=q^{:F^{A}\rightarrow B}\rightarrow q\bef f\quad.
\end{align*}
Substituting this code into the law~(\ref{eq:fold-naturality-from-profunctor-derivation2})
and applying to an arbitrary $p^{:P^{C,B}}=z^{:B}\times h^{:A\times C\rightarrow B}$,
we get
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (z\times h)\triangleright\gunderline{(f^{:B\rightarrow C})^{\downarrow P^{\bullet,B}}}\bef\text{fold}_{F}\bef\gunderline{f^{\uparrow Q^{B,\bullet}}}\\
{\color{greenunder}\text{definitions of liftings}:}\quad & \quad=(z\times h)\triangleright(\text{id}\boxtimes(h\rightarrow a\times b\rightarrow h(a\times f(b))))\gunderline{\bef}\text{fold}_{F}\gunderline{\bef}(q\rightarrow q\bef f)\\
{\color{greenunder}\triangleright\text{-notation}:}\quad & \quad=(z\times h)\triangleright(\text{id}\boxtimes(h\rightarrow a\times b\rightarrow h(a\times f(b))))\triangleright\text{fold}_{F}\triangleright(q\rightarrow q\bef f)\\
{\color{greenunder}\text{apply functions}:}\quad & \quad=\text{fold}_{F}(z\times(a\times b\rightarrow h(a\times f(b))))\bef f\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & (z\times h)\triangleright\gunderline{f^{\uparrow P^{C,\bullet}}}\bef\text{fold}_{F}\bef\gunderline{f^{\downarrow Q^{\bullet,C}}}=(z\times h)\triangleright(f\boxtimes(h^{:A\times C\rightarrow B}\rightarrow h\bef f))\bef\text{fold}_{F}\bef\text{id}\\
{\color{greenunder}\text{apply functions}:}\quad & \quad=\text{fold}_{F}(f(z)\times(h\bef f))\quad.
\end{align*}
We obtained a naturality law of \lstinline!fold!,
\[
\text{fold}_{F}(f(z)\times(h\bef f))=\text{fold}_{F}(z\times(a\times b\rightarrow h(a\times f(b))))\bef f\quad.
\]

These examples illustrate how we may derive the form of the naturality
law for any type signature by specializing the general law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
to specific profunctors $P$ and $Q$.

In the next subsections, we will prove\footnote{The proof builds upon ideas from the paper by E.~S.~Bainbridge et
al., \textsf{``}Functorial polymorphism\textsf{''} (Theor.~Comp.~Sci.~70, p.~35,
1990), see \texttt{\href{https://www.sciencedirect.com/science/article/pii/0304397590901517}{https://www.sciencedirect.com/science/article/pii/0304397590901517}}.
A more rigorous but significantly more complicated proof was given
by Joachim de Lataillade\index{Joachim de Lataillade} in the paper
\textsf{``}Dinatural terms in System $F$\textsf{''}, see \texttt{\href{https://www.irif.fr/~delatail/dinat.pdf}{https://www.irif.fr/$\sim$delatail/dinat.pdf}},
based on deriving the syntactic form of naturality laws in full detail. } that any fully parametric function $t^{A}:P^{A,A}\rightarrow Q^{A,A}$
is dinatural, i.e., satisfies its naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}).
Since the form of the law depends only on the type signature, all
functions $t^{A}$ satisfy the same naturality law. 

The restriction to \emph{fully parametric} programs\index{fully parametric!program}
is essential because it excludes, for instance, any use of mutable
variables, \lstinline!null! values, exceptions, run-time type identification,
run-time code loading, or values defined in external libraries that
are not known to be fully parametric. Code that uses those features
of Scala is not covered by parametricity theorems and may violate
naturality laws, as we have seen elsewhere in this book.

\paragraph{Dinatural expressions}

We note that the type constructor $P^{A,A}\rightarrow Q^{A,A}$ is
neither covariant nor contravariant in $A$, but it is itself a profunctor
that we may denote by $T$. The naturality law is simpler when formulated
via $T$:

\subsubsection{Statement \label{subsec:Statement-dinatural-expression-wedge-law}\ref{subsec:Statement-dinatural-expression-wedge-law}}

Using the profunctor $T^{X,Y}\triangleq P^{Y,X}\rightarrow Q^{X,Y}$,
the naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
is written as

\begin{wrapfigure}[9]{l}{0.24\columnwidth}%
\vspace{-1.5\baselineskip}
\[
\xymatrix{\xyScaleY{2.0pc}\xyScaleX{3.0pc} & T^{B,B}\ar[d]\sp(0.37){f^{\downarrow T^{\bullet,B}}}\\
T^{A,A}\ar[r]\sp(0.5){~~~f^{\uparrow T^{A,\bullet}}} & T^{A,B}
}
\]

\vspace{-0\baselineskip}
\end{wrapfigure}%

~\vspace{-0.8\baselineskip}

\begin{equation}
t^{A}\triangleright f^{\uparrow T^{A,\bullet}}=t^{B}\triangleright f^{\downarrow T^{\bullet,B}}\quad,\label{eq:dinaturality-wedge-law}
\end{equation}
\vspace{-0.8\baselineskip}

\noindent where the transformation $t$ is viewed as a value of type
$\forall Z.\,T^{Z,Z}$, while the function $f^{:A\rightarrow B}$
is arbitrary. This law is known as the \textbf{wedge law}\index{wedge law}
for $t$.

\subparagraph{Proof}

The two liftings for $T$ are expressed as
\begin{align*}
f^{\uparrow T^{A,\bullet}}:T^{A,A}\rightarrow T^{A,B}\quad, & \quad\quad t^{A}\triangleright f^{\uparrow T^{A,\bullet}}=p^{:P^{B,A}}\rightarrow t^{A}(p\triangleright f^{\downarrow P^{\bullet,A}})\triangleright f^{\uparrow Q^{A,\bullet}}=f^{\downarrow P^{\bullet,A}}\bef t^{A}\bef f^{\uparrow Q^{A,\bullet}}\quad,\\
f^{\downarrow T^{\bullet,B}}:T^{B,B}\rightarrow T^{A,B}\quad, & \quad\quad t^{B}\triangleright f^{\uparrow T^{\bullet,B}}=p^{:P^{B,A}}\rightarrow t^{B}(p\triangleright f^{\uparrow P^{B,\bullet}})\triangleright f^{\downarrow Q^{\bullet,B}}=f^{\uparrow P^{B,\bullet}}\bef t^{B}\bef f^{\downarrow Q^{\bullet,B}}\quad.
\end{align*}
It follows that Eq.~(\ref{eq:dinaturality-wedge-law}) is equivalent
to Eq.~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}),
which proves the statement.

Expressions $t$ of type $\forall Z.\,T^{Z,Z}$ satisfying the wedge
law~(\ref{eq:dinaturality-wedge-law}) are called \textbf{dinatural
expressions}\index{dinatural expression}. The wedge law formulates
the dinaturality condition for any expression $t$ with a type parameter,
whether $t$ is a \textsf{``}transformation\textsf{''} (i.e., a function) or not.
It can be derived from the parametricity theorem that any fully parametric
expression of type $\forall Z.\,T^{Z,Z}$ is dinatural.

\subsection{Composition of natural and dinatural transformations}

In the proof of the parametricity theorem, we will need to compose
some dinatural transformations. Composition of \emph{natural} transformations
is easily shown to be again natural. However, proving the same property
for dinatural transformations requires additional assumptions. To
show why, first consider the composition of two natural transformations
$u:P^{A}\rightarrow Q^{A}$ and $v:Q^{A}\rightarrow R^{A}$.

\subsubsection{Statement \label{subsec:Statement-natural-transformations-composition}\ref{subsec:Statement-natural-transformations-composition}}

The composition $t\triangleq u\bef v$ is a natural transformation,
$t:P^{A}\rightarrow R^{A}$, assuming that $u:P^{A}\rightarrow Q^{A}$
and $v:Q^{A}\rightarrow R^{A}$ satisfy their naturality laws, and
that $P$, $Q$, $R$ are functors.

\subparagraph{Proof}

The naturality laws for $u$ and $v$ are written with an arbitrary
$f^{:A\rightarrow B}$ as
\[
f^{\uparrow P}\bef u=u\bef f^{\uparrow Q}\quad,\quad\quad f^{\uparrow Q}\bef v=v\bef f^{\uparrow R}\quad.
\]
The required naturality law for $t$ is derived by a direct calculation,

\begin{wrapfigure}[6]{l}{0.4\columnwidth}%
\vspace{-1.5\baselineskip}
\[
\xymatrix{\xyScaleY{2.0pc}\xyScaleX{3.0pc}P^{A}\ar[d]\sp(0.45){f^{\uparrow P}}\ar[r]\sp(0.5){u} & Q^{A}\ar[d]\sp(0.45){f^{\uparrow Q}}\ar[r]\sp(0.5){v} & R^{A}\ar[d]\sp(0.45){f^{\uparrow R}}\\
P^{B}\ar[r]\sp(0.5){u} & Q^{B}\ar[r]\sp(0.5){v} & R^{B}
}
\]

\vspace{-0\baselineskip}
\end{wrapfigure}%

~\vspace{-1.8\baselineskip}

\begin{align*}
{\color{greenunder}\text{expect to equal }t\bef f^{\uparrow R}:}\quad & f^{\uparrow P}\bef\gunderline t=\gunderline{f^{\uparrow P}\bef u}\bef v\\
{\color{greenunder}\text{naturality of }u:}\quad & =u\bef\gunderline{f^{\uparrow Q}\bef v}\\
{\color{greenunder}\text{naturality of }v:}\quad & =\gunderline{u\bef v}\bef f^{\uparrow R}=t\bef f^{\uparrow R}\quad.
\end{align*}
This calculation shows that the above type diagram commutes.

In the proof of Statement~\ref{subsec:Statement-natural-transformations-composition},
we derived the naturality law for the composed transformation $t\triangleq u\bef v$
directly from the naturality laws of $u$ and $v$ with no further
assumptions. In particular, we did not need to assume that $u$ and
$v$ are implemented by fully parametric code. We will now see that
the composition property for dinatural transformations cannot be derived
in the same way. 

Consider two dinatural transformations $u:P^{A,A}\rightarrow Q^{A,A}$
and $v:Q^{A,A}\rightarrow R^{A,A}$, where $P$, $Q$, $R$ are any
profunctors. The naturality laws of $u$ and $v$ are written as
\[
f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef f^{\uparrow Q^{A,\bullet}}=f^{\uparrow P^{B,\bullet}}\bef u^{B}\bef f^{\downarrow Q^{\bullet,B}}\quad,\quad\quad f^{\downarrow Q^{\bullet,A}}\bef v^{A}\bef f^{\uparrow R^{A,\bullet}}=f^{\uparrow Q^{B,\bullet}}\bef v^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.
\]
 The composition $t\triangleq u\bef v$ has type signature $t:P^{A,A}\rightarrow R^{A,A}$,
and so its naturality law is
\begin{equation}
f^{\downarrow P^{\bullet,A}}\bef t^{A}\bef f^{\uparrow R^{A,\bullet}}=f^{\uparrow P^{B,\bullet}}\bef t^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.\label{eq:dinaturality-naturality-law-for-t-derivation1}
\end{equation}
Can we derive that law by combining the naturality laws of $u$ and
$v$? Note that \emph{both} sides of the laws of $u$ and $v$ contain
a lifting of $f$ to $Q$, while Eq.~(\ref{eq:dinaturality-naturality-law-for-t-derivation1})
does not contain that lifting. The function $f^{:A\rightarrow B}$
does not satisfy any known equations that we could use in the proof.
So, we do not have any law that would allow us to eliminate $f^{\uparrow Q}$
or $f^{\downarrow Q}$ from the laws of $u$ and $v$. (If, e.g.,
we knew that $f$ is invertible, we would use a function $g^{:B\rightarrow A}$
satisfying $f\bef g=\text{id}$ and $g\bef f=\text{id}$ and so eliminate
$f$. But Eq.~(\ref{eq:dinaturality-naturality-law-for-t-derivation1})
must hold for arbitrary functions $f^{:A\rightarrow B}$.) No matter
how we combine those laws, starting from one side or from the other,
the resulting expressions will always contain $f^{\uparrow Q}$ and/or
$f^{\downarrow Q}$. Similarly, starting from any side of Eq.~(\ref{eq:dinaturality-naturality-law-for-t-derivation1}),
we cannot \emph{introduce} the function $f^{\uparrow Q}$ or $f^{\downarrow Q}$
into the expression. So, we are unable to obtain an expression in
which we could use the laws of $u$ and $v$. 

\begin{wrapfigure}[12]{l}{0.55\columnwidth}%
\vspace{-1.6\baselineskip}
\[
\xymatrix{\xyScaleY{2.3pc}\xyScaleX{2.3pc} & P^{A,A}\ar[r]\sp(0.5){u} & Q^{A,A}\ar[r]\sp(0.5){v}\ar[rd]\sb(0.45){f^{\uparrow Q^{A,\bullet}}\negthickspace\negthickspace} & R^{A,A}\ar[rd]\sp(0.45){f^{\uparrow R^{A,\bullet}}}\\
P^{B,A}\ar[ru]\sp(0.5){f^{\downarrow P^{\bullet,A}}\negthickspace\negthickspace\negthickspace\negthickspace}\ar[rd]\sb(0.5){f^{\uparrow P^{B,\bullet}}\negthickspace\negthickspace} & Q^{B,A}\ar[ru]\sb(0.55){\negthickspace\negthickspace f^{\downarrow Q^{\bullet,A}}}\ar[rd]\sp(0.5){f^{\uparrow Q^{B,\bullet}}\negthickspace\negthickspace} &  & Q^{A,B} & R^{A,B}\\
 & P^{B,B}\ar[r]\sp(0.5){u} & Q^{B,B}\ar[r]\sp(0.5){v}\ar[ru]\sp(0.5){f^{\downarrow Q^{\bullet,B}}\negthickspace\negthickspace\negthickspace} & R^{B,B}\ar[ru]\sb(0.55){\negthickspace\negthickspace f^{\downarrow R^{\bullet,B}}}
}
\]

\vspace{-1.1\baselineskip}
\caption{Composition of dinatural transformations.\label{fig:Type-diagram-for-composition-of-dinatural-transformations}}

\vspace{-0\baselineskip}
\end{wrapfigure}%

\noindent The type diagram in Figure~\ref{fig:Type-diagram-for-composition-of-dinatural-transformations}
combines the diagrams for the laws of $u$ and $v$ in an attempt
to derive the law of $t$. We can see a value of type $Q^{B,A}$ acting
as a \textsf{``}source\textsf{''} of some arrows. It means that we are required to
have a value of type $Q^{B,A}$ in order for the complete diagram
to commute. But we are given only an arbitrary value $p:P^{B,A}$,
and we are required to fill out the remainder of the diagram starting
from that value. We cannot compute a value of type $Q^{B,A}$ from
$p:P^{B,A}$ because we are given only a \textsf{``}diagonal\textsf{''} transformation
$P^{A,A}\rightarrow Q^{A,A}$ and not a function of type $P^{X,Y}\rightarrow Q^{X,Y}$
with arbitrary type parameters $X$, $Y$. So, we cannot prove in
this way that the diagram commutes. 

Nevertheless, it is true that the composition of dinatural transformations
is dinatural \textemdash{} as long as the code is fully parametric,
which is always assumed by the parametricity theorem.

\subsubsection{Statement \label{subsec:Statement-dinatural-transformation-composition}\ref{subsec:Statement-dinatural-transformation-composition}}

Given any profunctors $P$, $Q$, $R$ and any two fully parametric
dinatural transformations $u:P^{A,A}\rightarrow Q^{A,A}$ and $v:Q^{A,A}\rightarrow R^{A,A}$,
the transformation $t\triangleq u\bef v$ is also dinatural.

\subparagraph{Proof}

We need to show that the naturality law holds for $t$: for any $p^{:P^{B,A}}$
and $f^{:A\rightarrow B}$,
\begin{align}
 &  & p\triangleright f^{\downarrow P^{\bullet,A}}\bef\gunderline{t^{A}}\bef f^{\uparrow R^{A,\bullet}} & \overset{?}{=}p\triangleright f^{\uparrow P^{B,\bullet}}\bef\gunderline{t^{B}}\bef f^{\downarrow R^{\bullet,B}}\quad.\nonumber \\
{\color{greenunder} & \text{substitute }t\triangleq u\bef v:}\quad & p\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef v^{A}\bef f^{\uparrow R^{A,\bullet}} & \overset{?}{=}p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\bef v^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.\label{eq:dinaturality-composition-derivation1}
\end{align}
It is given that the naturality laws of $u$ and $v$ both hold: for
any $p^{:P^{B,A}}$ and $q^{:Q^{B,A}}$,
\[
p\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef f^{\uparrow Q^{A,\bullet}}=p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\bef f^{\downarrow Q^{\bullet,B}}\quad,\quad\quad q\triangleright f^{\downarrow Q^{\bullet,A}}\bef v^{A}\bef f^{\uparrow R^{A,\bullet}}=q\triangleright f^{\uparrow Q^{B,\bullet}}\bef v^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.
\]
We will be able to establish Eq.~(\ref{eq:dinaturality-composition-derivation1})
only if we find a suitable value $q:Q^{B,A}$ that correctly fits
the inner square of the type diagram in Figure~\ref{fig:Type-diagram-for-composition-of-dinatural-transformations},
which can be written as the equation

\begin{wrapfigure}{l}{0.32\columnwidth}%
\vspace{-1.6\baselineskip}
\[
\xymatrix{\xyScaleY{2.3pc}\xyScaleX{2.6pc} & Q^{A,A}\ar[rd]\sb(0.45){f^{\uparrow Q^{A,\bullet}}\negthickspace\negthickspace}\\
Q^{B,A}\ar[ru]\sb(0.55){\negthickspace\negthickspace f^{\downarrow Q^{\bullet,A}}}\ar[rd]\sp(0.5){f^{\uparrow Q^{B,\bullet}}\negthickspace\negthickspace} &  & Q^{A,B}\\
 & Q^{B,B}\ar[ru]\sp(0.5){f^{\downarrow Q^{\bullet,B}}\negthickspace\negthickspace\negthickspace}
}
\]

\vspace{-1.6\baselineskip}
\end{wrapfigure}%

~
\[
q\triangleright f^{\downarrow Q^{\bullet,A}}\bef f^{\uparrow Q^{A,\bullet}}=q\triangleright f^{\uparrow Q^{B,\bullet}}\bef f^{\downarrow Q^{\bullet,B}}\quad.
\]

\noindent By the commutativity law of the profunctor $Q$ (see Section~\ref{subsec:Proof-of-the-profunctor-commutativity-law}),
this equation holds for \emph{any} value $q^{:Q^{B,A}}$. It remains
to show that a suitable value $q$ can be found such that we may replace
$p\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}$ by $q\triangleright f^{\downarrow Q^{\bullet,A}}$
and $p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}$ by $q\triangleright f^{\uparrow Q^{B,\bullet}}$
in Eq.~(\ref{eq:dinaturality-composition-derivation1}). In other
words, $q$ must satisfy the equations
\begin{equation}
\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad q\triangleright f^{\downarrow Q^{\bullet,A}}=p\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\quad,\quad\quad q\triangleright f^{\uparrow Q^{B,\bullet}}=p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\quad.\label{eq:dinaturality-conditions-on-q-derivation1}
\end{equation}
We will complete the proof of law if we are able to compute a suitable
value $q$ for \emph{any} given $f^{:A\rightarrow B}$, $p^{:P^{B,A}}$,
and $u^{A}:P^{A,A}\rightarrow Q^{A,A}$. However, such $q$ does not
always exist without further assumptions. A simple counterexample
is found by choosing the profunctor $Q^{X,Y}\triangleq X\rightarrow Y$
and setting
\[
u^{A}\triangleq\_^{:P^{A,A}}\rightarrow\text{id}^{:A\rightarrow A}\quad.
\]
With this choice of $u$, the value $p$ is ignored, and so we can
simplify Eq.~(\ref{eq:dinaturality-conditions-on-q-derivation1})
to
\[
f^{:A\rightarrow B}\bef q^{:B\rightarrow A}=\text{id}^{:A\rightarrow A}\quad,\quad\quad q^{:B\rightarrow A}\bef f^{:A\rightarrow B}=\text{id}^{:B\rightarrow B}\quad.
\]
This is possible only if the function $q^{:B\rightarrow A}$ is the
inverse of $f$; but, of course, not all functions $f$ are invertible.
Also, the existence of an inverse for a function $f^{:A\rightarrow B}$
means that the types $A$ and $B$ are \emph{equivalent} ($A\cong B$)
due to the isomorphism $f$, which is clearly not true for arbitrary
types $A$ and $B$.

To proceed with the proof, we use a trick: we first prove the naturality
law of $t$ when restricted to isomorphic types $A\cong B$ and to
invertible functions $f^{:A\rightarrow B}$. Then we use the fact
that a fully parametric function $t$ cannot use any type information
about $A$ and $B$ or any information about the function $f$, and
so the naturality law must actually hold for all types $A$, $B$
and for all $f^{:A\rightarrow B}$.

When the function $f^{:A\rightarrow B}$ has an inverse $g^{:B\rightarrow A}$,
we may define $q$ according to
\[
q^{:Q^{B,A}}\triangleq p^{:P^{B,A}}\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef g^{\downarrow Q^{\bullet,A}}\quad.
\]
This will satisfy the first requirement in Eq.~(\ref{eq:dinaturality-conditions-on-q-derivation1})
because
\begin{align*}
{\color{greenunder}\text{expect to equal }p\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}:}\quad & q\triangleright f^{\downarrow Q^{\bullet,A}}=p^{:P^{B,A}}\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef\gunderline{g^{\downarrow Q^{\bullet,A}}\bef f^{\downarrow Q^{\bullet,A}}}\\
{\color{greenunder}\text{composition under lifting}:}\quad & =p^{:P^{B,A}}\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef(\gunderline{g\bef f})^{\downarrow Q^{\bullet,A}}\\
{\color{greenunder}\text{use }g\bef f=\text{id}:}\quad & =p^{:P^{B,A}}\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\quad.
\end{align*}
The second of the requirements in Eq.~(\ref{eq:dinaturality-conditions-on-q-derivation1})
is then also satisfied:
\begin{align*}
{\color{greenunder}\text{expect to equal }p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}:}\quad & q\triangleright f^{\uparrow Q^{B,\bullet}}=p^{:P^{B,A}}\triangleright f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef g^{\downarrow Q^{\bullet,A}}\bef f^{\uparrow Q^{B,\bullet}}\\
{\color{greenunder}\text{commutativity law of }Q:}\quad & =p^{:P^{B,A}}\triangleright\gunderline{f^{\downarrow P^{\bullet,A}}\bef u^{A}\bef f^{\uparrow Q^{A,\bullet}}}\bef g^{\downarrow Q^{\bullet,B}}\\
{\color{greenunder}\text{naturality law of }u:}\quad & =p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\bef\gunderline{f^{\downarrow Q^{\bullet,B}}\bef g^{\downarrow Q^{\bullet,B}}}\\
{\color{greenunder}\text{composition under lifting}:}\quad & =p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\bef(\gunderline{f\bef g})^{\downarrow Q^{\bullet,B}}=p\triangleright f^{\uparrow P^{B,\bullet}}\bef u^{B}\quad.
\end{align*}

We can now substitute Eq.~(\ref{eq:dinaturality-conditions-on-q-derivation1})
into Eq.~(\ref{eq:dinaturality-composition-derivation1}) and obtain
\[
q\triangleright f^{\downarrow Q^{\bullet,A}}\bef v^{A}\bef f^{\uparrow R^{A,\bullet}}\overset{?}{=}q\triangleright f^{\uparrow Q^{B,\bullet}}\bef v^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.
\]
This holds due to the given naturality law of $v$. So, we have shown
that the naturality law~(\ref{eq:dinaturality-naturality-law-for-t-derivation1})
of $t$ is satisfied for \emph{invertible} functions $f^{:A\rightarrow B}$.

It remains to remove the assumption of invertibility of $f^{:A\rightarrow B}$.
At this point, we need to use the fact that the code of $u$ and $v$
is fully parametric. Then so is the code of $t\triangleq u\bef v$.
The naturality law of $t$ has the form of Eq.~(\ref{eq:dinaturality-naturality-law-for-t-derivation1}).
Both sides of that equation can be written out as some code that combines
the code of $t$ and the code of the required liftings of $f$. As
we have seen in Chapter~\ref{chap:Functors,-contrafunctors,-and}
and in the proof of the profunctor commutativity law (Section~\ref{subsec:Proof-of-the-profunctor-commutativity-law}),
all lifting code is fully parametric and uses the arbitrary function
$f$ only by applying $f$ to some arguments. So, the code corresponding
to both sides of Eq.~(\ref{eq:dinaturality-naturality-law-for-t-derivation1})
is fully parametric and involves the function $f$ as an unevaluated
value (a \textsf{``}free variable\index{free variable}\textsf{''}). We can symbolically
write that code as the equation 
\[
a(f)=b(f)\quad,
\]
where $a(f)$ and $b(f)$ are some expressions of type $P^{B,A}\rightarrow R^{A,B}$
that are built up from the nine code constructions from Table~\ref{tab:nine-pure-code-constructions}.
It follows that the code of $a(f)$ and $b(f)$ may apply the given
function $f$ but cannot use the inverse function $g$ assumed in
the proof above. So, the only possibility for the equation $a(f)=b(f)$
to hold is when the programs $a(f)$ and $b(f)$ are \emph{equal}
\emph{as symbolic expressions}\footnote{This step in the proof seems plausible but is not rigorously derived
in the present version of this book.} up to equivalence transformations $g=x\rightarrow g(x)$ and $y\triangleright(x\rightarrow g(x))=g(y)$.
So, the equation $a(f)=b(f)$ must hold for arbitrary functions $f$
and for arbitrary types $A$, $B$. 

This concludes the proof of Statement~\ref{subsec:Statement-dinatural-transformation-composition}.

The proof of the main parametricity theorem is made shorter if we
use the following property: If the type parameters of a natural transformation
are substituted by some profunctors, we will obtain a dinatural transformation.
Two versions of this property are proved next.

\subsubsection{Statement \label{subsec:Statement-dinatural-substitute-functor-in-natural}\ref{subsec:Statement-dinatural-substitute-functor-in-natural}}

Suppose $u^{A}:G^{A}\rightarrow H^{A}$ is a natural transformation,
and suppose $P^{X,Y}$ is some fully parametric profunctor. Define
the transformation
\[
t^{A}:G^{P^{A,A}}\rightarrow H^{P^{A,A}}\quad,\quad\quad t^{A}\triangleq u^{P^{A,A}}\quad,
\]
by using the unmodified code of the function $u^{A}$ with a substituted
type parameter, $P^{A,A}$ instead of $A$. Then $t$ is a dinatural
transformation between profunctors $G^{P^{A,A}}$ and $H^{P^{A,A}}$.

\subparagraph{Proof}

By assumption, the naturality law of $u$ holds for any $k^{:A\rightarrow B}$,
\[
k^{\uparrow G}\bef u^{B}=u^{A}\bef k^{\uparrow H}\quad.
\]
The naturality law of $t$ is an equality of functions of type $G^{P^{B,A}}\rightarrow H^{P^{A,B}}$
for an arbitrary $f^{:A\rightarrow B}$:
\[
\big(f^{\downarrow P^{\bullet,A}}\big)^{\uparrow G}\bef t^{A}\bef\big(f^{\uparrow P^{\bullet,A}}\big)^{\uparrow H}\overset{?}{=}\big(f^{\uparrow P^{B,\bullet}}\big)^{\uparrow G}\bef t^{B}\bef\big(f^{\downarrow P^{\bullet,B}}\big)^{\uparrow H}\quad.
\]
Begin the calculation with the left-hand side of that law:
\begin{align*}
{\color{greenunder}\text{use }t^{A}=u^{P^{A,A}}:}\quad & \big(f^{\downarrow P^{\bullet,A}}\big)^{\uparrow G}\bef\underline{t^{A}}\bef\big(f^{\uparrow P^{A,\bullet}}\big)^{\uparrow H}=\gunderline{\big(f^{\downarrow P^{\bullet,A}}\big)^{\uparrow G}\bef u^{P^{A,A}}}\bef\big(f^{\uparrow P^{A,\bullet}}\big)^{\uparrow H}\\
{\color{greenunder}\text{naturality of }u:}\quad & =u^{P^{B,A}}\bef\gunderline{\big(f^{\downarrow P^{\bullet,A}}\big)^{\uparrow H}\bef\big(f^{\uparrow P^{A,\bullet}}\big)^{\uparrow H}}=u^{P^{B,A}}\bef\big(f^{\downarrow P^{\bullet,A}}\bef f^{\uparrow P^{A,\bullet}}\big)^{\uparrow H}\quad.
\end{align*}
Now write the right-hand side:
\begin{align*}
{\color{greenunder}\text{use }t^{B}=u^{P^{B,B}}:}\quad & \big(f^{\uparrow P^{B,\bullet}}\big)^{\uparrow G}\bef\gunderline{t^{B}}\bef\big(f^{\downarrow P^{\bullet,B}}\big)^{\uparrow H}=\gunderline{\big(f^{\uparrow P^{B,\bullet}}\big)^{\uparrow G}\bef u^{P^{B,B}}}\bef\big(f^{\downarrow P^{\bullet,B}}\big)^{\uparrow H}\\
{\color{greenunder}\text{naturality of }u:}\quad & =u^{P^{B,A}}\bef\gunderline{\big(f^{\uparrow P^{B,\bullet}}\big)^{\uparrow H}\bef\big(f^{\downarrow P^{\bullet,B}}\big)^{\uparrow H}}=u^{P^{B,A}}\bef\big(f^{\uparrow P^{B,\bullet}}\bef f^{\downarrow P^{\bullet,B}}\big)^{\uparrow H}\quad.
\end{align*}
The difference between the left-hand side and the right-hand side
is now only in the order of applying lifted functions $f$. By the
profunctor commutativity law of $P$, we have
\[
f^{\downarrow P^{\bullet,A}}\bef f^{\uparrow P^{A,\bullet}}=f^{\uparrow P^{B,\bullet}}\bef f^{\downarrow P^{\bullet,B}}\quad,
\]
which completes the proof.

The following more general version of the same property will be also
useful.

\subsubsection{Statement \label{subsec:Statement-dinatural-substitute-profunctor-in-natural}\ref{subsec:Statement-dinatural-substitute-profunctor-in-natural}}

Suppose $G^{X,Y}$ and $H^{X,Y}$ are some bifunctors, $P^{X,Y}$
and $Q^{X,Y}$ are some profunctors, all fully parametric, and $u^{A,B}:G^{A,B}\rightarrow H^{A,B}$
is a natural transformation separately in the type parameters $A$
and $B$. Define the transformation 
\[
t^{A}:G^{P^{A,A},Q^{A,A}}\rightarrow H^{P^{A,A},Q^{A,A}}\quad,\quad\quad t\triangleq u^{P^{A,A,},Q^{A,A}}\quad,
\]
by using the unmodified code of $u^{A,B}$ with substituted type parameters,
$P^{A,A}$ and $Q^{A,A}$ instead of $A$ and $B$. Then $t$ is a
dinatural transformation between profunctors $G^{P^{A,A},Q^{A,A}}$
and $H^{P^{A,A},Q^{A,A}}$.

\subparagraph{Proof}

The assumed naturality law of $u$ holds separately with respect to
both type parameters,
\[
f^{\uparrow G^{\bullet,B}}\bef u^{B,B}=u^{A,B}\bef f^{\uparrow H^{\bullet,B}}\quad,\quad\quad f^{\uparrow G^{A,\bullet}}\bef u^{A,B}=u^{A,A}\bef f^{\uparrow H^{A,\bullet}}\quad.
\]
 The naturality law of $t$ is written, after substituting $t=u$,
as
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (f^{\downarrow P})^{\uparrow G^{\bullet,Q^{B,A}}}\bef(f^{\downarrow Q})^{\uparrow G^{P^{A,A},\bullet}}\bef u^{P^{A,A},Q^{A,A}}\bef(f^{\uparrow P})^{\uparrow H^{\bullet,Q^{A,A}}}\bef(f^{\uparrow Q})^{\uparrow H^{P^{A,B},\bullet}}\\
{\color{greenunder}\text{right-hand side}:}\quad & \quad\overset{?}{=}(f^{\uparrow P})^{\uparrow G^{\bullet,Q^{B,A}}}\bef(f^{\uparrow Q})^{\uparrow G^{P^{B,B},\bullet}}\bef u^{P^{B,B},Q^{B,B}}\bef(f^{\downarrow P})^{\uparrow H^{\bullet,Q^{B,B}}}\bef(f^{\downarrow Q})^{\uparrow H^{P^{A,B},\bullet}}\quad.
\end{align*}
The naturality law of $u$ allows us to move all $G$-lifted functions
to the right of $u$, where they become $H$-lifted. The law becomes
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & u^{P^{B,A},Q^{B,A}}\bef(f^{\downarrow P})^{\uparrow H^{\bullet,Q^{B,A}}}\bef(f^{\downarrow Q})^{\uparrow H^{P^{A,A},\bullet}}\bef(f^{\uparrow P})^{\uparrow H^{\bullet,Q^{A,A}}}\bef(f^{\uparrow Q})^{\uparrow H^{P^{A,B},\bullet}}\\
{\color{greenunder}\text{right-hand side}:}\quad & \quad\overset{?}{=}u^{P^{B,A},Q^{B,A}}\bef(f^{\uparrow P})^{\uparrow H^{\bullet,Q^{B,A}}}\bef(f^{\uparrow Q})^{\uparrow H^{P^{B,B},\bullet}}\bef(f^{\downarrow P})^{\uparrow H^{\bullet,Q^{B,B}}}\bef(f^{\downarrow Q})^{\uparrow H^{P^{A,B},\bullet}}\quad.
\end{align*}
The remaining difference between the two sides is only in the application
order of lifted functions $f$. All those liftings commute due to
the profunctor and bifunctor commutativity laws.

\subsection{Proof of the parametricity theorem}

We will now prove that any fully parametric $t^{A}$ expressed as
\[
t^{A}:P^{A,A}\rightarrow Q^{A,A}\quad,\quad\quad t\triangleq p^{:P^{A,A}}\rightarrow\text{expression}\quad,
\]
satisfies the naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}).
It is assumed that the function body, denoted by \textsf{``}\lstinline!expression!\textsf{''},
is some fully parametric code built up from the nine code constructions
of Table~\ref{tab:nine-pure-code-constructions}.

The proof goes by induction on the structure of \textsf{``}\lstinline!expression!\textsf{''}.
The base case contains the first two constructions (\textsf{``}use unit value\textsf{''}
and \textsf{``}use argument\textsf{''}) since those constructions do not assume any
previous code. We will need to prove that the law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
holds in those cases. The inductive step covers the remaining seven
constructions, which create larger code expressions from smaller ones.
We will need to prove that the code for $t$ satisfies the law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
under the inductive assumption that all previous code expressions
already satisfy their versions of that law.

For example, the product construction (\textsf{``}create tuple\textsf{''}) combines
two previously available code expressions ($a$ and $b$) into a tuple
$a\times b$. The corresponding transformations are $p\rightarrow a$,
$p\rightarrow b$, and $p\rightarrow a\times b$. Note that the naturality
laws have different forms for each of the transformations $p\rightarrow a$,
$p\rightarrow b$, and $p\rightarrow a\times b$ because those functions
have different types. Assuming that the profunctor transformations
$p\rightarrow a$ and $p\rightarrow b$ already satisfy their respective
naturality laws, we will need to prove that $p\rightarrow a\times b$
also satisfies the corresponding law. (The proof is in item 5 below.)

We will now prove the naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
for each of the nine code constructions.

\paragraph{1) Use unit value}

This is a base case where \textsf{``}\lstinline!expression!\textsf{''} is just the
unit value, $1$. The code of $t$ is a constant function that always
returns that value: $t\triangleq p^{:P^{A,A}}\rightarrow1$. In that
case, $Q^{X,Y}\triangleq\bbnum 1$ and so the liftings for $Q$ consist
of identity functions, $f^{\downarrow Q^{\bullet,B}}=\text{id}$ and
$f^{\uparrow Q^{A,\bullet}}=\text{id}$. We can also write $t=\_\rightarrow1$.
Both sides of the law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
are then functions that ignore their arguments and always return $1$.

A quicker way of proving the dinaturality of $t$ is to use Statement~\ref{subsec:Statement-dinatural-substitute-functor-in-natural}
with $G^{A}\triangleq A$ and $H^{A}\triangleq\bbnum 1$.

\paragraph{2) Use argument }

This is a base case where \textsf{``}\lstinline!expression!\textsf{''} is just the
given argument of the transformation, i.e., the code of $t$ is $t\triangleq p\rightarrow p$.
Since $t$ is an identity function, it has type $t^{A}:P^{A,A}\rightarrow P^{A,A}$,
and so we must have $P=Q$. The naturality law follows from Statement~\ref{subsec:Statement-dinatural-substitute-functor-in-natural}
with $G^{A}\triangleq H^{A}\triangleq A$.

The rest of the proof goes over the remaining code constructions,
which are inductive steps.

\paragraph{3) Create function}

The \textsf{``}\lstinline!expression!\textsf{''} is a nameless function $z\rightarrow r$,
so $t\triangleq p^{:P^{A,A}}\rightarrow z\rightarrow r$, where the
sub-expression $r$ may use both $p$ and $z$ as bound variables.
Since the code of $r$ is fully parametric, the types of $r$ and
$z$ must be expressible as $R^{A,A}$ and $Z^{A,A}$, where $R^{X,Y}$
and $Z^{X,Y}$ are some profunctors. The form of $t$ implies that
$Q^{X,Y}\triangleq Z^{Y,X}\rightarrow R^{X,Y}$ (note the reverse
order of parameters in $Z^{Y,X}$). The inductive assumption says
that the naturality law is already satisfied by \emph{any} transformation
whose \textsf{``}\lstinline!expression!\textsf{''} is just $r$. Such a transformation
must have code of the form $u\triangleq s\rightarrow r$, where the
type of the argument $s$ must contain the bound variables $p$ and
$z$ that may be used in $r$. So, we set $s=p\times z$ and write
the code of $u$ as $u=p^{:P^{A,A}}\times z^{:Z^{A,A}}\rightarrow r$.
We then have
\begin{equation}
t=p^{:P^{A,A}}\rightarrow z^{:Z^{A,A}}\rightarrow u(p\times z)\quad,\quad\quad u=p^{:P^{A,A}}\times z^{:Z^{A,A}}\rightarrow t(p)(z)\quad.\label{eq:dinaturality-u-function-type-derivation0}
\end{equation}

Denoting the profunctor $U^{X,Y}\triangleq P^{X,Y}\times Z^{X,Y}$,
we write the naturality law for $u:U^{A,A}\rightarrow R^{A,A}$ as
\begin{equation}
f^{\downarrow U^{\bullet,A}}\bef u^{A}\bef f^{\uparrow R^{A,\bullet}}=f^{\uparrow U^{B,\bullet}}\bef u^{B}\bef f^{\downarrow R^{\bullet,B}}\quad.\label{eq:dinaturality-u-function-type-derivation1}
\end{equation}
This equation holds by the inductive assumption. We will derive the
naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
for $t$ from Eq.~(\ref{eq:dinaturality-u-function-type-derivation1})
by using the type definitions $U^{X,Y}\triangleq P^{X,Y}\times Z^{X,Y}$
and $Q^{X,Y}\triangleq Z^{Y,X}\rightarrow R^{X,Y}$ to express the
liftings of $U$ and $Q$ through the liftings of $P$, $R$, and
$Z$ via the standard functor/contrafunctor codes:
\begin{align}
f^{\downarrow U^{\bullet,A}}=f^{\downarrow P^{\bullet,A}}\boxtimes f^{\downarrow Z^{\bullet,A}}\quad, & \quad\quad f^{\uparrow U^{B,\bullet}}=f^{\uparrow P^{B,\bullet}}\boxtimes f^{\uparrow Z^{B,\bullet}}\quad,\label{eq:dinaturality-u-function-type-derivation2}\\
f^{\uparrow Q^{A,\bullet}}=q^{:Q^{A,A}}\rightarrow f^{\downarrow Z^{\bullet,A}}\bef q\bef f^{\uparrow R^{A,\bullet}}\quad, & \quad\quad f^{\downarrow Q^{\bullet,B}}=q^{:Q^{B,B}}\rightarrow f^{\uparrow Z^{B,\bullet}}\bef q\bef f^{\downarrow R^{\bullet,B}}\quad.\label{eq:dinaturality-u-function-type-derivation3}
\end{align}
Substituting the relevant formulas into the left-hand side of Eq.~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
and applying to $p^{:P^{B,A}}$, we find
\begin{align*}
{\color{greenunder}\text{Eq.~(\ref{eq:dinaturality-u-function-type-derivation0})}:}\quad & p\triangleright f^{\downarrow P^{\bullet,A}}\bef\gunderline t\bef\gunderline{f^{\uparrow Q^{A,\bullet}}}=p\triangleright f^{\downarrow P^{\bullet,A}}\,\gunderline{\triangleright\big(p}\rightarrow z\rightarrow u(p\times z)\big)\,\gunderline{\triangleright\big(q^{:Q^{A,A}}}\rightarrow f^{\downarrow Z^{\bullet,A}}\bef q\bef f^{\uparrow R^{A,\bullet}}\big)\\
 & =\gunderline{f^{\downarrow Z^{\bullet,A}}\bef\big(}z\rightarrow u((p\triangleright f^{\downarrow P^{\bullet,A}})\times\gunderline z)\big)\bef f^{\uparrow R^{A,\bullet}}=z\rightarrow u\big(\gunderline{(p\triangleright f^{\downarrow P^{\bullet,A}})\times(z\triangleright f^{\downarrow Z^{\bullet,A}})}\big)\triangleright f^{\uparrow R^{A,\bullet}}\\
 & =z\rightarrow\gunderline{u\big(}(p\times z)\triangleright\big(f^{\downarrow P^{\bullet,A}}\boxtimes f^{\downarrow Z^{\bullet,A}}\big)\gunderline{\big)}\triangleright f^{\uparrow R^{A,\bullet}}=z\rightarrow(p\times z)\triangleright\big(\gunderline{f^{\downarrow P^{\bullet,A}}\boxtimes f^{\downarrow Z^{\bullet,A}}}\big)\triangleright u\triangleright f^{\uparrow R^{A,\bullet}}\\
{\color{greenunder}\text{Eq.~(\ref{eq:dinaturality-u-function-type-derivation2})}:}\quad & =z\rightarrow(p\times z)\triangleright\gunderline{f^{\downarrow U^{\bullet,A}}\bef u\bef f^{\uparrow R^{A,\bullet}}}\\
{\color{greenunder}\text{Eq.~(\ref{eq:dinaturality-u-function-type-derivation1})}:}\quad & =z\rightarrow(p\times z)\triangleright\gunderline{f^{\uparrow U^{B,\bullet}}}\bef\gunderline u\bef f^{\downarrow R^{\bullet,B}}\\
{\color{greenunder}\text{Eq.~(\ref{eq:dinaturality-u-function-type-derivation2})}:}\quad & =z\rightarrow\gunderline{(p\times z)\triangleright\big(}f^{\uparrow P^{B,\bullet}}\boxtimes f^{\uparrow Z^{B,\bullet}}\big)\triangleright\big(p\times z\rightarrow t(p)(z)\big)\triangleright f^{\downarrow R^{\bullet,B}}\\
 & =z\rightarrow t\big(p\triangleright f^{\uparrow P^{B,\bullet}}\big)\big(z\triangleright f^{\uparrow Z^{B,\bullet}}\big)\triangleright f^{\downarrow R^{\bullet,B}}.
\end{align*}
Now apply the right-hand side of Eq.~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors})
to the same value $p$:
\begin{align*}
{\color{greenunder}\text{use Eq.~(\ref{eq:dinaturality-u-function-type-derivation3})}:}\quad & p\triangleright f^{\uparrow P^{B,\bullet}}\bef t\bef\gunderline{f^{\downarrow Q^{\bullet,B}}}=\gunderline{p\triangleright f^{\uparrow P^{B,\bullet}}\triangleright t\triangleright\big(}q^{:Q^{B,B}}\rightarrow f^{\uparrow Z^{B,\bullet}}\bef q\bef f^{\downarrow R^{\bullet,B}}\big)\\
 & =f^{\uparrow Z^{B,\bullet}}\bef t(p\triangleright f^{\uparrow P^{B,\bullet}})\bef f^{\downarrow R^{\bullet,B}}=z\rightarrow z\triangleright f^{\uparrow Z^{B,\bullet}}\triangleright t(p\triangleright f^{\uparrow P^{B,\bullet}})\triangleright f^{\downarrow R^{\bullet,B}}\\
 & =z\rightarrow t\big(p\triangleright f^{\uparrow P^{B,\bullet}}\big)\big(z\triangleright f^{\uparrow Z^{B,\bullet}}\big)\triangleright f^{\downarrow R^{\bullet,B}}\quad.
\end{align*}
We obtain the same expression as for the left-hand side, which proves
that the law holds.

The proof does not assume that the expression $t(p)(r)$ actually
uses both arguments $p$ and $r$. So, the law holds also for functions
that ignore some of their arguments.

\paragraph{4) Use function }

The \textsf{``}\lstinline!expression!\textsf{''} is a function application such as
$f(z)$. Then we may write the code of $t$ as $t\triangleq p^{:P^{A,A}}\rightarrow f(z)$,
where $f$ and $z$ are some other expressions. Since these expressions
are parts of the code $p\rightarrow f(z)$, it must be possible to
compute $f$ and $z$ separately, given $p$. So, we can write fully
parametric code for the two transformations $u\triangleq p\rightarrow f$
and $v\triangleq p\rightarrow z$. The inductive assumption is that
the naturality law holds for any transformations whose code contains
$f$ or $z$ as the function body. So, we may use the naturality laws
for $u$ and $v$.

What are the possible types of $f$ and $z$? Since $z$ is computed
by fully parametric code, the type of $z$ can be expressed as $z^{:Z^{A,A}}$
via some profunctor $Z^{X,Y}$. To match the types, the function $f$
must have type $Z^{A,A}\rightarrow Q^{A,A}$. It will be convenient
to express $t$ as a composition of two transformations: the first,
denoted by $s$, simply gathers $f$ and $z$ in a tuple,
\[
s:P^{A,A}\rightarrow(Z^{A,A}\rightarrow Q^{A,A})\times Z^{A,A}\quad,\quad\quad s\triangleq p\rightarrow u(p)\times v(p)\quad.
\]
The second, denoted $w$, applies the function:
\[
w:(Z^{A,A}\rightarrow Q^{A,A})\times Z^{A,A}\rightarrow Q^{A,A}\quad,\quad\quad w\triangleq f\times z\rightarrow f(z)\quad.
\]
We will show separately that $s$ and $w$ are dinatural. Since both
$s$ and $w$ are implemented via fully parametric code, it will follow
by Statement~\ref{subsec:Statement-dinatural-transformation-composition}
that the composition $t=s\bef w$ is also dinatural.

Defining the profunctor $G^{X,Y}\triangleq Z^{Y,X}\rightarrow Q^{X,Y}$,
we write the type signatures of $u$ and $v$ as 
\[
u:P^{A,A}\rightarrow G^{A,A}\quad,\quad\quad v:P^{A,A}\rightarrow Z^{A,A}\quad.
\]
Since $u$ and $v$ are dinatural, we may apply the derivation in
item 5 below (which does not use any other assumptions) with $R\triangleq G$
and $S\triangleq Z$ to show that the transformation $s\triangleq p\rightarrow u(p)\times v(p)$
of type $P^{A,A}\rightarrow G^{A,A}\times Z^{A,A}$ is also dinatural.

It remains to show that the transformation $w\triangleq g\times z\rightarrow z\triangleright g$
is dinatural. For brevity, we will omit type annotations and write
$f^{\uparrow G}$ instead of $f^{\uparrow G^{X,\bullet}}$ and $f^{\downarrow G}$
instead of $f^{\downarrow G^{\bullet,Y}}$, with $G$ and other profunctors.
Since profunctors have one contravariant and one covariant type parameter,
the choice of the type parameter in a lifting will remain unambiguous.
The naturality law is then written as
\begin{equation}
(f^{\downarrow G}\boxtimes f^{\downarrow Z})\bef w\bef f^{\uparrow Q}\overset{?}{=}(f^{\uparrow G}\boxtimes f^{\uparrow Z})\bef w\bef f^{\downarrow Q}\quad.\label{eq:dinaturality-law-application-derivation1}
\end{equation}
It helps to write out the liftings to $G$. For arbitrary $g^{:G^{B,A}}$
and $f^{:A\rightarrow B}$, we have $g\triangleright f^{\downarrow G^{\bullet,A}}$
of type $G^{A,A}=Z^{A,A}\rightarrow Q^{A,A}$, so
\begin{align*}
g^{:G^{B,A}}\triangleright f^{\downarrow G} & =z^{:Z^{A,A}}\rightarrow(z\triangleright f^{\uparrow Z})\triangleright g\bef f^{\downarrow Q}=f^{\uparrow Z}\bef g\bef f^{\downarrow Q}\quad.
\end{align*}
Similarly
\[
g^{:G^{B,A}}\triangleright f^{\uparrow G}=z^{:Z^{B,B}}\rightarrow(z\triangleright f^{\downarrow Z})\triangleright g\bef f^{\uparrow Q}=f^{\downarrow Z}\bef g\bef f^{\uparrow Q}\quad.
\]
Now apply the left-hand side of the naturality law~(\ref{eq:dinaturality-law-application-derivation1})
to an arbitrary $g^{:G^{B,A}}\times z^{:Z^{B,A}}$:
\begin{align*}
{\color{greenunder}\text{definitions of }\boxtimes\text{ and }w:}\quad & (g^{:G^{B,A}}\times z^{:Z^{B,A}})\triangleright(f^{\downarrow G}\boxtimes f^{\downarrow Z})\bef w\bef f^{\uparrow Q}=(z\triangleright f^{\downarrow Z})\triangleright(\gunderline{g\triangleright f^{\downarrow G}})\bef f^{\uparrow Q}\\
{\color{greenunder}\text{definition of }g\triangleright f^{\downarrow G}:}\quad & =(z\triangleright f^{\downarrow Z})\triangleright f^{\uparrow Z}\bef g\bef f^{\downarrow Q}\bef f^{\uparrow Q}=z\triangleright f^{\downarrow Z}\bef f^{\uparrow Z}\bef g\bef f^{\downarrow Q}\bef f^{\uparrow Q}\quad.
\end{align*}
Applying the right-hand side of Eq.~(\ref{eq:dinaturality-law-application-derivation1})
to $g\times z$ gives similarly
\begin{align*}
{\color{greenunder}\text{definitions of }\boxtimes\text{ and }w:}\quad & (g^{:G^{B,A}}\times z^{:Z^{B,A}})\triangleright(f^{\uparrow G}\boxtimes f^{\uparrow Z})\bef w\bef f^{\downarrow Q}=(z\triangleright f^{\uparrow Z})\triangleright(\gunderline{g\triangleright f^{\uparrow G}})\bef f^{\downarrow Q}\\
{\color{greenunder}\text{definition of }g\triangleright f^{\uparrow G}:}\quad & =(z\triangleright f^{\uparrow Z})\triangleright f^{\downarrow Z}\bef g\bef f^{\uparrow Q}\bef f^{\downarrow Q}=z\triangleright f^{\uparrow Z}\bef f^{\downarrow Z}\bef g\bef f^{\uparrow Q}\bef f^{\downarrow Q}\quad.
\end{align*}
The two sides of Eq.~(\ref{eq:dinaturality-law-application-derivation1})
now differ only by the order of application of lifted functions $f^{\uparrow Z}$,
$f^{\downarrow Z}$, $f^{\uparrow Q}$, and $f^{\downarrow Q}$. These
applications commute by the profunctor commutativity laws of $Z$
and $Q$:
\[
f^{\downarrow Z}\bef f^{\uparrow Z}=f^{\uparrow Z}\bef f^{\downarrow Z}\quad,\quad\quad f^{\downarrow Q}\bef f^{\uparrow Q}=f^{\uparrow Q}\bef f^{\downarrow Q}\quad.
\]
This concludes the proof of Eq.~(\ref{eq:dinaturality-law-application-derivation1})
and so proves the entire item 4.

\paragraph{5) Create tuple }

The \textsf{``}\lstinline!expression!\textsf{''} is a tuple, so the code of $t$
is of the form $t\triangleq p^{:P^{A,A}}\rightarrow r\times s$ where
$r$ and $s$ are some expressions. In general, the types of $r$
and $s$ will be given by some profunctors $R$ and $S$, so that
$Q^{X,Y}=R^{X,Y}\times S^{X,Y}$. Since the code of $t$ computes
$r\times s$ from $p$, we should be able to compute $r$ and $s$
separately from $p$. It means that we have well-defined transformations
$u\triangleq p\rightarrow r$ and $v\triangleq p\rightarrow s$ having
types $u:P^{A.A}\rightarrow R^{A,A}$ and $v:P^{A,A}\rightarrow S^{A,A}$.
The inductive assumption is that the naturality law already holds
for any transformation whose function body is $r$ or $s$. So, we
may assume that the law holds separately for $u$ and $v$:
\[
f^{\downarrow P}\bef u\bef f^{\uparrow R}=f^{\uparrow P}\bef u\bef f^{\downarrow R}\quad,\quad\quad f^{\downarrow P}\bef v\bef f^{\uparrow S}=f^{\uparrow P}\bef v\bef f^{\downarrow S}\quad.
\]
The lifting for $Q$ is expressed through the liftings for $R$ and
$S$ as
\[
f^{\uparrow Q}=f^{\uparrow R}\boxtimes f^{\uparrow S}\quad,\quad\quad f^{\downarrow Q}=f^{\downarrow R}\boxtimes f^{\downarrow S}\quad.
\]
We can now verify the naturality law of $t$ by expressing $t=p\rightarrow u(p)\times v(p)=\Delta\bef(u\boxtimes v)$:
\begin{align*}
{\color{greenunder}\text{expect }f^{\uparrow P}\bef t\bef f^{\downarrow Q}:}\quad & f^{\downarrow P}\bef\gunderline t\bef\gunderline{f^{\uparrow Q}}=\gunderline{f^{\downarrow P}\bef\Delta}\bef(u\boxtimes v)\bef\big(f^{\uparrow R}\boxtimes f^{\uparrow S}\big)\\
{\color{greenunder}\text{naturality of }\Delta:}\quad & =\Delta\bef(f^{\downarrow P}\boxtimes f^{\downarrow P})\bef(u\boxtimes v)\bef\big(f^{\uparrow R}\boxtimes f^{\uparrow S}\big)=\Delta\bef\big(\gunderline{f^{\downarrow P}\bef u\bef f^{\uparrow R}}\big)\boxtimes\big(\gunderline{f^{\downarrow P}\bef v\bef f^{\uparrow S}}\big)\\
{\color{greenunder}\text{inductive assumption}:}\quad & =\Delta\bef\big(f^{\uparrow P}\bef u\bef f^{\downarrow R}\big)\boxtimes\big(f^{\uparrow P}\bef v\bef f^{\downarrow S}\big)=\gunderline{\Delta\bef\big(f^{\uparrow P}\boxtimes f^{\uparrow P}\big)}\bef(u\boxtimes v)\bef\big(\gunderline{f^{\downarrow R}\boxtimes f^{\downarrow S}}\big)\\
{\color{greenunder}\text{naturality of }\Delta:}\quad & =f^{\uparrow P}\bef\gunderline{\Delta\bef(u\boxtimes v)}\bef f^{\downarrow Q}=f^{\uparrow P}\bef t\bef f^{\downarrow Q}\quad.
\end{align*}


\paragraph{6) Use tuple }

The \textsf{``}\lstinline!expression!\textsf{''} contains a tuple accessor, such
as $\pi_{1}$. It is sufficient to prove the law for $\pi_{1}$, since
all tuple accessors work similarly. So, we assume that the type $P$
is a product, $P^{X,Y}\triangleq Q^{X,Y}\times R^{X,Y}$, and that
the code of $t$ is of the form $t\triangleq\pi_{1}=p^{:Q^{A,A}\times R^{A,A}}\rightarrow p\triangleright\pi_{1}$.
This is just the natural transformation $\pi_{1}$ with substituted
type parameters, so the dinaturality of $t$ follows from Statement~\ref{subsec:Statement-dinatural-substitute-profunctor-in-natural}
with $G^{X,Y}\triangleq X\times Y$, $H^{X,Y}\triangleq X$, $P\triangleq Q$,
and $Q\triangleq R$.

\paragraph{7) Create disjunctive value }

The \textsf{``}\lstinline!expression!\textsf{''} contains a case class constructor
such as \lstinline!Left!, \lstinline!Right!, or \lstinline!Some!.
We may assume that $Q^{X,Y}=R^{X,Y}+S^{X,Y}$ for some profunctors
$R$, $S$, and that the code of $t$ is of the form $t\triangleq p^{:P^{A,A}}\rightarrow r^{:R^{A,A}}+\bbnum 0^{:S^{A,A}}$
where $r$ is some expression of type $R^{A,A}$. It is sufficient
to prove the law for $t=p\rightarrow r+\bbnum 0$, because all other
disjunctive cases such as $t=p\rightarrow\bbnum 0+s$ work analogously.

Since the code of $t$ is fully parametric, the value $r$ can be
also computed from $p$ using fully parametric code. So the inductive
assumption is that the transformation $u\triangleq p\rightarrow r$
is already known to be dinatural, with the law
\[
f^{\downarrow P}\bef u\bef f^{\uparrow R}=f^{\uparrow P}\bef u\bef f^{\downarrow R}\quad.
\]
 The naturality law for $t$ is
\[
f^{\downarrow P}\bef t\bef f^{\uparrow Q}\overset{?}{=}f^{\uparrow P}\bef t\bef f^{\downarrow Q}\quad.
\]
The liftings to $Q$ are disjunctive functions written in the matrix
notation as
\[
f^{\uparrow Q^{B,\bullet}}=\,\begin{array}{|c||cc|}
 & R^{B,B} & S^{B,B}\\
\hline R^{B,A} & f^{\uparrow R^{B,\bullet}} & \bbnum 0\\
S^{B,A} & \bbnum 0 & f^{\uparrow S^{B,\bullet}}
\end{array}\quad,\quad\quad f^{\downarrow Q^{\bullet,A}}=\,\begin{array}{|c||cc|}
 & R^{A,A} & S^{A,A}\\
\hline R^{B,A} & f^{\downarrow R^{\bullet,A}} & \bbnum 0\\
S^{B,A} & \bbnum 0 & f^{\downarrow S^{\bullet,A}}
\end{array}\quad.
\]
Omitting type annotations, we apply the naturality law to a value
$p^{:P^{B,A}}$ and get
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & p\triangleright f^{\downarrow P}\bef t\bef f^{\uparrow Q}=\big((p\triangleright f^{\downarrow P}\triangleright u)+\bbnum 0\big)\triangleright\,\begin{array}{||cc|}
f^{\uparrow R} & \bbnum 0\\
\bbnum 0 & f^{\uparrow S}
\end{array}\\
{\color{greenunder}\text{apply disjunctive function}:}\quad & \quad=p\triangleright f^{\downarrow P}\bef u\bef f^{\uparrow R}\quad.\\
{\color{greenunder}\text{right-hand side}:}\quad & p\triangleright f^{\uparrow P}\bef t\bef f^{\downarrow Q}=\big((p\triangleright f^{\uparrow P}\triangleright u)+\bbnum 0\big)\triangleright\,\begin{array}{||cc|}
f^{\downarrow R} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S}
\end{array}\\
{\color{greenunder}\text{apply disjunctive function}:}\quad & \quad=p\triangleright\gunderline{f^{\uparrow P}\bef u\bef f^{\downarrow R}}\\
{\color{greenunder}\text{dinaturality of }u:}\quad & \quad=p\triangleright f^{\downarrow P}\bef u\bef f^{\uparrow R}\quad.
\end{align*}
Both sides are now equal.

\paragraph{8) Use disjunctive value }

The \textsf{``}\lstinline!expression!\textsf{''} is a pattern match, so the code
of $t$ is of the form 
\[
t\triangleq p^{:P^{A,A}}\rightarrow e\triangleright\,\begin{array}{||c|}
g\\
h
\end{array}\quad,
\]
where $g$ and $h$ are known functions. The expressions $e$, $g$,
and $h$ are used as part of the code, and so they must be all fully
parametric and computable from $p$. We then choose the inductive
assumption that the transformations $p\rightarrow e$, $p\rightarrow g$,
$p\rightarrow h$ are dinatural. As in item 4, we can represent $t$
as a composition of two transformations, 
\[
t=u\bef v\quad,\quad\quad u\triangleq p\rightarrow e\times g\times h\quad,\quad\quad v\triangleq e\times g\times h\rightarrow e\triangleright\,\begin{array}{||c|}
g\\
h
\end{array}\quad.
\]
To show that the transformation $u$ is dinatural, we apply the argument
in item 5 twice. It remains to show that $v$ is dinatural. Since
the code of $v$ is fully parametric, it will follow by Statement~\ref{subsec:Statement-dinatural-transformation-composition}
that the composition $t=u\bef v$ is dinatural.

Assume that the types of $e$, $g$, $h$ are $e:R^{A,A}+S^{A,A}$,
$g:R^{A,A}\rightarrow Q^{A,A}$, and $h:S^{A,A}\rightarrow Q^{A,A}$,
where $R^{X,Y}$ and $S^{X,Y}$ are some profunctors. For convenience,
let us define the profunctors $K$ and $L$ by
\[
K^{X,Y}\triangleq R^{Y,X}\rightarrow Q^{X,Y}\quad,\quad\quad L^{X,Y}\triangleq S^{Y,X}\rightarrow Q^{X,Y}\quad.
\]
The type of $v$ is then $v^{A}:(R^{A,A}+S^{A,A})\times K^{A,A}\times L^{A,A}\rightarrow Q^{A,A}$,
so its naturality law is 
\[
\big(f^{\downarrow(R+S)}\boxtimes f^{\downarrow K}\boxtimes f^{\downarrow L}\big)\bef v\bef f^{\uparrow Q}\overset{?}{=}\big(f^{\uparrow(R+S)}\boxtimes f^{\uparrow K}\boxtimes f^{\uparrow L}\big)\bef v\bef f^{\downarrow Q}\quad.
\]
The code of the required liftings is defined by
\begin{align*}
f^{\downarrow(R+S)}=\,\begin{array}{||cc|}
f^{\downarrow R} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S}
\end{array}\quad, & \quad\quad f^{\uparrow(R+S)}=\,\begin{array}{||cc|}
f^{\uparrow R} & \bbnum 0\\
\bbnum 0 & f^{\uparrow S}
\end{array}\quad,\\
g\triangleright f^{\downarrow K}=f^{\uparrow R}\bef g\bef f^{\downarrow Q}\quad, & \quad\quad g\triangleright f^{\uparrow K}=f^{\downarrow R}\bef g\bef f^{\uparrow Q}\quad,\\
h\triangleright f^{\downarrow L}=f^{\uparrow S}\bef h\bef f^{\downarrow Q}\quad, & \quad\quad h\triangleright f^{\uparrow L}=f^{\downarrow S}\bef h\bef f^{\uparrow Q}\quad.
\end{align*}
Apply the left-hand side of the naturality law to a value $e^{:R^{B,A}+S^{B,A}}\times g^{:R^{A,B}\rightarrow Q^{B,A}}\times h^{:S^{A,B}\rightarrow Q^{B,A}}$:
\begin{align*}
{\color{greenunder}\text{expand }\boxtimes:}\quad & \gunderline{e\times g\times h\triangleright\big(}f^{\downarrow(R+S)}\boxtimes f^{\downarrow K}\boxtimes f^{\downarrow L}\big)\bef v\bef f^{\uparrow Q}=(e\triangleright f^{\downarrow(R+S)})\times(\gunderline{g\triangleright f^{\downarrow K}})\times(\gunderline{h\triangleright f^{\downarrow L}})\triangleright v\bef f^{\uparrow Q}\\
{\color{greenunder}\text{definition of }v:}\quad & =e\triangleright\,\begin{array}{||cc|}
f^{\downarrow R} & \bbnum 0\\
\bbnum 0 & f^{\downarrow S}
\end{array}\,\bef\,\begin{array}{||c|}
g\triangleright f^{\downarrow K}\\
h\triangleright f^{\downarrow L}
\end{array}\,\bef f^{\uparrow Q}\\
{\color{greenunder}\text{composition}:}\quad & =e\triangleright\,\begin{array}{||c|}
f^{\downarrow R}\bef f^{\uparrow R}\bef g\bef f^{\downarrow Q}\\
f^{\downarrow S}\bef f^{\uparrow S}\bef h\bef f^{\downarrow Q}
\end{array}\,\bef f^{\uparrow Q}=e\triangleright\,\begin{array}{||c|}
f^{\downarrow R}\bef f^{\uparrow R}\bef g\bef f^{\downarrow Q}\bef f^{\uparrow Q}\\
f^{\downarrow S}\bef f^{\uparrow S}\bef h\bef f^{\downarrow Q}\bef f^{\uparrow Q}
\end{array}\quad.
\end{align*}
Apply the right-hand side to the same value:
\begin{align*}
{\color{greenunder}\text{expand }\boxtimes:}\quad & \gunderline{e\times g\times h\triangleright\big(}f^{\uparrow(R+S)}\boxtimes f^{\uparrow K}\boxtimes f^{\uparrow L}\big)\bef v\bef f^{\downarrow Q}=(e\triangleright f^{\uparrow(R+S)})\times(\gunderline{g\triangleright f^{\uparrow K}})\times(\gunderline{h\triangleright f^{\uparrow L}})\triangleright v\bef f^{\downarrow Q}\\
{\color{greenunder}\text{definition of }v:}\quad & =e\triangleright\,\begin{array}{||cc|}
f^{\uparrow R} & \bbnum 0\\
\bbnum 0 & f^{\uparrow S}
\end{array}\,\bef\,\begin{array}{||c|}
g\triangleright f^{\uparrow K}\\
h\triangleright f^{\uparrow L}
\end{array}\,\bef f^{\downarrow Q}\\
{\color{greenunder}\text{composition}:}\quad & =e\triangleright\,\begin{array}{||c|}
f^{\uparrow R}\bef f^{\downarrow R}\bef g\bef f^{\uparrow Q}\\
f^{\uparrow S}\bef f^{\downarrow S}\bef h\bef f^{\uparrow Q}
\end{array}\,\bef f^{\downarrow Q}=e\triangleright\,\begin{array}{||c|}
f^{\uparrow R}\bef f^{\downarrow R}\bef g\bef f^{\uparrow Q}\bef f^{\downarrow Q}\\
f^{\uparrow S}\bef f^{\downarrow S}\bef h\bef f^{\uparrow Q}\bef f^{\downarrow Q}
\end{array}\quad.
\end{align*}
The two sides of the naturality law now differ only by the application
order of lifted functions $f$. Applying the profunctor commutativity
law of $R$, $S$, and $Q$, we find that the two sides of the naturality
law are equal.

\paragraph{9) Use recursion}

Here the \textsf{``}\lstinline!expression!\textsf{''} is the value of the function
$t$ itself, so the code of $t$ is $t\triangleq p^{:P^{A,A}}\rightarrow\overline{t}$
(the overline in $\overline{t}$ denotes the recursive use of $t$).
We treat $\overline{t}$ as just code for some function that is already
known (by the inductive assumption) to obey its naturality law. Assume
that the type of $t$ is $P^{A,A}\rightarrow Q^{A,A}$; then the recursive
invocation $\overline{t}$ has the type $Q^{A,A}$ (which is a recursive
type assumed to be equivalent to $P^{A,A}\rightarrow Q^{A,A}$). The
naturality law of $t$ applied to an arbitrary $p^{:P^{B,A}}$ is
\[
p\triangleright f^{\downarrow P}\bef t\bef f^{\uparrow Q}\overset{?}{=}p\triangleright f^{\uparrow P}\bef t\bef f^{\downarrow Q}\quad.
\]
Substituting $t=p\rightarrow\overline{t}$, we find
\[
p\triangleright f^{\downarrow P}\triangleright(p\rightarrow\overline{t})\triangleright f^{\uparrow Q}=\overline{t}\triangleright f^{\uparrow Q}\overset{?}{=}p\triangleright f^{\uparrow P}\triangleright(p\rightarrow\overline{t})\triangleright f^{\downarrow Q}=\overline{t}\triangleright f^{\downarrow Q}\quad.
\]
It remains to show that 
\[
\overline{t}\triangleright f^{\uparrow Q}\overset{?}{=}\overline{t}\triangleright f^{\downarrow Q}\quad.
\]
This is the \textsf{``}wedge law\textsf{''} of $\overline{t}$, which follows from
the assumed dinaturality of $\overline{t}$ by Statement~\ref{subsec:Statement-dinatural-expression-wedge-law}.

This concludes the proof of what is commonly known as \textsf{``}the\textsf{''} parametricity
theorem: Any fully parametric function with a type parameter will
automatically satisfy a naturality law. For functions having several
type parameters, we can fix all the type parameters except one and
apply the parametricity theorem separately, obtaining one naturality
law for each type parameter.

\subsection{Uniqueness of functor and contrafunctor typeclass instances\label{sec:Uniqueness-of-functor-and-contrafunctor}}

An important consequence of the parametricity theorem is the fact
that functors and contrafunctors can be implemented in only one way.

\subsubsection{Statement \label{subsec:Statement-functor-is-unique}\ref{subsec:Statement-functor-is-unique}}

A \index{fully parametric!functor}\textbf{fully parametric functor},
i.e., a functor $F$ whose type is a combination of the constructions
of Table~\ref{tab:six-pure-type-constructions}, has a unique lawful
and fully parametric implementation of \lstinline!fmap!. 

\subparagraph{Proof}

Section~\ref{subsec:f-Functor-constructions} derived lawful and
fully parametric implementations of the \lstinline!fmap! method for
all functors $F$ build up from the six type constructions. The naturality
laws obtained from the parametricity theorem must use precisely those
\textsf{``}standard\textsf{''} implementations of \lstinline!fmap!, because the proof
of the parametricity theorem significantly depends on the code of
those implementations. Throughout this book, the standard lifting
code is denoted by $\text{fmap}_{F}(f)$ or by $f^{\uparrow F}$.
Now suppose that there exists \emph{another} lawful and fully parametric
implementation of \lstinline!fmap! for $F$, denoted by $\text{fmap}_{F}^{\prime}(f)$:
\[
\text{fmap}_{F}^{\prime}:\left(A\rightarrow B\right)\rightarrow F^{A}\rightarrow F^{B}\quad,\quad\quad\text{fmap}_{F}^{\prime}(f^{:A\rightarrow B})=\text{???}^{:F^{A}\rightarrow F^{B}}\quad.
\]
We will now show that $\text{fmap}_{F}^{\prime}=\text{fmap}_{F}$.
Let us fix the type parameter $A$ and apply the parametricity theorem
to $\text{fmap}_{F}^{\prime}$ with respect to $B$. The resulting
naturality law involves an arbitrary $g^{:B\rightarrow C}$:
\[
\text{fmap}_{F}^{\prime}(f^{:A\rightarrow B}\bef g^{:B\rightarrow C})\overset{!}{=}\text{fmap}_{F}^{\prime}(f)\bef g^{\uparrow F}\quad.
\]
Within the naturality law, the lifting $g^{\uparrow F}$ must use
the \textsf{``}standard\textsf{''} lifting code $g^{\uparrow F}\triangleq\text{fmap}_{F}(g)$.
By assumption, $\text{fmap}_{F}^{\prime}$ is lawful, so we may use
its composition law and write
\[
\text{fmap}_{F}^{\prime}(f\bef g)=\text{fmap}_{F}^{\prime}(f)\bef\text{fmap}_{F}^{\prime}(g)\overset{!}{=}\text{fmap}_{F}^{\prime}(f)\bef g^{\uparrow F}\quad.
\]
Since $f^{:A\rightarrow B}$ is arbitrary, we can choose $A=B$ and
$f=\text{id}^{:B\rightarrow B}$ to obtain
\[
\text{fmap}_{F}^{\prime}(\text{id})\bef\text{fmap}_{F}^{\prime}(g)\overset{!}{=}\text{fmap}_{F}^{\prime}(\text{id})\bef g^{\uparrow F}\quad.
\]
The identity law for $\text{fmap}_{F}^{\prime}$ gives $\text{fmap}_{F}^{\prime}(\text{id})=\text{id}$,
so we can simplify the last equation to
\[
\text{fmap}_{F}^{\prime}(g)\overset{!}{=}g^{\uparrow F}=\text{fmap}_{F}(g)\quad.
\]
This must hold for arbitrary $g^{:B\rightarrow C}$, which proves
that $\text{fmap}_{F}^{\prime}=\text{fmap}_{F}$.

\subsubsection{Statement \label{subsec:Statement-contrafunctor-is-unique}\ref{subsec:Statement-contrafunctor-is-unique}}

A contrafunctor $H$ whose type expression is a combination of the
six type constructions (Table~\ref{tab:six-pure-type-constructions})
has a unique fully parametric implementation of a lawful \lstinline!cmap!
method. 

\subparagraph{Proof}

We use similar arguments as in the proof of Statement~\ref{subsec:Statement-functor-is-unique}.
For any lawful, fully parametric alternative implementation $\text{cmap}_{H}^{\prime}$,
the parametricity theorem gives the naturality law
\[
\text{cmap}_{H}^{\prime}(f^{:A\rightarrow B}\bef g^{:B\rightarrow C})\overset{!}{=}(g^{:B\rightarrow C})^{\downarrow H}\bef\text{cmap}_{H}^{\prime}(f)\quad.
\]
By assumption, the identity and composition law hold for $\text{cmap}_{H}^{\prime}$.
Setting $f=\text{id}^{:B\rightarrow B}$, we get
\[
\text{cmap}_{H}^{\prime}(\text{id}\bef g)=\text{cmap}_{H}^{\prime}(g)\overset{!}{=}g^{\downarrow H}\bef\text{cmap}_{H}^{\prime}(\text{id})=g^{\downarrow H}\quad.
\]
This must hold for arbitrary $g^{:B\rightarrow C}$, which shows that
$\text{cmap}_{H}^{\prime}(g)=g^{\downarrow H}=\text{cmap}_{H}(g)$
as required.

\section{Summary}

We have proved three parametricity results that apply to all fully
parametric code:
\begin{itemize}
\item The lifting methods of any bifunctor, profunctor, or bi-contrafunctor
obey the commutativity law such as Eq.~(\ref{eq:f-fmap-fmap-bifunctor-commutativity}).
Because of this, any fully parametric type constructor $F^{A,B}$
which is a functor separately with respect to $A$ and $B$ is always
a bifunctor whose \lstinline!bimap! method satisfies the composition
law~(\ref{eq:f-bimap-composition-law}). Similar properties hold
for profunctors and for bi-contrafunctors. The proof goes by induction
on the exponential-polynomial type expression of $F^{A,B}$, which
must be built up via the six type constructions (Table~\ref{tab:six-pure-type-constructions}).
\item Any function of type $P^{A,A}\rightarrow Q^{A,A}$ (where $P$, $Q$
are profunctors) obeys the general naturality law~(\ref{eq:naturality-law-for-dinatural-transformations-of-profunctors}),
which allows us to derive a specific naturality law for any fully
parametric function. The form of the law depends only on the function\textsf{'}s
type signature and applies to all fully parametric implementations
of that type signature. The proof goes by induction on the structure
of the expression, which must be built up via the nine code constructions
(Table~\ref{tab:nine-pure-code-constructions}).
\item The lifting methods of functors and contrafunctors can be implemented
in only one way once the identity and composition laws are imposed.
The unique correct implementations are defined by the standard procedures
shown in Chapter~\ref{chap:Functors,-contrafunctors,-and}, and there
are no other inequivalent implementations. Here we do not distinguish
\emph{equivalent} implementations such as $f(x)$ and $(y\rightarrow y)(f)(x)$,
which are syntactically different programs that will always give the
same results. So, there is only one lawful implementation of the \lstinline!Functor!
or \lstinline!Contrafunctor! typeclass instances for a given type
constructor. (For many other typeclasses, such as \lstinline!Filterable!
or \lstinline!Monad!, a given type constructor may have several inequivalent
and lawful typeclass instances.)
\end{itemize}

\chapter{A humorous disclaimer}

\emph{The following text is quoted in part from an anonymous online
source (~\textsf{``}Project Guten Tag\textsf{''}) dating back at least to 1997. The
original text is no longer available on the Internet.}

\medskip{}

\noun{Warranto Limitensis; Disclamatantus Damagensis}

Solus exceptus \textsf{``}Rectum Replacator Refundiens\textsf{''} describitus ecci,
\begin{enumerate}
\item Projectus (etque nunquam partum quis hic etext remitibus cum \noun{Project
Guten Tag}$^{\text{TM}}$ identificator) disclamabat omni liabilitus
tuus damagensis, pecuniensisque, includibantus pecunia legalitus,
et 
\item \noun{Remedia Negligentitia Non Habet Tuus, Warrantus Destructi\-bus
Contractus Nullibus Ni Liabilitus Sumus, Inclutatibus Non Limitatus
Destructio Directibus, Consequentius, Punitio, O Incidentus, Non Sunt
Si Nos Notificat Vobis}. 
\end{enumerate}
Sit discubriatus defectus en etextum sic entram diariam noventam recibidio,
pecuniam tuum refundatorium receptorus posset, sic scribatis vendor.
Sit veniabat medium physicalis, vobis idem reternat et replacator
possit copius. Sit venitabat electronicabilis, sic viri datus chansus
segundibus. 

\noun{Hic Etext Venid \textsf{``}Como-asi\textsf{''}. Nihil Warranti Nunquam Classum,
Expressito Ni Implicato, Le Macchen Como Si Etexto Bene Sit O Il Medio
Bene Sit, Inclutat Et Non Limitat Warranti Mercatensis, Appropriatensis
Purposem. }

Statuen varias non permitatent disclamabaris ni warranti implicatoren
ni exclusioni limitatio damagaren consequentialis, ecco lo qua disclamatori
exclusato\-rique non vobis applicant, et potat optia alia legali.

\twocolumn

\chapter{GNU Free Documentation License\label{sec:GFDL} }

{\footnotesize{}Version 1.2, November 2002}{\footnotesize\par}

{\tiny{}Copyright (c) 2000,2001,2002 Free Software Foundation, Inc.
59 Temple Place, Suite 330, Boston, MA 02111-1307, USA}{\tiny\par}

{\tiny{}Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.}{\tiny\par}

{\tiny{}\setcounter{subsection}{-1}}{\tiny\par}

\subsection*{{\tiny{}Preamble}}

{\tiny{}The purpose of this License is to make a manual, textbook,
or other functional and useful document free in the sense of freedom:
to assure everyone the effective freedom to copy and redistribute
it, with or without modifying it, either commercially or noncommercially.
Secondarily, this License preserves for the author and publisher a
way to get credit for their work, while not being considered responsible
for modifications made by others.}{\tiny\par}

{\tiny{}This License is a kind of \textquotedblleft copyleft\textsf{''}, which
means that derivative works of the document must themselves be free
in the same sense. It complements the GNU General Public License,
which is a copyleft license designed for free software.}{\tiny\par}

{\tiny{}We have designed this License in order to use it for manuals
for free software, because free software needs free documentation:
a free program should come with manuals providing the same freedoms
that the software does. But this License is not limited to software
manuals; it can be used for any textual work, regardless of subject
matter or whether it is published as a printed book. We recommend
this License principally for works whose purpose is instruction or
reference.}{\tiny\par}

\subsection{Applicability and definitions\label{subsec:1Applicability-and-definitions}}

{\tiny{}This License applies to any manual or other work, in any medium,
that contains a notice placed by the copyright holder saying it can
be distributed under the terms of this License. Such a notice grants
a world-wide, royalty-free license, unlimited in duration, to use
that work under the conditions stated herein. The \textquotedblleft Document\textsf{''},
below, refers to any such manual or work. Any member of the public
is a licensee, and is addressed as \textquotedblleft you\textsf{''}. You accept
the license if you copy, modify or distribute the work in a way requiring
permission under copyright law.}{\tiny\par}

{\tiny{}A \textquotedblleft Modified Version\textsf{''} of the Document means
any work containing the Document or a portion of it, either copied
verbatim, or with modifications and/or translated into another language.}{\tiny\par}

{\tiny{}A \textquotedblleft Secondary Section\textsf{''} is a named appendix
or a front-matter section of the Document that deals exclusively with
the relationship of the publishers or authors of the Document to the
Document\textsf{'}s overall subject (or to related matters) and contains nothing
that could fall directly within that overall subject. (Thus, if the
Document is in part a textbook of mathematics, a Secondary Section
may not explain any mathematics.) The relationship could be a matter
of historical connection with the subject or with related matters,
or of legal, commercial, philosophical, ethical or political position
regarding them.}{\tiny\par}

{\tiny{}The \textquotedblleft Invariant Sections\textsf{''} are certain Secondary
Sections whose titles are designated, as being those of Invariant
Sections, in the notice that says that the Document is released under
this License. If a section does not fit the above definition of Secondary
then it is not allowed to be designated as Invariant. The Document
may contain zero Invariant Sections. If the Document does not identify
any Invariant Sections then there are none.}{\tiny\par}

{\tiny{}The \textquotedblleft Cover Texts\textsf{''} are certain short passages
of text that are listed, as Front-Cover Texts or Back-Cover Texts,
in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may
be at most 25 words.}{\tiny\par}

{\tiny{}A \textquotedblleft Transparent\textsf{''} copy of the Document means
a machine-readable copy, represented in a format whose specification
is available to the general public, that is suitable for revising
the document straightforwardly with generic text editors or (for images
composed of pixels) generic paint programs or (for drawings) some
widely available drawing editor, and that is suitable for input to
text formatters or for automatic translation to a variety of formats
suitable for input to text formatters. A copy made in an otherwise
Transparent file format whose markup, or absence of markup, has been
arranged to thwart or discourage subsequent modification by readers
is not Transparent. An image format is not Transparent if used for
any substantial amount of text. A copy that is not \textquotedblleft Transparent\textsf{''}
is called \textquotedblleft Opaque\textsf{''}.}{\tiny\par}

{\tiny{}Examples of suitable formats for Transparent copies include
plain ASCII without markup, Texinfo input format, \LaTeX{} input format,
SGML or XML using a publicly available DTD, and standard-conforming
simple HTML, PostScript or PDF designed for human modification. Examples
of transparent image formats include PNG, XCF and JPG. Opaque formats
include proprietary formats that can be read and edited only by proprietary
word processors, SGML or XML for which the DTD and/or processing tools
are not generally available, and the machine-generated HTML, PostScript
or PDF produced by some word processors for output purposes only.}{\tiny\par}

{\tiny{}The \textsf{``}Title Page\textsf{''} means, for a printed book, the title
page itself, plus such following pages as are needed to hold, legibly,
the material this License requires to appear in the title page. For
works in formats which do not have any title page as such, \textquotedblleft Title
Page\textquotedblright{} means the text near the most prominent appearance
of the work\textsf{'}s title, preceding the beginning of the body of the text.}{\tiny\par}

{\tiny{}A section \textsf{``}Entitled XYZ\textsf{''} means a named subunit of the
Document whose title either is precisely XYZ or contains XYZ in parentheses
following text that translates XYZ in another language. (Here XYZ
stands for a specific section name mentioned below, such as \textquotedblleft Acknowledgements\textquotedblright ,
\textquotedblleft Dedications\textquotedblright , \textquotedblleft Endorsements\textquotedblright ,
or \textquotedblleft History\textquotedblright .) To \textquotedblleft Preserve
the Title\textquotedblright{} of such a section when you modify the
Document means that it remains a section \textquotedblleft Entitled
XYZ\textquotedblright{} according to this definition.}{\tiny\par}

{\tiny{}The Document may include Warranty Disclaimers next to the
notice which states that this License applies to the Document. These
Warranty Disclaimers are considered to be included by reference in
this License, but only as regards disclaiming warranties: any other
implication that these Warranty Disclaimers may have is void and has
no effect on the meaning of this License.}{\tiny\par}

\subsection{Verbatim copying\label{subsec:2Verbatim-copying}}

{\tiny{}You may copy and distribute the Document in any medium, either
commercially or noncommercially, provided that this License, the copyright
notices, and the license notice saying this License applies to the
Document are reproduced in all copies, and that you add no other conditions
whatsoever to those of this License. You may not use technical measures
to obstruct or control the reading or further copying of the copies
you make or distribute. However, you may accept compensation in exchange
for copies. If you distribute a large enough number of copies you
must also follow the conditions in section~\ref{subsec:3Copying-in-quantity}.}{\tiny\par}

{\tiny{}You may also lend copies, under the same conditions stated
above, and you may publicly display copies.}{\tiny\par}

\subsection{Copying in quantity\label{subsec:3Copying-in-quantity}}

{\tiny{}If you publish printed copies (or copies in media that commonly
have printed covers) of the Document, numbering more than 100, and
the Document\textsf{'}s license notice requires Cover Texts, you must enclose
the copies in covers that carry, clearly and legibly, all these Cover
Texts: Front-Cover Texts on the front cover, and Back-Cover Texts
on the back cover. Both covers must also clearly and legibly identify
you as the publisher of these copies. The front cover must present
the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition. Copying with
changes limited to the covers, as long as they preserve the title
of the Document and satisfy these conditions, can be treated as verbatim
copying in other respects.}{\tiny\par}

{\tiny{}If the required texts for either cover are too voluminous
to fit legibly, you should put the first ones listed (as many as fit
reasonably) on the actual cover, and continue the rest onto adjacent
pages.}{\tiny\par}

{\tiny{}If you publish or distribute Opaque copies of the Document
numbering more than 100, you must either include a machine-readable
Transparent copy along with each Opaque copy, or state in or with
each Opaque copy a computer-network location from which the general
network-using public has access to download using public-standard
network protocols a complete Transparent copy of the Document, free
of added material. If you use the latter option, you must take reasonably
prudent steps, when you begin distribution of Opaque copies in quantity,
to ensure that this Transparent copy will remain thus accessible at
the stated location until at least one year after the last time you
distribute an Opaque copy (directly or through your agents or retailers)
of that edition to the public.}{\tiny\par}

{\tiny{}It is requested, but not required, that you contact the authors
of the Document well before redistributing any large number of copies,
to give them a chance to provide you with an updated version of the
Document.}{\tiny\par}

\subsection{Modifications\label{subsec:4Modifications}}

{\tiny{}You may copy and distribute a Modified Version of the Document
under the conditions of sections~\ref{subsec:2Verbatim-copying}
and \ref{subsec:3Copying-in-quantity} above, provided that you release
the Modified Version under precisely this License, with the Modified
Version filling the role of the Document, thus licensing distribution
and modification of the Modified Version to whoever possesses a copy
of it. In addition, you must do these things in the Modified Version:}{\tiny\par}

{\tiny{}A. Use in the Title Page (and on the covers, if any) a title
distinct from that of the Document, and from those of previous versions
(which should, if there were any, be listed in the History section
of the Document). You may use the same title as a previous version
if the original publisher of that version gives permission.}{\tiny\par}

{\tiny{}B. List on the Title Page, as authors, one or more persons
or entities responsible for authorship of the modifications in the
Modified Version, together with at least five of the principal authors
of the Document (all of its principal authors, if it has fewer than
five), unless they release you from this requirement.}{\tiny\par}

{\tiny{}C. State on the Title page the name of the publisher of the
Modified Version, as the publisher.}{\tiny\par}

{\tiny{}D. Preserve all the copyright notices of the Document.}{\tiny\par}

{\tiny{}E. Add an appropriate copyright notice for your modifications
adjacent to the other copyright notices.}{\tiny\par}

{\tiny{}F. Include, immediately after the copyright notices, a license
notice giving the public permission to use the Modified Version under
the terms of this License, in the form shown in the Addendum below.}{\tiny\par}

{\tiny{}G. Preserve in that license notice the full lists of Invariant
Sections and required Cover Texts given in the Document\textsf{'}s license
notice.}{\tiny\par}

{\tiny{}H. Include an unaltered copy of this License.}{\tiny\par}

{\tiny{}I. Preserve the section Entitled \textsf{``}History\textsf{''}, Preserve its
Title, and add to it an item stating at least the title, year, new
authors, and publisher of the Modified Version as given on the Title
Page. If there is no section Entitled \textquotedblleft History\textquotedblright{}
in the Document, create one stating the title, year, authors, and
publisher of the Document as given on its Title Page, then add an
item describing the Modified Version as stated in the previous sentence.}{\tiny\par}

{\tiny{}J. Preserve the network location, if any, given in the Document
for public access to a Transparent copy of the Document, and likewise
the network locations given in the Document for previous versions
it was based on. These may be placed in the \textsf{``}History\textsf{''} section.
You may omit a network location for a work that was published at least
four years before the Document itself, or if the original publisher
of the version it refers to gives permission.}{\tiny\par}

{\tiny{}K. For any section Entitled \textsf{``}Acknowledgements\textsf{''} or \textsf{``}Dedications\textsf{''},
Preserve the Title of the section, and preserve in the section all
the substance and tone of each of the contributor acknowledgements
and/or dedications given therein.}{\tiny\par}

{\tiny{}L. Preserve all the Invariant Sections of the Document, unaltered
in their text and in their titles. Section numbers or the equivalent
are not considered part of the section titles.}{\tiny\par}

{\tiny{}M. Delete any section Entitled \textsf{``}Endorsements\textsf{''}. Such a
section may not be included in the Modified Version.}{\tiny\par}

{\tiny{}N. Do not retitle any existing section to be Entitled \textsf{``}Endorsements\textsf{''}
or to conflict in title with any Invariant Section.}{\tiny\par}

{\tiny{}O. Preserve any Warranty Disclaimers.}{\tiny\par}

{\tiny{}If the Modified Version includes new front-matter sections
or appendices that qualify as Secondary Sections and contain no material
copied from the Document, you may at your option designate some or
all of these sections as invariant. To do this, add their titles to
the list of Invariant Sections in the Modified Version\textsf{'}s license notice.
These titles must be distinct from any other section titles.}{\tiny\par}

{\tiny{}You may add a section Entitled \textsf{``}Endorsements\textsf{''}, provided
it contains nothing but endorsements of your Modified Version by various
parties \textemdash{} for example, statements of peer review or that
the text has been approved by an organization as the authoritative
definition of a standard.}{\tiny\par}

{\tiny{}You may add a passage of up to five words as a Front-Cover
Text, and a passage of up to 25 words as a Back-Cover Text, to the
end of the list of Cover Texts in the Modified Version. Only one passage
of Front-Cover Text and one of Back-Cover Text may be added by (or
through arrangements made by) any one entity. If the Document already
includes a cover text for the same cover, previously added by you
or by arrangement made by the same entity you are acting on behalf
of, you may not add another; but you may replace the old one, on explicit
permission from the previous publisher that added the old one.}{\tiny\par}

{\tiny{}The author(s) and publisher(s) of the Document do not by this
License give permission to use their names for publicity for or to
assert or imply endorsement of any Modified Version.}{\tiny\par}

\subsection*{{\tiny{}Combining documents}}

{\tiny{}You may combine the Document with other documents released
under this License, under the terms defined in section 4 above for
modified versions, provided that you include in the combination all
of the Invariant Sections of all of the original documents, unmodified,
and list them all as Invariant Sections of your combined work in its
license notice, and that you preserve all their Warranty Disclaimers.}{\tiny\par}

{\tiny{}The combined work need only contain one copy of this License,
and multiple identical Invariant Sections may be replaced with a single
copy. If there are multiple Invariant Sections with the same name
but different contents, make the title of each such section unique
by adding at the end of it, in parentheses, the name of the original
author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant
Sections in the license notice of the combined work.}{\tiny\par}

{\tiny{}In the combination, you must combine any sections Entitled
\textquotedblleft History\textquotedblright{} in the various original
documents, forming one section Entitled \textquotedblleft History\textquotedblright ;
likewise combine any sections Entitled \textquotedblleft Acknowledgements\textquotedblright ,
and any sections Entitled \textquotedblleft Dedications\textquotedblright .
You must delete all sections Entitled \textquotedblleft Endorsements.\textquotedblright{}}{\tiny\par}

\subsection*{{\tiny{}Collections of documents}}

{\tiny{}You may make a collection consisting of the Document and other
documents released under this License, and replace the individual
copies of this License in the various documents with a single copy
that is included in the collection, provided that you follow the rules
of this License for verbatim copying of each of the documents in all
other respects.}{\tiny\par}

{\tiny{}You may extract a single document from such a collection,
and distribute it individually under this License, provided you insert
a copy of this License into the extracted document, and follow this
License in all other respects regarding verbatim copying of that document.}{\tiny\par}

\subsection*{{\tiny{}Aggregation with independent works}}

{\tiny{}A compilation of the Document or its derivatives with other
separate and independent documents or works, in or on a volume of
a storage or distribution medium, is called an \textquotedblleft aggregate\textquotedblright{}
if the copyright resulting from the compilation is not used to limit
the legal rights of the compilation\textsf{'}s users beyond what the individual
works permit. When the Document is included an aggregate, this License
does not apply to the other works in the aggregate which are not themselves
derivative works of the Document.}{\tiny\par}

{\tiny{}If the Cover Text requirement of section~\ref{subsec:3Copying-in-quantity}
is applicable to these copies of the Document, then if the Document
is less than one half of the entire aggregate, the Document\textsf{'}s Cover
Texts may be placed on covers that bracket the Document within the
aggregate, or the electronic equivalent of covers if the Document
is in electronic form. Otherwise they must appear on printed covers
that bracket the whole aggregate.}{\tiny\par}

\subsection*{{\tiny{}Translation}}

{\tiny{}Translation is considered a kind of modification, so you may
distribute translations of the Document under the terms of section~\ref{subsec:4Modifications}.
Replacing Invariant Sections with translations requires special permission
from their copyright holders, but you may include translations of
some or all Invariant Sections in addition to the original versions
of these Invariant Sections. You may include a translation of this
License, and all the license notices in the Document, and any Warranty
Disclaimers, provided that you also include the original English version
of this License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original
version of this License or a notice or disclaimer, the original version
will prevail.}{\tiny\par}

{\tiny{}If a section in the Document is Entitled \textquotedblleft Acknowledgements\textquotedblright ,
\textquotedblleft Dedications\textquotedblright , or \textquotedblleft History\textquotedblright ,
the requirement (section~\ref{subsec:4Modifications}) to Preserve
its Title (section~\ref{subsec:1Applicability-and-definitions})
will typically require changing the actual title.}{\tiny\par}

\subsection*{{\tiny{}Termination}}

{\tiny{}You may not copy, modify, sublicense, or distribute the Document
except as expressly provided for under this License. Any other attempt
to copy, modify, sublicense or distribute the Document is void, and
will automatically terminate your rights under this License. However,
parties who have received copies, or rights, from you under this License
will not have their licenses terminated so long as such parties remain
in full compliance.}{\tiny\par}

\subsection*{{\tiny{}Future revisions of this license}}

{\tiny{}The Free Software Foundation may publish new, revised versions
of the GNU Free Documentation License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns. See \url{http://www.gnu.org/copyleft/}.}{\tiny\par}

{\tiny{}Each version of the License is given a distinguishing version
number. If the Document specifies that a particular numbered version
of this License \textquotedblleft or any later version\textquotedblright{}
applies to it, you have the option of following the terms and conditions
either of that specified version or of any later version that has
been published (not as a draft) by the Free Software Foundation. If
the Document does not specify a version number of this License, you
may choose any version ever published (not as a draft) by the Free
Software Foundation.}{\tiny\par}

\subsection*{\noun{\tiny{}Addendum}{\tiny{}: How to use this License for your
documents}}

{\tiny{}To use this License in a document you have written, include
a copy of the License in the document and put the following copyright
and license notices just after the title page:}{\tiny\par}

{\tiny{}Copyright (c) <year> <your name>. Permission is granted to
copy, distribute and/or modify this document under the terms of the
GNU Free Documentation License, Version 1.2 or any later version published
by the Free Software Foundation; with no Invariant Sections, no Front-Cover
Texts, and no Back-Cover Texts. A copy of the license is included
in the section entitled \textquotedblleft GNU Free Documentation License\textquotedblright .}{\tiny\par}

{\tiny{}If you have Invariant Sections, Front-Cover Texts and Back-Cover
Texts, replace the \textquotedblleft with...Texts.\textquotedblright{}
line with this:}{\tiny\par}

{\tiny{}with the Invariant Sections being <list their titles>, with
the Front-Cover Texts being <list>, and with the Back-Cover Texts
being <list>.}{\tiny\par}

{\tiny{}If you have Invariant Sections without Cover Texts, or some
other combination of the three, merge those two alternatives to suit
the situation.}{\tiny\par}

{\tiny{}If your document contains nontrivial examples of program code,
we recommend releasing these examples in parallel under your choice
of free software license, such as the GNU General Public License,
to permit their use in free software.}{\tiny\par}

\subsection*{{\tiny{}Copyright }}

{\tiny{}Copyright (c) 2000, 2001, 2002 Free Software Foundation, Inc.
59 Temple Place, Suite 330, Boston, MA 02111-1307, USA}{\tiny\par}

{\tiny{}Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.}{\tiny\par}
