
\chapter{The logic of types. III. The Curry-Howard correspondence\label{chap:3-3-The-formal-logic-curry-howard}}

\global\long\def\gunderline#1{\mathunderline{greenunder}{#1}}
\global\long\def\bef{\forwardcompose}
\global\long\def\bbnum#1{\custombb{#1}}
Fully parametric functions were defined in Section~\ref{sec:Fully-parametric-functions}.
These functions perform general operations that do not depend on any
specific data types such as \lstinline!Int! or \lstinline!String!.
An example of a fully parametric function is
\begin{lstlisting}
def before[A, B, C](f: A => B, g: B => C): A => C = { x => g(f(x)) }
\end{lstlisting}
We have also seen in Section~\ref{subsec:Deriving-a-function's}
that for certain functions of this kind, the code can be derived unambiguously
from the type signature.

There exists a mathematical theory (called the \textbf{Curry-Howard
correspondence}) that provides precise conditions for the possibility
of deriving a function's code from its type and a systematic derivation
algorithm. Technical details about the algorithm are found in Appendix~\ref{app:The-Curry-Howard-correspondence}.
This chapter describes the main results and applications of this theory
to functional programming.

\section{Values computed by fully parametric functions}

\subsection{Motivation}

Consider the Scala code of a fully parametric function,
\begin{lstlisting}
def f[A, B, ...]: ... = {
  ...
  val x: Either[A, B] = ... // Some expression here.
  ...
}
\end{lstlisting}
If this program compiles without type errors, it means that the types
match and, in particular, that the function \lstinline!f! is able
to compute a value \lstinline!x! of type \lstinline!Either[A, B]!.

It is sometimes \emph{impossible} to compute a value of a certain
type within the body of a fully parametric function. For example,
the fully parametric function \lstinline!fmap! shown in Section~\ref{subsec:Disjunctive-Example-option-1}
cannot compute any values of type \lstinline!A!,
\begin{lstlisting}
def fmap[A, B](f: A => B): Option[A] => Option[B] = {
  val x: A = ??? // Cannot compute x here!
  ... }
\end{lstlisting}
The reason is that a fully parametric function cannot compute values
of type \lstinline!A! from scratch, without using previously given
values of type \lstinline!A! and without applying a function that
returns values of type \lstinline!A!. In \lstinline!fmap!, no values
of type \lstinline!A! are given as arguments; the given function
\lstinline!f: A => B! returns values of type \lstinline!B! and not
\lstinline!A!. The code of \lstinline!fmap! must do pattern matching
on an \lstinline!Option[A]!:
\begin{lstlisting}
def fmap[A, B](f: A => B): Option[A] => Option[B] = {
  case None       => 
    val x: A = ??? // Cannot compute x here!
  case Some(a)    =>
    val x: A = a   // Can compute x in this scope.
}
\end{lstlisting}
Since the case \lstinline!None! has no values of type \lstinline!A!,
we are unable to compute a value \lstinline!x! in that scope (as
long as \lstinline!fmap! remains a fully parametric function). Being
able to compute \lstinline!x:A! ``within the body of the function''
means that, if needed, function should be able to \emph{return} \lstinline!x!
as a result value. This requires being able to compute \lstinline!x!
in all cases, not just within one part of the \lstinline!match! expression. 

The body of \lstinline!fmap! also cannot compute any values of type
\lstinline!B!. Since no arguments of type \lstinline!B! are given,
the only way of obtaining a value of type \lstinline!B! would be
to apply the function \lstinline!f: A => B! to \emph{some} value
of type \lstinline!A!; but we just saw that the body of \lstinline!fmap!
cannot compute any values of type \lstinline!A!.

Another example of being unable to compute a value of a certain type
is
\begin{lstlisting}
def before[A, B, C](f: A => B, g: B => C): A => C = {
  //  val h: C => A = ??? // Cannot compute h here!
  a => g(f(a))            // Can compute a value of type A => C.
}
\end{lstlisting}
The body of \lstinline!before! may only use the arguments \lstinline!f!
and \lstinline!g!. It is possible to obtain a value of type \lstinline!A => C!
by composing \lstinline!f! and \lstinline!g!, but it is impossible
to compute a value \lstinline!h! of type \lstinline!C => A!, no
matter what code we try to write. The reason is that the body of \lstinline!before!
has no given values of type \lstinline!A! and no functions that return
values of type \lstinline!A!, so a nameless function such as \lstinline!(c:C) => ???!
cannot compute its return value of type \lstinline!A!. Since a fully
parametric function cannot create values of an arbitrary type \lstinline!A!
from scratch, we see no possibility of computing \lstinline!h! within
the body of \lstinline!before!.

Can we prove rigorously that a value of type \lstinline!C => A! cannot
be computed within the body of \lstinline!before!? Could a clever
trick produce a value of that type? So far, we only saw informal arguments
about whether values of certain types can be computed. To make the
arguments rigorous, we need to translate statements such as ``\emph{a
fully parametric function} \lstinline!before! \emph{can compute a
value of type} \lstinline!C => A!'' into mathematical formulas,
with rigorous rules for proving them true or false.

In Section~\ref{subsec:Disjunctions-and-conjunctions}, we denoted
by ${\cal CH}(A)$ the proposition ``the ${\cal C}$ode ${\cal H}$as
a value of type $A$''. By ``the code'' we now mean the body of
a given fully parametric function. So, the notation ${\cal CH}(A)$
is not completely adequate because the validity of the proposition
${\cal CH}(A)$ depends not only on the choice of the type $A$ but
also on the place in the code fragment where the value of type $A$
needs to be computed. What exactly is this additional dependency?
In the above examples, we used the \emph{types} of a function's arguments
when reasoning about getting a value of a given type $A$. Thus, a
precise description of the proposition ${\cal CH}(A)$ is 
\begin{align}
{\color{greenunder}{\cal CH}\text{-proposition}:}\quad & \text{a fully parametric function having arguments of types}\nonumber \\
 & X,Y,...,Z\text{ can compute a value of type }A\quad.\label{eq:ch-CH-proposition-def}
\end{align}
Here $X$, $Y$, ..., $Z$, $A$ may be either type parameters or
more complicated type expressions such as $B\Rightarrow C$ or $(C\Rightarrow D)\Rightarrow E$,
built from other type parameters.

If arguments of types $X$, $Y$, ..., $Z$ are given, it means we
already have values of these types. So, the propositions ${\cal CH}(X)$,
${\cal CH}(Y)$, ..., ${\cal CH}(Z)$ will be true. Thus, proposition~(\ref{eq:ch-CH-proposition-def})
is equivalent to ``${\cal CH}(A)$ assuming ${\cal CH}(X)$, ${\cal CH}(Y)$,
..., ${\cal CH}(Z)$''. In mathematical logic, a statement of this
form is called a \textbf{sequent}\index{sequent!example} and is denoted
by
\begin{equation}
{\cal CH}(X),{\cal CH}(Y),...,{\cal CH}(Z)\vdash{\cal CH}(A)\quad.\label{eq:ch-example-sequent}
\end{equation}
The assumptions ${\cal CH}(X)$, ${\cal CH}(Y)$, ..., ${\cal CH}(Z)$
are called \textbf{premises}\index{sequent!premises} and the proposition
${\cal CH}(A)$ is called the \textbf{goal}\index{sequent!goal}.
So, showing rigorously the possibility of computing values in functions
means proving that sequents of the form~(\ref{eq:ch-example-sequent})
are true. Our previous examples are denoted by the following sequents:
\begin{align*}
{\color{greenunder}\text{\texttt{fmap} for \texttt{Option}}:}\quad & {\cal CH}(A\Rightarrow B)\vdash{\cal CH}(\text{\texttt{Option[A]}}\Rightarrow\text{\texttt{Option[B]}})\\
{\color{greenunder}\text{the function \texttt{before}}:}\quad & {\cal CH}(A\Rightarrow B),{\cal CH}(B\Rightarrow C)\vdash{\cal CH}(A\Rightarrow C)\\
{\color{greenunder}\text{value of type }A\text{ within \texttt{fmap}}:}\quad & {\cal CH}(A\Rightarrow B),{\cal CH}(\text{\texttt{Option[A]}})\vdash{\cal CH}(A)\\
{\color{greenunder}\text{value of type }C\Rightarrow A\text{ within \texttt{before}}:}\quad & {\cal CH}(A\Rightarrow B),{\cal CH}(B\Rightarrow C)\vdash{\cal CH}(C\Rightarrow A)
\end{align*}
Calculations in formal logic are called ``proofs''. So, in this
section we gave informal arguments towards proving the first two sequents
and disproving the last two. We will now develop tools for proving
such sequents rigorously.

A proposition ${\cal CH}(A)$ may be true for one set of premises
such as ${\cal CH}(X)$, ${\cal CH}(Y)$, ..., ${\cal CH}(Z)$ but
false for another. Here and in the following sections, we will be
reasoning about ${\cal CH}$-propositions within the body of a \emph{chosen}
fully parametric function, i.e.~with a fixed set of premises. We
will then temporarily omit the premises and use the shorter notation
${\cal CH}(A)$.

\subsection{Type notation and ${\cal CH}$-propositions for standard type constructions\label{subsec:Type-notation-and-standard-type-constructions}}

In Section~\ref{subsec:Disjunctions-and-conjunctions} we saw examples
of reasoning about ${\cal CH}$-propositions for case classes and
for disjunctive types. We will now extend this reasoning systematically
to all type constructions that programs could use. A special \textbf{type
notation}\index{type notation} explained in this section will help
us write type expressions more concisely. (See Appendix~\ref{chap:Appendix-Notations}
for reference on the type notation.)

There are \emph{seven} standard type constructions present in all
functional languages: the \lstinline!Unit! type, the void type (called
\lstinline!Nothing! in Scala), the primitive types, the tuple types,
the disjunctive types, the function types, and the parameterized types.
We will now derive the rules for writing ${\cal CH}$-propositions
for each of these type constructions.

\paragraph{Rule for \lstinline!Unit! type}

The \lstinline!Unit! type has only a single value \lstinline!()!,
and this value can be \emph{always} computed since it does not depend
on having any previous data:
\begin{lstlisting}
def f[...]: ... = {
  ...
  val x: Unit = () // We can always compute a `Unit` value.
  ...
\end{lstlisting}
So the proposition ${\cal CH}($\lstinline!Unit!$)$ is always true.
In the type notation, the \lstinline!Unit! type is denoted by $\bbnum 1$.

Named \lstinline!Unit! types\index{unit type!named} also have a
single value that is always possible to compute. For example,
\begin{lstlisting}
final case class N1()
\end{lstlisting}
defines a named \lstinline!Unit! type, and it can be computed as
\begin{lstlisting}
val x: N1 = N1()
\end{lstlisting}
So, the proposition ${\cal CH}($\lstinline!N1!$)$ is always true.
Named \lstinline!Unit! types are denoted by $\bbnum 1$, just as
the \lstinline!Unit! type itself.

\paragraph{Rule for the void type}

The Scala type \lstinline!Nothing! has no values, so the proposition
${\cal CH}($\lstinline!Nothing!$)$ is always false. The type \lstinline!Nothing!
is denoted by $\bbnum 0$ in the type notation.

\paragraph{Rule for primitive types}

For a specific primitive (or library-defined) type such as \lstinline!Int!
or \lstinline!String!, the corresponding ${\cal CH}$-proposition
is \emph{always true} because we could use any constant value, e.g.
\begin{lstlisting}
def f[...]: ... {
   ...
   val x: String = "abc" // We can always compute a `String` value.
   ...
\end{lstlisting}


\paragraph{Rule for tuple types}

To compute a value of a tuple type \lstinline!(A, B)! requires to
compute a value of type \lstinline!A! \emph{and} a value of type
\lstinline!B!. This is expressed by the logic formula ${\cal CH}($\lstinline!(A, B)!$)={\cal CH}(A)\wedge{\cal CH}(B)$.
A similar formula holds for case classes, as Eq.~(\ref{eq:curry-howard-example-case-class})
shows. In the type notation, the tuple \lstinline!(A, B)! is written
as $A\times B$. Tuples and case classes with more than two parts
are denoted similarly as $A\times B\times...\times C$. For example,
the Scala definition
\begin{lstlisting}
case class Person(firstName: String, lastName: String, age: Int)
\end{lstlisting}
is written in the type notation as $\text{String}\times\text{String}\times\text{Int}$.
So, the rule for tuple types is
\[
{\cal CH}\left(A\times B\times...\times C\right)={\cal CH}(A)\wedge{\cal CH}(B)\wedge...\wedge{\cal CH}(C)\quad.
\]


\paragraph{Rule for disjunctive types}

A disjunctive type may consist of several case classes. Having a value
of a disjunctive type means to have a value of (at least) one of those
case classes. An example of translating this relationship into a formula
was shown by Eq.~(\ref{eq:curry-howard-example-disjunction}). For
the standard disjunctive type \lstinline!Either[A, B]!, we have the
logical formula ${\cal CH}($\lstinline!Either[A, B]!$)={\cal CH}(A)\vee{\cal CH}(B)$.
In the type notation, the Scala type \lstinline!Either[A, B]! is
written as $A+B$. A longer example: the Scala definition
\begin{lstlisting}
sealed trait RootsOfQ
final case class NoRoots() extends RootsOfQ
final case class OneRoot(x: Double) extends RootsOfQ
final case class TwoRoots(x: Double, y: Double) extends RootsOfQ
\end{lstlisting}
is translated to the type notation as
\[
\text{RootsOfQ}=1+\text{Double}+\text{Double}\times\text{Double}\quad.
\]
The type notation is significantly shorter because it omits all case
class names and part names from the type definitions. In this notation,
the rule for disjunctive types is
\[
{\cal CH}\left(A+B+...+C\right)={\cal CH}(A)\vee{\cal CH}(B)\vee...\vee{\cal CH}(C)\quad.
\]


\paragraph{Rule for function types}

Consider now a function type such as \lstinline!A => B!. (This type
is written in the type notation as $A\Rightarrow B$.) To compute
a value of that type, we need to write code such as
\begin{lstlisting}
val f: A => B = { (a: A) =>
  ??? // Compute a value of type B in this scope.
}
\end{lstlisting}
The inner scope of the function needs to compute a value of type $B$,
and the given value \lstinline!a:A! may be used for that. So, ${\cal CH}(A\Rightarrow B)$
is true if and only if we are able to compute a value of type $B$
when we are given a value of type $A$. To translate this statement
into the language of logical propositions, we need to use the logical
implication, ${\cal CH}(A)\Rightarrow{\cal CH}(B)$, which means that
${\cal CH}(B)$ can be proved if ${\cal CH}(A)$ already holds. So
the rule for function types is
\[
{\cal CH}(A\Rightarrow B)={\cal CH}(A)\Rightarrow{\cal CH}(B)\quad.
\]


\paragraph{Rule for parameterized types}

Consider a function with type parameters, e.g.
\begin{lstlisting}
def f[A, B]: A => (A => B) => B = { x => g => g(x) }
\end{lstlisting}
Being able to define the body of such a function is equivalent to
being able to compute a value of type \lstinline!A => (A => B) => B!
for \emph{all} possible types \lstinline!A! and \lstinline!B!. In
the notation of formal logic, this is written as
\[
{\cal CH}\left(\forall(A,B).\,A\Rightarrow(A\Rightarrow B)\Rightarrow B\right)
\]
and is equivalent to
\[
\forall(A,B).\,{\cal CH}\left(A\Rightarrow(A\Rightarrow B)\Rightarrow B\right)\quad.
\]
The code notation for the parameterized function \lstinline!f! is
\[
f^{A,B}:A\Rightarrow\left(A\Rightarrow B\right)\Rightarrow B\quad,
\]
and its type can be written as
\[
\forall(A,B).\,A\Rightarrow\left(A\Rightarrow B\right)\Rightarrow B\quad.
\]
The symbol $\forall$ means ``for all'' and is known as the \index{universal quantifier}\textbf{universal
quantifier} in logic.

In Scala, longer type expressions can be named and their names (called
\textbf{type aliases}\index{type aliases}) can be used to make code
shorter. Type aliases may also contain type parameters. Defining and
using a type alias for the type of the function \lstinline!f! looks
like this,
\begin{lstlisting}
type F[A, B] = A => (A => B) => B
def f[A, B]: F[A, B] = { x => g => g(x) }
\end{lstlisting}
This is written in the type notation as
\begin{align*}
F^{A,B} & \triangleq A\Rightarrow\left(A\Rightarrow B\right)\Rightarrow B\quad,\\
f^{A,B}:F^{A,B} & =x^{:A}\Rightarrow g^{:A\Rightarrow B}\Rightarrow g(x)\quad,
\end{align*}
or equivalently (although somewhat less readably)
\[
f:\big(\forall(A,B).\,F^{A,B}\big)=\forall(A,B).\,x^{:A}\Rightarrow g^{:A\Rightarrow B}\Rightarrow g(x)\quad.
\]

In Scala 3, the function \lstinline!f! can be written as a value
via the syntax
\begin{lstlisting}
val f: [A, B] => A => (A => B) => B = {
  [A, B] => (x: A) => (g: A => B) => g(x)
}
\end{lstlisting}
This syntax corresponds more closely to the mathematical notation
shown above.

So, the rule for parameterized types with the type notation $F^{A}$
is
\[
{\cal CH}(\forall A.\:F^{A})=\forall A.\,{\cal CH}(F^{A})\quad.
\]

Case classes and disjunctive types use \emph{names} for the types
and their parts. However, those names only add convenience for programmers
and do not affect the computational properties of types. So, the type
notation allows us to use nameless type expressions.

Table~\ref{tab:ch-correspondence-type-notation-CH-propositions}
summarizes the type notation and also shows how to translate it into
logic formulas involving propositions of the form ${\cal CH}(...)$.

The precedence\index{type notation!operator precedence} of operators
in the type notation is chosen to have fewer parentheses in the type
expressions that are frequently used. Here are the rules of precedence:
\begin{itemize}
\item The type product operators ($\times$) group stronger than the disjunctive
operators ($+$), so that type expressions such as $A+B\times C$
have the same operator precedence as in standard arithmetic. That
is, $A+B\times C$ means $A+\left(B\times C\right)$. This convention
makes type expressions easier to reason about (for people familiar
with arithmetic).
\item The function type arrows ($\Rightarrow$) group weaker than the operators
$+$ and $\times$, so that often-used types such as $A\Rightarrow\bbnum 1+B$
(representing \lstinline!A => Option[B]!) or $A\times B\Rightarrow C$
(representing \lstinline!((A, B)) => C!) can be written without any
parentheses. Type expressions such as $\left(A\Rightarrow B\right)\times C$
will require parentheses but are used less often.
\item The type quantifiers group weaker than all other operators, so we
can write types such as $\forall A.\,A\Rightarrow A\Rightarrow A$
without parentheses. Type quantifiers are most often placed outside
a type expression. When this is not the case, parentheses are necessary,
e.g.~in the type expression $\left(\forall A.\,A\Rightarrow A\right)\Rightarrow\bbnum 1$.
\end{itemize}
\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Type in Scala} & \textbf{Type notation} & \textbf{${\cal CH}$-proposition}\tabularnewline
\hline 
\hline 
{\small{}type parameter} \lstinline!A! & $A$ & ${\cal CH}(A)$\tabularnewline
\hline 
{\small{}tuple} \lstinline!(A, B)! & $A\times B$ & ${\cal CH}(A)$ $\wedge$ ${\cal CH}(B)$\tabularnewline
\hline 
{\small{}disjunctive type} \lstinline!Either[A, B]! & $A+B$ & ${\cal CH}(A)$ $\vee$ ${\cal CH}(B)$\tabularnewline
\hline 
{\small{}function type} \lstinline!A => B! & $A\Rightarrow B$ & ${\cal CH}(A)$ $\Rightarrow$ ${\cal CH}(B)$\tabularnewline
\hline 
\lstinline!Unit! {\small{}or a ``named''} \lstinline!Unit! {\small{}type} & $\bbnum 1$ & ${\cal CH}(\bbnum 1)=True$\tabularnewline
\hline 
\lstinline!Nothing! {\small{}(the void type)} & $\bbnum 0$ & ${\cal CH}(\bbnum 0)=False$\tabularnewline
\hline 
{\small{}parameterized method} \lstinline!def f[A]: F[A]! & $f^{A}:F^{A}$ & $\forall A.\,{\cal CH}(F^{A})$\tabularnewline
\hline 
{\small{}parameterized value (Scala 3)} \lstinline![A] => F[A]! & $\forall A.\,F^{A}$ & $\forall A.\,{\cal CH}(F^{A})$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{The correspondence between type constructions and ${\cal CH}$-propositions.\label{tab:ch-correspondence-type-notation-CH-propositions}}
\end{table}


\subsection{Solved examples: Type notation\index{solved examples}}

From now on, we will prefer to write types in the type notation rather
than in the Scala syntax. Let us get some experience in translating
between type notation and Scala code.

\subsubsection{Example \label{subsec:Example-ch-dupl-function}\ref{subsec:Example-ch-dupl-function}}

Define a function \lstinline!delta! taking an argument \lstinline!x!
and returning the pair \lstinline!(x, x)!. Derive the most general
type for this function. Write the type signature of \lstinline!delta!
in the type notation, and translate it into a ${\cal CH}$-proposition.
Simplify the ${\cal CH}$-proposition if possible.

\subparagraph{Solution}

Begin by writing the code of the function:
\begin{lstlisting}
def delta(x: ...) = (x, x)
\end{lstlisting}
To derive the most general type for \lstinline!delta!, first assume
\lstinline!x:A!, where \lstinline!A! is a type parameter; then the
tuple \lstinline!(x, x)! has type \lstinline!(A, A)!. We do not
see any constraints on the type parameter \lstinline!A!. So the type
parameter represents an arbitrary type and needs to be added to the
type signature of \lstinline!delta!:
\begin{lstlisting}
def delta[A](x: A): (A, A) = (x, x)
\end{lstlisting}
We find that the most general type of \lstinline!delta! is \lstinline!A => (A, A)!.

It is convenient to use the letter $\Delta$ for the function \lstinline!delta!.
In the type notation, the type signature of $\Delta$ is written as
\[
\Delta^{A}:A\Rightarrow A\times A\quad.
\]
So the proposition ${\cal CH}(\Delta)$ (meaning ``the function $\Delta$
can be implemented'') is
\[
{\cal CH}(\Delta)=\forall A.{\cal \,CH}\left(A\Rightarrow A\times A\right)\quad.
\]
In the type expression $A\Rightarrow A\times A$, the product symbol
($\times$) binds stronger than the function arrow ($\Rightarrow$),
so the parentheses in $A\Rightarrow\left(A\times A\right)$ may be
omitted.

Using the rules for transforming ${\cal CH}$-propositions, we rewrite
\begin{align*}
 & {\cal CH}(A\Rightarrow A\times A)\\
{\color{greenunder}\text{rule for function types}:}\quad & ={\cal CH}(A)\Rightarrow{\cal CH}(A\times A)\\
{\color{greenunder}\text{rule for tuple types}:}\quad & ={\cal CH}(A)\Rightarrow\left({\cal CH}(A)\wedge{\cal CH}(A)\right)\quad.
\end{align*}
Thus the proposition ${\cal CH}(\Delta)$ is equivalent to
\[
{\cal CH}(\Delta)=\forall A.\,{\cal CH}(A)\Rightarrow({\cal CH}(A)\wedge{\cal CH}(A))\quad.
\]


\subsubsection{Example \label{subsec:Example-ch-notation-function-1}\ref{subsec:Example-ch-notation-function-1}}

The standard disjunctive types \lstinline!Either[A, B]! and \lstinline!Option[A]!
are written in the type notation as
\begin{align*}
\text{Either}^{A,B} & \triangleq A+B\quad,\\
\text{Opt}^{A} & \triangleq\bbnum 1+A\quad.
\end{align*}
The type \lstinline!Either[A, B]! is written as $A+B$ by definition
of the disjunctive type operator ($+$). The type \lstinline!Option[A]!
has two disjoint cases, \lstinline!None! and \lstinline!Some[A]!.
The case class \lstinline!None! is a ``\index{unit type!named}named
\lstinline!Unit!'' and is denoted by $\bbnum 1$. The case class
\lstinline!Some[A]! contains a single value of type $A$. So, the
type notation for \lstinline!Option[A]! is $\bbnum 1+A$.

\subsubsection{Example \label{subsec:Example-ch-notation-function-1-a}\ref{subsec:Example-ch-notation-function-1-a}}

The Scala definition of the disjunctive type \lstinline!UserAction!,
\begin{lstlisting}
sealed trait UserAction
final case class SetName(first: String, last: String) extends UserAction
final case class SetEmail(email: String) extends UserAction
final case class SetUserId(id: Long) extends UserAction
\end{lstlisting}
is written in the type notation as
\begin{equation}
\text{UserAction}\triangleq\text{String}\times\text{String}+\text{String}+\text{Long}\quad.\label{eq:ch-example-case-class-type-notation}
\end{equation}
The type operation $\times$ groups stronger than $+$, as in arithmetic.
To derive the type notation~(\ref{eq:ch-example-case-class-type-notation}),
we first drop all names from case classes and get three nameless tuples
\lstinline!(String, String)!, \lstinline!(String)!, and \lstinline!(Long)!.
Each of these tuples is then converted into a product using the operator
$\times$, and all products are ``summed'' in the type notation
using the operator $+$.

\subsubsection{Example \label{subsec:Example-ch-notation-function-2}\ref{subsec:Example-ch-notation-function-2}}

The parameterized disjunctive type \lstinline!Either3! is a generalization
of \lstinline!Either!:
\begin{lstlisting}
sealed trait Either3[A, B, C]
final case class Left[A, B, C](x: A) extends Either3[A, B, C]
final case class Middle[A, B, C](x: B) extends Either3[A, B, C]
final case class Right[A, B, C](x: C) extends Either3[A, B, C]
\end{lstlisting}
This disjunctive type is written in the type notation as
\[
\text{Either3}^{A,B,C}\triangleq A+B+C\quad.
\]


\subsubsection{Example \label{subsec:Example-ch-notation-function-3}\ref{subsec:Example-ch-notation-function-3}}

Define a Scala type constructor \lstinline!F[A]! corresponding to
the type notation 
\[
F^{A}\triangleq\bbnum 1+\text{Int}\times A\times A+\text{Int}\times\left(\text{Int}\Rightarrow A\right)\quad.
\]


\subparagraph{Solution}

The formula for $F^{A}$ defines a disjunctive type \lstinline!F[A]!
with three parts. To implement \lstinline!F[A]! in Scala, we need
to choose names for each of the disjoint parts, which will become
case classes. For the purposes of this example, let us choose names
\lstinline!F1!, \lstinline!F2!, and \lstinline!F3!. Each of these
case classes needs to have the same type parameter \lstinline!A!.
So we begin writing the code as
\begin{lstlisting}
sealed trait F[A]
final case class F1[A](...) extends F[A]
final case class F2[A](...) extends F[A]
final case class F3[A](...) extends F[A]
\end{lstlisting}
Each of these case classes represents one part of the disjunctive
type: \lstinline!F1! represents $\bbnum 1$, \lstinline!F2! represents
$\text{Int}\times A\times A$, and \lstinline!F3! represents $\text{Int}\times\left(\text{Int}\Rightarrow A\right)$.
To define these case classes, we need to name their parts. The final
code is
\begin{lstlisting}
sealed trait F[A]
final case class F1[A]() extends F[A]          // Named `Unit`.
final case class F2[A](n: Int, x1: A, x2: A) extends F[A]
final case class F3[A](n: Int, f: Int => A)  extends F[A]
\end{lstlisting}
The names \lstinline!n!, \lstinline!x1!, \lstinline!x2!, and \lstinline!f!
are chosen purely for convenience.

\subsubsection{Example \label{subsec:Example-ch-notation-function-4}\ref{subsec:Example-ch-notation-function-4}}

Write the type signature of the function
\begin{lstlisting}
def fmap[A, B](f: A => B): Option[A] => Option[B]
\end{lstlisting}
in the type notation.

\subparagraph{Solution}

This is a curried function, so we first rewrite the type signature
as
\begin{lstlisting}
def fmap[A, B]: (A => B) => Option[A] => Option[B]
\end{lstlisting}
The type notation for \lstinline!Option[A]! is $\bbnum 1+A$. Now
we can write the type signature of \lstinline!fmap! as
\begin{align*}
 & \text{fmap}^{A,B}:\left(A\Rightarrow B\right)\Rightarrow\bbnum 1+A\Rightarrow\bbnum 1+B\quad,\\
{\color{greenunder}\text{or equivalently}:}\quad & \text{fmap}:\forall(A,B).\,\left(A\Rightarrow B\right)\Rightarrow\bbnum 1+A\Rightarrow\bbnum 1+B\quad.
\end{align*}
We do not put parentheses around $\bbnum 1+A$ and $\bbnum 1+B$ because
the function arrows ($\Rightarrow$) group weaker than the other type
operations. Parentheses around $\left(A\Rightarrow B\right)$ are
required.

\subsection{Exercises: Type notation\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-type-notation-1}\ref{subsec:Exercise-type-notation-1}}

Define a Scala disjunctive type \lstinline!Q[T,A]! corresponding
to the type notation
\[
Q^{T,A}\triangleq\bbnum 1+T\times A+\text{Int}\times(T\Rightarrow T)+\text{String}\times A\quad.
\]


\subsubsection{Exercise \label{subsec:Exercise-type-notation-2}\ref{subsec:Exercise-type-notation-2}}

Rewrite \lstinline!Either[(A, Int), Either[(A, Char), (A, Float)]]!
in the type notation.

\subsubsection{Exercise \label{subsec:Exercise-type-notation-3}\ref{subsec:Exercise-type-notation-3}}

Define a Scala type \lstinline!OptE[A, B]! written in the type notation
as $\text{OptE}^{A,B}\triangleq\bbnum 1+A+B$.

\subsubsection{Exercise \label{subsec:Exercise-type-notation-4}\ref{subsec:Exercise-type-notation-4}}

Write a Scala type signature for the fully parametric function 
\[
\text{flatMap}^{A,B}:\bbnum 1+A\Rightarrow\left(A\Rightarrow\bbnum 1+B\right)\Rightarrow\bbnum 1+B
\]
and implement this function, preserving information as much as possible.

\section{The logic of ${\cal CH}$-propositions}

\subsection{Motivation and first examples\label{subsec:ch-Motivation-and-first-examples}}

So far, we were able to convert statements such as ``\emph{a fully
parametric function can compute values of type} $A$'' into logical
propositions of the form ${\cal CH}(A)$ that we called ${\cal CH}$-propositions.
The next step is to determine the proof rules suitable for reasoning
about ${\cal CH}$-propositions.

Formal logic uses axioms and derivation rules for proving that certain
formulas are true or false. A simple example of a true formula is
``any proposition $\alpha$ is equivalent to itself'',
\[
\forall\alpha.\,\alpha=\alpha\quad.
\]
In logic, equivalence of propositions is usually understood as \index{implication}\textbf{implication}
($\Rightarrow$) in both directions: $\alpha=\beta$ means $\left(\alpha\Rightarrow\beta\right)\wedge\left(\beta\Rightarrow\alpha\right)$.
So, the above formula is the same as
\[
\forall\alpha.\,\alpha\Rightarrow\alpha\quad.
\]
If the proposition $\alpha$ is a ${\cal CH}$-proposition, $\alpha\triangleq{\cal CH}(A)$
for some type $A$, we obtain the formula
\begin{equation}
\forall A.\,{\cal CH}(A)\Rightarrow{\cal CH}(A)\quad.\label{eq:ch-type-sig-1}
\end{equation}
We expect true ${\cal CH}$-propositions to correspond to types that
\emph{can} be computed in a fully parametric function. Let us see
if this example fits our expectations. We can rewrite Eq.~(\ref{eq:ch-type-sig-1})
as
\begin{align*}
 & \forall A.\,\gunderline{{\cal CH}(A)\Rightarrow{\cal CH}(A)}\\
{\color{greenunder}\text{rule for function types}:}\quad & =\gunderline{\forall A}.\,{\cal CH}\left(A\Rightarrow A\right)\\
{\color{greenunder}\text{rule for parameterized types}:}\quad & ={\cal CH}\left(\forall A.\,A\Rightarrow A\right)\quad.
\end{align*}
The last line shows the ${\cal CH}$-proposition that corresponds
to the function type $\forall A.\,A\Rightarrow A$. Translating the
type notation into a Scala type signature, we get
\begin{lstlisting}
def f[A]: A => A
\end{lstlisting}
This type signature can be easily implemented,
\begin{lstlisting}
def f[A]: A => A = { x => x }
\end{lstlisting}
So, in this example we see how we converted a true formula in logic
into the type of a value \lstinline!f! that can be implemented.

While the formula $\forall\alpha.\,\alpha=\alpha$ may be self-evident,
the point of using formal logic is to obtain the precise set of axioms
and proof rules that allow us to deduce all correct formulas systematically,
without need for intuition or guessing. What is the set of axioms
and proof rules suitable for the task?

A well-known set of logical rules is called \index{Boolean logic}Boolean
logic. In that logic, each proposition is either $True$ or $False$,
and the implication operation ($\Rightarrow$) is defined by 
\begin{equation}
\left(\alpha\Rightarrow\beta\right)\triangleq\left((\neg\alpha)\vee\beta\right)\quad.\label{eq:ch-definition-of-implication-in-Boolean-logic}
\end{equation}
The truth of formulas can be checked by substituting either $True$
or $False$ in every variable and computing the truth value of the
formula in all possible cases. The result is arranged into a \index{truth table}truth
table. The basic operations (disjunction, conjunction, negation, and
implication) have the following truth tables:
\begin{center}
{\small{}}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
{\small{}$\alpha$} & {\small{}$\beta$} & \textbf{\small{}$\alpha\vee\beta$} & \textbf{\small{}$\alpha\wedge\beta$} & \textbf{\small{}$\neg\alpha$} & \textbf{\small{}$\alpha\Rightarrow\beta$}\tabularnewline
\hline 
\hline 
{\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$True$} & {\small{}$False$} & {\small{}$True$} & {\small{}$False$} & {\small{}$False$} & {\small{}$False$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$True$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
\end{tabular}{\small\par}
\par\end{center}

So the formula $\alpha\Rightarrow\alpha$ has the value $True$ whether
$\alpha$ itself is $True$ or $False$. This check is sufficient
to show that $\forall\alpha.\,\alpha\Rightarrow\alpha$ is true in
Boolean logic.

Here is the truth table for the formula $\forall(\alpha,\beta).\,(\alpha\wedge\beta)\Rightarrow\alpha$;
that formula is true in Boolean logic since all values in the last
column are $True$:
\begin{center}
{\small{}}%
\begin{tabular}{|c|c|c|c|}
\hline 
{\small{}$\alpha$} & {\small{}$\beta$} & \textbf{\small{}$\alpha\wedge\beta$} & {\small{}$(\alpha\wedge\beta)\Rightarrow\alpha$}\tabularnewline
\hline 
\hline 
{\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$True$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
\end{tabular}{\small\par}
\par\end{center}

The formula $\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\wedge\beta)$
is not true in Boolean logic, which we can see from the following
truth table (one value in the last column is $False$):
\begin{center}
{\small{}}%
\begin{tabular}{|c|c|c|c|}
\hline 
{\small{}$\alpha$} & {\small{}$\beta$} & \textbf{\small{}$\alpha\wedge\beta$} & {\small{}$\alpha\Rightarrow(\alpha\wedge\beta)$}\tabularnewline
\hline 
\hline 
{\small{}$True$} & {\small{}$True$} & {\small{}$True$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$True$} & {\small{}$False$} & {\small{}$False$} & {\small{}$False$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$True$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
{\small{}$False$} & {\small{}$False$} & {\small{}$False$} & {\small{}$True$}\tabularnewline
\hline 
\end{tabular}{\small\par}
\par\end{center}

Table~\ref{tab:Logical-formulas-Boolean-theorems} shows more examples
of logical formulas that are true in Boolean logic. Each formula is
first given in terms of ${\cal CH}$-propositions (we denoted $\alpha\triangleq{\cal CH}(A)$
and $\beta\triangleq{\cal CH}(B)$ for brevity) and then into a Scala
type signature of a function that can be implemented.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Logic formula} & \textbf{Type formula} & \textbf{Scala code}\tabularnewline
\hline 
\hline 
{\footnotesize{}$\forall\alpha.\,\alpha\Rightarrow\alpha$} & {\footnotesize{}$\forall A.\,A\Rightarrow A$} & \lstinline!def id[A](x: A): A = x!\tabularnewline
\hline 
{\footnotesize{}$\forall\alpha.\,\alpha\Rightarrow True$} & {\footnotesize{}$\forall A.\,A\Rightarrow\bbnum 1$} & \lstinline!def toUnit[A](x: A): Unit = ()!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\vee\beta)$} & {\footnotesize{}$\forall(A,B).\,A\Rightarrow A+B$} & \lstinline!def toL[A, B](x: A): Either[A, B] = Left(x)!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\wedge\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,A\times B\Rightarrow A$} & \lstinline!def first[A, B](p: (A, B)): A = p._1!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\beta\Rightarrow\alpha)$} & {\footnotesize{}$\forall(A,B).\,A\Rightarrow(B\Rightarrow A)$} & \lstinline!def const[A, B](x: A): B => A = (_ => x)!\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logical formulas that are true theorems in Boolean logic.\label{tab:Logical-formulas-Boolean-theorems}}
\end{table}

Table~\ref{tab:Logical-formulas-not-Boolean-theorems} some examples
of formulas that are \emph{not true} in Boolean logic. Translated
into type formulas and then into Scala, these formulas yield type
signatures that \emph{cannot} be implemented by fully parametric functions.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Logic formula} & \textbf{Type formula} & \textbf{Scala type signature}\tabularnewline
\hline 
\hline 
{\footnotesize{}$\forall\alpha.\,True\Rightarrow\alpha$} & {\footnotesize{}$\forall A.\,\bbnum 1\Rightarrow A$} & \lstinline!def f[A](x: Unit): A = ???!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\vee\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,A+B\Rightarrow A$} & \lstinline!def f[A,B](x: Either[A, B]): A=???!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,\alpha\Rightarrow(\alpha\wedge\beta)$} & {\footnotesize{}$\forall(A,B).\,A\Rightarrow A\times B$} & \lstinline!def f[A,B](p: A): (A, B) = ???!\tabularnewline
\hline 
{\footnotesize{}$\forall(\alpha,\beta).\,(\alpha\Rightarrow\beta)\Rightarrow\alpha$} & {\footnotesize{}$\forall(A,B).\,(A\Rightarrow B)\Rightarrow A$} & \lstinline!def f[A,B](x: A => B): A = ???!\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logical formulas that are \emph{not} true in Boolean logic.\label{tab:Logical-formulas-not-Boolean-theorems}}
\end{table}

At first sight, it appears from these examples that whenever a logical
formula is true in Boolean logic, the corresponding type signature
can be implemented in code, and vice versa. However, this is \emph{incorrect}:
the rules of Boolean logic are not suitable for reasoning about types
in a functional language. Below we will see some examples of formulas
that are true in Boolean logic but yield unimplementable type signatures.

\subsection{Example: Failure of Boolean logic for type reasoning\label{subsec:Example:-Failure-of-Boolean-logic}}

To see an explicit example of obtaining an incorrect result when using
Boolean logic to reason about values computed by fully parametric
functions, consider the following type,
\begin{equation}
\forall(A,B,C).\,\left(A\Rightarrow B+C\right)\Rightarrow\left(A\Rightarrow B\right)+\left(A\Rightarrow C\right)\quad,\label{eq:ch-example-boolean-bad-type}
\end{equation}
which corresponds to the Scala type signature
\begin{lstlisting}
def bad[A, B, C](g: A => Either[B, C]): Either[A => B, A => C] = ???
\end{lstlisting}
The function \lstinline!bad! cannot be implemented as a fully parametric
function. To see why, consider that the only available data is a function
$g^{:A\Rightarrow B+C}$, which returns values of type $B$ or $C$
depending (in some unknown way) on the input value of type $A$. The
function \lstinline!bad! must return either a function of type $A\Rightarrow B$
or a function of type $A\Rightarrow C$. How can the code of \lstinline!bad!
make this decision? The only input data is the function $g$ that
takes an argument of type $A$. We could imagine applying $g$ to
various arguments of type $A$ and to see whether $g$ returns a $B$
or a $C$. However, the type $A$ is arbitrary, and a fully parametric
function cannot produce a value of type $A$ in order to apply $g$
to it. So the decision about whether to return $A\Rightarrow B$ or
$A\Rightarrow C$ must be independent of the function $g$; that decision
must be hard-coded in the function \lstinline!bad!.

Suppose we hard-coded the decision to return a function of type $A\Rightarrow B$.
How can we create a function of type $A\Rightarrow B$ in the body
of \lstinline!bad!? Given a value $x^{:A}$ of type $A$, we would
need to compute some value of type $B$. Since the type $B$ is arbitrary
(it is a type parameter), we cannot produce a value of type $B$ from
scratch. The only potential source of values of type $B$ is the given
function $g$. The only way of using $g$ is to apply it to $x^{:A}$.
However, for some $x$, the value $g(x)$ may have type \lstinline!Right(c)!
where \lstinline!c! is of type $C$. In that case, we will have a
value of type $C$, not $B$. So, in general, we cannot guarantee
that we can always obtain a value of type $B$ from a given value
$x^{:A}$. This means we cannot build a function of type $A\Rightarrow B$
out of the function $g$. Similarly, we cannot build a function of
type $A\Rightarrow C$ out of $g$. 

Whether we decide to return $A\Rightarrow B$ or $A\Rightarrow C$,
we will not be able to return a result value of the required type,
as we just saw. We must conclude that we cannot implement \lstinline!bad!
as a fully parametric function.

We could try to switch between $A\Rightarrow B$ and $A\Rightarrow C$
depending on a given value of type $A$. This, however, corresponds
to a different type signature: 
\[
\forall(A,B,C).\,\left(A\Rightarrow B+C\right)\Rightarrow A\Rightarrow\left(A\Rightarrow B\right)+\left(A\Rightarrow C\right)\quad.
\]
This type signature \emph{can} be implemented, for instance, by this
Scala code:
\begin{lstlisting}
def q[A, B, C](g: A => Either[B, C]): A => Either[A=>B, A=>C] = { a =>
  g(a) match {
    case Left(b) => Left(_ => b)
    case Right(c) => Right(_ => c)
  }
}
\end{lstlisting}
But this is not the required type signature~(\ref{eq:ch-example-boolean-bad-type}).

Now let us convert the type signature~(\ref{eq:ch-example-boolean-bad-type})
into a ${\cal CH}$-proposition:
\begin{align}
 & \forall(\alpha,\beta,\gamma).\,\left(\alpha\Rightarrow\left(\beta\vee\gamma\right)\right)\Rightarrow\left(\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)\right)\quad,\label{eq:abc-example-classical-logic-bad}\\
\text{where}\quad & \alpha\triangleq{\cal CH}(A),\quad\beta\triangleq{\cal CH}(B),\quad\gamma\triangleq{\cal CH}(C)\quad.\nonumber 
\end{align}
It turns out that this formula is true \emph{in Boolean logic}. To
prove this, we need to show that Eq.~(\ref{eq:abc-example-classical-logic-bad})
is equal to $True$ for any Boolean values of the variables $\alpha$,
$\beta$, $\gamma$. One way is to rewrite the expression~(\ref{eq:abc-example-classical-logic-bad})
using the rules of Boolean logic, such as Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic}):
\begin{align*}
 & \gunderline{\alpha\Rightarrow}\left(\beta\vee\gamma\right)\\
{\color{greenunder}\text{definition of }\Rightarrow\text{ via Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =(\neg\alpha)\vee\beta\vee\gamma\quad,\\
 & \gunderline{\left(\alpha\Rightarrow\beta\right)}\vee\gunderline{\left(\alpha\Rightarrow\gamma\right)}\\
{\color{greenunder}\text{definition of }\Rightarrow\text{ via Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{(\neg\alpha)}\vee\beta\vee\gunderline{(\neg\alpha)}\vee\gamma\\
{\color{greenunder}\text{property }x\vee x=x\text{ in Boolean logic}:}\quad & =(\neg\alpha)\vee\beta\vee\gamma\quad,
\end{align*}
showing that $\alpha\Rightarrow(\beta\vee\gamma)$ is in fact \emph{equal}
to $\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)$
in Boolean logic.

Let us also give a proof via truth value reasoning. The only possibility
for an implication $X\Rightarrow Y$ to be $False$ is when $X=True$
and $Y=False$. So, Eq.~(\ref{eq:abc-example-classical-logic-bad})
can be $False$ only if $\left(\alpha\Rightarrow(\beta\vee\gamma)\right)=True$
and $\left(\alpha\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\gamma\right)=False$.
A disjunction can be false only when both parts are false; so we must
have both $\left(\alpha\Rightarrow\beta\right)=False$ and $\left(\alpha\Rightarrow\gamma\right)=False$.
This is only possible if $\alpha=True$ and $\beta=\gamma=False$.
But, with these value assignments, we find $\left(\alpha\Rightarrow(\beta\vee\gamma)\right)=False$
rather than $True$ as we assumed. It follows that we cannot ever
make Eq.~(\ref{eq:abc-example-classical-logic-bad}) equal to $False$.
This proves Eq.~(\ref{eq:abc-example-classical-logic-bad}) to be
true in Boolean logic.

\subsection{The rules of proof for ${\cal CH}$-propositions\label{subsec:The-rules-of-proof}}

Section~\ref{subsec:Example:-Failure-of-Boolean-logic} shows that
some true formulas in Boolean logic do not correspond to types of
\emph{implementable} fully parametric functions. However, we have
also seen several other examples where Boolean logic does provide
correct results: some true formulas correspond to implementable type
signatures, while some false formulas correspond to non-implementable
type signatures.

Instead of guessing whether the rules of Boolean logic are suitable,
let us derive the suitable logical axioms and proof rules systematically.

The proposition ${\cal CH}(A)$ is true when a value of type $A$
can be computed by a fully parametric function with a given type signature.
To describe all possible ways of computing a value of type $A$, we
need to enumerate all possible ways of writing code within a fully
parametric function. The requirement of parametricity means that we
are not allowed to use any specific types such as \lstinline!Int!
or \lstinline!String!. We are only allowed to work with values of
unknown types described by the given type parameters. We cannot use
any concrete values such as \lstinline!123! or \lstinline!"hello"!,
or any library functions that work with specific (non-parametric)
types; however, we are permitted to use fully parametric types, such
as \lstinline!Either[A, B]! or \lstinline!Option[A]!. The allowed
eight code constructs are illustrated in this code fragment:
\begin{lstlisting}
def f[A, B, ...](a: A, b: B)... = { // (A given type signature.)
  val x1: Unit = ()            // 1) Create a value of type Unit.
  val x2: A = a                // 2) Use a given argument.
  val x3 = { x: A => ... }     // 3) Create a function.
  val x4: D = x3(x2)           // 4) Use a function.
  val x5: (A, B) = (a, b)      // 5) Create a tuple.
  val x6: B = x5._2            // 6) Use a tuple.
  val x7: Option[A] = Some(x2) // 7) Create values of a disjunctive type.
  val x8 = x7 match { ... }    // 8) Use values of a disjunctive type.
}
\end{lstlisting}
A value of type $X$ can be computed (i.e.~${\cal CH}(X)$ is true)
if and only if we can create a sequence of computed values such as
\lstinline!x1!, \lstinline!x2!, ..., each being the result of one
of these eight code constructs, ending with a value of type $X$.
So, each of the eight code constructs should correspond to a logical
rule for proving a ${\cal CH}$-proposition.

A set of axioms and proof rules defines a \textbf{formal logic}\index{formal logic}.
So, we will now write the proof rules that will define \emph{the}
logic appropriate for reasoning about ${\cal CH}$-propositions.

Because each proof rule will be obtained from a specific code construct,
any ${\cal CH}$-proposition such as ${\cal CH}(X)$ proved by applying
a sequence of these rules will automatically correspond to a code
fragment that combines the relevant code constructs to compute a value
of type $X$. Conversely, any code computing a value of type $X$
must be a combination of some of the eight code constructs, and that
combination can be automatically translated into a sequence of applications
of proof rules in the logic to produce a proof of the proposition
${\cal CH}(X)$.

Let us now write down the proof rules that follow from the eight code
constructs. We will need to consider the full formulation~(\ref{eq:ch-CH-proposition-def})
of ${\cal CH}$-propositions and write them as sequents such as Eq.~(\ref{eq:ch-example-sequent}).
For brevity, we define $\alpha\triangleq{\cal CH}(A)$, $\beta\triangleq{\cal CH}(B)$,
etc. It is also customary to use the letter $\Gamma$ to denote a
set of premises, such as ${\cal CH}(X)$, ${\cal CH}(Y)$, ..., ${\cal CH}(Z)$
in Eq.~(\ref{eq:ch-example-sequent}). So, we can write a shorter
formula $\Gamma\vdash\alpha$ instead of the sequent~(\ref{eq:ch-example-sequent}).

With these notations, we will now enumerate all the possible ways
of proving that a ${\cal CH}$-proposition is true. We assume that
the set of premises $\Gamma$ is known.

\paragraph{1) Create a \lstinline!Unit! value}

At any place in the code, we may write the expression \lstinline!()!
of type \lstinline!Unit!. This expression corresponds to a proof
of the proposition ${\cal CH}(\bbnum 1)$ with any set $\Gamma$ of
premises (even with an empty set of premises). So, the sequent $\Gamma\vdash{\cal CH}(\bbnum 1)$
is always true. The code corresponding to the proof of this sequent
is an expression that creates a value of the \lstinline!Unit! type:
\[
\text{Proof}\left(\Gamma\vdash{\cal CH}(\bbnum 1)\right)=1\quad,
\]
where we denoted by $1$ the value \lstinline!()!.

In formal logic, a sequent that is found to be always true, such as
our $\Gamma\vdash{\cal CH}(\bbnum 1)$, is called an \textbf{axiom}\index{logical axiom}
and is written in the following notation,
\[
\frac{}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{create unit})\quad\quad.
\]
The ``fraction with a label'' represents a proof rule. The denominator
of the ``fraction'' is the target sequent that we need to prove.
The numerator of the ``fraction'' can have zero or more other sequents
that need to be proved before the target sequent can be proved. In
this case, the set of previous sequents is empty: the target sequent
is an axiom and so requires no previous sequents for its proof. The
label ``$\text{create unit}$'' is an arbitrary name used to refer
to the rule.

\paragraph{2) Use a given argument}

At any place within the code of a fully parametric function, we may
use one of the function's arguments, say $x^{:A}$. If some argument
has type $A$, it means that $\alpha\triangleq{\cal CH}(A)$ belongs
to the set of premises of the sequent we are trying to prove. To indicate
this, we may write the set of premises as ``$\Gamma,\alpha$''.
The code construct \lstinline!x:A! shows that we can compute a value
of type $A$, i.e.~show that $\alpha$ is true, given these premises.
This is expressed by the sequent $\Gamma,\alpha\vdash\alpha$. The
proof of this sequent corresponds to an expression that returns one
of the given arguments (which we here called $x^{:A}$),
\[
\text{Proof}\big(\Gamma,\alpha\vdash\alpha\big)=x^{:A}\quad.
\]
This sequent is an axiom since its proof requires no previous sequents,
The formal logic notation for this axiom is
\[
\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use arg})\quad\quad.
\]


\paragraph{3) Create a function}

At any place in the code, we may compute a nameless function of type,
say, $A\Rightarrow B$, by writing \lstinline!(x:A) => expr! as long
as a value \lstinline!expr! of type $B$ can be computed in the inner
scope of the function. The code for \lstinline!expr! is also required
to be fully parametric; it may use \lstinline!x! and/or other values
visible in that scope. So we now need to answer the question of whether
a fully parametric function can compute a value of type $B$, given
an argument of type $A$ as well as all other arguments previously
given to the parent function. This question is answered by a sequent
whose premises contain one more proposition, ${\cal CH}(A)$, in addition
to all previously available premises. Translating this into the language
of ${\cal CH}$-propositions, we find that we will prove the sequent
\[
\Gamma\vdash{\cal CH}(A\Rightarrow B)\quad=\quad\Gamma\vdash{\cal CH}(A)\Rightarrow{\cal CH}(B)\quad\triangleq\quad\Gamma\vdash\alpha\Rightarrow\beta
\]
if we can prove the sequent $\Gamma,{\cal CH}(A)\vdash{\cal CH}(B)=\Gamma,\alpha\vdash\beta$.
In the notation of formal logic, this is a \textbf{derivation rule}\index{derivation rule}
(rather than an axiom) and is written as
\[
\frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})\quad\quad.
\]
The \textbf{turnstile}\index{turnstile} symbol, $\vdash$, groups
weaker than other operators. So, we can write sequents such as $(\Gamma,\alpha)\vdash(\beta\Rightarrow\gamma)$
with fewer parentheses: $\Gamma,\alpha\vdash\beta\Rightarrow\gamma$.

What code corresponds to the ``$\text{create function}$'' rule?
The proof of $\Gamma\vdash\alpha\Rightarrow\beta$ depends on a proof
of another sequent. So, the corresponding code must be a \emph{function}
that takes a proof of the previous sequent as an argument and returns
a proof of the new sequent. By the CH correspondence, a proof of a
sequent corresponds to a code expression of the type given by the
goal of the sequent; the expression may use arguments of types corresponding
to the premises of the sequent. So, a proof of the sequent $\Gamma,\alpha\vdash\beta$
is an expression \lstinline!exprB! of type $B$ that may use a given
value of type $A$ as well as any other arguments given previously.
Then we can write the proof code for the sequent $\Gamma\vdash\alpha\Rightarrow\beta$
as the nameless function \lstinline!(x:A) => exprB!. This function
has type $A\Rightarrow B$ and requires us to already have a suitable
\lstinline!exprB!. This exactly corresponds to the proof rule ``$\text{create function}$''.
We may write the corresponding code as
\[
\text{Proof}(\Gamma\vdash{\cal CH}(A)\Rightarrow{\cal CH}(B))=x^{:A}\Rightarrow\text{Proof}(\Gamma,x^{:A}\vdash{\cal CH}(B))\quad.
\]
Here we wrote $x^{:A}$ instead of ${\cal CH}(A)$ since the value
$x^{:A}$ is a proof of the proposition ${\cal CH}(A)$. We will see
in Section~\ref{subsec:Example:-Proving-a-ch-proposition} how premises
such as $\Gamma,x^{:A}$ are implemented in code.

\paragraph{4) Use a function}

At any place in the code, we may apply an already defined function
of type $A\Rightarrow B$ to an already computed value of type $A$.
The result will be a value of type $B$. This corresponds to assuming
${\cal CH}(A\Rightarrow B)$ and ${\cal CH}(A)$, and then deriving
${\cal CH}(B)$. The formal logic notation for this proof rule is
\[
\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})\quad\quad.
\]
The code corresponding to this proof rule takes previously computed
values \lstinline!x:A! and \lstinline!f:A => B!, and writes the
expression \lstinline!f(x)!. This can be written as a function application,
\[
\text{Proof}(\Gamma\vdash\beta)=\text{Proof}\left(\Gamma\vdash\alpha\Rightarrow\beta\right)(\text{Proof}(\Gamma\vdash\alpha))\quad.
\]


\paragraph{5) Create a tuple}

If we have already computed some values \lstinline!x:A! and \lstinline!y:B!,
we may write the expression \lstinline!(x, y)! and so compute a value
of the tuple type \lstinline!(A, B)!. The proof rule is
\[
\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})\quad\quad.
\]
We can write the corresponding code expression as
\[
\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)=\text{Proof}\left(\Gamma\vdash\alpha\right)\times\text{Proof}\left(\Gamma\vdash\beta\right)\quad,
\]
where we write $a\times b$ to represent a pair of two values.

This rule describes creating a pair of values. A larger tuple, such
as \lstinline!(w, x, y, z)!, can be expressed via nested pairs, e.g.~as
\lstinline!(w, (x, (y, z)))!. So it is sufficient to have a sequent
rule for creating pairs; this rule can express the sequent rules for
creating all other tuples, and we do not need to define separate rules
for, say, $\Gamma\vdash\alpha\wedge\beta\wedge\gamma$.

\paragraph{6) Use a tuple}

If we already have a value \lstinline!t:(A,B)! of a tuple type $A\times B$,
we can extract one of the parts of the tuple and obtain a value of
type \lstinline!A! or a value of type \lstinline!B!. The code is
\lstinline!t._1! and \lstinline!t._2! respectively, and the corresponding
sequent proof rules are
\[
\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)\quad\quad\quad\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\beta}\quad(\text{use tuple-}2)\quad\quad.
\]
The code can be written as
\begin{align*}
\text{Proof}\left(\Gamma\vdash\alpha\right) & =\nabla_{1}\left(\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)\right)\quad,\\
\text{Proof}\left(\Gamma\vdash\beta\right) & =\nabla_{2}\left(\text{Proof}\left(\Gamma\vdash\alpha\wedge\beta\right)\right)\quad,
\end{align*}
where we introduced the notation $\nabla_{1}$ and $\nabla_{2}$ to
mean the Scala code \lstinline!_._1! and \lstinline!_._2!.

Since all tuples can be expressed through pairs, it is sufficient
to have proof rules for pairs.

\paragraph{7) Create a disjunctive value}

The disjunctive type \lstinline!Either[A, B]! corresponding to the
disjunction $\alpha\vee\beta$ can be used to define any other disjunctive
type; for instance, a disjunctive type with three parts can be expressed
as \lstinline!Either[A, Either[B, C]]!. So it is sufficient to have
proof rules for a disjunction of \emph{two} propositions.

There are two ways of creating a value of the type \lstinline!Either[A, B]!:
the code expressions are \lstinline!Left(x:A)! and \lstinline!Right(y:B)!.
The values \lstinline!x:A! or \lstinline!y:B! must have been computed
previously (and correspond to previously proved sequents). So, the
sequent proof rules are
\[
\frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Left})\quad\quad\quad\frac{\Gamma\vdash\beta}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Right})\quad\quad.
\]
The corresponding code can be written as 
\begin{align*}
\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right) & =\text{Left}(\text{Proof}\left(\Gamma\vdash\alpha\right))\quad,\\
\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right) & =\text{Right}(\text{Proof}\left(\Gamma\vdash\beta\right))\quad.
\end{align*}


\paragraph{8) Use a disjunctive value}

The only way we may use a value of disjunctive type \lstinline!Either[A, B]!
is by pattern matching on it, which will compute a result of some
other type \lstinline!C!. The code is
\begin{lstlisting}
val result: C = (e: Either[A, B]) match {
  case Left(x:A)    => expr1(x)
  case Right(y:B)   => expr2(y)
}
\end{lstlisting}
Here, \lstinline!expr1(x)! must be an expression of type \lstinline!C!
computed using \lstinline!x:A! and any previously available arguments
(i.e.~the premises $\Gamma$). Similarly, \lstinline!expr2(y)! must
be an expression of type \lstinline!C! computed using \lstinline!y:B!
and previous arguments. It is clear that \lstinline!expr1(x)! represents
a proof of a sequent with an additional premise of type \lstinline!A!,
i.e.~$\Gamma,\alpha\vdash\gamma$, where we denoted $\gamma\triangleq{\cal CH}(C)$.
Similarly, \lstinline!expr2(y)! is a proof of the sequent $\Gamma,\beta\vdash\gamma$.
So, the proof rule corresponding to the \lstinline!match! expression
is
\[
\frac{\Gamma\vdash\alpha\vee\beta\quad\quad\Gamma,\alpha\vdash\gamma\quad\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use Either})\quad\quad.
\]
The code can be written as 
\[
\text{Proof}\left(\Gamma\vdash\gamma\right)=\text{Proof}\left(\Gamma\vdash\alpha\vee\beta\right)\text{ match }\begin{cases}
\text{have }x^{:A}: & \text{Proof}\left(\Gamma,x^{:A}\vdash\gamma\right)\\
\text{have }y^{:B}: & \text{Proof}(\Gamma,y^{:B}\vdash\gamma)
\end{cases}\quad.
\]

Table~\ref{tab:Proof-rules-for-constructive-logic} summarizes the
eight proof rules derived in this section. These proof rules define
a logic known as the \textbf{\index{intuitionistic propositional logic}intuitionistic
propositional logic} or \textbf{\index{constructive propositional logic}constructive
propositional logic}. We will call this logic ``constructive'' for
short.

\begin{table}
\begin{align*}
{\color{greenunder}\text{axioms}:}\quad & \frac{~}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{create unit})\quad\quad\quad\quad\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use arg})\\
{\color{greenunder}\text{derivation rules}:}\quad & \frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})\\
 & \frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})\\
 & \frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})\\
 & \frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)\quad\quad\quad\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\beta}\quad(\text{use tuple-}2)\\
 & \frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Left})\quad\quad\quad\frac{\Gamma\vdash\beta}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Right})\\
 & \frac{\Gamma\vdash\alpha\vee\beta\quad\quad\Gamma,\alpha\vdash\gamma\quad\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use Either})
\end{align*}

\caption{Proof rules for the constructive logic.\label{tab:Proof-rules-for-constructive-logic}}
\end{table}


\subsection{Example: Proving a ${\cal CH}$-proposition and deriving code\label{subsec:Example:-Proving-a-ch-proposition}}

The task is to implement a fully parametric function with type signature
\begin{lstlisting}
def f[A, B]: ((A => A) => B) => B = ???
\end{lstlisting}

Being able to implement this function is the same as being able to
compute a value of type 
\[
F\triangleq\forall(A,B).\,((A\Rightarrow A)\Rightarrow B)\Rightarrow B\quad.
\]
Since the type parameters $A$ and $B$ are arbitrary, the body of
the fully parametric function \lstinline!f! cannot use any previously
defined values of types $A$ or $B$. So, the task is formulated as
computing a value of type $F$ with no previously defined values.
This is written as the sequent $\Gamma\vdash{\cal CH}(F)$, where
the set $\Gamma$ of premises is empty, $\Gamma=\emptyset$. Rewriting
this sequent using the rules of Table~\ref{tab:ch-correspondence-type-notation-CH-propositions},
we get
\begin{equation}
\forall(\alpha,\beta).\;\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta\quad,\label{eq:ch-example-sequent-2}
\end{equation}
where we denoted $\alpha\triangleq{\cal CH}(A)$ and $\beta\triangleq{\cal CH}(B)$. 

The next step is to prove the sequent~(\ref{eq:ch-example-sequent-2})
using the logic proof rules of Section~\ref{subsec:The-rules-of-proof}.
For brevity, we will omit the quantifier $\forall(\alpha,\beta)$
since it will be present in front of every sequent.

Begin by looking for a proof rule whose ``denominator'' has a sequent
similar to Eq.~(\ref{eq:ch-example-sequent-2}), i.e.~has an implication
($p\Rightarrow q$) in the goal. We have only one rule that can prove
a sequent of the form $\Gamma\vdash(p\Rightarrow q$); this is the
rule ``$\text{create function}$''. That rule requires us to already
have a proof of the sequent $(\Gamma,p)\vdash q$. So, we use this
rule with $\Gamma=\emptyset$, $p=(\alpha\Rightarrow\alpha)\Rightarrow\beta$,
and $q=\beta$: 
\[
\frac{(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta}{\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta}\quad.
\]
We now need to prove the sequent $(\alpha\Rightarrow\alpha)\Rightarrow\beta)\vdash\beta$,
which we can write as $\Gamma_{1}\vdash\beta$ where we defined $\Gamma_{1}\triangleq[(\alpha\Rightarrow\alpha)\Rightarrow\beta]$
to be the set containing the single premise $(\alpha\Rightarrow\alpha)\Rightarrow\beta$. 

There are no proof rules that derive a sequent with an explicit premise
of the form of an implication $p\Rightarrow q$. However, we have
a rule called ``$\text{use function}$'' that derives a sequent
by assuming another sequent containing an implication. We would be
able to use that rule,
\[
\frac{\Gamma_{1}\vdash\alpha\Rightarrow\alpha\quad\quad\Gamma_{1}\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta}{\Gamma_{1}\vdash\beta}\quad,
\]
if we could prove the two sequents $\Gamma_{1}\vdash\alpha\Rightarrow\alpha$
and $\Gamma_{1}\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$.
To prove these sequents, note that the rule ``$\text{create function}$''
applies to $\Gamma_{1}\vdash\alpha\Rightarrow\alpha$ as follows,
\[
\frac{\Gamma_{1},\alpha\vdash\alpha}{\Gamma_{1}\vdash\alpha\Rightarrow\alpha}\quad.
\]
The sequent $\Gamma_{1},\alpha\vdash\alpha$ is proved directly by
the axiom ``$\text{use arg}$''. The sequent $\Gamma_{1}\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$
is also proved by the axiom ``$\text{use arg}$'' because $\Gamma_{1}$
already contains $(\alpha\Rightarrow\alpha)\Rightarrow\beta$.

The proof of the sequent~(\ref{eq:ch-example-sequent-2}) is now
complete and can be visualized as a tree (Figure~\ref{fig:Proof-of-the-sequent-example-2}).
The next step is to derive the code from this proof. 

To do that, we combine the code expressions that correspond to each
of the proof rules we used. We need to retrace the proof backwards,
starting from the leaves of the tree and going towards the root, and
to assemble the $\text{Proof}\left(...\right)$ code expressions one
by one.

\begin{figure}
\begin{centering}
 \Tree[ .$\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta $ [ .\smaller{rule ``$\text{create function}$''} [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta$ [ .\smaller{rule ``$\text{use function}$''} [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\alpha\Rightarrow\alpha$ [ .\smaller{rule ``$\text{create function}$''} [ .$\Gamma_1,\alpha\vdash\alpha$ \smaller{axiom ``$\text{use arg}$''} ] ] ] [ .$(\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta$ \smaller{axiom ``$\text{use arg}$''} ] ] ] ] ]
\par\end{centering}
\caption{Proof tree for the sequent~(\ref{eq:ch-example-sequent-2}).\label{fig:Proof-of-the-sequent-example-2}}

\end{figure}

Begin with the left-most leaf ``$\text{use arg}$''. This rule corresponds
to the code $x^{:A}$,
\[
\text{Proof}\left(\Gamma_{1},\alpha\vdash\alpha\right)=x^{:A}\quad.
\]
Here $x^{:A}$ must be a proof of the premise $\alpha$ in the sequent
$\Gamma_{1},\alpha\vdash\alpha$. So, we need to use the same $x^{:A}$
when we write the code for the previous rule, ``$\text{create function}$'':
\[
\text{Proof}\left(\Gamma_{1}\vdash\alpha\Rightarrow\alpha\right)=(x^{:A}\Rightarrow\text{Proof}\left(\Gamma_{1},\alpha\vdash\alpha\right))=(x^{:A}\Rightarrow x)\quad.
\]
The right-most leaf ``$\text{use arg}$'' corresponds to the code
$f^{:(A\Rightarrow A)\Rightarrow B}$, where $f$ is the premise contained
in $\Gamma_{1}$. So we can write
\[
\text{Proof}\left(\Gamma_{1}\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta\right)=f^{:(A\Rightarrow A)\Rightarrow B}\quad.
\]
The previous rule, ``$\text{use function}$'', combines the two
preceding proofs:
\begin{align*}
 & \text{Proof}\left((\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta\right)\\
 & =\text{Proof}(\Gamma_{1}\vdash(\alpha\Rightarrow\alpha)\Rightarrow\beta)\left(\text{Proof}(\Gamma_{1}\vdash\alpha\Rightarrow\alpha)\right)\\
 & =f(x^{:A}\Rightarrow x)\quad.
\end{align*}
Keep going backwards and find that the rule applied before ``$\text{use function}$''
is ``$\text{create function}$''. We need to provide the same $f^{:\left(A\Rightarrow A\right)\Rightarrow B}$
as in the premise above, and so we obtain the code
\begin{align*}
 & \text{Proof}\left(\emptyset\vdash((\alpha\Rightarrow\alpha)\Rightarrow\beta)\Rightarrow\beta\right)\\
 & =f^{:\left(A\Rightarrow A\right)\Rightarrow B}\Rightarrow\text{Proof}\left((\alpha\Rightarrow\alpha)\Rightarrow\beta\vdash\beta\right)\\
 & =f^{:\left(A\Rightarrow A\right)\Rightarrow B}\Rightarrow f(x^{:A}\Rightarrow x)\quad.
\end{align*}
This is the final code expression that implements the type $(\left(A\Rightarrow A\right)\Rightarrow B)\Rightarrow B$.
In this way, we have systematically derived the code from the type
signature of a function. This function can be implemented in Scala
as
\begin{lstlisting}
def f[A, B]: ((A => A) => B) => B = { f => f(x => x) }
\end{lstlisting}

We found the proof tree in Figure~\ref{fig:Proof-of-the-sequent-example-2}
by guessing how to combine various proof rules. If we \emph{somehow}
find a proof tree for a sequent, we can prove the sequent and derive
the corresponding code. However, it is not always obvious how to combine
the proof rules for a given initial sequent, particularly when disjunctive
types are present. This is so because the rules of Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
do not provide an algorithm for finding a proof tree automatically
for a given initial sequent. It turns out that such an algorithm exists
(the ``LJT algorithm'', see Appendix~\ref{app:The-Curry-Howard-correspondence}).
That algorithm can find proofs and derive the code for any fully parametric
type signature with type parameters, tuples, disjunctive types, and
function types (if the given type signature can be implemented).

The library \texttt{curryhoward}\footnote{\texttt{\href{https://github.com/Chymyst/curryhoward}{https://github.com/Chymyst/curryhoward}}}
implements the LJT algorithm. Here are some examples of using this
library. We will run the \texttt{ammonite}\footnote{\texttt{\href{http://ammonite.io/\#Ammonite-Shell}{http://ammonite.io/\#Ammonite-Shell}}}
shell to load the library more easily.

Consider the type signature 
\[
\forall(A,B).\,\left(\left(\left(\left(A\Rightarrow B\right)\Rightarrow A\right)\Rightarrow A\right)\Rightarrow B\right)\Rightarrow B\quad.
\]
It is not immediately clear whether it is even possible to implement
a function with this type signature. It turns out that it \emph{is}
possible, and the code can be derived automatically with help of the
LJT algorithm. The library does this via the method \lstinline!implement!:
\begin{lstlisting}
@ import $ivy.`io.chymyst::curryhoward:0.3.7`, io.chymyst.ch._

@ def f[A, B]: ((((A => B) => A) => A) => B) => B = implement
defined function f

@ println(f.lambdaTerm.prettyPrint)
a => a (b => b (c => a (d => c)))
\end{lstlisting}
The automatically derived code for the function \lstinline!f! is
shown in a short code notation as $a\Rightarrow a\left(b\Rightarrow b\left(c\Rightarrow a\left(d\Rightarrow c\right)\right)\right)$.
The function \lstinline!f! has been already compiled and is ready
to be used in any subsequent code.

A compile-time error occurs when trying to use a type signature that
cannot be implemented as a fully parametric function:
\begin{lstlisting}
@ def g[A, B]: ((A => B) => A) => A = implement
cmd3.sc:1: type ((A => B) => A) => A cannot be implemented
def g[A, B]: ((A => B) => A) => A = implement
                                    ^
Compilation Failed
\end{lstlisting}
The logical formula corresponding to this type signature is 
\begin{equation}
\forall(\alpha,\beta).\,\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\Rightarrow\beta\quad.\label{eq:ch-example-3-peirce-law}
\end{equation}
This formula, known as ``Peirce's law'',\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Peirce\%27s_law}{https://en.wikipedia.org/wiki/Peirce\%27s\_law}}}
is another example of the failure of Boolean logic to describe the
logic of types in functional programming languages. Peirce's law is
a true theorem in Boolean logic, but it does not hold in the constructive
logic (i.e.~it cannot be derived using the proof rules of Table~\ref{tab:Proof-rules-of-constructive-and-boolean}).
If we try to implement \lstinline!g[A, B]! with the type signature
shown above, we will fail to write fully parametric code for \lstinline!g!
that compiles without type errors. We will fail because no such code
exists, \textendash{} not because we are insufficiently clever. The
LJT algorithm can \emph{prove} that the given type signature cannot
be implemented; the \texttt{curryhoward} library will then print an
error message, and compilation will fail.

As another example, let us verify that the type signature from Section~\ref{subsec:Example:-Failure-of-Boolean-logic}
cannot be implemented as a fully parametric function:
\begin{lstlisting}
@ def bad[A, B, C](g: A => Either[B, C]): Either[A => B, A => C] = implement
cmd4.sc:1: type (A => Either[B, C]) => Either[A => B, A => C] cannot be implemented
def bad[A, B, C](g: A => Either[B, C]): Either[A => B, A => C] = implement
                                                                 ^
Compilation Failed
\end{lstlisting}


\section{Solved examples: Equivalence of types}

We found a correspondence between types, code, logical propositions,
and proofs, which is known as the \textbf{Curry-Howard correspondence}\index{Curry-Howard correspondence}.
An example of the CH correspondence is that a proof of the logical
proposition
\begin{equation}
\forall(\alpha,\beta).\,\alpha\Rightarrow\left(\beta\Rightarrow\alpha\right)\label{eq:ch-proposition-example-2}
\end{equation}
corresponds to the code of the function 
\begin{lstlisting}
def f[A, B]: A => (B => A) = { x => _ => x }
\end{lstlisting}
With the CH correspondence in mind, we may say that the function \lstinline!f!'s
code ``is'' the proof of the proposition~(\ref{eq:ch-proposition-example-2}).
In this sense, the \emph{existence} of the code \lstinline!x => _ => x!
with the type $A\Rightarrow(B\Rightarrow A)$ ``is'' a proof of
the logical formula~(\ref{eq:ch-proposition-example-2}).

The Curry-Howard correspondence maps logic formulas such as $(\alpha\vee\beta)\wedge\gamma$
into type expressions such as $\left(A+B\right)\times C$. We have
seen that types behave similarly to logic formulas in one respect:
A logic formula is a true theorem when the corresponding type signature
can be implemented as a fully parametric function, and vice versa.

It turns out that the similarity ends here. In other respects, type
expressions behave as \emph{arithmetic} expressions and not as logic
formulas. For this reason, the type notation used in this book denotes
disjunctive types by $A+B$ and tuples by $A\times B$, which is designed
to remind us of arithmetic expressions (such as $1+2$ and $2\times3$)
rather than of logic formulas (such as $A\vee B$ and $A\wedge B$). 

The most important use of the type notation is for writing equations
with types. Can we write type equations using the arithmetic intuition,
such as 
\begin{equation}
\left(A+B\right)\times C=A\times C+B\times C\quad?\label{eq:ch-example-distributive}
\end{equation}
In this section, we will learn how to check whether one type expression
is equivalent to another.

\subsection{Identity in logic does not correspond to type equivalence\label{subsec:Identity-in-logic-not-type-equivalence}}

The CH correspondence maps Eq.~(\ref{eq:ch-example-distributive})
into the logic formula
\begin{equation}
\forall(A,B,C).\,\left(A\vee B\right)\wedge C=\left(A\wedge C\right)\vee\left(B\wedge C\right)\quad.\label{eq:ch-example-distributive-1}
\end{equation}
This formula is the well-known ``distributive law''\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Distributive_property\#Rule_of_replacement}{https://en.wikipedia.org/wiki/Distributive\_property\#Rule\_of\_replacement}}}
valid in Boolean logic as well as in the constructive logic. Since
a logical equation $P=Q$ means $P\Rightarrow Q$ and $Q\Rightarrow P$,
the distributive law~(\ref{eq:ch-example-distributive-1}) means
that the two formulas hold,
\begin{align}
 & \forall(A,B,C).\,\left(A\vee B\right)\wedge C\Rightarrow\left(A\wedge C\right)\vee\left(B\wedge C\right)\quad,\label{eq:ch-example-distributive-1a}\\
 & \forall(A,B,C).\,\left(A\wedge C\right)\vee\left(B\wedge C\right)\Rightarrow\left(A\vee B\right)\wedge C\quad.\label{eq:ch-example-distributive-1b}
\end{align}
The CH correspondence maps these logical formulas to fully parametric
functions with types
\begin{lstlisting}
def f1[A, B, C]: ((Either[A, B], C))    => Either[(A, C), (B, C)] = ???
def f2[A, B, C]: Either[(A, C), (B, C)] => (Either[A, B], C) = ???
\end{lstlisting}
In the type notation, these type signatures are written as
\begin{align*}
 & f_{1}:\forall(A,B,C).\,\left(A+B\right)\times C\Rightarrow A\times C+B\times C\quad,\\
 & f_{2}:\forall(A,B,C).\;A\times C+B\times C\Rightarrow\left(A+B\right)\times C\quad.
\end{align*}
Since the two logical formulas (\ref{eq:ch-example-distributive-1a})\textendash (\ref{eq:ch-example-distributive-1b})
are true theorems in constructive logic, we expect to be able to implement
the functions \lstinline!f1! and \lstinline!f2!. It is not straightforward
to guess how to combine the proof rules of Table~\ref{tab:Proof-rules-of-constructive-and-boolean}
to obtain proofs of Eqs.~(\ref{eq:ch-example-distributive-1a})\textendash (\ref{eq:ch-example-distributive-1b}).
So, instead of deriving the implementations of \lstinline!f1! and
\lstinline!f2! from the CH correspondence, we will write the Scala
code directly.

To implement \lstinline!f1!, we need to do pattern matching on the
argument:
\begin{lstlisting}
def f1[A, B, C]: ((Either[A, B], C)) => Either[(A, C), (B, C)] = {
  case (Left(a), c)   => Left((a, c))  // No other choice here.
  case (Right(b), c)  => Right((b, c)) // No other choice here.
}
\end{lstlisting}
In both cases, we have only one possible expression of the correct
type.

Similarly, the implementation of \lstinline!f2! leaves us no choices:
\begin{lstlisting}
def f2[A, B, C]: Either[(A, C), (B, C)] => (Either[A, B], C) = {
  case Left((a, c))   => (Left(a), c)  // No other choice here.
  case Right((b, c))  => (Right(b), c) // No other choice here.
}
\end{lstlisting}

The code of \lstinline!f1! and \lstinline!f2! never discards any
given values; in other words, these functions appear to preserve information.
We can formulate this property rigorously as a requirement that an
arbitrary value \lstinline!x: (Either[A, B], C)! is mapped by \lstinline!f1!
to some value \lstinline!y: Either[(A, C), (B, C)]! and then mapped
by \lstinline!f2! back to \emph{the same} value \lstinline!x!. Similarly,
any value \lstinline!y: Either[(A, C), (B, C)]! should be transformed
by \lstinline!f2! and then by \lstinline!f1! back to the \emph{same
value} \lstinline!y!. 

Let us write these conditions as equations,
\[
\forall x^{:(A+B)\times C}.\,f_{2}(f_{1}(x))=x\quad,\quad\quad\forall y^{:A\times C+B\times C}.\,f_{1}\left(f_{2}(y)\right)=y\quad.
\]
If we show that these equations hold, it will follow that all the
information in a value $x^{:(A+B)\times C}$ is completely preserved
inside the value $y\triangleq f_{1}(x)$; the original value $x$
can be recovered as $x=f_{2}(y)$. Conversely, all the information
in a value $y^{:A\times C+B\times C}$ is preserved inside $x\triangleq f_{2}(y)$
and can be recovered by applying $f_{1}$. Since the values $x^{:(A+B)\times C}$
and $y^{:A\times C+B\times C}$ are arbitrary, it will follow the
\emph{data types} themselves, $\left(A+B\right)\times C$ and $A\times C+B\times C$,
carry equivalent information. Such types are called \textbf{equivalent}\index{types!equivalent}
or \textbf{isomorphic}\index{types!isomorphic}\index{isomorphic types}.

Generally, we say that types $P$ and $Q$ are \textbf{equivalent}
or \textbf{isomorphic} (denoted $P\cong Q$) \index{type equivalence}when
there exist functions $f_{1}^{:P\Rightarrow Q}$ and $f_{2}^{:Q\Rightarrow P}$
that are inverses of each other. We can write these conditions using
the notation $(f_{1}\bef f_{2})(x)\triangleq f_{2}(f_{1}(x))$ as
\[
f_{1}\bef f_{2}=\text{id}\quad,\quad\quad f_{2}\bef f_{1}=\text{id}\quad.
\]
(We omit type annotations since we already checked that the types
match. In Scala, the forward composition $f_{1}\bef f_{2}$ is the
function \lstinline!f1 andThen f2!.) If these conditions hold, there
is a one-to-one correspondence between the values of types $P$ and
$Q$. This is the same as to say that the data types $P$ and $Q$
``carry equivalent information''.

To verify that the Scala functions \lstinline!f1! and \lstinline!f2!
defined above are inverses of each other, we first check if $f_{1}\bef f_{2}=\text{id}$.
Applying $f_{1}\bef f_{2}$ means to apply $f_{1}$ and then to apply
$f_{2}$ to the result. Begin by applying $f_{1}$ to an arbitrary
value $x^{:(A+B)\times C}$. A value $x$ of that type can be in only
one of the two disjoint cases: a tuple \lstinline!(Left(a), c)! or
a tuple \lstinline!(Right(b), c)!, for some values \lstinline!a:A!,
\lstinline!b:B!, and \lstinline!c:C!. The Scala code of \lstinline!f1!
maps these tuples to \lstinline!Left((a, c))! and to \lstinline!Right((b, c))!
respectively; we can see this directly from the code of \lstinline!f1!.
We then apply $f_{2}$ to those values, which maps them back to a
tuple \lstinline!(Left(a), c)! or a tuple \lstinline!(Right(b), c)!
respectively, according to the code of \lstinline!f2!. These tuples
are exactly the value $x$ we started with. So, applying $f_{1}\bef f_{2}$
to an arbitrary $x^{:(A+B)\times C}$ does not change the value $x$;
this is the same as to say that $f_{1}\bef f_{2}=\text{id}$.

To check whether $f_{2}\bef f_{1}=\text{id}$, we apply $f_{2}$ to
an arbitrary value $y^{:A\times C+B\times C}$, which must be one
of the two disjoint cases, \lstinline!Left((a, c))! or \lstinline!Right((b, c))!.
The code of \lstinline!f2! maps these two cases into tuples \lstinline!(Left(a), c)!
and \lstinline!(Right(b), c)! respectively. Then we apply \lstinline!f1!
and map these tuples back to \lstinline!Left((a, c))! and \lstinline!Right((b, c))!
respectively. It follows that applying $f_{2}$ and then $f_{1}$
will always recover the initial value $y$. In other words, $f_{2}\bef f_{1}=\text{id}$.

By looking at the code of \lstinline!f1! and \lstinline!f2!, we
can directly observe that these functions are inverses of each other:
the tuple pattern \lstinline!(Left(a), c)! is mapped to \lstinline!Left((a, c))!,
and the pattern \lstinline!(Right(b), c)! to \lstinline!Right((b, c))!,
or vice versa. It is then visually clear that no information is lost
and that the original values are restored by function compositions
$f_{1}\bef f_{2}$ or $f_{2}\bef f_{1}$.

We find that the logical identity~(\ref{eq:ch-example-distributive-1})
leads to an equivalence of the corresponding types,
\begin{equation}
\left(A+B\right)\times C\cong A\times C+B\times C\quad.\label{eq:ch-distributive-law-types}
\end{equation}

Consider another example of a logical identity: the associativity
law for conjunction,
\begin{equation}
\left(\alpha\wedge\beta\right)\wedge\gamma=\alpha\wedge\left(\beta\wedge\gamma\right)\quad.\label{eq:ch-example-associativity-conjunction}
\end{equation}
The corresponding types are $(A\times B)\times C$ and $A\times(B\times C)$;
in Scala, \lstinline!((A, B), C)! and \lstinline!(A, (B, C))!. We
can define functions that convert between these types without information
loss:
\begin{lstlisting}
def f3[A, B, C]: (((A, B), C)) => (A, (B, C)) = { case ((a, b), c) => (a, (b, c)) }
def f4[A, B, C]: (A, (B, C)) => (((A, B), C)) = { case (a, (b, c)) => ((a, b), c) }
\end{lstlisting}
By applying these functions to arbitrary values of types \lstinline!((A, B), C)!
and \lstinline!(A, (B, C))!, it is easy to see that the functions
\lstinline!f3! and \lstinline!f4! are inverses of each other. This
is also directly visible in the code: the nested tuple pattern \lstinline!((a, b), c)!
is mapped to the pattern \lstinline!(a, (b, c))! and back. So, the
types $\left(A\times B\right)\times C$ and $A\times\left(B\times C\right)$
are equivalent, and we can write $A\times B\times C$ without parentheses.

Does a logical identity always correspond to an equivalence of types?
This turns out to be \emph{not} so. A simple example of a logical
identity that does not correspond to a type equivalence is
\begin{equation}
True\vee\alpha=True\quad.\label{eq:ch-example-logic-identity-2}
\end{equation}
Since the CH correspondence maps the logical constant $True$ into
the \lstinline!Unit! type $\bbnum 1$, the corresponding types are
$\bbnum 1+A$ and $\bbnum 1$. The type denoted by $\bbnum 1+A$ is
the same as \lstinline!Option[A]! in Scala. So, the corresponding
type equivalence would be $\bbnum 1+A\cong\bbnum 1$ (in Scala, \lstinline!Option[A]!$\cong$\lstinline!Unit!).
It is intuitively clear that this type equivalence does not hold:
an \lstinline!Option[A]! may carry a value of type \lstinline!A!,
which cannot possibly be stored in a value of type \lstinline!Unit!.
We can verify this intuition rigorously by proving that any fully
parametric functions with type signatures $g_{1}:\bbnum 1+A\Rightarrow\bbnum 1$
and $g_{2}:\bbnum 1\Rightarrow\bbnum 1+A$ will not satisfy $g_{1}\bef g_{2}=\text{id}$.
To verify this, we note that $g_{2}:\bbnum 1\Rightarrow\bbnum 1+A$
must be a Scala function with the type signature
\begin{lstlisting}
def g2[A]: Unit => Option[A] = ???
\end{lstlisting}
Such a function must always return \lstinline!None!, since a fully
parametric function cannot produce values of an arbitrary type \lstinline!A!
from scratch. Therefore, $g_{1}\bef g_{2}$ is also a function that
always returns \lstinline!None!. The function $g_{1}\bef g_{2}$
has type signature $\bbnum 1+A\Rightarrow\bbnum 1+A$ or, in Scala
syntax, \lstinline!Option[A] => Option[A]!, and is not equal to the
identity function, because the identity function does not \emph{always}
return \lstinline!None!.

Another example of a logical identity without a type equivalence is
the distributive law 
\begin{equation}
\forall(A,B,C).\,\left(A\wedge B\right)\vee C=\left(A\vee C\right)\wedge\left(B\vee C\right)\quad,\label{eq:ch-example-distributive-2}
\end{equation}
which is ``dual'' to the law~(\ref{eq:ch-example-distributive-1}),
i.e.~it is obtained from Eq.~(\ref{eq:ch-example-distributive-1})
by swapping all conjunctions ($\wedge$) with disjunctions ($\vee$).
In logic, a dual formula to an identity is often also an identity.
The CH correspondence maps Eq.~(\ref{eq:ch-example-distributive-2})
into the type equation
\begin{equation}
\forall(A,B,C).\,\left(A\times B\right)+C=\left(A+C\right)\times\left(B+C\right)\quad.\label{eq:ch-example-incorrect-identity-2}
\end{equation}
However, the types $A\times B+C$ and $\left(A+C\right)\times\left(B+C\right)$
are \emph{not} equivalent. This can be seen by looking at the possible
code of functions $g_{3}:\left(A+C\right)\times\left(B+C\right)\Rightarrow A\times B+C$
and $g_{4}:A\times B+C\Rightarrow\left(A+C\right)\times\left(B+C\right)$.
The code of $g_{3}$ is
\begin{lstlisting}
def g3[A,B,C]: ((Either[A, C], Either[B, C])) => Either[(A, B), C] = {
  case (Left(a), Left(b))      => Left((a, b)) // No other choice.
  case (Left(a), Right(c))     => Right(c)     // No other choice.
  case (Right(c), Left(b))     => Right(c)     // No other choice.
  case (Right(c1), Right(c2))  => Right(c1)    // Must discard c1 or c2 here!
} // May return Right(c2) instead of Right(c1) in the last line.
\end{lstlisting}
In the last line, we have a choice of returning \lstinline!Right(c1)!
or \lstinline!Right(c2)!. Whichever we choose, we will lose information
because we will have discarded one of the given values \lstinline!c1!,
\lstinline!c2!. After evaluating $g_{3}$, we will not be able to
restore \emph{both} \lstinline!c1! and \lstinline!c2! \textendash{}
no matter what code we write for $g_{4}$. So, the composition $g_{3}\bef g_{4}$
cannot be equal to the identity function. The type equation~(\ref{eq:ch-example-incorrect-identity-2})
is incorrect.

We conclude that a logical identity ${\cal CH}(P)={\cal CH}(Q)$ guarantees,
via the CH correspondence, that we can implement \emph{some} fully
parametric functions of types $P\Rightarrow Q$ and $Q\Rightarrow P$.
However, it is not guaranteed that these functions are inverses of
each other, i.e.~that the type conversions $P\Rightarrow Q$ or $Q\Rightarrow P$
have no information loss. So, the type equivalence $P\cong Q$ does
not automatically follow from the logical identity ${\cal CH}(P)={\cal CH}(Q)$.

The CH correspondence means that we can compute \emph{some} value
$x^{:X}$ of a given type $X$ when the proposition ${\cal CH}(X)$
holds. However, the CH correspondence does not guarantee that the
computed value $x^{:X}$ will satisfy any additional properties or
laws.

\subsection{Arithmetic identities correspond to type equivalences}

Looking at the examples of equivalent types, we notice that correct
type equivalences correspond to \emph{arithmetical} identities rather
than \emph{logical} identities. For instance, the logical identity
in Eq.~(\ref{eq:ch-example-distributive-1}) leads to the type equivalence~(\ref{eq:ch-distributive-law-types}),
which looks like a standard identity of arithmetic, such as
\[
(1+10)\times20=1\times20+10\times20\quad.
\]
The logical identity in Eq.~(\ref{eq:ch-example-distributive-2}),
which does \emph{not} yield a type equivalence, leads to an incorrect
arithmetic equation~\ref{eq:ch-example-incorrect-identity-2}, e.g.~$\left(1\times10\right)+20\neq\left(1+20\right)\times\left(10+20\right)$.
Similarly, the associativity law~(\ref{eq:ch-example-associativity-conjunction})
leads to a type equivalence and to the arithmetic identity
\[
\left(a\times b\right)\times c=a\times\left(b\times c\right)\quad,
\]
while the logical identity in Eq.~(\ref{eq:ch-example-logic-identity-2}),
which does not yield a type equivalence, leads to an incorrect arithmetic
statement ($\forall a.\,1+a=1$).

Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction}
summarizes these and other examples of logical identities, with the
corresponding type equivalences. In all rows, quantifiers such as
$\forall\alpha$ or $\forall(A,B)$ are implied when necessary.

Because we chose the type notation to be similar to the ordinary arithmetic
notation, it is easy to translate a possible type equivalence into
an arithmetic equation. In all cases, valid arithmetic identities
correspond to type equivalences, and failures to obtain a type equivalence
correspond to incorrect arithmetic identities. With regard to type
equivalence, types such as $A+B$ and $A\times B$ behave similarly
to arithmetic expressions such as $10+20$ and $10\times20$ and not
similarly to logical formulas such as $\alpha\vee\beta$ and $\alpha\wedge\beta$.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|}
\hline 
\textbf{Logical identity} & \textbf{Type equivalence (if it holds)}\tabularnewline
\hline 
\hline 
$True\vee\alpha=True$ & $\bbnum 1+A\not\cong\bbnum 1$\tabularnewline
\hline 
$True\wedge\alpha=\alpha$ & $\bbnum 1\times A\cong A$\tabularnewline
\hline 
$False\vee\alpha=\alpha$ & $\bbnum 0+A\cong A$\tabularnewline
\hline 
$False\wedge\alpha=False$ & $\bbnum 0\times A\cong\bbnum 0$\tabularnewline
\hline 
$\alpha\vee\beta=\beta\vee\alpha$ & $A+B\cong B+A$\tabularnewline
\hline 
$\alpha\wedge\beta=\beta\wedge\alpha$ & $A\times B\cong B\times A$\tabularnewline
\hline 
$\left(\alpha\vee\beta\right)\vee\gamma=\alpha\vee\left(\beta\vee\gamma\right)$ & $\left(A+B\right)+C\cong A+\left(B+C\right)$\tabularnewline
\hline 
$\left(\alpha\wedge\beta\right)\wedge\gamma=\alpha\wedge\left(\beta\wedge\gamma\right)$ & $\left(A\times B\right)\times C\cong A\times\left(B\times C\right)$\tabularnewline
\hline 
$\left(\alpha\vee\beta\right)\wedge\gamma=\left(\alpha\wedge\gamma\right)\vee\left(\beta\wedge\gamma\right)$ & $\left(A+B\right)\times C\cong A\times C+B\times C$\tabularnewline
\hline 
$\left(\alpha\wedge\beta\right)\vee\gamma=\left(\alpha\vee\gamma\right)\wedge\left(\beta\vee\gamma\right)$ & $\left(A\times B\right)+C\not\cong\left(A+C\right)\times\left(B+C\right)$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logical identities using disjunction and conjunction, with the equivalent
types.\label{tab:Logical-identities-with-disjunction-and-conjunction}}
\end{table}

We already verified the first line and the last three lines of Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction}.
Other identities are verified in a similar way. Let us begin with
the 3rd and 4th lines of Table~\ref{tab:Logical-identities-with-disjunction-and-conjunction},
which involve the proposition $False$ and the corresponding \index{void type}void
type $\bbnum 0$, defined in Scala as \lstinline!Nothing!. Reasoning
about the void type needs a special technique that we will now develop
while verifying the type isomorphisms $\bbnum 0\times A\cong\bbnum 0$
and $\bbnum 0+A\cong A$.

\subsubsection{Example \label{subsec:ch-Example-0-times-A}\ref{subsec:ch-Example-0-times-A}\index{solved examples}}

Verify the type equivalence $\bbnum 0\times A\cong\bbnum 0$.

\subparagraph{Solution}

Recall that the type notation $\bbnum 0\times A$ represents the Scala
tuple type \lstinline!(Nothing, A)!. To demonstrate that the type
\lstinline!(Nothing, A)! is equivalent to the type \lstinline!Nothing!,
we need to show that the type \lstinline!(Nothing, A)! has \emph{no}
values. Indeed, how could we create a value of type, say, \lstinline!(Nothing, Int)!?
We would need to fill both parts of the tuple. We have values of type
\lstinline!Int!, but we can never get a value of type \lstinline!Nothing!.
So, regardless of the type \lstinline!A!, it is impossible to create
any values of type \lstinline!(Nothing, A)!. In other words, the
set of values of the type \lstinline!(Nothing, A)! is empty; but
that is the definition of the type \lstinline!Nothing!. The types
\lstinline!(Nothing, A)! (denoted by $\bbnum 0\times A$) and \lstinline!Nothing!
(denoted by $\bbnum 0$) are both void and therefore equivalent.

\subsubsection{Example \label{subsec:ch-Example-0-plus-A}\ref{subsec:ch-Example-0-plus-A}}

Verify the type equivalence $\bbnum 0+A\cong A$.

\subparagraph{Solution}

Recall that the type notation $\bbnum 0+A$ represents the Scala type
\lstinline!Either[Nothing, A]!. We need to show that any value of
that type can be mapped without loss of information to a value of
type \lstinline!A!, and vice versa. This means implementing functions
$f_{1}:\bbnum 0+A\Rightarrow A$ and $f_{2}:A\Rightarrow\bbnum 0+A$
such that $f_{1}\bef f_{2}=\text{id}$ and $f_{2}\bef f_{1}=\text{id}$.

The argument of $f_{1}$ is of type \lstinline!Either[Nothing, A]!.
How can we create a value of that type? Our only choices are to create
a \lstinline!Left(x)! with \lstinline!x:Nothing!, or to create a
\lstinline!Right(y)! with \lstinline!y:A!. However, we cannot create
a value \lstinline!x! of type \lstinline!Nothing! because the type
\lstinline!Nothing! has \emph{no} values; so we cannot create a \lstinline!Left(x)!.
The only remaining possibility is to create a \lstinline!Right(y)!
with some value \lstinline!y! of type \lstinline!A!. So, any values
of type $\bbnum 0+A$ must be of the form \lstinline!Right(y)!, and
we can extract that \lstinline!y! to obtain a value of type \lstinline!A!:
\begin{lstlisting}
def f1[A]: Either[Nothing, A] => A = {
  case Right(y) => y
// No need for case Left(x) => ... since no x can be given there.
}
\end{lstlisting}
For the same reason, there is only one implementation of the function
\lstinline!f2!,
\begin{lstlisting}
def f2[A]: A => Either[Nothing, A] = { y => Right(y) }
\end{lstlisting}
It is clear from the code that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other.

We have just seen that a value of type $\bbnum 0+A$ is always a \lstinline!Right(y)!
with some \lstinline!y:A!. Similarly, a value of type $A+\bbnum 0$
is always a \lstinline!Left(x)! with some \lstinline!x:A!. So, we
will use the notation $A+\bbnum 0$ and $\bbnum 0+A$ to \emph{denote}
the \lstinline!Left! and the \lstinline!Right! parts of the disjunctive
type \lstinline!Either!. This notation agrees with the behavior of
the Scala compiler, which will infer the types \lstinline!Either[A, Nothing] !or
\lstinline!Either[Nothing, A]! for these parts:
\begin{lstlisting}
def toLeft[A, B]: A => Either[A, B] = x => Left(x)
def toRight[A, B]: B => Either[A, B] = y => Right(y)

scala> toLeft(123)
res0: Either[Int, Nothing] = Left(123)

scala> toRight("abc")
res1: Either[Nothing, String] = Right("abc")
\end{lstlisting}
We can write the functions \lstinline!toLeft! and \lstinline!toRight!
in a short code notation as 
\[
\text{toLeft}^{A,B}=x^{:A}\Rightarrow x^{:A}+\bbnum 0^{:B}\quad,\quad\quad\text{toRight}^{A,B}=y^{:B}\Rightarrow\bbnum 0^{:A}+y^{:B}\quad.
\]
In this notation, a value of the disjunctive type is shown without
using Scala class names such as \lstinline!Either!, \lstinline!Right!,
and \lstinline!Left!. This shortens the writing and enables us to
perform code reasoning faster.

The type annotation $\bbnum 0^{:A}$ is helpful to remind ourselves
about the type parameter $A$ used e.g.~by the disjunctive value
$\bbnum 0^{:A}+y^{:B}$ in the body of \lstinline!toRight[A, B]!.
Without this type annotation, $\bbnum 0+y^{:B}$ means a value of
type \lstinline!Either[A, B]! where the parameter $A$ is left unspecified
and should be determined by matching the types of other expressions.

In the notation $\bbnum 0+y^{:B}$, we use the symbol $\bbnum 0$
rather than an ordinary zero ($0$), to avoid suggesting that $0$
is a value of type $\bbnum 0$. The void type $\bbnum 0$ has no values,
unlike the \lstinline!Unit! type, $\bbnum 1$, which has a value
denoted by $1$ in the code notation.

\subsubsection{Example \label{subsec:ch-Example-1xA}\ref{subsec:ch-Example-1xA}}

Verify the type equivalence $A\times\bbnum 1\cong A$.

\subparagraph{Solution}

The corresponding Scala types are the tuple \lstinline!(A, Unit)!
and the type \lstinline!A!. We need to implement functions $f_{1}:\forall A.\,A\times\bbnum 1\Rightarrow A$
and $f_{2}:\forall A.\,A\Rightarrow A\times\bbnum 1$ and to demonstrate
that they are inverses of each other. The Scala code for these functions
is
\begin{lstlisting}
def f1[A]: ((A, Unit)) => A = { case (a, ()) => a }
def f2[A]: A => (A, Unit) = { a => (a, ()) }
\end{lstlisting}
Let us first write a proof by reasoning directly with Scala code:
\begin{lstlisting}
(f1 andThen f2)((a,())) == f2(f1((a,())) == f2(a) == (a, ())
(f2 andThen f1)(a) == f1(f2(a)) == f1((a, ())) = a
\end{lstlisting}
Now let us write a proof in the code notation. The codes of $f_{1}$
and $f_{2}$ are
\begin{align*}
f_{1} & =a^{:A}\times1\Rightarrow a\quad,\\
f_{2} & =a^{:A}\Rightarrow a\times1\quad,
\end{align*}
where we denoted by $1$ the value \lstinline!()! of the \lstinline!Unit!
type. We find
\begin{align*}
(f_{1}\bef f_{2})(a^{:A}\times1) & =f_{2}\left(f_{1}(a\times1)\right)=f_{2}\left(a\right)=a\times1\quad,\\
(f_{2}\bef f_{1})(a^{:A}) & =f_{1}(f_{2}(a))=f_{1}(a\times1)=a\quad.
\end{align*}
This shows that both compositions are identity functions. Another
way of writing the proof is by computing the function compositions
symbolically, without applying to an $a^{:A}$,
\begin{align*}
f_{1}\bef f_{2} & =\left(a\times1\Rightarrow a\right)\bef\left(a\Rightarrow a\times1\right)=\left(a\times1\Rightarrow a\times1\right)=\text{id}^{A\times\bbnum 1}\quad,\\
f_{2}\bef f_{1} & =\left(a\Rightarrow a\times1\right)\bef\left(a\times1\Rightarrow a\right)=\left(a\Rightarrow a\right)=\text{id}^{A}\quad.
\end{align*}


\subsubsection{Example \label{subsec:ch-Example-A+B}\ref{subsec:ch-Example-A+B}}

Verify the type equivalence $A+B\cong B+A$.

\subparagraph{Solution}

The corresponding Scala types are \lstinline!Either[A, B]! and \lstinline!Either[B, A]!.
We use pattern matching to implement the functions required for the
type equivalence:
\begin{lstlisting}
def f1[A, B]: Either[A, B] => Either[B, A] = {
  case Left(a)    => Right(a) // No other choice here.
  case Right(b)   => Left(b)  // No other choice here.
}
def f2[A, B]: Either[B, A] => Either[A, B] = f1[B, A]
\end{lstlisting}
The functions \lstinline!f1! and \lstinline!f2! are implemented
by the same code, and the code can be derived unambiguously from the
type signatures. For instance, the line \lstinline!case Left(a) => ...!
is required to return a value of type \lstinline!Either[B, A]! by
using only a given value \lstinline!a:A!. The only way of doing that
is by returning \lstinline!Right(a)!.

It is clear from the code that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other (i.e.~the function \lstinline!f1! is
its own inverse). To verify that rigorously, we need show that \lstinline!f1 andThen f2!
is equal to an identity function. The function \lstinline!f1 andThen f2!
applies \lstinline!f2! to the result of \lstinline!f1!. The code
of \lstinline!f1! contains two \lstinline!case ...! lines, each
returning a result. So, we need to apply \lstinline!f2! separately
in each line. Evaluate the code symbolically:
\begin{lstlisting}
f1 andThen f2 == {
  case Left(a)    => f2(Right(a))
  case Right(b)   => f2(Left(b))
} == {
  case Left(a)    => Left(a)
  case Right(b)   => Right(b)
}
\end{lstlisting}
The result is a function of type \lstinline!Either[A, B] => Either[A, B]!
that does not change its argument; so it is equal to the identity
function. 

Let us now write the function \lstinline!f1! in the code notation
and perform the same derivation. We will also develop a useful notation
for functions operating on disjunctive types.

The pattern matching construction in the Scala code of \lstinline!f1!
contains a pair of functions with types \lstinline!A => Either[B, A]!
and \lstinline!B => Either[B, A]!. One of these functions is chosen
depending on whether the argument of \lstinline!f1! has type $A+\bbnum 0$
or $\bbnum 0+B$. So, we may write the code of \lstinline!f1! as
\[
f_{1}\triangleq x^{:A+B}\Rightarrow\begin{cases}
\text{if }x=a^{:A}+\bbnum 0^{:B}\quad: & \bbnum 0^{:B}+a^{:A}\\
\text{if }x=\bbnum 0^{:A}+b^{:B}\quad: & b^{:B}+\bbnum 0^{:A}
\end{cases}
\]
Since both the argument and the result of $f_{1}$ are disjunctive
types with $2$ parts each, it is convenient to write the code of
$f_{1}$ as a $2\times2$ matrix that maps the input parts to the
output parts:\index{disjunctive type!matrix notation}
\[
f_{1}\triangleq\begin{array}{|c||cc|}
 & B & A\\
\hline A~ &  & a^{:A}\Rightarrow a\\
B~ & b^{:B}\Rightarrow b & 
\end{array}\quad.
\]
The rows of the matrix correspond to the \lstinline!case! rows in
the Scala code; there is one row for each part of the disjunctive
type of the argument. The columns of the matrix correspond to the
parts of the disjunctive type of the result.\index{pattern matching!matrix notation}
The double line marks the input types of the functions.

The code of $f_{2}$ is written similarly; let us rename arguments
for clarity:
\[
f_{2}\triangleq\begin{array}{|c||cc|}
 & A & B\\
\hline B~ &  & y^{:B}\Rightarrow y\\
A~ & x^{:A}\Rightarrow x & 
\end{array}\quad.
\]
The forward composition $f_{1}\bef f_{2}$ is computed by the standard
rules of row-by-column matrix multiplication.\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Matrix_multiplication}{https://en.wikipedia.org/wiki/Matrix\_multiplication}}}
The empty places in matrices are ignored, and the non-empty expressions
are composed as functions:
\begin{align*}
f_{1}\bef f_{2} & =\begin{array}{|c||cc|}
 & B & A\\
\hline A~ &  & a^{:A}\Rightarrow a\\
B~ & b^{:B}\Rightarrow b & 
\end{array}\bef\begin{array}{|c||cc|}
 & A & B\\
\hline B~ &  & y^{:B}\Rightarrow y\\
A~ & x^{:A}\Rightarrow x & 
\end{array}\\
{\color{greenunder}\text{use matrix multiplication}:}\quad & =\begin{array}{|c||cc|}
 & A & B\\
\hline A~ & (a^{:A}\Rightarrow a)\bef(x^{:A}\Rightarrow x) & \\
B~ &  & (b^{:B}\Rightarrow b)\bef(y^{:B}\Rightarrow y)
\end{array}\\
{\color{greenunder}\text{function composition}:}\quad & =\begin{array}{|c||cc|}
 & A & B\\
\hline A~ & \text{id} & \\
B~ &  & \text{id}
\end{array}=\text{id}^{:A+B\Rightarrow A+B}\quad.
\end{align*}
Several features of the matrix notation are helpful in such calculations.
The parts of the code of $f_{1}$ are automatically composed with
the corresponding parts of the code of $f_{2}$. To check that the
types match in the function composition, we just need to compare the
types in the output row $\begin{array}{||cc|}
B & A\end{array}$ of $f_{1}$ with the input column $\begin{array}{|c||}
B\\
A
\end{array}$ of $f_{2}$. Once we verified that all types match, we may omit the
type annotations and write the derivation as
\begin{align*}
f_{1}\bef f_{2} & =\begin{array}{||cc|}
 & a^{:A}\Rightarrow a\\
b^{:B}\Rightarrow b & 
\end{array}\bef\begin{array}{||cc|}
 & y^{:B}\Rightarrow y\\
x^{:A}\Rightarrow x & 
\end{array}\\
{\color{greenunder}\text{use matrix multiplication}:}\quad & =\begin{array}{||cc|}
(a^{:A}\Rightarrow a)\bef(x^{:A}\Rightarrow x) & \\
 & (b^{:B}\Rightarrow b)\bef(y^{:B}\Rightarrow y)
\end{array}\\
{\color{greenunder}\text{function composition}:}\quad & =\begin{array}{||cc|}
\text{id} & \\
 & \text{id}
\end{array}=\text{id}\quad.
\end{align*}
The identity function is represented by the diagonal matrix $\begin{array}{||cc|}
\text{id} & \\
 & \text{id}
\end{array}\,$, just as in standard matrix notation.

\subsubsection{Exercise \label{subsec:ch-Exercise-AxB}\ref{subsec:ch-Exercise-AxB}\index{exercises}}

Verify the type equivalence $A\times B\cong B\times A$.

\subsubsection{Exercise \label{subsec:ch-Exercise-A+B+C}\ref{subsec:ch-Exercise-A+B+C}}

Verify the type equivalence $\left(A+B\right)+C\cong A+\left(B+C\right)$.
The equivalences $\left(A+B\right)+C\cong A+\left(B+C\right)$ and
$\left(A\times B\right)\times C\cong A\times\left(B\times C\right)$,
which was verified in Section~\ref{subsec:Identity-in-logic-not-type-equivalence},
permit us to write the type notation as $A+B+C$ and $A\times B\times C$
without the parentheses.

\subsubsection{Exercise \label{subsec:ch-Exercise-A+B2}\ref{subsec:ch-Exercise-A+B2}}

Verify the type equivalence 
\[
\left(A+B\right)\times\left(A+B\right)=A\times A+\bbnum 2\times A\times B+B\times B\quad,
\]
where $\bbnum 2$ denotes the \lstinline!Boolean! type.

\subsection{Type cardinalities and type equivalence}

To understand why type equivalences are related to arithmetic identities,
consider the question of how many different values a given type can
have.

Begin by counting the number of distinct values for simple types.
For example, the \lstinline!Unit! type has only one distinct value;
the type \lstinline!Nothing! has zero values; the \lstinline!Boolean!
type has two distinct values, \lstinline!true! and \lstinline!false!;
and the type \lstinline!Int! has $2^{32}$ distinct values.

It is more difficult to count the number of distinct values in a type
such as \lstinline!String!, which is equivalent to a list of unknown
length, \lstinline!List[Char]!. However, each computer's memory is
limited, so there will exist a maximum length for values of type \lstinline!String!,
and so the total number of possible different strings will be finite
(at least, for any given computer).

For a given type $A$, let us denote by $\left|A\right|$ the number
of distinct values of type $A$. The number $\left|A\right|$ is called
the \index{cardinality}\textbf{cardinality} of type $A$; this is
the same as the number of elements in the set of all values of type
$A$. Since any computer's memory is finite, and since we may assume
that we are already working with the largest possible computer, then
there will be \emph{finitely} many different values of a given type
$A$ that can exist in the computer. So, we may assume that $\left|A\right|$
is always a finite integer value. This assumption will simplify our
reasoning. We will not actually need to compute the precise number
of, say, all the different possible strings; it is sufficient to know
that the set of all strings is finite, so that we can denote its cardinality
by $\left|\text{String}\right|$.

The next step is to consider the cardinality of types such as $A\times B$
and $A+B$. If the types $A$ and $B$ have cardinalities $\left|A\right|$
and $\left|B\right|$, it follows that the set of all distinct pairs
\lstinline!(A, B)! has $\left|A\right|\times\left|B\right|$ elements.
So the cardinality of the type $A\times B$ is equal to the (arithmetic)
product of the cardinalities of $A$ and $B$. The set of all pairs
\[
\left\{ (a,b):a\in A,b\in B\right\} 
\]
is also known as the \index{Cartesian product}\textbf{Cartesian product}
of sets $A$ and $B$, and is denoted by $A\times B$. For this reason,
the tuple type is also called the \index{product type}\textbf{product
type}. Accordingly, the type notation adopts the symbol $\times$
for the product type.

The set of all distinct values of the type $A+B$, i.e.~of the Scala
type \lstinline!Either[A, B]!, is a disjoint union of the set of
values of the form \lstinline!Left(a)! and the set of values of the
form \lstinline!Right(b)!. It is clear that the cardinalities of
these sets are equal to $\left|A\right|$ and $\left|B\right|$ respectively.
So the cardinality of the type \lstinline!Either[A, B]! is equal
to $\left|A\right|+\left|B\right|$. For this reason, disjunctive
types such as \lstinline!Either[A, B]! are also called \index{sum type}\textbf{sum
types}, and the type notation adopts the symbol $+$ for these types.

We can write our conclusions as
\begin{align*}
\left|A\times B\right| & =\left|A\right|\times\left|B\right|\quad,\\
\left|A+B\right| & =\left|A\right|+\left|B\right|\quad.
\end{align*}
The type notation, $A\times B$ for the pairs and $A+B$ for the disjunctive
types, translates directly into type cardinalities.

The last step is to notice that two types can be equivalent, $P\cong Q$,
only if their cardinalities are equal, $\left|P\right|=\left|Q\right|$.
When the cardinalities are not equal, $\left|P\right|\neq\left|Q\right|$,
it will be impossible to have a one-to-one correspondence between
the sets of values of type $P$ and values of type $Q$. So it will
be impossible to convert values from type $P$ to type $Q$ and back
without loss of information.

We conclude that types are equivalent when a logical identity \emph{and}
an arithmetic identity hold.

The presence of both identities does not automatically guarantee a
useful type equivalence. The fact that information in one type can
be identically stored in another type does not necessarily mean that
it is helpful to do so in a given application.

For example, the types \lstinline!Option[Option[A]]! and \lstinline!Either[Boolean, A]!
are equivalent because both types contain $2+\left|A\right|$ distinct
values. The short notation for these types is $\bbnum 1+\bbnum 1+A$
and $\bbnum 2+A$ respectively (the type Boolean is denoted by $\bbnum 2$
since it has only two distinct values). 

One could easily write code to convert between these types without
loss of information:
\begin{lstlisting}
def f1[A]: Option[Option[A]] => Either[Boolean, A] = {
  case None           => Left(false) // Or maybe Left(true)?
  case Some(None)     => Left(true)
  case Some(Some(x))  => Right(x)
}
def f2[A]: Either[Boolean, A] => Option[Option[A]] = {
  case Left(false)    => None
  case Left(true)     => Some(None)
  case Right(x)       => Some(Some(x))
}
\end{lstlisting}
A sign of trouble is the presence of an arbitrary choice in this code.
In \lstinline!f1!, we could map \lstinline!None! to \lstinline!Left(false)!
or to \lstinline!Left(true)!, and adjust the rest of the code accordingly;
the type equivalence would still hold. So, formally speaking, these
types \emph{are} equivalent, but there is no ``natural'' choice
of the conversion functions \lstinline!f1! and \lstinline!f2! that
will work correctly in all applications, because the meaning of these
data types is application-dependent. This type equivalence is ``accidental''\index{type equivalence!accidental}.

\subsubsection{Example \label{subsec:ch-Example-cardinality-option-either}\ref{subsec:ch-Example-cardinality-option-either}\index{solved examples}}

Are the types \lstinline!Option[A]! and \lstinline!Either[Unit, A]!
equivalent? Check whether the corresponding logic identity and arithmetic
identity hold.

\paragraph{Solution}

Begin by writing the given types in the type notation: \lstinline!Option[A]!
is written as $\bbnum 1+A$, and \lstinline!Either[Unit, A]! is written
also as $\bbnum 1+A$. This already indicates, by looking at the notation
alone, that the types are equivalent. However, let us verify explicitly
that the type notation is not misleading us.

To establish type equivalence, we need to implement two fully parametric
functions
\begin{lstlisting}
def f1[A]: Option[A] => Either[Unit, A] = ???
def f2[A]: Either[Unit, A] => Option[A] = ???
\end{lstlisting}
such that $f_{1}\bef f_{2}=\text{id}$ and $f_{2}\bef f_{1}=\text{id}$.
It is straightforward to implement \lstinline!f1! and \lstinline!f2!:
\begin{lstlisting}
def f1[A]: Option[A] => Either[Unit, A] = {
  case None      => Left(())
  case Some(x)   => Right(x)
}
def f2[A]: Either[Unit, A] => Option[A] = {
  case Left(())   => None
  case Right(x)   => Some(x)
}
\end{lstlisting}
The code clearly shows that \lstinline!f1! and \lstinline!f2! are
inverses of each other; this verifies the type equivalence.

The logic identity is $True\vee A=True\vee A$ and holds trivially.
It remains to check the arithmetic identity, which relates the number
of distinct values of types \lstinline!Option[A]! and \lstinline!Either[Unit, A]!.
Assume that the number of distinct values of type \lstinline!A! is
$\left|A\right|$. Any possible value of type \lstinline!Option[A]!
must be either \lstinline!None! or \lstinline!Some(x)!, where \lstinline!x!
is a value of type \lstinline!A!. So the number of distinct values
of type \lstinline!Option[A]! is $1+\left|A\right|$. All possible
values of type \lstinline!Either[Unit, A]! are of the form \lstinline!Left(())!
or \lstinline!Right(x)!, where \lstinline!x! is a value of type
\lstinline!A!. So the number of distinct values of type \lstinline!Either[Unit, A]!
is $1+\left|A\right|$. We see that the arithmetic identity holds:
the types \lstinline!Option[A]! and \lstinline!Either[Unit, A]!
have equally many distinct values.

This example shows that the type notation is helpful for reasoning
about type equivalences. The answer to the question was found immediately
when we wrote the type notation, $\bbnum 1+A$, for the given types.

\subsection{Type equivalence involving function types}

Until now, we have looked at product types and disjunctive types.
Let us now consider type constructions involving function types.

Consider two types $A$ and $B$, whose cardinalities are known as
$\left|A\right|$ and $\left|B\right|$. What is the cardinality of
the set of all maps between given sets $A$ and $B$? In other words,
how many distinct values does the function type $A\Rightarrow B$
have? A function \lstinline!f: A => B! needs to select a value of
type $B$ for each possible value of type $A$. Therefore, the number
of different functions \lstinline!f: A => B! is $\left|B\right|^{\left|A\right|}$
(the arithmetic exponent\index{exponent}, $\left|B\right|$ to the
power $\left|A\right|$).

For the types $A=B=\text{Int}$, we have $\left|A\right|=\left|B\right|=2^{32}$,
and so the estimate will give 
\[
\left|A\Rightarrow B\right|=\left(2^{32}\right)^{\left(2^{32}\right)}=2^{32\times2^{32}}=2^{2^{37}}\approx10^{4.1\times10^{10}}\quad.
\]
In fact, most of these functions will map integers to integers in
a complicated (and practically useless) way and will be impossible
to implement on a realistic computer because their code will be much
longer than the available memory. So, the number of practically implementable
functions of type $A\Rightarrow B$ is often much smaller than $\left|B\right|^{\left|A\right|}$.
Nevertheless, the estimate $\left|B\right|^{\left|A\right|}$ is useful
since it shows the number of distinct functions that are possible
in principle.

Let us now see what logic identities and arithmetic identities are
available for type expressions involving function types. Table~\ref{tab:Logical-identities-with-function-types}
lists the available identities and the corresponding type equivalences.
(In the last column, we defined $a\triangleq\left|A\right|$, $b\triangleq\left|B\right|$,
and $c\triangleq\left|C\right|$ for brevity.) 

It is notable that no logic identity is available for the formula
$\alpha\Rightarrow\left(\beta\vee\gamma\right)$, and correspondingly
no type equivalence is available for the type expression $A\Rightarrow B+C$
(although there is an identity for $A\Rightarrow B\times C$). The
presence of type expressions of the form $A\Rightarrow B+C$ makes
type reasoning more complicated because they cannot be transformed
into equivalent formulas with simpler parts.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Logical identity (if holds)} & \textbf{Type equivalence} & \textbf{Arithmetic identity}\tabularnewline
\hline 
\hline 
$\left(True\Rightarrow\alpha\right)=\alpha$ & $\bbnum 1\Rightarrow A\cong A$ & $a^{1}=a$\tabularnewline
\hline 
$\left(False\Rightarrow\alpha\right)=True$ & $\bbnum 0\Rightarrow A\cong\bbnum 1$ & $a^{0}=1$\tabularnewline
\hline 
$\left(\alpha\Rightarrow True\right)=True$ & $A\Rightarrow\bbnum 1\cong\bbnum 1$ & $1^{a}=1$\tabularnewline
\hline 
$\left(\alpha\Rightarrow False\right)\neq False$ & $A\Rightarrow\bbnum 0\not\cong\bbnum 0$ & $0^{a}\neq0$\tabularnewline
\hline 
$\left(\alpha\vee\beta\right)\Rightarrow\gamma=\left(\alpha\Rightarrow\gamma\right)\wedge\left(\beta\Rightarrow\gamma\right)$ & $A+B\Rightarrow C\cong\left(A\Rightarrow C\right)\times\left(B\Rightarrow C\right)$ & $c^{a+b}=c^{a}\times c^{b}$\tabularnewline
\hline 
$(\alpha\wedge\beta)\Rightarrow\gamma=\alpha\Rightarrow\left(\beta\Rightarrow\gamma\right)$ & $A\times B\Rightarrow C\cong A\Rightarrow B\Rightarrow C$ & $c^{a\times b}=\left(c^{b}\right)^{a}$\tabularnewline
\hline 
$\alpha\Rightarrow\left(\beta\wedge\gamma\right)=\left(\alpha\Rightarrow\beta\right)\wedge\left(\alpha\Rightarrow\gamma\right)$ & $A\Rightarrow B\times C\cong\left(A\Rightarrow B\right)\times\left(A\Rightarrow C\right)$ & $\left(b\times c\right)^{a}=b^{a}\times c^{a}$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Logical identities and arithmetic identities for function types.\label{tab:Logical-identities-with-function-types}}
\end{table}

We will now prove some of the type identities in Table~\ref{tab:Logical-identities-with-function-types}.

\subsubsection{Example \label{subsec:ch-Example-type-identity-f}\ref{subsec:ch-Example-type-identity-f}\index{solved examples}}

Verify the type equivalence $\bbnum 1\Rightarrow A\cong A$.

\subparagraph{Solution}

Recall that the type notation $\bbnum 1\Rightarrow A$ means the Scala
function type \lstinline!Unit => A!. There is only one value of type
\lstinline!Unit!, so the choice of a function of the type \lstinline!Unit => A!
is the same as the choice of a value of type \lstinline!A!. Thus,
the number of distinct values of the type $\bbnum 1\Rightarrow A$
is $\left|A\right|$, and the arithmetic identity holds.

To verify the type equivalence explicitly, we need to implement two
functions
\begin{lstlisting}
def f1[A]: (Unit => A) => A = ???
def f2[A]: A => Unit => A = ???
\end{lstlisting}
The first function needs to produce a value of type \lstinline!A!,
given an argument of the function type \lstinline!Unit => A!. The
only possibility is to apply that function to the value of type \lstinline!Unit!;
we can always produce that value as \lstinline!()!:
\begin{lstlisting}
def f1[A]: (Unit => A) => A = (h: Unit => A) => h(())
\end{lstlisting}
Implementing \lstinline!f2! is straightforward; we can just discard
the \lstinline!Unit! argument:
\begin{lstlisting}
def f2[A]: A => Unit => A = (x: A) => _ => x
\end{lstlisting}
It remains to show that the functions \lstinline!f1! and \lstinline!f2!
are inverses of each other. Let us perform the proof using Scala code
and then using the code notation.

Writing Scala code, compute \lstinline!f1(f2(x))! for an arbitrary
\lstinline!x:A!. Substituting the code, we get
\begin{lstlisting}
f1(f2(x)) == f1(_ => x) == (_ => x)(()) == x
\end{lstlisting}
Now compute \lstinline!f2(f1(h))! for arbitrary \lstinline!h: Unit => A!
in Scala code:
\begin{lstlisting}
f2(f1(h)) == f2(h(())) == { _ => h(()) }
\end{lstlisting}
How can we show that the function \lstinline!{_ => h(())}! is equal
to \lstinline!h!? Whenever we apply equal functions to equal arguments,
they return equal results. In our case, the argument of \lstinline!h!
is of type \lstinline!Unit!, so we only need to verify that the result
of applying \lstinline!h! to the value \lstinline!()! is the same
as the result of applying \lstinline!{_ => h(())}! to \lstinline!()!.
In other words, we need to apply both sides to an additional argument
\lstinline!()!:
\begin{lstlisting}
f2(f1(h))(()) == { _ => h(()) } (()) == h(())
\end{lstlisting}
This completes the proof.

For comparison, let us show the same proof in the code notation. The
functions $f_{1}$ and $f_{2}$ are 
\begin{align*}
 & f_{1}\triangleq h^{:\bbnum 1\Rightarrow A}\Rightarrow h(1)\quad,\\
 & f_{2}\triangleq x^{:A}\Rightarrow1\Rightarrow x\quad.
\end{align*}
Now write the function compositions in both directions:
\begin{align*}
 & f_{1}\bef f_{2}=(h^{:\bbnum 1\Rightarrow A}\Rightarrow h(1))\bef(x^{:A}\Rightarrow1\Rightarrow x)\\
{\color{greenunder}\text{compute composition}:}\quad & =\left(h\Rightarrow1\Rightarrow h(1)\right)\\
{\color{greenunder}\text{note that }1\Rightarrow h(1)\text{ is the same as }h:}\quad & =\left(h\Rightarrow h\right)=\text{id}\quad.\\
 & f_{2}\bef f_{1}=(x^{:A}\Rightarrow1\Rightarrow x)\bef(h^{:\bbnum 1\Rightarrow A}\Rightarrow h(1))\\
{\color{greenunder}\text{compute composition}:}\quad & =x\Rightarrow(1\Rightarrow x)(1)\\
{\color{greenunder}\text{apply function}:}\quad & =\left(x\Rightarrow x\right)=\text{id}\quad.
\end{align*}

The type $\bbnum 1\Rightarrow A$ is equivalent to the type $A$,
but these types are not the same. The most important difference between
these types is that a value of type $A$ is available immediately,
while a value of type $\bbnum 1\Rightarrow A$ is a function that
still needs to be applied to an argument (of type $\bbnum 1$) before
a value of type $A$ is obtained. The type $\bbnum 1\Rightarrow A$
may represent an ``on-call''\index{on-call value} value of type
$A$; that is, a value computed on demand every time. (See Section~\ref{subsec:Lazy-values-iterators-and-streams}
for more details about ``on-call'' values.)

The void type\index{void type} $\bbnum 0$ needs special reasoning. 

\subsubsection{Example \label{subsec:ch-Example-type-identity-0-to-A}\ref{subsec:ch-Example-type-identity-0-to-A}}

Verify the type equivalence $\bbnum 0\Rightarrow A\cong\bbnum 1$.

\subparagraph{Solution}

What could be a function $f^{:\bbnum 0\Rightarrow A}$ from the type
$\bbnum 0$ to a type $A$? Since there exist no values of type $\bbnum 0$,
the function $f$ will never be applied to any arguments and so \emph{does
not need} to compute any actual values of type $A$. So, $f$ is a
function whose body may be empty; or at least it does not need to
contain any expressions of type $A$. In Scala, such a function can
be written as
\begin{lstlisting}
def absurd[A]: Nothing => A = { ??? }
\end{lstlisting}
This code will compile without type errors. An equivalent code is
\begin{lstlisting}
def absurd[A]: Nothing => A = { x => ??? }
\end{lstlisting}
The symbol \lstinline!???! is defined in the Scala library and represents
code that is ``not implemented''. Trying to evaluate this symbol
will produce an error:
\begin{lstlisting}
scala> ???
scala.NotImplementedError: an implementation is missing
  scala.Predef$.$qmark$qmark$qmark(Predef.scala:288) 
\end{lstlisting}
Since the function \lstinline!absurd! is impossible to apply to an
argument, this error will never happen. So, we can pretend that the
result value (which will never be computed or returned) has any required
type, e.g.~the type $A$.

Let us now verify that there exists \emph{only one} function of type
$\bbnum 0\Rightarrow A$. Suppose there are two such functions, $f^{:\bbnum 0\Rightarrow A}$
and $g^{:\bbnum 0\Rightarrow A}$. Are $f$ and $g$ different functions?
We would see that $f$ and $g$ are different only if we had a value
$x$ such that $f(x)\neq g(x)$, where $x$ must have type $\bbnum 0$.
However, there are \emph{no} values of type $\bbnum 0$, and so we
will never be able to find such $x$. It follows that any two functions
$f$ and $g$ of type $\bbnum 0\Rightarrow A$ are equal. In other
words, there exists only \emph{one} distinct value of type $\bbnum 0\Rightarrow A$;
i.e.~the cardinality of the type $\bbnum 0\Rightarrow A$ is $1$.
So, the type $\bbnum 0\Rightarrow A$ is equivalent to the type $\bbnum 1$.

\subsubsection{Example \label{subsec:ch-Example-type-identity-A-0}\ref{subsec:ch-Example-type-identity-A-0}}

Show that $A\Rightarrow\bbnum 0\not\cong\bbnum 0$ and $A\Rightarrow\bbnum 0\not\cong\bbnum 1$.

\subparagraph{Solution}

To prove that two types are \emph{not} equivalent, it is sufficient
to show that their type cardinalities are different. Let us determine
the cardinality of the type $A\Rightarrow\bbnum 0$, assuming that
the cardinality of $A$ is known. We note that a function of type,
say, $\text{Int}\Rightarrow\bbnum 0$ is impossible to implement.
(If we had such a function $f^{:\text{Int}\Rightarrow\bbnum 0}$,
we could evaluate, say, $x\triangleq f(123)$ and obtain a value $x$
of type $\bbnum 0$, which is impossible by definition of the type
$\bbnum 0$. It follows that $\left|\text{Int}\Rightarrow\bbnum 0\right|=0$.
However, Example~\ref{subsec:ch-Example-type-identity-0-to-A} shows
that $\bbnum 0\Rightarrow\bbnum 0$ has cardinality $1$. So, the
cardinality $\left|A\Rightarrow\bbnum 0\right|=1$ if the type $A$
is itself $\bbnum 0$ but $\left|A\Rightarrow\bbnum 0\right|=0$ for
all other types $A$. We conclude that the type $A\Rightarrow\bbnum 0$
is not equivalent to $\bbnum 0$ or $\bbnum 1$ for all $A$; it is
equivalent to $\bbnum 0$ only for non-void types $A$.

\subsubsection{Example \label{subsec:ch-Example-type-identity-2}\ref{subsec:ch-Example-type-identity-2}}

Verify the type equivalence $A\Rightarrow\bbnum 1\cong\bbnum 1$.

\subparagraph{Solution}

There is only one fully parametric function that returns $\bbnum 1$:
\begin{lstlisting}
def f[A]: A => Unit = { _ => () }
\end{lstlisting}
The function $f$ cannot use its argument of type $A$ since nothing
is known about that type. So the code of $f$ \emph{must} discard
its argument and return the fixed value \lstinline!()! of type \lstinline!Unit!.
In the code notation, this function is written as
\[
f^{:A\Rightarrow\bbnum 1}\triangleq\left(\_\Rightarrow1\right)\quad.
\]
We can show that there exist only \emph{one} distinct function of
type $A\Rightarrow\bbnum 1$ (that is, the type $A\Rightarrow\bbnum 1$
has cardinality $1$). Assume that $f$ and $g$ are two such functions,
and try to find a value $x^{:A}$ such that $f(x)\neq g(x)$. We cannot
find any such $x$ because $f(x)=1$ and $g(x)=1$ for all $x$. So,
any two functions $f$ and $g$ of type $A\Rightarrow\bbnum 1$ must
be equal to each other. Any type having cardinality $1$ is equivalent
to the \lstinline!Unit! type, $\bbnum 1$. So $A\Rightarrow\bbnum 1\cong\bbnum 1$.

\subsubsection{Example \label{subsec:ch-Example-type-identity-5}\ref{subsec:ch-Example-type-identity-5}}

Verify the type equivalence 
\[
A+B\Rightarrow C\cong(A\Rightarrow C)\times(B\Rightarrow C)\quad.
\]


\subparagraph{Solution}

Begin by implementing two functions with type signatures
\begin{lstlisting}
def f1[A,B,C]: (Either[A, B] => C) => (A => C, B => C) = ???
def f2[A,B,C]: ((A => C, B => C)) => Either[A, B] => C = ???
\end{lstlisting}
The code can be derived unambiguously from the type signatures. For
the first function, we need to produce a pair of functions of type
\lstinline!(A => C, B => C)!. Can we produce the first part of that
pair? Computing a function of type \lstinline!A => C! means that
we need to produce a value of type \lstinline!C! given an arbitrary
value \lstinline!a:A!. The available data is a function of type \lstinline!Either[A, B] => C!
called, say, \lstinline!h!. We can apply that function to \lstinline!Left(a)!
and obtain a value of type \lstinline!C! as required. So, a function
of type \lstinline!A => C! is computed as \lstinline!a => h(Left(a))!.
Similarly, we produce a function of type \lstinline!B => C!. The
code is
\begin{lstlisting}
def f1[A,B,C]: (Either[A, B] => C) => (A => C, B => C) =
  (h: Either[A, B] => C) => (a => h(Left(a)), b => h(Right(b)))
\end{lstlisting}
A code notation for this function is
\begin{align*}
 & f_{1}:\left(A+B\Rightarrow C\right)\Rightarrow\left(A\Rightarrow C\right)\times\left(B\Rightarrow C\right)\quad,\\
 & f_{1}\triangleq h^{:A+B\Rightarrow C}\Rightarrow\left(a^{:A}\Rightarrow h(a+\bbnum 0^{:B})\right)\times\left(b^{:B}\Rightarrow h(\bbnum 0^{:A}+b)\right)\quad.
\end{align*}

For the function \lstinline!f2!, we need to apply pattern matching
to both curried arguments and then return a value of type \lstinline!C!.
This can be achieved in only one way:
\begin{lstlisting}
def f2[A,B,C]: ((A => C, B => C)) => Either[A, B] => C = { case (f, g) =>
  {
    case Left(a)    => f(a)
    case Right(b)   => g(b)
  }
}
\end{lstlisting}
A code notation for this function can be written as
\begin{align*}
 & f_{2}:\left(A\Rightarrow C\right)\times\left(B\Rightarrow C\right)\Rightarrow A+B\Rightarrow C\quad,\\
 & f_{2}\triangleq f^{:A\Rightarrow C}\times g^{:B\Rightarrow C}\Rightarrow\begin{array}{|c||c|}
 & C\\
\hline A & a\Rightarrow f(a)\\
B & b\Rightarrow g(b)
\end{array}\quad.
\end{align*}
The matrix in the last line has only one column because the result
type, $C$, is not known to be a disjunctive type. We may simplify
the functions, e.g.~$a\Rightarrow f(a)$ into $f$, and write
\[
f_{2}\triangleq f^{:A\Rightarrow C}\times g^{:B\Rightarrow C}\Rightarrow f^{:A\Rightarrow C}\times g^{:B\Rightarrow C}\Rightarrow\begin{array}{|c||c|}
 & C\\
\hline A & f\\
B & g
\end{array}\quad.
\]

It remains to verify that $f_{1}\bef f_{2}=\text{id}$ and $f_{2}\bef f_{1}=\text{id}$.
To compute the composition $f_{1}\bef f_{2}$, we write (omitting
types)
\begin{align*}
f_{1}\bef f_{2} & =\left(h\Rightarrow(a\Rightarrow h(a+\bbnum 0))\times(b\Rightarrow h(\bbnum 0+b))\right)\bef\bigg(f\times g\Rightarrow\begin{array}{||c|}
f\\
g
\end{array}\,\bigg)\\
{\color{greenunder}\text{compute composition}:}\quad & =h\Rightarrow\begin{array}{||c|}
a\Rightarrow h(a+\bbnum 0)\\
b\Rightarrow h(\bbnum 0+b)
\end{array}\quad.
\end{align*}
To proceed, we need to simplify the expressions $h(a+\bbnum 0)$ and
$h(\bbnum 0+b)$. We rewrite the argument $h$ (an arbitrary function
of type $A+B\Rightarrow C$) in the matrix notation:
\[
h\triangleq\begin{array}{|c||c|}
 & C\\
\hline A & a\Rightarrow p(a)\\
B & b\Rightarrow q(b)
\end{array}=\begin{array}{|c||c|}
 & C\\
\hline A & p\\
B & q
\end{array}\quad,
\]
where $p^{:A\Rightarrow C}$ and $q^{:B\Rightarrow C}$ are new arbitrary
functions. Since we already checked the types, we can omit all type
annotations and express $h$ as
\[
h\triangleq\begin{array}{||c|}
p\\
q
\end{array}\quad.
\]
To evaluate expressions such as $h(a+\bbnum 0)$ and $h(\bbnum 0+b)$,
we need to use one of the rows of the column matrix $h$. The correct
row will be selected \emph{automatically} by the standard rules of
matrix multiplication if we place a row vector to the left of the
matrix, and if we use the convention of omitting any terms containing
$\bbnum 0$:
\begin{align*}
 & \begin{array}{|cc|}
a & \bbnum 0\end{array}\triangleright\begin{array}{||c|}
p\\
q
\end{array}=a\triangleright p\quad,\\
 & \begin{array}{|cc|}
\bbnum 0 & b\end{array}\triangleright\begin{array}{||c|}
p\\
q
\end{array}=b\triangleright q\quad.
\end{align*}
Here we used the symbol $\triangleright$ to separate an argument
from a function when the argument is written to the \emph{left} of
the function. The symbol $\triangleright$ (pronounced ``pipe'')
is defined by $x\triangleright f\triangleq f(x)$. In Scala, this
operation is available as \lstinline!x.pipe(f)! as of Scala 2.13.

We write values of disjunctive types, such as $a+\bbnum 0$, as if
they were row vectors:
\begin{equation}
h(a+\bbnum 0)=(a+\bbnum 0)\triangleright h=\begin{array}{|cc|}
a & \bbnum 0\end{array}\triangleright h\quad.\label{eq:forward-notation-}
\end{equation}
With these notations, we can compute further:
\begin{align*}
 & h(a+\bbnum 0)=\begin{array}{|cc|}
a & \bbnum 0\end{array}\triangleright\begin{array}{||c|}
p\\
q
\end{array}=a\triangleright p=p(a)\quad,\\
 & h(\bbnum 0+b)=\begin{array}{|cc|}
\bbnum 0 & b\end{array}\triangleright\begin{array}{||c|}
p\\
q
\end{array}=b\triangleright q=q(b)\quad.
\end{align*}
Now we can complete the proof of $f_{1}\bef f_{2}=\text{id}$:
\begin{align*}
f_{1}\bef f_{2} & =h\Rightarrow\begin{array}{||c|}
a\Rightarrow h(a+\bbnum 0)\\
b\Rightarrow h(\bbnum 0+b)
\end{array}\\
{\color{greenunder}\text{previous equations}:}\quad & =\begin{array}{||c|}
p\\
q
\end{array}\Rightarrow\begin{array}{||c|}
a\Rightarrow p(a)\\
b\Rightarrow q(b)
\end{array}\\
{\color{greenunder}\text{simplify functions}:}\quad & =\bigg(\begin{array}{||c|}
p\\
q
\end{array}\Rightarrow\begin{array}{||c|}
p\\
q
\end{array}\,\bigg)=\text{id}\quad.
\end{align*}

To prove that $f_{2}\bef f_{1}=\text{id}$, use the notation~(\ref{eq:forward-notation-}):
\begin{align*}
f_{2}\bef f_{1} & =\bigg(f\times g\Rightarrow\begin{array}{||c|}
f\\
g
\end{array}\,\bigg)\bef\left(h\Rightarrow(a\Rightarrow h(a+\bbnum 0))\times(b\Rightarrow h(\bbnum 0+b))\right)\\
{\color{greenunder}\text{compute composition}:}\quad & =f\times g\Rightarrow\big(a\Rightarrow\begin{array}{|cc|}
a & \bbnum 0\end{array}\triangleright\begin{array}{||c|}
f\\
g
\end{array}\,\big)\times\big(b\Rightarrow\begin{array}{|cc|}
\bbnum 0 & b\end{array}\triangleright\begin{array}{||c|}
f\\
g
\end{array}\,\big)\\
{\color{greenunder}\text{matrix notation}:}\quad & =f\times g\Rightarrow(a\Rightarrow\gunderline{a\triangleright f})\times(b\Rightarrow\gunderline{b\triangleright g})\\
{\color{greenunder}\text{definition of }\triangleright:}\quad & =f\times g\Rightarrow\gunderline{\left(a\Rightarrow f(a)\right)}\times\gunderline{\left(b\Rightarrow g(b)\right)}\\
{\color{greenunder}\text{simplify functions}:}\quad & =\left(f\times g\Rightarrow f\times g\right)=\text{id}\quad.
\end{align*}

In this way, we have proved that $f_{1}$ and $f_{2}$ are mutual
inverses. The proofs appear long because we took time to motivate
and introduce new notation for applying matrices to row vectors. Given
this notation, the proof for $f_{1}\bef f_{2}=\text{id}$ can be written
as
\begin{align*}
f_{1}\bef f_{2} & =\left(h\Rightarrow(a\Rightarrow(a+\bbnum 0)\triangleright h)\times(b\Rightarrow(\bbnum 0+b)\triangleright h)\right)\bef\bigg(f\times g\Rightarrow\begin{array}{||c|}
f\\
g
\end{array}\bigg)\\
{\color{greenunder}\text{compute composition}:}\quad & =h\Rightarrow\begin{array}{||c|}
a\Rightarrow\left|\begin{array}{cc}
a & \bbnum 0\end{array}\right|\triangleright h\\
b\Rightarrow\left|\begin{array}{cc}
\bbnum 0 & b\end{array}\right|\triangleright h
\end{array}=\begin{array}{||c|}
p\\
q
\end{array}\Rightarrow\begin{array}{||c|}
a\Rightarrow\begin{array}{|cc|}
a & \bbnum 0\end{array}\triangleright\begin{array}{||c|}
p\\
q
\end{array}\\
b\Rightarrow\begin{array}{|cc|}
\bbnum 0 & b\end{array}\,\triangleright\,\begin{array}{||c|}
p\\
q
\end{array}
\end{array}\\
{\color{greenunder}\text{matrix notation}:}\quad & =\begin{array}{||c|}
p\\
q
\end{array}\Rightarrow\begin{array}{||c|}
a\Rightarrow a\triangleright p\\
b\Rightarrow b\triangleright q
\end{array}=\big(\begin{array}{||c|}
p\\
q
\end{array}\Rightarrow\begin{array}{||c|}
p\\
q
\end{array}\,\big)=\text{id}\quad.
\end{align*}
The code notation makes proofs shorter than they would be if we were
manipulating code in Scala syntax. From now on, we will prefer to
use the code notation in the proofs, keeping in mind that Scala code
can be unambiguously recovered from the code notation, and vice versa.

\subsubsection{Example \label{subsec:ch-Example-type-identity-6}\ref{subsec:ch-Example-type-identity-6}}

Verify the type equivalence 
\[
A\times B\Rightarrow C\cong A\Rightarrow B\Rightarrow C\quad.
\]


\subparagraph{Solution}

Begin by implementing the two functions
\begin{lstlisting}
def f1[A,B,C]: (((A, B)) => C) => A => B => C = ???
def f2[A,B,C]: (A => B => C) => ((A, B)) => C = ???
\end{lstlisting}
The Scala code can be derived from the type signatures without ambiguity:
\begin{lstlisting}
def f1[A,B,C]: (((A, B)) => C) => A => B => C = g => a => b => g((a, b))
def f2[A,B,C]: (A => B => C) => ((A, B)) => C = h => { case (a, b) => h(a)(b) }
\end{lstlisting}
Write these functions in the code notation:
\begin{align*}
 & f_{1}=g^{:A\times B\Rightarrow C}\Rightarrow a^{:A}\Rightarrow b^{:B}\Rightarrow g(a\times b)\quad,\\
 & f_{2}=h^{:A\Rightarrow B\Rightarrow C}\Rightarrow\left(a\times b\right)^{:A\times B}\Rightarrow h(a)(b)\quad.
\end{align*}
We denote by $\left(a\times b\right)^{:A\times B}$ the argument of
type \lstinline!(A, B)! with pattern matching implied. This notation
allows us to write shorter code notation involving tupled arguments.

Compute the function composition $f_{1}\bef f_{2}$:
\begin{align*}
f_{1}\bef f_{2} & =(g\Rightarrow\gunderline{a\Rightarrow b\Rightarrow g(a\times b)})\bef\left(h\Rightarrow a\times b\Rightarrow h(a)(b)\right)\\
{\color{greenunder}\text{substitute }h=a\Rightarrow b\Rightarrow g(a\times b):}\quad & =g\Rightarrow\gunderline{a\times b\Rightarrow g(a\times b)}\\
{\color{greenunder}\text{simplify function}:}\quad & =\left(g\Rightarrow g\right)=\text{id}\quad.
\end{align*}
Compute the function composition $f_{2}\bef f_{1}$:
\begin{align*}
f_{2}\bef f_{1} & =(h\Rightarrow\gunderline{a\times b\Rightarrow h(a)(b)})\bef\left(g\Rightarrow a\Rightarrow b\Rightarrow g(a\times b)\right)\\
{\color{greenunder}\text{substitute }g=a\times b\Rightarrow h(a)(b):}\quad & =h\Rightarrow a\Rightarrow\gunderline{b\Rightarrow h(a)(b)}\\
{\color{greenunder}\text{simplify function }b\Rightarrow h(a)(b):}\quad & =h\Rightarrow\gunderline{a\Rightarrow h(a)}\\
{\color{greenunder}\text{simplify function }a\Rightarrow h(a)\text{ to }h:}\quad & =\left(h\Rightarrow h\right)=\text{id}\quad.
\end{align*}


\subsubsection{Exercise \label{subsec:ch-Exercise-type-identity-4}\ref{subsec:ch-Exercise-type-identity-4}\index{exercises}}

Verify the type equivalence 
\[
A\Rightarrow B\times C\cong\left(A\Rightarrow B\right)\times\left(A\Rightarrow C\right)\quad.
\]


\section{Summary}

What tasks can we perform now?
\begin{itemize}
\item Use the short type notation for reasoning about types to:
\begin{itemize}
\item Decide type equivalence using the rules of arithmetic.
\item Simplify type expressions before writing code.
\end{itemize}
\item Convert a fully parametric type signature into a logical formula to:
\begin{itemize}
\item Decide whether the type signature can be implemented in code.
\item If possible, derive the code using the CH correspondence.
\end{itemize}
\end{itemize}
What tasks cannot be performed with these tools?
\begin{itemize}
\item Automatically generate code for a recursive function. (Propositional
logic cannot describe recursion.)
\item Automatically generate code satisfying a property (e.g.\ isomorphism).
We may generate the code, but it is not guaranteed that properties
will hold. The workaround is to verify the required properties manually,
after deriving the code.
\item Express complicated conditions (e.g.\ ``array is sorted'') in a
type signature. This can be done using \textbf{dependent types}\index{dependent type}
(i.e.~types that depend on run-time values in an arbitrary way) \textendash{}
an advanced technique that Scala does not fully support. Programming
languages such as Coq, Agda, and Idris support full dependent types,
but cannot automatically generate code from dependent type signatures.
\item Generate code using type constructors with known properties (e.g.\ \lstinline!.map!).
\end{itemize}
As an example of using type constructors with properties, consider
this type signature:
\begin{lstlisting}
def q[A]: Array[A] => (A => Option[B]) => Array[Option[B]]
\end{lstlisting}
Can we generate the code of this function from its type signature?
We know that the Scala library defines a \lstinline!.map! method
on the \lstinline!Array! type constructor, so the implementation
of \lstinline!q! is simple,
\begin{lstlisting}
def q[A]: Array[A] => (A => Option[B]) => Array[Option[B]] = { arr => f => arr.map(f) }
\end{lstlisting}
However, it is hard to create an \emph{algorithm} that can derive
this implementation automatically from the type signature of \lstinline!q!
via the Curry-Howard correspondence. The algorithm would have to convert
the type signature of \lstinline!q! into the logical formula 
\begin{equation}
{\cal CH}(\text{Array}^{A})\Rightarrow{\cal CH}(A\Rightarrow\text{Opt}^{B})\Rightarrow{\cal CH}(\text{Array}^{\text{Opt}^{B}})\quad.\label{eq:ch-example-quantified-proposition}
\end{equation}
To derive an implementation, the algorithm would need to use the available
\lstinline!.map! method for \lstinline!Array!. That method has a
type signature such as
\[
\text{map}:\forall(A,B).\,\text{Array}^{A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow\text{Array}^{B}\quad.
\]
To derive the ${\cal CH}$-proposition~(\ref{eq:ch-example-quantified-proposition}),
the algorithm will need to assume that the ${\cal CH}$-proposition
\begin{equation}
{\cal CH}\left(\forall(A,B).\,\text{Array}^{A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow\text{Array}^{B}\right)\label{eq:ch-example-quantified-proposition-2}
\end{equation}
already holds, i.e.~that Eq.~(\ref{eq:ch-example-quantified-proposition-2})
is one of the premises of a sequent to be proved. Reasoning about
propositions such as Eq.~(\ref{eq:ch-example-quantified-proposition-2})
requires \index{first-order logic}\textbf{first-order logic} \textendash{}
a logic whose proof rules can handle quantified types such as $\forall(A,B)$\emph{
inside} premises. However, first-order logic is \textbf{undecidable\index{undecidable logic@\textbf{undecidable logic}}}:
no algorithm can guarantee finding a proof or showing the absence
of a proof in all cases. 

The constructive propositional logic (with the rules listed in Section~\ref{subsec:The-rules-of-proof})
is \textbf{decidable} and has an algorithm that either finds a proof
or disproves any given formula. However, that logic cannot handle
premises containing type quantifiers such as $\forall(A,B)$ inside,
because all the available rules have the quantifiers implicitly placed
\emph{outside} the premises. 

So, code for functions such as \lstinline!q! can only be derived
by trial and error, informed by intuition. This book will help functional
programmers to acquire the necessary intuition and technique.

\subsection{Solved examples\index{solved examples}}

\subsubsection{Example \label{subsec:ch-solvedExample-1}\ref{subsec:ch-solvedExample-1}}

Find the cardinality of the type \lstinline!P = Option[Option[Boolean] => Boolean]!.
Write \lstinline!P! in the type notation and simplify to an equivalent
type.

\subparagraph{Solution}

Begin with the type \lstinline!Option[Boolean]!, which can be either
\lstinline!None! or \lstinline!Some(x)! with an \lstinline!x:Boolean!.
Since the type \lstinline!Boolean! has $2$ possible values, the
type \lstinline!Option[Boolean]! has $3$ values:
\[
\left|\text{Opt}^{\text{Bool}}\right|=\left|\bbnum 1+\text{Bool}\right|=1+\left|\text{Bool}\right|=3\quad.
\]
In the type notation, \lstinline!Boolean! is denoted by the symbol
$\bbnum 2$, and the type \lstinline!Option[Boolean]! by $\bbnum 1+\bbnum 2$.
So, the type notation $\bbnum 1+\bbnum 2$ is consistent with the
cardinality $3$ of that type,
\[
\left|\bbnum 1+\text{Bool}\right|=\left|\bbnum 1+\bbnum 2\right|=1+2=3\quad.
\]

The function type \lstinline!Option[Boolean] => Boolean! is denoted
by $\bbnum 1+\bbnum 2\Rightarrow\bbnum 2$. Its cardinality is computed
as the arithmetic power 
\[
\left|\text{Opt}^{\text{Bool}}\Rightarrow\text{Bool}\right|=\left|\bbnum 1+\bbnum 2\Rightarrow\bbnum 2\right|=\left|\bbnum 2\right|^{\left|\bbnum 1+\bbnum 2\right|}=2^{3}=8\quad.
\]
Finally, the we write \lstinline!P! in the type notation as 
\[
P=\bbnum 1+\left(\bbnum 1+\bbnum 2\Rightarrow\bbnum 2\right)
\]
and find 
\[
\left|P\right|=\left|\bbnum 1+\left(\bbnum 1+\bbnum 2\Rightarrow\bbnum 2\right)\right|=1+\left|\bbnum 1+\bbnum 2\Rightarrow\bbnum 2\right|=1+8=9\quad.
\]


\subsubsection{Example \label{subsec:ch-solvedExample-2}\ref{subsec:ch-solvedExample-2}}

Implement a Scala type \lstinline!P[A]! for the type notation 
\[
P^{A}\triangleq1+A+\text{Int}\times A+(\text{String}\Rightarrow A)\quad.
\]


\subparagraph{Solution}

To translate type notation into Scala code, begin by defining the
disjunctive types as case classes (with names chosen for convenience).
In this case, $P^{A}$ is a disjunctive type with four parts, so we
will need four case classes:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A](???) extends P[A]
final case class P2[A](???) extends P[A]
final case class P3[A](???) extends P[A]
final case class P4[A](???) extends P[A]
\end{lstlisting}
Each of the case classes represents one part of the disjunctive type.
Now we write the contents for each of the case classes, in order to
implement the data in each of the disjunctive parts:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A]() extends P[A]
final case class P2[A](x: A) extends P[A]
final case class P3[A](n: Int, x: A) extends P[A]
final case class P4[A](f: String => A) extends P[A]
\end{lstlisting}


\subsubsection{Example \label{subsec:ch-solvedExample-2a}\ref{subsec:ch-solvedExample-2a}}

Find an equivalent disjunctive type for the type \lstinline!P = (Either[A, B], Either[C, D])!.

\subparagraph{Solution}

Begin by writing the given type in the type notation. The tuple becomes
the product type, and \lstinline!Either! becomes the disjunctive
or ``sum'' type:
\[
P\triangleq(A+B)\times(C+D)\quad.
\]
We can use the usual rules of arithmetic to expand brackets in this
type expression and to obtain an equivalent type:
\[
P\cong A\times C+A\times D+B\times C+B\times D\quad.
\]
This type is disjunctive, with $4$ parts.

\subsubsection{Example \label{subsec:ch-solvedExample-3}\ref{subsec:ch-solvedExample-3}}

Show that the following type equivalences do \emph{not} hold: $A+A\not\cong A$
and $A\times A\not\cong A$, although the corresponding logical identities
hold.

\subparagraph{Solution}

Note that the arithmetic equalities do not hold, $A+A\neq A$ and
$A\times A\ne A$. This already indicates that the types are not equivalent.
To build further intuition, consider that a value of type $A+A$ (in
Scala, \lstinline!Either[A, A]!) is a \lstinline!Left(a)! or a \lstinline!Right(a)!
for some \lstinline!a:A!. In the code notation, it is either $a^{:A}+\bbnum 0$
or $\bbnum 0+a^{:A}$. So, a value of type $A+A$ contains a value
of type $A$ with an additional information about whether it is the
first or the second part of the disjunctive type. We cannot represent
all that information in a single value of type $A$. 

A value of type $A\times A$ contains two (possibly different) values
of type $A$, which cannot be represented by a single value of type
$A$ without loss of information.

However, the corresponding logical identities $\alpha\vee\alpha=\alpha$
and $\alpha\wedge\alpha=\alpha$ hold. To see that, we could derive
the four formulas
\begin{align*}
\alpha\vee\alpha\Rightarrow\alpha\quad, & \quad\quad\alpha\Rightarrow\alpha\vee\alpha\quad,\\
\alpha\wedge\alpha\Rightarrow\alpha\quad, & \quad\quad\alpha\Rightarrow\alpha\wedge\alpha\quad,
\end{align*}
using the proof rules of Section~\ref{subsec:The-rules-of-proof}.
Alternatively, we may use the CH correspondence and show that the
type signatures
\begin{align*}
\forall A.\,A+A\Rightarrow A\quad, & \quad\quad\forall A.\,A\Rightarrow A+A\quad,\\
\forall A.\,A\times A\Rightarrow A\quad, & \quad\quad\forall A.\,A\Rightarrow A\times A\quad
\end{align*}
can be implemented via fully parametric functions. For a programmer,
it is easier to write code than to guess the correct sequence of proof
rules. For the first pair of type signatures, we find
\begin{lstlisting}
def f1[A]: Either[A, A] => A = {
  case Left(a)    => a   // No other choice here.
  case Right(a)   => a   // No other choice here.
}
def f2[A]: A => Either[A, A] = { a => Left(a) } // Can be also Right(a).
\end{lstlisting}
The presence of an arbitrary choice, to return \lstinline!Left(a)!
or \lstinline!Right(a)!, is a warning sign showing that additional
information is required to create a value of type \lstinline!Either[A, A]!.
This is precisely the information that is present in the type $A+A$
but missing in the type $A$.

The code notation for these functions is
\begin{align*}
 & f_{1}\triangleq\begin{array}{|c||c|}
 & A\\
\hline A & a\Rightarrow a\\
A & a\Rightarrow a
\end{array}=\begin{array}{|c||c|}
 & A\\
\hline A & \text{id}\\
A & \text{id}
\end{array}\quad,\\
 & f_{2}\triangleq a^{:A}\Rightarrow a+\bbnum 0^{:A}=\begin{array}{|c||cc|}
 & A & A\\
\hline A & a\Rightarrow a & 
\end{array}=\begin{array}{|c||cc|}
 & A & A\\
\hline A & \text{id} & 
\end{array}\quad.
\end{align*}
The composition of these functions is not equal to identity:
\[
f_{1}\bef f_{2}=\begin{array}{||c|}
\text{id}\\
\text{id}
\end{array}\bef\begin{array}{||cc|}
\text{id} & \end{array}=\begin{array}{||cc|}
\text{id} & \\
\text{id} & 
\end{array}\neq\text{id}=\begin{array}{||cc|}
\text{id} & \\
 & \text{id}
\end{array}\quad.
\]

For the second pair of type signatures, the code is
\begin{lstlisting}
def f1[A]: ((A, A)) => A = { case (a1, a2) => a1 } // Can be also `a2`.
cef f2[A]:   A => (A, A) = { x => (x, x) }     // No other choice here.
\end{lstlisting}
It is clear that the first function loses information when it returns
\lstinline!a1! and discards \lstinline!a2! (or vice versa).

The code notation for these functions is
\begin{align*}
 & f_{1}\triangleq a_{1}^{:A}\times a_{2}^{:A}\Rightarrow a_{1}\quad,\\
 & f_{2}\triangleq a^{:A}\Rightarrow a\times a\quad.
\end{align*}
The composition of these functions is not equal to identity:
\begin{align*}
f_{1}\bef f_{2} & =\left(a_{1}\times a_{2}\Rightarrow a_{1}\right)\bef\left(a\Rightarrow a\times a\right)\\
 & =\left(a_{1}\times a_{2}\Rightarrow a_{1}\times a_{1}\right)\neq\text{id}=\left(a_{1}\times a_{2}\Rightarrow a_{1}\times a_{2}\right)\quad.
\end{align*}

We have implemented all four type signatures as fully parametric functions,
which shows that the corresponding logical formulas are all true (i.e.~can
be derived using the proof rules). However, the functions cannot be
inverses of each other. So, the type equivalences do not hold.

\subsubsection{Example \label{subsec:ch-solvedExample-4}\ref{subsec:ch-solvedExample-4}}

Show that $\left(\left(A\wedge B\right)\Rightarrow C\right)\neq(A\Rightarrow C)\vee(B\Rightarrow C)$
in the constructive logic, although the equality holds in Boolean
logic. This is another example of the failure of Boolean logic to
provide correct reasoning for types.

\subparagraph{Solution}

Begin by rewriting the logical equality as two implications,
\begin{align*}
 & (A\wedge B\Rightarrow C)\Rightarrow(A\Rightarrow C)\vee(B\Rightarrow C)\quad,\\
 & \left((A\Rightarrow C)\vee(B\Rightarrow C)\right)\Rightarrow\left(\left(A\wedge B\right)\Rightarrow C\right)\quad.
\end{align*}
It is sufficient to show that one of these implications is incorrect.
Rather than looking for a proof tree in the constructive logic (which
would be difficult, since it would require us to demonstrate that
\emph{no} proof tree exists), let us use the CH correspondence. So
the task is to implement fully parametric functions with the type
signatures
\begin{align*}
 & (A\times B\Rightarrow C)\Rightarrow(A\Rightarrow C)+(B\Rightarrow C)\quad,\\
 & (A\Rightarrow C)+(B\Rightarrow C)\Rightarrow A\times B\Rightarrow C\quad.
\end{align*}
 For the first type signature:
\begin{lstlisting}
def f1[A,B,C]: (((A, B)) => C) => Either[A => C, B => C] = { k => ??? }
\end{lstlisting}
We are required to return either a \lstinline!Left(g)! with \lstinline!g: A => C!,
or a \lstinline!Right(h)! with \lstinline!h: B => C!. The only given
data is a function \lstinline!k! of type $A\times B\Rightarrow C$,
so the decision of whether to return a \lstinline!Left! or a \lstinline!Right!
must be hard-coded in the function \lstinline!f1! independently of
\lstinline!k!. Can we produce a function \lstinline!g! of type \lstinline!A => C!?
Given a value of type \lstinline!A!, we would need to return a value
of type \lstinline!C!. The only way to obtain a value of type \lstinline!C!
is by applying \lstinline!k! to some arguments. But to apply \lstinline!k!,
we need a value of type \lstinline!B!, which we do not have. So we
cannot produce a \lstinline!g: A => C!. Similarly, we cannot produce
a function \lstinline!h! of type \lstinline!B => C!.

We can repeat the same argument in the type notation: To return a
value of type $(A\Rightarrow C)+(B\Rightarrow C)$ means to return
either $g^{:A\Rightarrow C}+\bbnum 0$ or $\bbnum 0+h^{:B\Rightarrow C}$.
The decision must be hard-coded since the only data is a function
$k^{:A\times B\Rightarrow C}$. The only way of computing a $g^{:A\Rightarrow C}$
would be to partially apply $k^{:A\times B\Rightarrow C}$ to a value
of type $B$. However, we cannot obtain any values of type $B$. Similarly,
we cannot compute any $h^{:B\Rightarrow C}$.

The inverse type signature is implementable:
\begin{lstlisting}
def f2[A,B,C]: Either[A => C, B => C] => ((A, B)) => C = {
  case Left(g)    => { case (a, b) => g(a) }
  case Right(h)   => { case (a, b) => h(b) }
}
\end{lstlisting}
The matrix notation for this function is
\[
f_{2}\triangleq\begin{array}{|c||c|}
 & A\times B\Rightarrow C\\
\hline A\Rightarrow C & g^{:A\Rightarrow C}\Rightarrow a\times b\Rightarrow g(a)\\
B\Rightarrow C & h^{:B\Rightarrow C}\Rightarrow a\times b\Rightarrow h(b)
\end{array}\quad.
\]

Let us now show that the identity 
\begin{equation}
((\alpha\wedge\beta)\Rightarrow\gamma)=((\alpha\Rightarrow\gamma)\vee(\beta\Rightarrow\gamma))\label{eq:ch-example-identity-boolean-not-constructive}
\end{equation}
holds in Boolean logic. A straightforward calculation is to simplify
the Boolean expression using Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic}),
which only holds in Boolean logic (but not in the constructive logic).
We find
\begin{align*}
{\color{greenunder}\text{left-hand side of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})}:}\quad & \left(\alpha\wedge\beta\right)\gunderline{\Rightarrow}\,\gamma\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{\neg(\alpha\wedge\beta)}\vee\gamma\\
{\color{greenunder}\text{use de Morgan's law}:}\quad & =\neg\alpha\vee\neg\beta\vee\gamma\quad.\\
{\color{greenunder}\text{right-hand side of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})}:}\quad & (\gunderline{\alpha\Rightarrow\gamma})\vee(\gunderline{\beta\Rightarrow\gamma})\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\neg\alpha\vee\gunderline{\gamma}\vee\neg\beta\vee\gunderline{\gamma}\\
{\color{greenunder}\text{use identity }\gamma\vee\gamma=\gamma:}\quad & =\neg\alpha\vee\neg\beta\vee\gamma\quad.
\end{align*}
Both sides of Eq.~(\ref{eq:ch-example-identity-boolean-not-constructive})
are equal to the same formula, $\neg\alpha\vee\neg\beta\vee\gamma$,
so the identity holds.

This proof does not work in the constructive logic because neither
the Boolean formula~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})
nor the law of de Morgan,
\[
\neg(\alpha\wedge\beta)=\left(\neg\alpha\vee\neg\beta\right)\quad,
\]
can be derived from the rules of the constructive logic.

Another way of proving the Boolean identity~(\ref{eq:ch-example-identity-boolean-not-constructive})
is to enumerate all possible truth values for the variables $\alpha$,
$\beta$, and $\gamma$. The left-hand side, $\left(\alpha\wedge\beta\right)\Rightarrow\gamma$,
can be $False$ only if $\alpha\wedge\beta=True$ (that is, both $\alpha$
and $\beta$ are $True$) and $\gamma=False$; for all other truth
values of $\alpha$, $\beta$, and $\gamma$, the formula $\left(\alpha\wedge\beta\right)\Rightarrow\gamma$
is $True$. Let us determine when the right-hand side, $(\alpha\Rightarrow\gamma)\vee(\beta\Rightarrow\gamma)$,
can be $False$. This can happen only if both parts of the disjunction
are $False$; that means $\alpha=True$, $\beta=True$, and $\gamma=False$.
So, the two sides of the identity~(\ref{eq:ch-example-identity-boolean-not-constructive})
are both $True$ or both $False$ with any choice of truth values
of $\alpha$, $\beta$, and $\gamma$. In Boolean logic, this is sufficient
to prove the identity~(\ref{eq:ch-example-identity-boolean-not-constructive}).

It is important to note that the rules of the constructive logic are
not equivalent to checking whether some propositions are $True$ or
$False$. In this sense, constructive logic does not imply that every
proposition is either $True$ or $False$. This is not intuitive and
requires getting used to.

\subsubsection{Example \label{subsec:ch-solvedExample-5}\ref{subsec:ch-solvedExample-5}}

Denote $\text{Read}^{E,T}\triangleq E\Rightarrow T$ and implement
fully parametric functions with types $A\Rightarrow\text{Read}^{E,A}$
and $\text{Read}^{E,A}\Rightarrow(A\Rightarrow B)\Rightarrow\text{Read}^{E,B}$.

\subparagraph{Solution}

Begin by defining a type alias for the type $\text{Read}^{E,T}$:
\begin{lstlisting}
type Read[E, T] = E => T
\end{lstlisting}
The first type signature has only one implementation:
\begin{lstlisting}
def p[E, A]: A => Read[E, A] = { x => _ => x }
\end{lstlisting}
The argument of type $E$ must be discarded: we cannot use it while
computing a value of type \lstinline!A! given an \lstinline!x:A!.

The second type signature has three type parameters. It is the type
signature of the standard function \lstinline!map!:
\begin{lstlisting}
def map[E, A, B]: Read[E, A] => (A => B) => Read[E, B] = ???
\end{lstlisting}
Expanding the type alias, we see that the two curried arguments are
functions of types $E\Rightarrow A$ and $A\Rightarrow B$. The forward
composition of these functions is a function of type $E\Rightarrow B$,
or $\text{Read}^{E,B}$, which is exactly what we are required to
return. So the code can be written as

\begin{lstlisting}
def map[E, A, B]: (E => A) => (A => B) => E => B = { r => f => r andThen f }
\end{lstlisting}
If we did not notice this shortcut, we would reason differently: We
are required to compute a value of type $B$ given \emph{three} curried
arguments $r^{:E\Rightarrow A}$, $f^{:A\Rightarrow B}$, and $e^{:E}$.
Write this requirement as
\[
\text{map}\triangleq r^{:E\Rightarrow A}\Rightarrow f^{:A\Rightarrow B}\Rightarrow e^{:E}\Rightarrow???^{:B}\quad,
\]
The symbol $\text{???}^{:B}$ is called a \index{typed hole}\textbf{typed
hole}; it stands for a value that we are still figuring out how to
compute, but whose type is already known. Typed holes are supported
in Scala by an experimental compiler plugin.\footnote{\texttt{\href{https://github.com/cb372/scala-typed-holes}{https://github.com/cb372/scala-typed-holes}}}
The plugin will print the known information about the typed hole.

To fill the typed hole $\text{???}^{:B}$, we need a value of type
$B$. Since no arguments have type $B$, the only way of getting a
value of type $B$ is to apply $f^{:A\Rightarrow B}$ to some value
of type $A$. So we write
\[
\text{map}\triangleq r^{:E\Rightarrow A}\Rightarrow f^{:A\Rightarrow B}\Rightarrow e^{:E}\Rightarrow f(???^{:A})\quad.
\]
The only way of getting an $A$ is to apply $r$ to a value of type
$E$,
\[
\text{map}\triangleq r^{:E\Rightarrow A}\Rightarrow f^{:A\Rightarrow B}\Rightarrow e^{:E}\Rightarrow f(r(???^{:E}))\quad.
\]
We have exactly one value of type $E$, namely $e^{:E}$. So the code
must be 
\[
\text{map}^{E,A,B}\triangleq r^{:E\Rightarrow A}\Rightarrow f^{:A\Rightarrow B}\Rightarrow e^{:E}\Rightarrow f(r(e))\quad.
\]
Translate this to the Scala syntax:
\begin{lstlisting}
def map[E, A, B]: (E => A) => (A => B) => E => B = { r => f => e => f(r(e)) }
\end{lstlisting}
We may now notice that the expression $e\Rightarrow f(r(e))$ is a
function composition $r\bef f$ applied to $e$, and simplify the
code accordingly.

\subsubsection{Example \label{subsec:ch-solvedExample-6}\ref{subsec:ch-solvedExample-6}}

Show that the type signature \lstinline!Read[A, T] => (A => B) => Read[B, T]!
cannot be implemented as a fully parametric function.

\subparagraph{Solution}

Expand the type signature and try to implement this function:
\begin{lstlisting}
def m[A, B, T] : (A => T) => (A => B) => B => T = { r => f => b => ??? }
\end{lstlisting}
Given values $r^{:A\Rightarrow T}$, $f^{:A\Rightarrow B}$, and $b^{:B}$,
we need to compute a value of type $T$:
\[
m=r^{:A\Rightarrow T}\Rightarrow f^{:A\Rightarrow B}\Rightarrow b^{:B}\Rightarrow???^{:T}\quad.
\]
The only way of getting a $T$ is to apply $r$ to some value of type
$A$,
\[
m=r^{:A\Rightarrow T}\Rightarrow f^{:A\Rightarrow B}\Rightarrow b^{:B}\Rightarrow r(???^{:A})\quad.
\]
However, we do not have any values of type $A$. We have a function
$f^{:A\Rightarrow B}$ that \emph{consumes} values of type $A$, and
we cannot use $f$ to produce any values of type $A$. So we seem
to be unable to fill the typed hole $\text{???}^{:A}$ and implement
the function \lstinline!m!.

In order to verify that \lstinline!m! is unimplementable, we need
to prove that the logical formula
\begin{equation}
\forall(\alpha,\beta,\tau).\,(\alpha\Rightarrow\tau)\Rightarrow(\alpha\Rightarrow\beta)\Rightarrow(\beta\Rightarrow\tau)\label{eq:ch-example-boolean-formula-3}
\end{equation}
is not true in the constructive logic. We could use the \texttt{curryhoward}
library for that:
\begin{lstlisting}
@ def m[A, B, T] : (A => T) => (A => B) => B => T = implement
cmd1.sc:1: type (A => T) => (A => B) => B => T cannot be implemented
def m[A, B, T] : (A => T) => (A => B) => B => T = implement
                                                ^
Compilation Failed
\end{lstlisting}
Another way is to check whether this formula is true in Boolean logic.
A formula that holds in constructive logic will always hold in Boolean
logic, because all rules shown in Section~\ref{subsec:The-rules-of-proof}
preserve Boolean truth values (see Section~\ref{subsec:Relationship-between-Boolean}
for more details). It follows that any formula that fails to hold
in Boolean logic will also not hold in constructive logic. 

It is relatively easy to check whether a given Boolean formula is
always equal to $True$. Simplifying Eq.~(\ref{eq:ch-example-boolean-formula-3})
with the rules of Boolean logic, we find
\begin{align*}
 & (\alpha\Rightarrow\tau)\,\gunderline{\Rightarrow}\,(\alpha\Rightarrow\beta)\,\gunderline{\Rightarrow}\,(\beta\Rightarrow\tau)\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\neg(\gunderline{\alpha\Rightarrow\tau})\vee\neg(\gunderline{\alpha\Rightarrow\beta})\vee(\gunderline{\beta\Rightarrow\tau})\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{\neg(\neg\alpha\vee\tau)}\vee\gunderline{\neg(\neg\alpha\vee\beta)}\vee(\neg\beta\vee\tau)\\
{\color{greenunder}\text{use de Morgan's law}:}\quad & =\left(\alpha\wedge\neg\tau\right)\vee\gunderline{\left(\alpha\wedge\neg\beta\right)\vee\neg\beta}\vee\tau\\
{\color{greenunder}\text{use identity }(p\wedge q)\vee q=q:}\quad & =\gunderline{\left(\alpha\wedge\neg\tau\right)}\vee\neg\beta\vee\gunderline{\tau}\\
{\color{greenunder}\text{use identity }(p\wedge\neg q)\vee q=p\vee q:}\quad & =\alpha\vee\neg\beta\vee\tau\quad.
\end{align*}
This formula is not identically $True$: it is $False$ when $\alpha=\tau=False$
and $\beta=True$. So, Eq.~(\ref{eq:ch-example-boolean-formula-3})
is not true in Boolean logic, and thus is not true in constructive
logic. By the CH correspondence, we conclude that the type signature
of \lstinline!m! cannot be implemented by a fully parametric function.

\subsubsection{Example \label{subsec:ch-solvedExample-7}\ref{subsec:ch-solvedExample-7}}

Define the type constructor $P^{A}\triangleq\bbnum 1+A+A$ and implement
\lstinline!map! for it,
\[
\text{map}^{A,B}:P^{A}\Rightarrow(A\Rightarrow B)\Rightarrow P^{B}\quad.
\]
To check that \lstinline!map! preserves information, verify the law
\lstinline!map(p)(x => x) == p! for all \lstinline!p: P[A]!.

\subparagraph{Solution}

It is implied that \lstinline!map! should be fully parametric and
information-preserving. Begin by defining a Scala type for the notation
$\bbnum 1+A+A$:
\begin{lstlisting}
sealed trait P[A]
final case class P1[A]() extends P[A]
final case class P2[A](x: A) extends P[A]
final case class P3[A](x: A) extends P[A]
\end{lstlisting}
Now implement the required type signature. Each time we find a choice
in the implementation, we will choose to preserve information as much
as possible.
\begin{lstlisting}
def map[A, B]: P[A] => (A => B) => P[B] = p => f => p match {
  case P1() => P1()         // No choice here.
  case P2(x) => ???
  case P3(x) => ???
}
\end{lstlisting}
In the case \lstinline!P2(x)!, we are required to produce a value
of type $P^{B}$ from a value $x^{:A}$ and a function $f^{:A\Rightarrow B}$.
Since $P^{B}$ is a disjunctive type with three parts, we can produce
a value of type $P^{B}$ in three different ways: \lstinline!P1()!,
\lstinline!P2(...)!, and \lstinline!P3(...)!. If we return \lstinline!P1()!,
we will lose the information about the value \lstinline!x!. If we
return \lstinline!P3(...)!, we will preserve the information about
\lstinline!x! but lose the information that the input value was a
\lstinline!P2! rather than a \lstinline!P3!. So we should return
\lstinline!P2(...)! in that scope; in that way, we preserve the entire
input information. 

The value under \lstinline!P2(...)! must be of type $B$, and the
only way of getting a value of type $B$ is to apply $f$ to $x$.
So, we return \lstinline!P2(f(x))!.

Similarly, in the case \lstinline!P3(x)!, we should return \lstinline!P3(f(x))!.
The final code of \lstinline!map! is
\begin{lstlisting}
def map[A, B]: P[A] => (A => B) => P[B] = p => f => p match {
  case P1() => P1()         // No choice here.
  case P2(x) => P2(f(x))    // Preserve information.
  case P3(x) => P3(f(x))    // Preserve information.
}
\end{lstlisting}

To verify the given law, we first write a matrix notation for \lstinline!map!:
\[
\text{map}^{A,B}\triangleq p^{:\bbnum 1+A+A}\Rightarrow f^{:A\Rightarrow B}\Rightarrow p\triangleright\begin{array}{|c||ccc|}
 & \bbnum 1 & B & B\\
\hline \bbnum 1 & \text{id} &  & \\
A &  & f & \\
A &  &  & f
\end{array}\quad.
\]
The required law is written as an equation
\[
\text{map}(p)(\text{id})=p\quad.
\]
Substituting the code notation for \lstinline!map!, we verify the
law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{map}(p)(\text{id})\\
{\color{greenunder}\text{apply map()() to arguments}:}\quad & =p\triangleright\begin{array}{||ccc|}
\text{id} &  & \\
 & \text{id} & \\
 &  & \text{id}
\end{array}\\
{\color{greenunder}\text{identity in matrix notation}:}\quad & =p\triangleright\text{id}\\
{\color{greenunder}\triangleright\text{-notation}:}\quad & =\text{id}(p)=p\quad.
\end{align*}


\subsubsection{Example \label{subsec:ch-solvedExample-8}\ref{subsec:ch-solvedExample-8}}

Implement \lstinline!map! and \lstinline!flatMap! for \lstinline!Either[L, R]!,
applied to the type parameter \lstinline!L!.

\subparagraph{Solution}

For a type constructor, say, $P^{A}$, the standard type signatures
for \lstinline!map! and \lstinline!flatMap! are
\begin{align*}
\text{map} & :P^{A}\Rightarrow(A\Rightarrow B)\Rightarrow P^{B}\quad,\\
\text{flatMap} & :P^{A}\Rightarrow(A\Rightarrow P^{B})\Rightarrow P^{B}\quad.
\end{align*}
If a type constructor has more than one type parameter, e.g.~$P^{A,S,T}$,
one can define the functions \lstinline!map! and \lstinline!flatMap!
applied to a chosen parameter. For example, when applied to the type
parameter $A$, the type signatures are 
\begin{align*}
\text{map} & :P^{A,S,T}\Rightarrow(A\Rightarrow B)\Rightarrow P^{B,S,T}\quad,\\
\text{flatMap} & :P^{A,S,T}\Rightarrow(A\Rightarrow P^{B,S,T})\Rightarrow P^{B,S,T}\quad.
\end{align*}
Being ``applied to the type parameter $A$'' means that the other
type parameters $S,T$ in $P^{A,S,T}$ remain fixed while the type
parameter $A$ is replaced by $B$ in the type signatures of \lstinline!map!
and \lstinline!flatMap!.

For the type \lstinline!Either[L, R]! (i.e.~$L+R$), we keep the
type parameter $R$ fixed while $L$ is replaced by $M$. So we obtain
the type signatures
\begin{align*}
\text{map} & :L+R\Rightarrow(L\Rightarrow M)\Rightarrow M+R\quad,\\
\text{flatMap} & :L+R\Rightarrow(L\Rightarrow M+R)\Rightarrow M+R\quad.
\end{align*}
Implementing these functions is straightforward:
\begin{lstlisting}
def map[L,M,R]: Either[L, R] => (L => M) => Either[M, R] = e => f => e match {
  case Left(x)    => Left(f(x))
  case Right(y)   => Right(y)
}
def flatMap[L,M,R]: Either[L, R] => (L => Either[M, R]) => Either[M, R] = e => f => e match {
  case Left(x)    => f(x)
  case Right(y)   => Right(y)
}
\end{lstlisting}
The code notation for these functions is
\begin{align*}
\text{map} & \triangleq e^{:L+R}\Rightarrow f^{:L\Rightarrow M}\Rightarrow e\triangleright\begin{array}{|c||cc|}
 & M & R\\
\hline L & f & \\
R &  & \text{id}
\end{array}\quad,\\
\text{flatMap} & \triangleq e^{:L+R}\Rightarrow f^{:L\Rightarrow M+R}\Rightarrow e\triangleright\begin{array}{|c||c|}
 & M+R\\
\hline L & f\\
R & y^{:R}\Rightarrow\bbnum 0^{:M}+y
\end{array}\quad.
\end{align*}
Note that we cannot split $f$ into the $M$ and $R$ columns since
$f(x^{:L})$ could return either part of the disjunction $M+R$.

\subsubsection{Example \label{subsec:ch-solvedExample-9}\ref{subsec:ch-solvedExample-9}{*}}

Define a type $\text{State}^{S,A}\equiv S\Rightarrow A\times S$ and
implement the functions:

\textbf{(a)} $\text{pure}^{S,A}:A\Rightarrow\text{State}^{S,A}$

\textbf{(b)} $\text{map}^{S,A,B}:\text{State}^{S,A}\Rightarrow(A\Rightarrow B)\Rightarrow\text{State}^{S,B}$

\textbf{(c)} $\text{flatMap}^{S,A,B}:\text{State}^{S,A}\Rightarrow(A\Rightarrow\text{State}^{S,B})\Rightarrow\text{State}^{S,B}$

\subparagraph{Solution}

It is assumed that all functions must be fuly parametric, and preserve
as much information as possible. We define
\begin{lstlisting}
type State[S, A] = S => (A, S)
\end{lstlisting}

\textbf{(a)} The type signature is $A\Rightarrow S\Rightarrow A\times S$,
and there is only one implementation,
\begin{lstlisting}
def pure[S, A]: A => State[S, A] = a => s => (a, s)
\end{lstlisting}
In the short code notation, this is written as
\[
\text{pu}^{S,A}\triangleq a^{:A}\Rightarrow s^{:A}\Rightarrow a\times s\quad.
\]

\textbf{(b)} The explicit type signature is 
\[
\text{map}^{S,A,B}:(S\Rightarrow A\times S)\Rightarrow(A\Rightarrow B)\Rightarrow S\Rightarrow B\times S\quad.
\]
Begin writing a Scala implementation:
\begin{lstlisting}
def map[S, A, B]: State[S, A] => (A => B) => State[S, B] = { t => f => s => ??? }
\end{lstlisting}
We need to compute a value of $B\times S$ from the curried arguments
$t^{:S\Rightarrow A\times S}$, $f^{:A\Rightarrow B}$, and $s^{:S}$.
We begin writing the code of \lstinline!map! as a typed hole,
\[
\text{map}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow B}\Rightarrow s^{:S}\Rightarrow\text{???}^{:B}\times\text{???}^{:S}\quad.
\]
The only way of getting a value of type $B$ is by applying $f$ to
a value of type $A$:
\[
\text{map}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow B}\Rightarrow s^{:S}\Rightarrow f(\text{???}^{:A})\times\text{???}^{:S}\quad.
\]
To fill the typed hole, we need a value of type $A$. The only possibility
of obtaining a value of type $A$ is by applying $t$ to a value of
type $S$; we already have such a value, $s^{:S}$. Computing $t(s)$
yields a pair of type $A\times S$, from which we may take the first
part (of type $A$) to fill the typed hole $\text{???}^{:A}$. The
second part of the pair is a value of type $S$ that we may use to
fill the second typed hole, $\text{???}^{:S}$. So the Scala code
is
\begin{lstlisting}
def map[S, A, B]: State[S, A] => (A => B) => State[S, B] = { t => f => s =>
  val (a, s2) = t(s)
  (f(a), s2)          // We could also return `(f(a), s)` here.
}
\end{lstlisting}
Why not return the original value \lstinline!s! in the tuple $B\times S$,
instead of the new value \lstinline!s2!? The reason is that we would
like to preserve information as much as possible. If we return \lstinline!(f(a), s)!
in the function \lstinline!map!, we will have discarded the computed
value \lstinline!s2!, which is a loss of information.

To write the code notation for \lstinline!map!, we need to destructure
the pair that $t(s)$ returns. We can write explicit destructuring
code like this:
\[
\text{map}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow B}\Rightarrow s^{:S}\Rightarrow(a^{:A}\times s_{2}^{:S}\Rightarrow f(a)\times s_{2})(t(s))\quad.
\]
If we temporarily denote by $q$ the destructuring function 
\[
q\triangleq(a^{:A}\times s_{2}^{:S}\Rightarrow f(a)\times s_{2})\quad,
\]
we will notice that the expression $s\Rightarrow q(t(s))$ is a function
composition applied to $s$. So, we rewrite $s\Rightarrow q(t(s))$
as the composition $t\bef q$ and obtain shorter code, 
\[
\text{map}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow B}\Rightarrow t\bef(a^{:A}\times s^{:S}\Rightarrow f(a)\times s)\quad.
\]
Shorter formulas are often easier to reason about in derivations (although
not necessarily easier to read when converted to program code).

\textbf{(c)} The required type signature is
\[
\text{flatMap}^{S,A,B}:(S\Rightarrow A\times S)\Rightarrow(A\Rightarrow S\Rightarrow B\times S)\Rightarrow S\Rightarrow B\times S\quad.
\]
We perform t reasoning with typed holes:
\[
\text{flatMap}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow S\Rightarrow B\times S}\Rightarrow s^{:S}\Rightarrow\text{???}^{:B}\times???^{:S}\quad.
\]
To fill $\text{???}^{:B}$, we need to apply $f$ to some arguments,
since $f$ is the only function that returns any values of type $B$.
A saturated application of $f$ will yield a value of type $B\times S$,
which we can return without change:
\[
\text{flatMap}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow S\Rightarrow B\times S}\Rightarrow s^{:S}\Rightarrow f(\text{???}^{:A})(\text{???}^{:S})\quad.
\]
To fill the new typed holes, we need to apply $t$ to an argument
of type $S$. We have only one given value $s^{:S}$ of type $S$,
so we must compute $t(s)$ and destructure it:
\[
\text{flatMap}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow S\Rightarrow B\times S}\Rightarrow s^{:S}\Rightarrow\left(a\times s_{2}\Rightarrow f(a)(s_{2})\right)(t(s))\quad.
\]
Translating this notation into Scala code, we obtain
\begin{lstlisting}
def flatMap[S,A,B]: State[S, A] => (A => State[S, B]) => State[S, B] = {
  t => f => s =>
    val (a, s2) = t(s)
    f(a)(s2)            // We could also return `f(a)(s)` here.
}
\end{lstlisting}
As before, in order to preserve information, we choose not to discard
the computed value \lstinline!s2!.

The code notation for \lstinline!flatMap! can be simplified to
\[
\text{flatMap}\triangleq t^{:S\Rightarrow A\times S}\Rightarrow f^{:A\Rightarrow S\Rightarrow B\times S}\Rightarrow t\bef\left(a\times s\Rightarrow f(a)(s)\right)\quad.
\]


\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:ch-Exercise-0}\ref{subsec:ch-Exercise-0}}

Find the cardinality of the Scala type \lstinline!Option[Boolean => Option[Boolean]]!.
Show that this type is equivalent to \lstinline!Option[Boolean] => Boolean!,
and argue that the equivalence is accidental\index{type equivalence!accidental}
and not ``natural''.

\subsubsection{Exercise \label{subsec:ch-Exercise-1-a}\ref{subsec:ch-Exercise-1-a}}

Verify the type equivalences $A+A\cong\bbnum 2\times A$ and $A\times A\cong\bbnum 2\Rightarrow A$,
where $\bbnum 2$ denotes the \lstinline!Boolean! type.

\subsubsection{Exercise \label{subsec:ch-Exercise-1}\ref{subsec:ch-Exercise-1}}

Show that $A\Rightarrow(B\vee C)\neq(A\Rightarrow B)\wedge(A\Rightarrow C)$
in logic.

\subsubsection{Exercise \label{subsec:ch-Exercise-2}\ref{subsec:ch-Exercise-2}}

Write the type notation for \lstinline!Either[(A, Int), Either[(A, Char), (A, Float)]]!.
Transform this type into an equivalent type of the form $A\times(...)$.

\subsubsection{Exercise \label{subsec:ch-Exercise-3}\ref{subsec:ch-Exercise-3}}

Define a type $\text{OptE}^{T,A}\triangleq\bbnum 1+T+A$ and implement
information-preserving \lstinline!map! and \lstinline!flatMap! for
it, applied to the type parameter $A$. Get the same result using
the equivalent type $(\bbnum 1+A)+T$, i.e. \lstinline!Either[Option[A], T]!.
The required type signatures are
\begin{align*}
\text{map}^{A,B,T} & :\text{OptE}^{T,A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow\text{OptE}^{T,B}\quad,\\
\text{flatMap}^{A,B,T} & :\text{OptE}^{T,A}\Rightarrow(A\Rightarrow\text{OptE}^{T,B})\Rightarrow\text{OptE}^{T,B}\quad.
\end{align*}


\subsubsection{Exercise \label{subsec:ch-Exercise-4}\ref{subsec:ch-Exercise-4}}

Implement the \lstinline!map! function for \lstinline!P[A]! (see
Example~\ref{subsec:ch-solvedExample-2}). The required type signature
is $P^{A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow P^{B}$.

\subsubsection{Exercise \label{subsec:ch-Exercise-4-1}\ref{subsec:ch-Exercise-4-1}}

For the type constructor $Q^{T,A}$ defined in Exercise~\ref{subsec:Exercise-type-notation-1},
define the \lstinline!map! function with the type signature
\[
\text{map}^{T,A,B}:Q^{T,A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow Q^{T,B}\quad.
\]
The implementation should preserve information as much as possible.

\subsubsection{Exercise \label{subsec:Exercise-disjunctive-6}\ref{subsec:Exercise-disjunctive-6}}

Define a recursive type constructor $\text{Tr}_{3}$ as $\text{Tr}_{3}{}^{A}\triangleq\bbnum 1+A\times A\times A\times\text{Tr}_{3}{}^{A}$
and implement the \lstinline!map! function for it, with the standard
type signature
\[
\text{map}^{A,B}:\text{Tr}_{3}{}^{A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow\text{Tr}_{3}{}^{B}\quad.
\]


\subsubsection{Exercise \label{subsec:ch-Exercise-5}\ref{subsec:ch-Exercise-5}}

Implement fully parametric functions with the following types:

\textbf{(a)} $A+Z\Rightarrow(A\Rightarrow B)\Rightarrow B+Z$

\textbf{(b)} $A+Z\Rightarrow B+Z\Rightarrow(A\Rightarrow B\Rightarrow C)\Rightarrow C+Z$

\textbf{(c)} $\text{flatMap}^{E,A,B}:\text{Read}^{E,A}\Rightarrow(A\Rightarrow\text{Read}^{E,B})\Rightarrow\text{Read}^{E,B}$

\textbf{(d)} $\text{State}^{S,A}\Rightarrow\left(S\times A\Rightarrow S\times B\right)\Rightarrow\text{State}^{S,B}$

\subsubsection{Exercise \label{subsec:ch-Exercise-7}\ref{subsec:ch-Exercise-7}{*}}

Denote $\text{Cont}^{R,T}\triangleq\left(T\Rightarrow R\right)\Rightarrow R$
and implement the functions:

\textbf{(a)} $\text{map}^{R,T,U}:\text{Cont}^{R,T}\Rightarrow(T\Rightarrow U)\Rightarrow\text{Cont}^{R,U}$

\textbf{(b)} $\text{flatMap}^{R,T,U}:\text{Cont}^{R,T}\Rightarrow(T\Rightarrow\text{Cont}^{R,U})\Rightarrow\text{Cont}^{R,U}$

\subsubsection{Exercise \label{subsec:ch-Exercise-8}\ref{subsec:ch-Exercise-8}{*}}

Denote $\text{Search}^{Z,T}\triangleq\left(T\Rightarrow Z\right)\Rightarrow T$
and implement the functions:

\textbf{(a)} $\text{map}^{Z,A,B}:\text{Search}^{Z,A}\Rightarrow\left(A\Rightarrow B\right)\Rightarrow\text{Search}^{Z,B}$

\textbf{(b)} $\text{flatMap}^{Z,A,B}:\text{Search}^{Z,A}\Rightarrow(A\Rightarrow\text{Search}^{Z,B})\Rightarrow\text{Search}^{Z,B}$

\section{Discussion}

\subsection{Using the Curry-Howard correspondence for writing code}

This chapter shows how the CH correspondence performs two practically
important tasks of type-level reasoning: checking whether a type signature
can be implemented as a fully parametric function, and determining
whether two types are equivalent. The first task is accomplished by
mapping type expressions into formulas in the constructive logic and
by applying the proof rules of that logic. The second task is accomplished
by mapping type expressions into \emph{arithmetic} formulas and applying
the ordinary rules of arithmetic.

Fully parametric functions can be often derived from their type signatures
alone. It is useful for a programmer to know that certain type signatures,
such as
\begin{lstlisting}
def f[A, B]: A => (A => B) => B
\end{lstlisting}
have only one possible implementation, while other type signatures,
such as
\begin{lstlisting}
def f[A, B]: A => (B => A) => B
\end{lstlisting}
cannot be implemented as fully parametric functions.

It is helpful to know that the code derivation can be performed by
an algorithm using tools such as the \texttt{curryhoward} library.
However, in most cases it is more beneficial if a programmer is able
to derive an implementation by hand, or to see quickly that an implementation
is impossible. Exercises in this chapter build up the required technique.
The type notation introduced in this book is designed to help programmers
to recognize patterns in type expressions and to reason about them
more easily.

Throughout this chapter, we required all functions to be fully parametric.
The reason is that the CH correspondence becomes informative only
with parameterized types and with fully parametric functions. For
concrete types, e.g.\ \lstinline!Int!, one can always produce \emph{some}
value even with no previous data, so the proposition $\mathcal{CH}(\text{Int})$
is always true within any code.

Consider the function \lstinline!(x:Int) => x + 1! with type signature
\lstinline!Int => Int!. This type signature is insufficient to specify
the code of the function, because there are many different functions
with the same type signature, such as \lstinline!x => x + 2!, \lstinline!x => x + 3!,
etc. So, it is hopeless to try deriving the code from this type signature.
Only a fully parametric type signature, such as $A\Rightarrow\left(A\Rightarrow B\right)\Rightarrow B$,
can be informative enough (in some cases) for deriving the code automatically
and unambiguously. If we permit functions that are not fully parametric,
we will not be able to reason about implementability of type signatures
or about code derivation.

Information about the implementability of type signatures is given
by logical formulas involving ${\cal CH}$-propositions. Generally,
a ${\cal CH}$-proposition such as ${\cal CH}(A)$ is true when we
are able to compute a value of type $A$. The validity of ${\cal CH}$-propositions
means that we can compute \emph{some} values of given types, but it
does not give any information about the properties of those values,
such as whether they satisfy any additional laws. For this reason,
type equivalence is not determined by an equivalence of logical formulas.

It is useful for programmers to be able to reason about types and
transform type expressions to equivalent simpler types before starting
to write code. We have shown that a type equivalence corresponds to
\emph{each} standard arithmetic identity such as $\left(a+b\right)+c=a+\left(b+c\right)$,
$\left(a\times b\right)\times c=a\times(b\times c)$, $1\times a=a$,
$\left(a+b\right)\times c=a\times c+b\times c$, etc. So, we are allowed
to transform and simplify types as if they were arithmetic expressions,
e.g.~to rewrite
\[
\left(A+B\right)\times C+D\cong D+A\times C+B\times C\quad.
\]
The type notation makes this reasoning more intuitive (for people
familiar with arithmetic). 

These results apply to all type expressions built up using product
types, disjunctive types (also called ``sum'' types because they
correspond to arithmetic sums), and function types (also called ``exponential''
types because they correspond to arithmetic exponentials). Such types
may be called \textbf{exponential-polynomial}\index{exponential-polynomial type}
types.

There are no type constructions corresponding to subtraction or division,
so equations such as 
\begin{align*}
\left(1-t\right)\times\left(1+t\right) & =1-t\times t\quad,\\
\frac{t+t\times t}{t} & =1+t\quad,
\end{align*}
do not directly yield any type equivalences. However, consider this
well-known formula,
\[
\frac{1}{1-t}=1+t+t^{2}+t^{3}+...+t^{n}+...\quad.
\]
At first sight, this formula appears to involve subtraction, division,
and an infinite series, and thus cannot be directly translated into
a type equivalence. However, this formula can be rewritten as
\begin{equation}
L(t)=1+t+t^{2}+t^{3}+...+t^{n}\times L(t)\quad,\label{eq:ch-example-type-formula-list}
\end{equation}
which is finite and only contains additions and multiplications. So,
Eq.~(\ref{eq:ch-example-type-formula-list}) can be translated into
a type equivalence:
\[
L^{A}\cong1+A+A\times A+A\times A\times A+...+\underbrace{A\times...\times A}_{n}\times L^{A}\quad.
\]
This type formula (with $n=1$) is equivalent to a recursive definition
of the \lstinline!List! type,
\[
\text{List}^{A}\triangleq1+A\times\text{List}^{A}\quad.
\]


\subsection{Implications for designing new programming languages}

The functional programming paradigm assumes that programmers may use
the seven standard type constructions (Section~\ref{subsec:Type-notation-and-standard-type-constructions})
and the eight standard code constructions (Section~\ref{subsec:The-rules-of-proof}).
These constructions are foundational in the sense that all types and
all code can be expressed via these constructions. A programming language
that does not directly support some of these constructions cannot
be considered a functional programming language.

A remarkable consequence of the CH correspondence is that the type
system of any programming language (functional or not) is mapped into
a \emph{certain} \emph{logic}, i.e.~a system of logical operations
and proof rules. A logical operation will correspond to each of the
type constructions available in the programming language; a proof
rule will correspond to each of the available code constructions.
Functional programming languages that support all the standard type
and code constructions \textendash{} for instance, OCaml, Haskell,
F\#, Scala, Swift, etc., \textendash{} will be mapped into the constructive
logic with all standard logical operations available ($True$, $False$,
disjunction, conjunction, and implication). Languages such as C, C++,
Java, C\# are mapped into logics that do not have the disjunction
operation or the constants $True$ and $False$. In other words, these
languages are mapped into \emph{incomplete} logics where some theorems
will not be provable. (If ${\cal CH}(A)$ is true but not provable,
a value of type $A$ is not directly computable by programs, although
it could have been.) Languages such as Python, JavaScript, Ruby, Clojure
have no type checking and so are mapped to \emph{inconsistent} logics
where any proposition can be derived \textendash{} even propositions
normally considered $False$. 

Incompleteness of a logic means that the programming language is unable
to express directly certain computations with data, such as handling
data that belongs to a disjoint domain. Inconsistency of a logic means
that a program may ``derive $False$ from $True$'', i.e.~the program
appears to compute a value that is not actually available. In practice,
this usually means that a program \emph{crashes} because a value has
a wrong type, is ``null'', or is a pointer to an invalid memory
location. 

None of these errors will happen in a programming language whose logic
is complete and consistent and where types are checked at compile
time. 

So, the CH correspondence gives a mathematically justified procedure
for designing type systems in new programming languages. The procedure
contains the following steps:
\begin{itemize}
\item Choose a complete formal logic that is free of inconsistencies.
\item For each logical operation, provide a type construction in the language.
\item For each proof rule, provide a code construction in the language.
\end{itemize}
Programmers benefit if their programming language is designed according
to these principles.

Mathematicians have studied different logics such as modal logic,
temporal logic, or linear logic. For each logic, mathematicians have
determined the minimal complete sets of logical operations, axioms,
and proof rules that do not lead to inconsistency. Programming language
designers can pick and choose logics and translate them into minimal
programming languages whose programs are guaranteed to have correct
types. This mathematical guarantee (known as \index{type safety}\textbf{type
safety}) is a powerful help for programmers since it automatically
prevents a large class of programming errors.

Practically used programming languages will need to introduce many
more features than the minimal, mathematically necessary constructions
that correspond to the chosen logic. Programmers will still benefit
from type safety as long as the program stays within the mathematically
consistent subset of the language. 

\subsection{Uses of the void type}

Scala's \index{void type}void type (\lstinline!Nothing!) corresponds
to the logical constant $False$. The practical uses of $False$ are
quite limited. One use case is for a branch of a \lstinline!match!
/ \lstinline!case! expression that does not return a value because
it throws an \index{exception}exception. Such branches are considered
formally to return a value of type \lstinline!Nothing!, which can
then be mapped to a value of any other type (through the function
\lstinline!absurd[A]: Nothing => A!, see Example~\ref{subsec:ch-Example-type-identity-0-to-A}). 

To see how this trick is used, consider this code defining a value
\lstinline!x!,
\begin{lstlisting}
val x: Double = if (t >= 0.0) math.sqrt(t) else { throw new Exception("error") }
\end{lstlisting}
The \lstinline!else! branch does not return a value, but \lstinline!x!
is declared to be of type \lstinline!Double!. For this code to type-check,
both branches must return values of the same type. So, the compiler
needs to pretend that the \lstinline!else! branch also returns a
value of type \lstinline!Double!. The compiler first assigns the
type \lstinline!Nothing! to the expression \lstinline!throw ...!
and then implicitly uses the function \lstinline!absurd: Nothing => Double!
to convert that type to \lstinline!Double!. In this way, types will
match in the definition of the value \lstinline!x!. 

We will not use exceptions in this book: the functional programming
paradigm avoids exceptions because their presence significantly complicates
reasoning about code.

So far, none of our examples involved the logical \textbf{negation}\index{negation}
operation. It is defined as 
\[
\neg A\triangleq A\Rightarrow False\quad,
\]
and its practical use is as limited as that of $False$ and the void
type. However, logical negation plays an important role in Boolean
logic, which we will discuss next.

\subsection{Relationship between Boolean logic and constructive logic\label{subsec:Relationship-between-Boolean} }

We have seen that some true theorems of Boolean logic are not true
in constructive logic. For example, the Boolean identities $\neg\left(\neg\alpha\right)=\alpha$
and $\left(\alpha\Rightarrow\beta\right)=(\neg\alpha\vee\beta)$ do
not hold in the constructive logic. However, any theorem of constructive
logic is also a theorem of Boolean logic. The reason is that all eight
rules of constructive logic (Section~\ref{subsec:The-rules-of-proof})
are also true in Boolean logic.

To verify that a formula is true in Boolean logic, we only need to
check that the value of the formula is $True$ for all possible truth
values ($True$ or $False$) of its variables. A sequent such as $\alpha,\beta\vdash\gamma$
is true in Boolean logic if and only if $\gamma=True$ under the assumption
that $\alpha=\beta=True$. So, the sequent $\alpha,\beta\vdash\gamma$
is translated into the Boolean formula 
\[
\alpha,\beta\vdash\gamma=\left(\left(\alpha\wedge\beta\right)\Rightarrow\gamma\right)=\left(\neg\alpha\vee\neg\beta\vee\gamma\right)\quad.
\]
Table~\ref{tab:Proof-rules-of-constructive-and-boolean} translates
all proof rules of Section~\ref{subsec:The-rules-of-proof} into
Boolean formulas. The first two lines are axioms, while the subsequent
lines are Boolean theorems that can be verified by calculation.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|}
\hline 
\textbf{Constructive logic} & \textbf{Boolean logic}\tabularnewline
\hline 
\hline 
$\frac{}{\Gamma\vdash{\cal CH}(\bbnum 1)}\quad(\text{create unit})$ & {\small{}$\neg\Gamma\vee True=True$}\tabularnewline
\hline 
$\frac{~}{\Gamma,\alpha\vdash\alpha}\quad(\text{use arg})$ & {\small{}$\neg\Gamma\vee\neg\alpha\vee\alpha=True$}\tabularnewline
\hline 
$\frac{\Gamma,\alpha\vdash\beta}{\Gamma\vdash\alpha\Rightarrow\beta}\quad(\text{create function})$ & {\small{}$\left(\neg\Gamma\vee\neg\alpha\vee\beta\right)=\left(\neg\Gamma\vee\left(\alpha\Rightarrow\beta\right)\right)$}\tabularnewline
\hline 
$\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\alpha\Rightarrow\beta}{\Gamma\vdash\beta}\quad(\text{use function})$ & {\small{}$\left(\left(\neg\Gamma\vee\alpha\right)\wedge\left(\neg\Gamma\vee\left(\alpha\Rightarrow\beta\right)\right)\right)\Rightarrow\left(\neg\Gamma\vee\beta\right)$}\tabularnewline
\hline 
$\frac{\Gamma\vdash\alpha\quad\quad\Gamma\vdash\beta}{\Gamma\vdash\alpha\wedge\beta}\quad(\text{create tuple})$ & {\small{}$\left(\neg\Gamma\vee\alpha\right)\wedge\left(\neg\Gamma\vee\beta\right)=\left(\neg\Gamma\vee\left(\alpha\wedge\beta\right)\right)$}\tabularnewline
\hline 
$\frac{\Gamma\vdash\alpha\wedge\beta}{\Gamma\vdash\alpha}\quad(\text{use tuple-}1)$ & {\small{}$\left(\neg\Gamma\vee\left(\alpha\wedge\beta\right)\right)\Rightarrow\left(\neg\Gamma\vee\alpha\right)$}\tabularnewline
\hline 
$\frac{\Gamma\vdash\alpha}{\Gamma\vdash\alpha\vee\beta}\quad(\text{create Left})$ & {\small{}$\left(\neg\Gamma\vee\alpha\right)\Rightarrow\left(\neg\Gamma\vee\left(\alpha\vee\beta\right)\right)$}\tabularnewline
\hline 
$\frac{\Gamma\vdash\alpha\vee\beta\quad\quad\Gamma,\alpha\vdash\gamma\quad\quad\Gamma,\beta\vdash\gamma}{\Gamma\vdash\gamma}\quad(\text{use Either})$ & {\small{}$\left(\left(\neg\Gamma\vee\alpha\vee\beta\right)\wedge\left(\neg\Gamma\vee\neg\alpha\vee\gamma\right)\wedge\left(\neg\Gamma\vee\neg\beta\vee\gamma\right)\right)\Rightarrow\left(\neg\Gamma\vee\gamma\right)$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Proof rules of constructive logic are true also in the Boolean logic.\label{tab:Proof-rules-of-constructive-and-boolean}}
\end{table}

To simplify the calculations, note that all terms in the formulas
contain the operation $\left(\neg\Gamma\vee...\right)$ corresponding
to the context $\Gamma$. Now, if $\Gamma$ is $False$, the entire
formula becomes automatically $True$, and there is nothing else to
check. So, it remains to verify the formula in case $\Gamma=True$,
and then we can simply omit all instances of $\neg\Gamma$ in the
formulas. Let us show the Boolean derivations for the rules ``$\text{use function}$''
and ``$\text{use Either}$''; other formulas are checked in a similar
way. 
\begin{align*}
{\color{greenunder}\text{formula ``use function''}:}\quad & \left(\alpha\wedge\left(\alpha\Rightarrow\beta\right)\right)\Rightarrow\beta\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\gunderline{\neg}(\alpha\,\gunderline{\wedge}\,(\neg\alpha\,\gunderline{\vee}\,\beta))\vee\beta\\
{\color{greenunder}\text{de Morgan's laws}:}\quad & =\gunderline{\neg\alpha\vee(\alpha\wedge\neg\beta)}\vee\beta\\
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q\text{ with }p=\neg\alpha\text{ and }q=\beta:}\quad & =\neg\alpha\vee\gunderline{\neg\beta\vee\beta}\\
{\color{greenunder}\text{axiom ``use arg''}:}\quad & =True\quad.
\end{align*}
\begin{align*}
{\color{greenunder}\text{formula ``use Either''}:}\quad & \left(\left(\alpha\vee\beta\right)\wedge\left(\alpha\Rightarrow\gamma\right)\wedge\left(\beta\Rightarrow\gamma\right)\right)\Rightarrow\gamma\\
{\color{greenunder}\text{use Eq.~(\ref{eq:ch-definition-of-implication-in-Boolean-logic})}:}\quad & =\neg\left(\left(\alpha\vee\beta\right)\wedge\left(\neg\alpha\vee\gamma\right)\wedge\left(\neg\beta\vee\gamma\right)\right)\vee\gamma\\
{\color{greenunder}\text{de Morgan's laws}:}\quad & =\left(\neg\alpha\wedge\neg\beta\right)\vee\gunderline{\left(\alpha\wedge\neg\gamma\right)}\vee\gunderline{\left(\beta\wedge\neg\gamma\right)}\vee\gamma\\
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q:}\quad & =\gunderline{\left(\neg\alpha\wedge\neg\beta\right)\vee\alpha}\vee\beta\vee\gamma\\
{\color{greenunder}\text{identity }p\vee(\neg p\wedge q)=p\vee q:}\quad & =\gunderline{\neg\alpha\vee\alpha}\vee\beta\vee\gamma\\
{\color{greenunder}\text{axiom ``use arg''}:}\quad & =True\quad.
\end{align*}
Since each proof rule of the constructive logic is translated into
a true formula in Boolean logic, it follows that a proof tree in the
constructive logic will be translated into a tree of Boolean formulas
that have value $True$ for each axiom or proof rule. The result is
that any constructive proof for a sequent such as $\emptyset\vdash f(\alpha,\beta,\gamma)$
is translated into a chain of Boolean implications that look like
this,
\[
True=(...)\Rightarrow(...)\Rightarrow...\Rightarrow f(\alpha,\beta,\gamma)\quad.
\]
Since $\left(True\Rightarrow\alpha\right)=\alpha$, this chain proves
the Boolean formula $f(\alpha,\beta,\gamma)$.

For example, the proof tree shown in Figure~\ref{fig:Proof-of-the-sequent-example-2}
is translated into
\begin{align*}
{\color{greenunder}\text{axiom ``use arg''}:}\quad & True=\left(\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\neg\alpha\vee\alpha\right)\\
{\color{greenunder}\text{rule ``create function''}:}\quad & \Rightarrow\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\left(\alpha\Rightarrow\alpha\right)\quad.\\
{\color{greenunder}\text{axiom ``use arg''}:}\quad & True=\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\quad.\\
{\color{greenunder}\text{rule ``use function''}:}\quad & True\Rightarrow\left(\neg\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\vee\beta\right)\\
{\color{greenunder}\text{rule ``create function''}:}\quad & \Rightarrow\left(\left(\left(\alpha\Rightarrow\alpha\right)\Rightarrow\beta\right)\Rightarrow\beta\right)\quad.
\end{align*}

It is easier to check Boolean truth than to find a proof tree in constructive
logic (or to establish that no proof tree exists). So, if we find
that a formula is not true in Boolean logic, we know it is also not
true in constructive logic. This gives us a quick way of proving that
some type signatures are not implementable as fully parametric functions.
In addition to formulas shown in Table~\ref{tab:Logical-formulas-not-Boolean-theorems}
(Section~\ref{subsec:ch-Motivation-and-first-examples}), further
examples of formulas that are not true in Boolean logic are
\begin{align*}
 & \forall A.\,A\quad,\\
 & \forall(A,B).\,A\Rightarrow B\quad,\\
 & \forall(A,B).\,(A\Rightarrow B)\Rightarrow B\quad.
\end{align*}

Table~\ref{tab:Proof-rules-of-constructive-and-boolean} uses the
Boolean identity $\left(\alpha\Rightarrow\beta\right)=(\neg\alpha\vee\beta)$,
which does not hold in the constructive logic, to translate the constructive
axiom ``$\text{use arg}$'' into the Boolean axiom $\neg\alpha\vee\alpha=True$.
The formula $\neg\alpha\vee\alpha=True$ is known as the \textbf{law
of excluded middle}\footnote{\texttt{\href{https://en.wikipedia.org/wiki/Law_of_excluded_middle}{https://en.wikipedia.org/wiki/Law\_of\_excluded\_middle}}},
which is equivalent to saying that any proposition $\alpha$ is either
true or false. It is remarkable that the constructive logic \emph{does
not have} the ``law of excluded middle''; it is not an axiom nor
a derived theorem of constructive logic. 

To see why, consider what it would mean for $\neg\alpha\vee\alpha=True$
to hold in the constructive logic. The negation operation, $\neg\alpha$,
is defined as the implication $\alpha\Rightarrow False$. So, the
logical formula $\forall\alpha.\,\neg\alpha\vee\alpha$ corresponds
to the type $\forall A.\,\left(A\Rightarrow\bbnum 0\right)+A$. Can
we compute a value of this type in a fully parametric function? We
would need to compute either a value of type $A\Rightarrow\bbnum 0$
or a value of type $A$; this decision needs to be made in advance
independently of $A$, because the code of a fully parametric function
must operate in the same way for all types. Should we decide to return
$A$ or $A\Rightarrow\bbnum 0$? We certainly cannot compute a value
of type $A$ from scratch, since $A$ is a type parameter. As we have
seen in Example~\ref{subsec:ch-Example-type-identity-A-0}, a value
of type $A\Rightarrow\bbnum 0$ exists only if the type $A$ is itself
$\bbnum 0$; but we do not know that, and we are supposed to write
the same code for all types $A$. Since there are no values of type
$\bbnum 0$, and the type parameter $A$ could be, say, \lstinline!Int!,
we cannot compute a value of type $A\Rightarrow\bbnum 0$.

Example~\ref{subsec:ch-Example-type-identity-A-0} showed that the
type $A\Rightarrow\bbnum 0$ is equivalent to $\bbnum 0$ if $A$
is not itself void ($A\not\cong\bbnum 0$), and to $\bbnum 1$ otherwise.
Surely, any type $A$ is either void or not void. So, why exactly
is it impossible to implement a value of the type $\left(A\Rightarrow\bbnum 0\right)+A$?
We could say that if $A$ is void then $\left(A\Rightarrow\bbnum 0\right)\cong\bbnum 1$
is not void, and so at least one of the parts of the disjunction $\left(A\Rightarrow\bbnum 0\right)+A$
should always exist.

However, this reasoning is incorrect. When we implement functions,
we need to compute values of the required types and not merely show
that these values ``should exist''. Moreover, the functions' code
must work in the same general way for all types; the code cannot decide
what to do depending on a specific type. So, the requirement is to
compute a value of type $\left(A\Rightarrow\bbnum 0\right)+A$ in
the same way for an arbitrary type $A$. Proving that the value ``should
exist'' is insufficient; we need to know how to ``construct'' that
value in the code. The ``constructive\index{constructive logic}''
logic got its name from this requirement.

\begin{comment}
you have code like this it means that at some point in your program
in the expression you are able to compute a value X of type T now
of course we're assuming that your program is correct and running
and your expression is being evaluated correctly so if so you have
a value of type T let's denote this proposition by CH of T meaning
that code has a value of type T the curry habit respondents is a correspondence
between types and prepositions and also between values and proofs
on the one side there is program code in a functional language that
program has types and expressions that have those types or values
of those types on the other side of the correspondence there is formal
logic which has prepositions and these prepositions can be true or
false the formal logic has proofs of prepositions so true prepositions
follow from axioms or from other already proved prepositions so in
this tutorial I will explore this correspondence using Scala as usual
as in all my lectures

so what is this correspondence let's look at this table which summarizes
this correspondence so as we as we agreed the proposition that corresponds
to each type not to each value but to each type such as integer floating
point double string and so on so each type responds to a proposition
the tuple type corresponds to the situation that you have computed
to values so let's say a tuple of a and B it means that if you computed
X which has the type to pull up a and B it means you have computed
some a and you also have computed some B you cannot have a tuple if
you don't have both parts of the tuple so the proposition therefore
is that you your code has a and also your code has B so this is a
logical and operation on the propositions also called conjunction
the either type corresponds to the situation that you have computed
maybe a or maybe beep not both You certainly have computed one of
them though you just don't necessarily know which one so in the logic
it means you have computed a so code has a or code has B so this is
a logical or operation on the two propositions CH of a and CH of B
so in other words CH of tuple a B is equal to this CH of either a
B is equal to this what is the significance of a function type suppose
you have in your program Val X : a to me equals something it means
you are able to define a function or compute a value of function type
which is the same thing this function takes a and returns B so if
somebody gives you a value of type a you will be able to produce a
value of type B in other words if your code maybe at some later point
has a value of type a then your code will also be able to get a value
of type B so in the logical language this is an implication again
in logical operation of implication this proposition implies that
proposition so if you have an A then you also can have a B it doesn't
mean though that you have an A so that having the function expression
doesn't mean you have an A you have any values of type a at all but
should you get them at any point you would be able to apply this function
to them and get the B or the values of type B the unit type corresponds
to the true proposition to a position which is identically true proposition
that is always true regardless of anything you have computed or not
otherwise so why is that because it is because you always can have
a value of unit type what we have to say is Val X call an unit or
without a type of notation equals the empty parenthesis the empty
tuple you don't need any other values to be able to compute unit you
can always have it so code has unit always regardless of any other
values that the code has or doesn't have so it means that the proposition
that code has a value of type unit is always true nothing is a special
type defined in Scala that does not have any values so your code can
never have a value X of type nothing that's equal to something and
so the proposition that your code has a value of type nothing is a
false proposition is always false so in this way we can see the correspondence
between types and prepositions and the interpretation of these prepositions
is that your program can compute a value of a certain type I will
be using short notation for these things this this notation is not
always convenient this this is Scala syntax on the left in the middle
this is syntax of formal logic written in English in formal logic
one uses special symbols for and or implies true and false and these
symbols are not always convenient when you work with programs because
actually we're not going to work so much with logic as we were going
to work with types so the short notation can be used at the same time
for types and for logic for types and for prepositions in the short
notation we just represent the tuple type or the logical conjunction
buy this product symbol direct product in mathematics we represent
the disjunction or the or operation by the plus symbol or this logical
disjunction symbol and I will use both in different contexts I will
use this only when talking about logical prepositions I will use this
when talking about types and since we're we're going to be talking
types most of the time we're going to be using this most of the time
and why it is a plus I will explain in this tutorial there is a significance
to choosing the symbol plus and the symbol of the product rather than
logical symbols of conjunction and disjunction the implication I'm
going to also use this area because we already have the arrow that's
good enough instead of true and false I'm going to use 1 and 0 so
unit is 1 and nothing is 0 I'm not going to have a lot of experience
dealing with values of type nothing because I can't be any such values
but formal but sometimes it's convenient to have the notation for
this type so if we have a type parameter in a function it means that
the function is a value defined for any such type and in the logic
notation that is denoted like that for all T in other words for all
prepositions T something is true in the program it means for all types
T we can get that value or that function here's an example this function
in ask Scala C in the Scala syntax takes a value of type a and returns
a tuple having two values of type a now the type of this function
is this expression is for all a for all types a because a is a type
parameter variable we have this function type in the logical language
this corresponds to this logical proposition for any a from a follows
a and a because I'm using this symbol for the logical end now if you
think about this proposition it is valid indeed no matter what a is
if a is given then a is true and a is true now this proposition doesn't
seem to be very deep correspondingly this function is not very interesting
it just takes a value and duplicates it repeats it in the both parts
of the tuple not a very interesting function admittedly it's just
a very simple example that illustrates the correspondence between
types and propositions as another example let's consider a disjunction
type which is in Scala defined using trade since case classes so the
first example I have a simple disjunction type that represents some
kind of users action in some kind of application let's say and there
are three cases that the user can set the name and that that variant
that case has two two parts then the user can set email that is one
part that is a string and here it can set user ID with the one integer
value the short notation for this type is this you see all the names
are stripped there are emitted so user action set name set email first
last and so on all of that is emitted from the short notation only
the bare types are shown and you see the disjunction very clearly
and you see the parts of each part of the disjunction so the disjunction
has three parts corresponding to these three case classes the first
part has two strings so it's a tuple of two strings and the second
part is a single string and the third part is a single long integer
number in this way we can write short notation for the types that
we use making the type structure much clearer at the same time we
lose the information about names and that information is useful of
course while writing the program's because it reminds you what all
these parts mean what is this string as opposed to that string but
when you talk about types and their properties and the properties
of functions the logic of types when you reason about types it is
not helpful to know well this is the first name and that is the last
name this is helpful to know that you have a string and you have another
string that's why we will use the short notation for reasoning about
types at an abstract level and once you have finished that reasoning
you translate the short notation into scholar code putting in all
the names according to the actual significance of these values this
is of course very helpful for programmers to put put in these names
as the second example consider a parameterize to parameterize disjunction
type which I called either three so it's a disjunction of three possibilities
left middle and right and this is a very simple generalization of
the either type so the short notation for it is like this it's just
simple disjunction of a b and c and here i introduced another notation
for type parameters type parameters are written as sub s superscripts
at the type constructor so the type constructor either three has three
type parameters and they are written in the short notation as a superscript
this is a notation i invented if it proves to be useful i will keep
using it so far it's been pretty good however I'm open to changing
it if there is a better notation in this tutorial I will use this
notation so what can we get out of this correspondence so I would
like to show you several very useful things that you will get once
we realize that the correspondence exists between types and prepositions
and alternatively or other also a correspondence exists between proofs
of prepositions and program expressions actual code so there is a
lot of useful value to be extracted from this knowledge and so that's
first example of this value is that we have an in logic various theorems
or valid formulas as they are also called formulas that can be derived
in logic from axioms using the rules of derivation each logic has
certain axioms and certain rules of derivation and then those formulas
that you can derive in the logic are valid that terminology of formal
logic and they are also called theorems of logic if you wish so these
are on the left in this table some examples of theorems valid formulas
of logic so for example this is actually an axiom that for any proposition
a if you have that a is is Val is true than a is true now the second
example is also an axiom of logic that truth follows from anything
in other words you don't need to prove a proposition that is identically
true I'd like to stress that follows from it doesn't it has a very
specific technical meaning in logic it is not does not mean that it's
somehow causally follows or that you know a is a special proposition
that causes this to be true this is not the meaning of what would
have of implication of this a symbol is the implication symbol the
meaning of the symbol is that if we can prove that a is true we can
prove that this is true so the meaning of the implication symbol in
this logic is that if we can prove what's on the left then we can
prove what's on the right so if we can prove a then we can prove a
well that seems to be obviously true if we can prove a whatever that
a is then we can prove the true proposition but that is also immediately
obvious because the true proposition is identically true it does not
need to be proved it's already true and so it it it doesn't matter
what propositions we already proved here we can ignore all those proofs
and just have the true proposition now here's another example oh yeah
I wanted to show the code for all this so what is the identity all
this is the identity function this is an standard library in Scala
identity function defined let's say like this takes an argument X
of type a and returns a value of type a of course it just returns
X so the correspondence between proofs and code I'd like to illustrate
in this table is that if you have a proof of a it means you have a
proof that your code has the value of type a remember the interpretation
of our logical propositions is that code has a value of type T so
all of these in the short notation a means CH of a so code has a so
what is the proof that code has a label it's the value X the value
X itself the value of x is the proof that we have computed a value
of type a so if you have a value X it means of type of type a it means
you have proved the proposition a for your code and like I said all
these propositions are specific to each program so in each program
some of them can be true and false in other programs others will be
true or false so let's keep that in mind so in a specific program
if you have managed to compute a value of type a then in logic it
means you have proved that proposition is true see HIV is true so
proofs are expressions X was probably computed by some other part
of your code through some long expression let's say so that's the
proof that corresponds to the proof in the logical in on on the logical
side of this correspondence and now if you have a proof of a then
you're supposed to produce the proof away if you want to prove this
proposition well that's obvious you just reproduce the same proof
you were just given a proof of a a second ago so you just give that
proof back so you see there is a direct correspondence between proofs
and code let us see on further examples how this works here's a proposition
that if you have a proof obey then you can prove prove the true proposition
the code is that you take this X which is a proof of a you ignore
it and return the unit value so you ignore this X which is the only
way that you can do this you cannot use this X in order to prove that
you have unit cause you already have unit unit is this there's no
no more information and unity don't need X to get unit and you cannot
actually use X to get unit now because the only thing you have here
is X so there's no way for you to make unit value using X you you
have to ignore it unit value there's only one of them so no information
from X can pass into the unit value so in this way this code corresponds
to the proof of this proposition that whatever you have previously
proved you can prove truth by ignoring the proofs that you had before
because truth is already true so it's a proposition that identically
true let's just look at the next example a slightly more complicated
example for all a and for all B if a is given then A or B can be proved
so if a proof of a is given you can prove a or B how do you do that
while you produce a you reproduce the proof of Ali and that's good
enough you will never be able to prove B because you don't have any
proofs about B you only have a proof of a and so you always going
to return the left side of this disjunction but that's good enough
you have proved a disjunction what does the code do it takes X which
is of type A and returns left of X which is value of type either a
B it's one of the case classes from either so left of X is of type
either so you you see this is exactly equivalent to the way we prove
this proposition we don't try to produce any values of type B because
we can't we cannot ever produce right of B in this function because
we will never get any B but we can produce left of X which is in a
in the left side of the disjunction and that's good enough the next
example is this if you have a or a proof of a and remember this is
this is logical end if you have a proof of a and if you have a proof
of B then you have a proof of a how you can prove it how do you do
that well you already had a proof of a so you ignore this proof of
B you can't use it in any case you reproduce the proof of way that
you have been given what does the function do it takes a function
takes a tuple of type a B it takes the part of the tuple ignoring
the second part the first part has type a there is nothing else you
can do here you cannot produce a in any other way because a is an
unknown type so the only way you can get an A is to take the first
part of the tuple next example is more complicated from a follows
from B follows a now I have said in my previous tutorial that we have
a syntax that the implication symbol associates to the right and so
these parenthesis are unnecessary I just wrote them here for clarity
but they are unnecessary in our syntax and later I will stop doing
this so how can we prove this theorem this is still a valid theorem
all these examples are valid theorems how do we prove this theorem
so we have a proof of any if we have a proof of any then we're supposed
to return this what is this this is a something that will give you
a proof of a if you give it a proof of B if you give it a proof of
B so this thing this thing is something that can produce a proof of
a if somebody gives it a proof of B how will it produce the proof
of it it will take this one it will ignore this proof of B and it
will just reproduce this proof that was given previously that is how
it's going to be done accordingly the code does precisely the same
thing it takes an X which is of type A and it returns a value of type
B to a by taking an arbitrary Y of type B and returning this X so
we are ignoring this wine and we return the X that was given early
one so you see this is exactly one to one the proof of this proposition
is exactly the code of this function the proof of this proposition
is the code of this function in soon so we have this very hard correspondence
which is between types and prepositions and between proofs sorry between
code of functions code that has a certain type and the proof of that
proposition importantly invalid formulas cannot be implemented in
code so valid formulas can be implemented invalid formulas can not
here are some examples of invalid formulas they are invalid in the
sense that they are not theorems they cannot be proved and they're
false in some in this sense they're they're not not valid as statements
as propositions here's an example for any a from one follows a this
cannot be proved because you're supposed to produce a proof of a out
of a proof of the identically true statement well identically true
statement doesn't need any proof so if you say that you had this you
say nothing everybody has this nobody needs any work to prove this
so you're basically trying to produce a proof away from nothing and
it could be a false statement so you couldn't possibly have this you
couldn't possibly produce a proof of any statement out of essentially
no information another example of a non valid one theorem in formula
is that not just I just want to be to be sure that you understand
invalid not in the sense of syntax the syntax here is correct it is
not a theorem it is a false statement so for all a for all B if you
have a or B this is the or symbol logical disjunction if you have
a or B then a follows well that is not true because if you have a
or B it doesn't mean you have a you might have B in no a at all and
then you would not be able to produce a proof of it because you only
have a proof of B and a and B are completely different so a could
be false and B could be true it could not possibly produce a proof
of a false statement here another example of invalid formula is is
this so from a follows a and B so how are going to get B you're supposed
to produce a and B so a you have but B you don't have again the same
problem you're supposed to produce something you don't have this is
when logical proposition the logical statements are invalid they cannot
produce a proof of some some formula without the required information
about that formula you cannot just produce it out of nothing for any
be very important that is this is supposed to be without working for
any a and B it's not possible to do that another example slightly
more in complicated of why things cannot work is this for any a and
for any B if you have this then you're supposed to produce a now what
does it mean you have this this is a function or input in the logical
realm right we're in the logical domain so this is implication so
all you know is that if someone gives you a proof of a you can produce
a proof of B you cannot get a proof of a out of that knowledge so
that knowledge is actually the knowledge of how to take a proof of
a and make a proof of B out of it it is not knowledge about how to
prove a and so you cannot possibly derive a from that knowledge derive
the proof of ad from that knowledge you cannot now I am telling you
why they are false in order to prove that they are false you need
to study formal logic and this is a listen this is something that
can take a long time and necessarily bring a lot of illumination here
you would have to prove essentially that no combination of axioms
and derivation rules will produce this formula and this is a no this
is not a very obvious proof but it's quite kind of obvious why you
cannot prove this to me this is obvious because in order to prove
a you need information about how to prove a and all you have is information
about how to make a proof of B out of an already existing proof of
it so cannot get that so as I said valid formulas can be implemented
as well invalid formulas cannot so suppose I have a formula and I
want I want to decide whether it can be implemented or not so actually
as a since I'm interested in applications in functional programming
I'm actually only interested in the question of writing code so my
question therefore is given some logical formula or a type can i implement
it in code or not and if I can how so this is the central question
that I will be dealing with in this tutorial among other questions
here's an example now these formulas so far seem to be kind of trivial
this is might be a little less trivial now here's a here's an example
I have no idea how to implement this or if it's even possible very
complicated type if you implement if you interpret this as a type
so these are two type parameters a and B and then they're this higher-order
function of order like 5 so it's completely unclear at first sight
whether this formula this type expression corresponds to a function
that can be implemented let alone why you would like to use that function
what is there what is the usefulness of this function but it turns
out these functions can be used for certain cases but right now let's
concentrate on the question of how we get this code so suppose we
we know we need this type how do we get this code turns out mathematicians
have studied this question for a long time mathematicians were only
studying the logical domain of course not the program in domain since
about 1930 or so many mathematicians have studied this including church
tarski girdle lots of people in Poland and Germany in England and
in the United States it took a very long time about 50 years between
1913 and 1980 between the beginning of this activity when this was
first formulated as logic with these particular rules and the time
when the Curie Howard respondents was realized that or was it wasn't
was discovered at that time it became clear that these things have
a direct bearing on functional programming actually helping people
to write code because if you know how to prove things here you just
directly write the code and the first important thing that I need
to say here is that there is an algorithm for deciding this question
so in other words this is called constructive propositional logic
it has a decision algorithm an algorithm that takes any such expression
like this however complicated it can have tuples it can have these
junctions it can have implications in ested in any way whatsoever
it can have the unit or nothing or whatever lists it in whatever way
and there's an algorithm that takes this expression assuming that
all the types variables here a B and so on are universally quantified
so that is always the case so let's say for that case there's an algorithm
that decides whether this can be well approved whether there is a
this is a valid formula or can be proved and at the same time if it
can be proved what is the code that implements this function this
algorithm is constructive it is not just proving that this can be
derived it actually gives you the code that implements this function
so this algorithm doesn't have a name per se but it has been developed
by a number of people and there are many such alternative many alternatives
for this algorithm I will give you some links later but for now it's
important to say that this question is decidable so there's an algorithm
that answers this question whether this can be implemented and if
so how which is what is the code and I have started implementing this
algorithm in the library called Korea Harvard so I have a link here
to this library I'm going to I'm not going to look at this right now
okay we're going to look at this a little later after we have seen
more examples so one other thing that follows from the very hard correspondence
is the curious resemblance with arithmetic not with not only with
logic but also with the rhythmic check so notice that I have chosen
the symbols + + product this was intentional the logical symbols which
are the disjunction and the conjunction do not have the same properties
as the arithmetic operations of addition and multiplication they do
have properties that look similar so here are some standard identities
in logic for example a + 1 is a a and B is equal to B and a and so
there are some identities like this associativity of conjunction associativity
of disjunction distributivity of conjunction distributivity of disjunction
and then some other properties that have to do with implication these
are all theorems in logic these are these this you can prove and the
equal sign here can be read as double implication so from X follows
Y and from white follows X also so both from X follows Y and from
Y follows X that is what the equal sign means in logic now if you
mentally replace here the disjunction symbol symbol with a plus what
happens some of these identities remain true so in other words you
move from logic to arithmetic and then you ask what are still the
true identities well most of them are actually still true except this
one for example a plus one equal one that is certainly not true in
arithmetic also the distribution of disjunction is not true in arithmetic
so this would mean a plus B times C equals n plus B times a plus C
and that's obviously false in arithmetic in arithmetic only the distribution
of conjunction or or or distribution of a product over the sum that
works but not distribution of sum over product but in logic the conjunction
and the disjunction are perfectly symmetric if one is true the other
is also true so whatever statement is a theorem for disjunction the
same statement is a theorem for conjunction and vice-versa so logic
therefore has slightly different properties than arithmetic so you
cannot just blindly use plus instead of disjunction it will be misleading
if I would right in plus one equals one that's confusing or obviously
an arithmetic this is not true so then the natural question is what
is the actual correspondence here is a de correspondence with logic
or with arithmetic and this is the question we will explore next it
turns out that both correspondence with logic and with arithmetic
are useful so these are kind of two sides of the coracoid correspondence
the arithmetic correspondence is useful for certain things and logic
correspondence for other things so let me maybe comment a little bit
on these identities so what are these identities if we take say this
one the first identity each identity as I said is just notation for
two implications one going from left to right one going from what
right to left now these implications correspond to a code that has
a function going from here to here and a function going from here
to here so what do these two functions do they convert values between
the types so there are two functions going in the opposite directions
between two types what do they do so the interesting question is can
we somehow convert one type into the other and back without any loss
of information if so then the types would be equivalent we can encode
the same information in both types so then a natural question is do
these in the identities mean equivalence of types so equivalence of
types in the mathematical language is called an isomorphism and the
formal definition is that you need to have two functions one going
from A to B and one going from B to a such that the composition of
these functions in both directions is equal to the identity function
and if this is so you can find such two functions then the types a
and B are isomorphic or equivalent which is the same thing and the
interpretation of this is that the values of these types can be encoded
in the other types so there is a one-to-one correspondence between
the sets of values of these types so whatever information you have
that is in the value of this type can be included one-to-one with
no loss of information by this type and then vice versa and so the
functions F and G provide the recording the the different packaging
of the same information from a into B from B back into a and so when
you do the composition of these functions and the information is repackaged
and then repackaged back and it's the same information so the value
must actually remain the same after you do this round trip and let's
take the first example this is the type let's ask is this an isomorphism
of types or are these two types equivalent so I will use the symbol
the triple equals triple bar or a triple line to signify that the
types are equivalent and then not necessarily equal they're equivalent
so there isn't isomorphism in scala these types corresponds to a tuple
of a and unit or end to a type a respectively so if we want to demonstrate
that these are equivalent we need to build two functions let's call
them F 1 and F 2 in the code the functions have these types so the
first function has a type from a tuple of any unit to a second function
is type from a to the tuple of a unit notice a double parenthesis
so there's a parentheses around the tuple and extra parentheses here
around the arguments of function so in Scala a syntax is such that
if you have a tuple as an argument than you need double parenthesis
in the function type here actually the double parentheses are not
necessary because this is not an argument of the function this is
the final result type of the function I wrote these parentheses here
nevertheless but they are really not necessary I should delete them
from the slides so coming back to our question of isomorphism we need
to implement these two functions and check that their composition
in both directions is identity function here is the Scala code well
the Scala code is very simple you take a tuple of a and unit you want
to produce a well you just take a pattern match on the tuple you get
the a as a pattern variable out of it and you return the a the second
function is also very simple you take an A and you produce a tuple
first part of the tuple is a the second part of the tuple is the unit
so does the composition equal identity well it should because we take
this a we put it into the a type and then we take that a and put it
back into the first part of the tuple where it came from initially
so taking the tuple going to a going back to the tuple gives you the
again again the same tuple and vice versa getting an a into a tuple
and then stripping away the second part of the tuple again gives you
the same way so it's kind of obvious that both directions of the composition
give identity functions let us look at test code that implements this
so these are the two functions I'm I have deleted the code so that
I can write it here again so how would I write this code all I take
I look at the argument of the function it's a tuple so a natural way
of writing a function whose argument is a tuple is to do a pattern
match so I do curly braces because pattern match is a case expression
that it requires curly braces in the Scala syntax and then I say there's
a a and B as a pattern because it's the tuple is two parts so I I
have to let's see the type of a is capital a the type of D is unit
so then I can return so what do I need to return the value must be
of type a oh I have only one I have only one of thing of type a so
that's it the second one is even easier so I get a which is the function
sorry the argument of the function and I'm supposed to return a function
of this type so I return an expression I don't need parentheses because
it's so simple I don't need a case expression here so I don't need
curly braces either so I just take argument a and return a tuple of
pay and the unit value and the unit value is empty to call just this
this is the unit value I'm done so let me run this test but before
I run this test or rather while I run this test I want to show you
this way the way of verifying that something is a identity function
let's say what is how do we verify that we have to do this in some
clever way the way I chose to do it is that I use a library called
Scala check which allows you to check properties in a randomized way
so I can check that a composition of two functions here's what I do
I say for all integer n it must be that the composition of F 2 of
F 2 F 1 of F 2 of M should equal n so this should equal the special
syntax of the library and for always a function defined in the library
of course it's not going to go over all integers here it's going to
take some randomly selected integers but that's pretty much as good
as it gets in terms of checking such functions and the second is test
is that I say for all X of type string unit I have extra parentheses
here which are not necessary so for all X of type string unit f 2
of F 1 of X should equal X so this is the opposite direction of the
composition of the tool isomorphisms so if both of these are correct
then indeed F 1 and F 2 are the two isomorphisms that we require to
prove that this is so so this is how we write test code to check the
properties and prove that was four qualities hold note I'd note that
I put specific types here integer and string so this is true for all
types I just checked some specific types that's good enough these
functions don't do anything with values they just repackage them in
some way and so whatever that type is is going to not not going to
matter I just make sure that works with different types I'll choose
some random randomly some types specific type is integer in string
the second example is I'd like to see if this is a type isomorphism
remember we have this formula in the logic disjunction of a + 1 is
equal to 1 so 8 or true is true is this and type isomorphism if we
translate that from logic into types so that will become a plus 1
equals 1 right so the logical formula is valid but is the type formula
giving us a type isomorphism or not the fact is that it does not so
these two types in Scala are the option type which is disjunction
of a and unit end unit type obviously these types are not equivalent
the information in an option type may be a value of type a but the
unit type cannot possibly represent any values of type a or or anything
non-trivial a unit type only has one value which is the empty tuple
so clearly these types cannot be equivalent how can we see that in
code and they are not equivalent and what is the significance of the
fact that the logic formula is a valid theorem here is code that we
can try to use to falsify this exam so I will write this code live
just again to show you how these things look in actual code so we're
supposed to make a function that takes an argument of type either
of am unit so we should start curly braces perhaps and have something
lights or right now we don't need the case expressions or services
either any unit is in a you going to something so that's our function
will take this as an argument and we'll return unit that's good enough
friend we just return unit you're done well actually this is never
used so in Scala arguments that are not used can be replaced with
the underscore symbol which makes the code kind of a bit cryptic looking
but this is a very frequent usage and also you clearly say that you're
not going to use this argument let's take the second function so we
get the unit yeah well actually let me just go back there's nothing
else I could have written here there's no way that I can use this
value somehow to produce this empty tuple is empty tuple it's empty
there's no way I can put something in it so I could have a very complicated
value here doesn't help the only thing I have I can do is to return
the empty tuple here I am given an empty tuple so let's just this
you and I'm supposed to produce a value of type either of a and unit
so how can I do this well there are two possibilities in either a
left and there right well I can produce some left away or I can producer
right of unit now what would be a left of a I need some a to get the
left array so I could say left some a but what is this a actually
I don't have any values of type a so I cannot possibly produce a left
part of the disjunction I must produce the right part and the right
part has the unit value and there is only one unit value I can take
anyway so I can put this u in here if I feel like I'm very fancy now
this U is actually unit so this actually could be replaced with this
there's only one value of type unit I don't need to take it from the
argument that's going to be the same anyway so this is exactly the
same all right so now I have implemented these functions and there's
only one way of doing this so the thing is this test is going to the
identity test is going to fail and I write this as a test but that
test and that states specifies that there exists some value of type
V which is sorry some value V of type either of integer unit such
that F 2 of F 1 of V is not equal to e so if this however F 1 of F
2 of you should equal Newton so in one direction the composition works
and there's only one value to check anyway but in the other direction
the composition fails to work so there are some non-trivial values
of integer let's say in the left and this integer is going to be transformed
into unit and then back it's going to be transformed into the right
and that's not the same as what it was before sorry but we lost information
here we have left and then we lost it and then we cannot recover so
this test is how its work how it works how its shown so we find that
some logic identities give isomorphisms and some don't so what does
it mean well logic identity it just means that there exist these two
functions F 1 and F 2 they can be implemented that's all it says it
doesn't say that right because this is what the logical proposition
means by it's a very foundation of the Kurihara correspondence logical
proposition means we can compute a value of this type in the code
no more and no less we can compute some value whether this value is
useful or not is not clear but we can compute a valid that's what
this this logical theorem says yes we can write two functions that
map option a to unit and unit to option a or either actually more
more precisely this should be an either of a and unit option a is
equivalent to either of a lien unit so yes we can compute we can write
these functions we can implement functions of these types but the
composition of this function is not identity function these functions
these functions lose information or at least one of them loses information
and so for this reason the types are not equivalent so heuristically
types are not equivalent when they lose information sorry when when
the when the function that you write to map the types into each other
lose information that is the intuition that we are gaining so far
so every time we implemented a function that does not lose information
like this one we got a we put it back we got a we put it back so no
information is lost in the example here we got something we ignore
it we got something we ignore it in this case it's okay to ignore
unit because unit only has one value in this case out okay in this
case we are ignoring possibly a value of type a so that function loses
information so I put this in roads because this is just intuition
this is not something we compute the amount of information in the
function this is not something we compute but this is the intuition
I'm building about the functions here are some more examples of verifying
type equivalence the way to verify type equivalence is to try implementing
the functions that go from one type to the other and back so let me
go through these examples example 3 we have a function that takes
a tuple of nested shape so there's a tuple of a B and that tuple is
inside a tuple of that and C so this is how we write this in Scala
and we need an extra pair of parentheses because this is the argument
of a function so this is the function type the result is a tuple of
this shape we don't need extra parentheses because this is not an
argument of a function this is the result how do we implement this
function since the argument is a tuple it is natural to do a case
match so we write pattern variables let's say X Y Z like this you
can write any names let me just check the types ctrl shift B type
a Type B type C I'm supposed to return to pull of this shape well
this is quite obvious I'm done similarly here let me just pop you
this code over to make things quicker I'm supposed to yeah no listen
notice it tells me that the type is wrong well of course it's wrong
I haven't finished writing the code now I'm finished how do I test
that these functions are correct I write the for all then for all
what for all values Q of this type so I choose specific types for
a B and C I must choose specific types there is no way to check with
values of type a there is no such thing we need to use a specific
type when checking and when using this function on actual data of
course then I say this should equal this so for any Q F 2 of F 1 of
Q should equal Q so which means F 2 composition with F 1 or F 1 composition
was F 2 is identity function and similarly for the other direction
if 1lf to go to the tuples in the opposite order notice again I have
to put extra parentheses around the argument of the function this
is the Scala syntax if I put types so four types I need extra parenthesis
for for simple functions like this I don't need extra parenthesis
because I don't have to say types title are specified on the left
and I don't have to specify the money right but here when I do for
all the types are free not fit not specified anywhere and then I have
to specify the type for the function argument in Scala and then I
have to put the extra parentheses around it just just an aside about
syntax example for this is a {[}Music{]} theorem of logic if you interpret
plus as disjunction and times as conjunction it's also a theme theorem
of arithmetic just like this one by the way was also a theorem of
both logic and arithmetic interpreting our notation that way so how
do we check the des is true the two functions must have this type
the first function is a tuple argument is a tuple of two parts one
part is either of a B and the other part is see the result of this
function is an either of left part of the disjunction being a tuple
of AC the right part of the disjunction being a tuple of BC and the
function of two has the two types in the opposite order note extra
pair of parentheses around the to pole on the left side so how do
we check that this is true we need to write this code so how do you
write this code well since the argument the argument of this function
is a tuple we need to do a case match so let's say this is either
a B and C what is the type of this yeah either a B type of this this
C I'm always checking I always check these types when I do a case
match because if you make a mistake it might still compile and case
matches are less strict strictly checked in some cases not in all
cases but in some cases just to be sure I'd use IntelliJ to check
all the types in the case match now either a B is an either so I need
to figure out in which part of the disjunction I actually have a value
so this could be either A or B so I need to match on a B so I say
a B match and I open braces because it's going to be again a case
expression so knee braces case clauses expected yes of course I also
expect them now there's the symbol which are used to generate the
closest that's convenient especially if you have many clauses now
the name of this thing is inconvenient I'd like to make a billion
V because that's much more suggested so there are two cases in either
it could be a left away or a rightly if we are in a now in the we
do we need to return in either of AC or BC so what do we have here
we have a C and we have an a well clearly we can return an AC and
that will be in the left part of the disjunction so I returned left
of a tuple a see if I'm in the right here then I have a B and also
and have a see I don't have an a I'm C I remind you that the case
expression is such that these pattern variables are defined only within
this scope so if I'm here I don't have a anymore so the language prevents
me from making this mistake using a when I'm in the right part of
the disjunction okay so here I returned a b c obviously i can return
the bc because i have a B and I have a see now this is red because
actually it wants to have either of HC NB C and so this we see must
be in the right part of the disjunction I need an extra parenthesis
to cut it so tuple reform and as I'm done this function let me do
the same thing again now this is an either so I'm going to say either
AC or DC goes to what goes to something I need to mention it right
away because there's nothing else I can do I don't know if I'm here
or here in the left or in the right part of the disjunction um if
the value I'm given is the tuple AC in the left or there's a tuple
BC in the right part of the disjunction so let me do that BC BC match
and then I have the same case causes I generate and then let me rename
these variables as AC and this has BC because this will have type
AC and this will happen fantasy now apart from the red because well
the red though I haven't finished writing there is a yellow what is
this yellow we'll see it when I get rid of the red right now apparently
the red is more important for Italy I always look at what's yellow
it's often very helpful almost always okay I'm in the AC so there
is a further possibility no there is not AC is a tuple so it does
it not a disjunction is a conjunction or a product type so I have
a and I have see what am i required to produce a tuple of either a
B or an NC well I have a see obviously I can produce that in the second
part of the tuple the first part of the tuple needs to be an either
of a B so I have an A sorry I should have said first I have this a
and C of type 2 PO AC let me decompose the tuple like this so this
syntax says I'm introducing new variables a and C and decompose a
tuple into them or I could have just put these variables right here
saving me a line of code all right now what is the first thing it's
an either of a B well I have an a so that's going to be in the left
part of the either and I do the similar thing here I have a right
B C and I put a B C - right into the pattern now notice the pattern
can be nested it doesn't have to be so simple it can be right and
then further destructuring or specification of structure of the pattern
can believe can be given so that makes code easier to read and easier
to write okay let's look at what's yellow now the first thing that's
yellow convert match statements to pattern matching anonymous function
okay there's this symbol here which I click convert code the code
became much shorter so actually this syntax is already a function
that matches its argument with these cases and the argument is of
this type so I don't have to say X arrow X match that's just not necessary
to write at all a very common pattern and it just makes code shorter
what is this yellow on a actually suspicious shadowing now we know
what that means it means somebody already defined a variable a outside
of this code and now we introduce a pattern variable also called a
in the case expression and that shadows the a that somebody already
defined outside of this code no I didn't define any A's that are visible
all my eyes are these entry internal variables are not visible outside
who define this a well actually it's the test library it will define
the key so I'm not going to be able to get rid of it since I'm using
the test library in order to perform the testing and run the code
I'm not going to be able to get rid of this year but in or code I
won't be inside the test and so this a would not be yellow in ordinary
Co alright so I'm done so this actually works let me run this test
so I implemented the two functions and then I check that the two directions
of their composition take arbitrary values of this type and return
the same values and take arbitrary values of that type and return
those values so that's the way that this is going to be tested there's
its mass the next example is this one a slightly more complicated
thing a disjunction as an argument so the argument of the function
is disjunction the same as a tuple of two functions but look at how
that works so here I already wrote all the code but the way to write
this code is exactly the same as I was showing you previously you
just go step by step and figure out each part of the disjunction and
so on so let me go through this code more quickly the left-hand side
is this so it's an either a B as an argument going to C so my function
has a type argument being this either a B going to C that's the type
of the argument of the function and the result of the function is
this from A to C from B to C that's a two-port two functions so what
do I do well I take a P which has type either a B 2 C and I return
a tuple now tuple consists of two functions from A to C and from B
to C now from a it takes an A and then it applies so what can we do
with an A well we can put this into here pretending that we had an
either with a on the left and we put that as an argument into P and
we get a C out so that's what this code says P of left of a and B
goes to P of right only the function going back takes this as an argument
a tuple of two functions and it needs to return this as a function
now I remind you that parentheses around this function on the right
are not necessary because the arrow the function arrow is associative
to the right so these parentheses are implicitly here I don't have
to put them they don't change the meaning of the code so I can say
that I have now a match on the tuple which is here then in this tuple
if I take that the value of a Siemens function from A to C and then
I also take the value BC which is a function from B to C so I decompose
a tuple into two parts and then I return this expression which is
as I just showed it's equivalent to a function of the stored X going
to X match so this this syntax is the same just shorter now what is
this X well as X is obviously as either a B here and so we match on
this either a B and so let's just let me call this a and B to be more
visual so this matches on the value and if it's in the left then we
get an A and we put we can apply this function to a and get a C out
if it's in the B we can apply this function to be and get again a
C out so we get a C out in either case now the syntax becomes shorter
but somewhat more cryptic so that's why I wanted to put this in for
illustration purposes but in most cases this won't be necessary to
write and the test shows that this works the test is written like
you know to for all nested to four roles so the first for all P and
we'll get a function and then for all arguments of that function we
verify that this works the thing is these these functions return functions
we cannot directly verify that functions are equal we have to put
so if I want to verify that function a is function it function f is
equal to function G I'd have to put in all kinds of arguments into
F the same argument into G and check that the results are the same
this this is the only way to check that two functions are equal clearly
I'm not going to put in all possible arguments it's just impossible
so I'm going to just test with some randomly chosen set of arguments
and hope that's good enough and that will catch bugs if there are
some bugs so that is how this works example six is to show that this
is not an equivalence now this is actually a valid theorem in the
logic this is this theorem distribution of disjunction but it is not
true in arithmetic obviously if you put a plus here a plus B C is
not equal to a plus B times a plus C so rules do exactly the same
thing so this is an either of a and A to point B C and this is an
either of a B to pulled with either of AC now if we look at the code
see how this is implemented then what happens is that if you are let's
look at f/2 if you are in the left here but in the right here so this
this is a tuple of to either values so they can be independently chosen
this is on the left and this is on the right what's safe so if you're
in this situation how can you return an either of a and a tuple BC
you couldn't have a B you don't have a B its Europe here you're on
the left so you don't have a B and here you have a seat so you have
only an A and the C if you have only an A and the C there's no way
for you to return this tuple you don't have a B so the only thing
you can do is you can return this a in the left part of the either
and so the code returns the left a and here also returns left a and
there's only one case when it returns a right of BC it's when you
have B here and C here so clearly this code loses information so if
you are given a here on the left and see here on the right you're
ignoring the C you're not returning it returning a left of a and so
a function f2 loses information in this sense and sure enough in the
direction of F 1 F 2 we have a violation of the identity requirement
but in the other direction it works so this is not an isomorphism
between the two types but we can implement both functions F 1 and
F 2 so just as in there in the other case what we see is that the
logical theorem guarantees that we can implement the two functions
to implement from left to right and from right to left whereas these
functions do not actually satisfy the isomorphism wireman but logic
cannot guarantee that the arithmetic identity obviously does not hold
and we we see that this function of two loses information and so it
is not an equivalence of the two types and the tests here are slightly
more clever so I have a function check that takes type parameters
and it runs this with type parameters then I can put any kind of types
I want just for fun to check that this works and the arbitrary say
here I'm not going to talk about this much this is something you have
to do with this testing library but Scala check is a very powerful
library that allows you to check to verify equations properties requirements
and laws of this kind so we have seen the curious thing that actually
when the arithmetic law holds also the types are equivalent and the
arithmetic law does not hold then we lose information and types are
not equivalent even though the logical theorem holds so how can we
understand this what is the relationship between the logic and the
arithmetic side of the curry Harvard correspondence to understand
this consider the types that have finite sets of possible values for
example boolean type has only two possible values true and false now
in the computer most obviously integers have a finite set of values
very large set with a finite set our floating point numbers also have
a finite set of values so pretty much everything in the computer follow
falls into this class of types that have a finite set of possible
values it is convenient sometimes to think that integers are arbitrary
arbitrarily large or strings are arbitrarily long or arrays or arbitrarily
long but actually computers have finite memory and so you even it
it could be very large but it's still finite theoretically so let's
consider therefore without loss of generality only the types that
have finite sets of possible values and let's compute how many values
we have in the some type or in the disjunction and that's clearly
going to be the sum of the numbers of two of the two types so the
size of the type or cardinality of the set to use the terminology
from set theory the cardinality of the set is the same as the size
of the set how many elements are in the set so the size of the set
of the disjunction type a plus B is equal to the sum of the sizes
of the teller sets for a and for me that is clear because we have
a disjunction on the Left we have this many possibilities on the right
we have this many possibilities and there's no possibility of having
both so there's no intersection here is just joint Union or disjunction
the product type or the tuple obviously we can have any value of a
paired with any value of B and so there's on a times B possible values
in this type the function type provides the set of all maps between
the two sets and B and so it is the B to the power of a because for
each a we can choose any be that crisp to it and so this is B multiplied
by itself eight times and obviously then if two types are equivalent
then they must have the same number of values the same size of the
sets and if the set size is different the types cannot be equivalent
because you cannot repackage all the possible values in in the other
type without losing information and then come back and get the same
value back so because of this whenever they add the identity that
we had is a valid arithmetic identity that has a chance of being an
equivalence of types and whenever it is not a valid arithmetic identity
like this one for example is not a valid arithmetic identity there
is no ways in this case then that the types are equivalent this is
a valid arithmetic identity so the type and the types are equivalent
so I'm not trying to prove here that any arithmetic identity will
automatically give an equivalence of types but this is highly suggestive
pretty much any reasonable arithmetic identity of this sort like a
plus B equals B plus a and so on 8 plus 0 equals a any kind of reasonable
arithmetic identity will give an equivalence of types and certainly
if the arithmetic identity does not hold there is no way that the
types could be equivalent because the cardinality of their value sets
is different also note the curious identities that I listed here these
are identities related to powers these are arithmetic identities each
one of them gives rise to a type equivalents if you translate it according
to this formula so 8 \textasciicircum{} C corresponds to the function
from C to a and this is product so that responds to a tuple and so
here for example we have this identity which we verified in code but
this is C to the power a plus B equals C to the power a times C to
the power B obviously an arithmetic identity this one so not only
identities that have to do with multiplication and addition but also
powers exponentiation another case in other words as a rhythmic identity
gives rise to equivalents of types logic identities do not always
give a syrup give rise to equivalence of types what they give is that
you can have two functions from one type to the other and from the
other to the first now these functions guarantee that if you get a
value of one type you also can get a value of another type and vice
versa in other words if you can implement one type or compute a value
of one type and you can compute the value of another type but it tells
nothing about what values and how many different values you can compute
so logic identities give the equal implementable 'ti of two types
so if one is implementable the other is implementable we're here in
implementable is the same as you can write code to compute it or you
can write code to define that function because defining a function
is the same as computing a function value it's just different words
for the same thing since in in the functional programming functions
are values so defining a function means you compute a value of function
type so to summarize arithmetic and logic formulas have different
significance and arithmetic formulas are related related to type of
lavon's and logic formulas are required are related to being able
to implement types being able to compute values of this type so in
being able to implement at all is usually interesting for functions
but being able to say that one type is equivalent to another that
is usually interesting for data types for because because function
types are never compared much but nevertheless you can still treat
functions as values and all types are just types and you can use the
same reasoning about types but both functions and about data at the
level of types and so these are the two ways that the Kurihara correspondence
gives us information about types it arithmetic formulas with types
tell us which types are equivalent and that is important if I'm if
I'm trying to write my program and I need to know what types to use
for data and if one type is equivalent to another I could use one
or I could use another according to convenience I know I can always
repackage one to the other or back without any loss of information
and so I will understand how to choose those types more conveniently
I have more choices if I know which types are equivalent for this
I use arithmetic reasoning so I translate types into arithmetic formulas
and the reason with them pretty much like a reason about high high
school level algebra with polynomials and powers I have identities
like this I have basically all identities of high school algebra completely
translated into well I just showed them these are some examples and
all others {[}Music{]} all other identity is out of here for example
these are all valid as well as arithmetic identities all these identities
tell me how to design my types so I'm and this is what I mean when
I say reasoning about types this reasoning is specific answering of
questions of what types to use in my program and in order to find
out I can write down simple polynomial or power laws and simplify
types let's say like I simplify expressions in algebra so there are
different kinds of expressions such as exponential polynomial and
so on in algebra and the class of expressions that we have encountered
so far our exponential polynomial expressions that is expressions
made up of constants like 1 was a constant some products and Exponential's
corresponding to this in the functional programming we have what I
call X 2 X poly types or exponential polynomial types these are the
primitive types like integers string and so on these correspond to
various constants then there are type variables as well so and under
disjunctions tuples so these junctions responds to sums to post corresponds
to products so these are like either option case classes with Co trade
to post respond to products and functions correspond to exponential
so function types response to respond to Exponential's and so on now
in functional programming community currently terminology is that
algebraic types are what I here explains to be polynomial types so
types that have primitive types disjunctions and tuples and usually
not functions so usually they're not called algebraic types now the
word algebra is used in so many different meanings and senses that
I'd like to keep it very clear what exactly I'm talking about and
so I don't want to say algebraic types I want to say more specifically
polynomial types or exponential polynomial types and if there are
some other types I'll have different word for them so until now we
have not seen any other types except exponential polynomial types
and in fact these are the only widely used kinds of types in functional
programming and here is an example of reasoning with types that I
was talking about or algebraic reasoning that I described here's two
specific examples that I'm going to give right now the first is to
define a list of integers so the type that represents a list of integers
so we'll define this from first principles in in previous tutorials
I have used the standard library of Scala with sequences and maps
and all that now those are defined in the standard library not going
to write another standard library here but it's important to see how
types are defined and how recursive types like lists or arrays or
sequences can be defined using polynomial types of you just add recursion
to the types and you find you can do this so consider this definition
so there's an integer list in twist I'm defining a new type using
the syntax with a sealed trait and case class so there are two cases
one is empty which is a case object so it's an empty tuple and the
next one is non empty which has two parts one is integer and the other
is int list so it's referring to itself in the definition of the type
so in this sense it is recursive it's a recursive type this is allowed
so that you can do this install the short notation for this type looks
like this int list is defined as or is equivalent to one so this is
the one plus the product of integer and int list itself so this definition
is a recursive polynomial type as I would call it it's recursive in
a sense that this type refers to itself in its own definition now
we see the short notation is much clearer and very suggestive of various
algebraic manipulations let us add a type parameter so not just always
using integers in the elements of the list but let's do type a any
type a so the short notation for that would be like this okay when
you find exactly the same thing actually there the different ways
of defining this one is like this so we introduce a type parameter
and the case object extends this type parameter with nothing so this
is an example where we can use the type nothing we don't actually
have any values I've typed nothing and and because of because this
is an empty tuple basic this isn't named empty tuple we could have
actually said final case class nil and then empty parentheses and
that will be even more clear even clear that we're just putting a
name on to an empty tuple and that extends list with type equal to
nothing and then we define this case class with this strange name
double colon well this is kind of traditional in functional programming
to use this name this is also used in this scholar standard library
but the double colon is just this name you could you could call this
anything because you know Z Z Z if you want so it has again exactly
the same structure as this list except that it's using the type parameter
now for the value of instead of int and it refers to list of a recursively
extending Westham a short notation for this thing is this now we know
the laws for the types and the equivalence corresponds to algebraic
manipulations like an arithmetic so let's perform that kind of manipulation
so list away is 1 plus 8 times list of X let's say 1 plus 8 times
open parenthesis 1 plus 8 times open parenthesis 1 plus 8 times and
so on let's now use the arithmetic identities we know that they correspond
to type equivalence or isomorphism and we can expand this and we get
an expression which looks like this it's an infinite disjunction of
empty list lists of one element list of two elements list of three
elements and so on of course this expression doesn't really mean much
it and finish finish disjunction is not well-defined we should stop
it at some point putting a list at the end in some way so at last
we can only do this a finite number of times so this this triple period
it it should only be used a finite number of times so the last term
would be 8 times 8 times 8 times list a and that would be well-defined
it will be a well-defined equivalence of types but this is very suggestive
this is basically showing you that this recursive recursive definition
gives you an infinite disjunction and gives you a possibility of having
a list of any links very visually clear what's what this type is doing
and there's a curious analogy with calculus so imagine you have a
function list of T and this function satisfies an equation of this
sort so I'm replacing a with a real number here to have an analogy
with calculus we can solve this equation and we have list of t equals
to 1 divided by 1 minus t and we can expand this in series and we
get a very much the same expression as this infinite sum of all the
powers of T so this is just an analogy it is not directly useful for
functional programming because there's no way for functional programming
to make sense of dividing one by one minus T there's no - as far as
we have seen but it's a curious fun analogy and even derivatives have
an analogy in functional programming but I will not talk about this
right now let's go through some worked examples to kind of repeat
what we have seen in this tutorial and get a bit more experience solving
various problems using the Curie Howard correspondence and reasoning
about types the first example is we want to convert a type notation
into scholar code so this is something that we should be able to do
to convert it in both directions the Excalibur and write the short
type notation which is much easier to reason about when you have to
answer questions about types such as am i using the right type for
something do my functions need to be so complicated can I simplify
the type maybe so before even you write code you should ask such questions
and write down the types whenever you see that the type gets very
complicated reasoning about types is much easier in a short notation
than then in the code notation so for this reason it is useful to
be able to go between in a short notation and the code in both directions
so let's see how we can define suppose suppose some reason and gave
us this type and the short notation what is the definition of this
in scholar code here is my implementation so I define the type it's
easier to define the type like this I could have defined a case class
with a single value in it but then I would have to define extra name
because if I define a type there's just one name here my t case class
would look like this and then I have to say name like if {[}Music{]}
going to my Tivo so I would have to write all this and I would have
to invent another name if I its if this is useful do that otherwise
otherwise just with the type type alias as it s colder or type name
names type and it can have a type RAM I'm required to implement this
Junction so I need to define this auxiliary class or type this Junction
is this one so one T integer n times T and function string to G therefore
I have four cases one is the empty so that really represents the unit
type empty tuple I call this empty value T and there's a single value
T then there was a tea with integer and then there is a function string
to G so each of them has a name so I just chose names for the script
for this describing what these things do but in a real application
these will be names that means something more interesting to the programmer
and {[}Music{]} that's it so every time I put a type parameter in
on the case class and I say it extends that trait with that type type
parameter how do i declare values of this type well I just do it like
this there is no name on the case class field here I don't need to
put a name here I just say B goes to this and that becomes that type
here I'll give an example of some interesting function from boolean
to this so if the boolean is true then I return this case and if the
boolean is false I return the empty value it's up to me what I do
here but basically that's I can do that so that for integers I need
to return here a function from string to integer alert I put this
function which returns the length of the string into the case class
we'll listening so this is how I use the definition I just gave the
second example is to transform this type into an equivalent some type
so some types and disjunctions are the same thing this type is a tuple
the tuple of to either's and i want to transform this into an equivalent
some type so the first thing I do is I write this type in the short
notation so the short notation for this type is this so I'll use the
star for the product which is even more suggestive of the arithmetic
correspondence to Titus's and then to convert this to a simple d junction
i simply expand the brackets as in school-level algebra and the result
is this sum or disjunction so it means that if my first type t1 is
a tuple of to either's my second type has to be declared as a disjunction
of four parts so I declare a sealed trait with four case classes each
of them must have all the four type parameters and you see it becomes
quite verbose I have to say each time I have to repeat these four
parameters these these four I have to say extends t2 blah blah every
time and I have to say final case class every time so this is the
diversity of definition of disjunction however once this is done the
code is not not do both so this is so I defined four cases t2 has
t to AC t to LD t to be C and T to be D and each of them has two parts
in the in the tuple in the case class or named named tuple so the
two parts are all the types that I'm supposed to have so for example
a and C a and D and so on so once I have defined these types how do
i specify their equivalents so I know that this type is equivalent
to this one because I just expanded the brackets in the algebraic
polynomial expression and I know that such such operations always
give type type equivalences but in a particular code one part of the
program might give me a value of t1 and another part of the program
might require a value of t2 so they're equivalent but I need to transfer
one into the other so I need these functions let's call them f1 and
f2 ideon that repackage t1 and t2 and vice versa so let's write a
code for these functions so T 1 goes into t2 how do we do that well
t1 is a tuple to either's so to write a function that takes it to
focus on argument I started with a case match and I match the tuple
directly with the two arguments now each of these arguments is on
either so I match in a B and I have a case left and the case right
case left awake is right of B and in each case I also need to match
C D which is this second either so I have these four combinations
I first match a and then I match C and then I met or D and if I match
we then I also could have matched C or D and each time I return the
case class instance that corresponds to that choice there's nothing
else I can write here in this function really I'm just repackaging
the data in from from this format into this format and this entire
code in fact could have been generated automatically and writing a
library to do that it's not ready yet but this code is unique there's
only one way of writing it correctly it follows from the type from
this type expression algorithmically follows so it can be derived
automatically by a library however what's not jump ahead the function
that converts T 2 into T 1 is easier T 2 is a case class sorry T 2
is a sealed trait with four case classes so we directly match on T
2 with 4 cases note again this short syntax when I'm not writing this
because that is not necessary to write so I have two kids I have four
cases and in each case I directly return the tuple of to either values
but I'm supposed to Richard so if I have for example a and what say
AMD or B and C then I return right of be left of C right with the
right of D and so on and I check that this works next example is to
show that a plus a and equal a and eight times a equals a are not
type equivalences although they hold in logic so these are logical
theorems that are valid in logic but of course as arithmetic statements
these are wrong a plus three is not equal to away for any K and so
and and eight times a is also not equal to eight for any a in arithmetic
and so we expect that these are going to be two pairs of functions
that we can implement going from this to this and back but these functions
will not compose to an identity let's let's see if this is so implementing
the function that goes from either to a is very easy we match from
the either we have a left away we return a right away going back is
even easier we take an a and now we need to return an either so which
one do we return return left or right we must choose either right
or left now there's no information in the a in the argument there's
no information to tell us what to choose so this choice has to be
hard-coded here it has to be chosen once and for all a that's actually
the problem because the other type has two versions are very left
and right and here we lost that information and so coming back we
have to choose one of them now I would like to emphasize and this
is not a political choice here between right and left it has nothing
to do with politics and the names right and left were chosen simply
because it's the left side of the disjunction or the right side of
the disjunction so whatever we choose here we cannot satisfy identity
so this function if one already has lost the information and if initially
we had the left part of the disjunction we projected onto a and then
we go back to the right part of the disjunction so we did not recover
the initial value which was the left of a and that is the code that
shows that there exists some value that does not satisfy the dual
identity to show that the product of a and a is not equivalent to
any then we do very similar thing so this function is take a tuple
of a a and rich a so here again we can return the first part of the
tuple or we can return the second part of the tuple but we have to
choose which and we have to choose in the same way for all arguments
there is no information here that can guide us to choose the left
or the right part of the tuple the first or the second part of the
tuple so let's say we choose the first part of the tuple then we lose
the information in the second part and that's the information loss
that our intuition tells us this cannot be a type equivalence and
indeed it is not so the function going from a to a tuple of a a it
can only do one thing it can duplicate the value a into a tuple and
so obviously we cannot recover information from the initial tuple
we lost the second one and then we duplicate so let's say the tuple
1 2 will be converted here to 1 and then converted to 1 1 so that's
obviously not identity show that this is not a theorem in logic now
this means we are not able to implement one of the two directions
right so logical equivalence means that we we have from like logical
X equal 1 to Y X equal Y means we have X to Y and we have Y to X so
in the code we should be able to implement now if this does not hold
in logic and we should not be able to implement one of these directions
let's see how that works so we're trying to implement this we have
a function from a B to C and we need to return the function either
a function from A to C or a function of B 2 see so how can we do that
well we have to so let's write this actually let's write this function
a slightly different syntax it's probably a little confusing but let
me let me rewrite this into a syntax we've been using until now but
this is a completely equivalent syntax a function type parameters
and then argument : result type this is a standard way usually in
which the scholar programs are written but in this tutorial for clarity
I want to emphasize the types of everything and so on and in this
notation types are not so obvious so let me write write it in the
way that we've been doing so we just say the type is this go into
that then we have F ABC is the argument here somewhere extra yes that's
it so yeah so we take this as an argument which is this function from
a B to C and we're supposed to produce this either value now we need
to decide whether we produce a left or a right because there is no
information on the left here on in the argument to tell us which to
choose so let's suppose we decide the left this time so we return
the left of this so now we return the function that takes a and it
needs to return C now how do we turn C to produce a value of type
C the only way for us is to use this function f ABC which produces
a C given a pair of a B but we don't have a B we have only a we need
to do F ABC of a comma something of type B and we don't have that
so we cannot implement this function this is the usual way in which
we can see that some function cannot be implemented due to its type
is that we're supposed to produce a value of some type but there is
no value of that type nobody can give us that value and our arguments
are not enough we're not given enough data to produce that value so
then obviously this is a no-go in the other direction it works if
we have an either it's a left or a right if it's a left we have a
function from A to C we can take this tuple of a B take the first
element of it which is a put into that function we get to C so that
works the second works in the same way the second case so one direction
of the logical inference works logical implication works but the other
does not so f1 does not work that is in that is how if we cannot implement
a function f1 of this type and this is what it means that this statement
is not true in logic next example now these examples from now on are
more realistic so we will use the skills that we learned and we'll
see how it works with these functions that are more and more useful
in real programming so let us denote this type reader simply what
this is a function we're required to implement functions with these
types it goes into reader EA and linear EA to a b2 reader EB so we
define a type reader like this and these two functions let's call
them pure and map so pure takes a and returns reader in a reader EA
is just e to a so it takes a as a word return a function it takes
a and returns a function that takes e and returns a now we have this
a we don't use the e that's the only way to to do this so this is
a function we've seen before that ignores its second argument and
returns the first one the only difference here is that I'm using the
type constructor so so I remind you that this thing is called a type
constructor because this is similar to something that construct type
given some type variables or some type parameters it's quite similar
to a function the type level so a function that takes types and returns
other types and this function we define like this so type level functions
you can think about them in this way they can have several arguments
and they return expressions that are types type expressions just like
functions that take values return expressions that are values type
level functions take types as parameters and return type expressions
so these are type constructors let's look at the map function so we
take reader a a a b and we need to return with your EB so how do we
do that so let's say R is the Sridhar EA we need to return a function
so I'll remind you this is the right associativity of implication
so this is in parentheses but I just highlighted so we need to take
the three Duryea and return a function let's write the code for this
then we take a read Rea and return the function what does that function
that function takes a to be let's call this F of type A to B so IntelliJ
knows and it returns reader a B now what does a reader EB is a function
that takes e and returns be right so let's take a and we need to return
some be of type B so how can we get a value of type B the only way
is to apply this function to some to something of type a so let's
call it like that let's say F of a where a is some value of type a
how do we get the value of type a well we're given R which is of type
E to a and we have an E so we can apply this R to this e we get the
value of type a we apply F to that a we get a B and that's our final
result so now all of this seems to be very long-winded so let me write
a different syntax which is shorter collecting up one eye instead
of writing the types like this I will put each of these arguments
right next to the function name this is just a different syntax I
still need extra parentheses for each argument here remind you that
Scala needs parentheses around type arguments I'm sorry around types
of arguments of functions so f is this so I write exactly the same
function in a different syntax instead of this I put a colon because
this is the final result type and then I just write this very simple
code because II so you see this R is now here this F is now here this
e is here and then I have in lined everything F of R of e that's all
so this is the entire code now there's only one way to write this
code the types are such that there is no freedom there's only one
way to get a value of B there's only one way to get a value of a and
so there is an algorithm that I've mentioned before it takes the type
expression and produces the code I started to implement this algorithm
in the library and I can already in this library at this early stage
already implemented the part of the algorithm that deals with function
types and tuples in not not yet in every situation but function types
in every situation tuples in some situations so I'm still working
on this eventually I will implement entire algorithm and right now
the library can already do derive this function because it has nothing
but implications the next example is show that one cannot implement
this function now what is this function this will be very similar
to map except with respect to the first argument up to the second
so this is a map with respect to the second argument we map a with
a function it'll be into a B now if we want to map the first argument
this cannot be done and the reason is that we have a in R which is
a reader of a T which is a function a to T and we have a to B and
we need a function from B to T so how do we produce a function of
B to T we take B we need to produce a T but we don't have a T unless
we have an a right this is a function from A to G how can we get an
A well we can't get an A we cannot we can we cannot take a function
f and produce an A out of it because function f consumes an a it does
not produce an A so this is a no go saying exactly the situation when
you're required to produce a type but you're not given any any means
of computing the value of that type however if we were to reverse
this direction then we can implement this because now we have a B
we can use this function to get an A and that will solve our problem
here we needed an A we can get that a from an F which is a type between
so this works in other words it's like a map but with this area reversed
and this is called a contra map so this is a contrary motion so to
speak so in just an example showing you that some types can be implemented
other types cannot be implemented next example is to implement this
function so it's a map on a type 1 plus a when 1 plus a is option
of a we know that option hasn't mapped but let's but let's in a standard
library let's try to implement it ourselves and our idea is that we
should avoid information loss now what what would there be as information
loss the thing is that the option type has two cases none and some
B would say if we have a function that returns an option we can always
return this part of the disjunction because this is always available
this is the true part that it is empty and empty tuple unit type we
can always return it so we can always implement a function that ignores
its arguments and returns unit or in the case of option type this
is called none all right so this is named named unit on the language
of types this is just one unit type but this kind of implementation
that ignores its arguments and always returns unit loses information
so in order to fix fix that and require the information of not lost
we need some critique right Erie but that sure that shows what is
that information that needs to be not lost so let's make a criterion
that if we put some option value in here and if you put an identity
function in here so a function that certainly does not lose any information
then I should get the same option back as I as I put there so this
would be my criterion for not using information let's see how this
is done so we can get an option and a function f from A to B so now
I'm reading a syntax with arguments here because it's slightly less
typing but also the types are a little obscured however this is a
good exercise to go between the short notation which is this and a
function type in Scala which is this declaration of a function so
this is this is what represents a short notation here since my argument
is an option I need to match on it so it's a disjunction option as
a disjunction with two cases some a and none I have to mention it
if I have some value in there then I return non empty option with
transformed value there's no other way I can do it I need a value
of type B the only way I get a value of type B is is when I apply
F to some value of type a there's no other be given here anywhere
so I must apply F to a so that's what I do in the case of none there's
nothing else to do except return them because I there's no way to
reach find the value of B from from nothing or a value of a from nothing
the other possibility would be to always return them so this is what
I described before is the information loss option and now if I check
this the test will check that if I use the map and there is no information
lost or identity is always preserved and if I use the bad one then
there is some value of initial X so that the value is not preserved
and then I check it with with integer type so that's this test and
let's now implement a map and flatmap in the same way for the either
type now for the either type usually one prefers R over L I will show
you what that means one prefers because there is a choice in implementing
happen flat map for the either type for the option type there is no
such choice due to this criterion but for the either type information
loss is not a problem the problem is there are two sides and let me
show you what that what that is so I need to map I have an argument
which is an either of L are a function from R to Z and I need to return
on either of LT so I match from the either and if I have a right then
I apply the function f and I map on the right but if I have a left
then I have a value L I cannot transform L with this function so I
have to return it unmodified so this is what it means to prefer the
right it means that the type is given such that it is the right one
that is being transformed and the left one is not being transformed
and I could of course put here a different type like L to change to
the R 2 T and then I would have to put their left transformation now
let's look at flat map the typical signature flat map let me remind
you what flat map is it's the first time we look at it in this tutorial
so if I have a sequence and I'm mapping with X going to sequence of
X X X let's say the result of this would be a sequence of sequences
will be a sequence of sequence 1 1 1 comma sequence 2 2 2 comma sequence
3 3 3 and then I do flatten and that gives me a flat not nested sequence
1 1 1 2 2 2 3 3 3 and the combination of map and flatten is shortened
to a function called flat map so what is this type of flat map so
flat map takes a sequence of T so something of type sequence of T
then I say dot flat map so this is actually implicitly one argument
of flat map because I'm putting a dot here so in an object-oriented
syntax and then I have a function from T to sequence of possibly well
let's save sequence of T also and the result is sequence of T now
I could have transformed the X here and some other type and then it
would have been at this other type it will be a sequence of sequence
of you flattened which is a sequence of you so flat map has this sequence
Pastore has this type signature it takes on the sequences it takes
sequence of T it takes a function from T into sequence of you and
it returns a sequence of you the exactly analogous type signature
for flat map on either would be that it takes an either of LR it takes
a function from R to either of LT so else stays the same so we are
preferring the right this is the convention L stays the same R is
transformed but the transform function returns an either just like
here the transform function returns a sequence not just a single you
with a sequence of you here it returns an either of LT but the result
is not an either of either the result is a note nested either that
is what flat map is supposed to do let's see if we can implement this
we can we match from this either if it's in the right then we call
this function because in the right we have the value R so we call
this function we get an either we imagine that again if that is a
right and we return a right if that is a left we return the left so
we basically return what that function returns so actually I could
simplify this code into this if we are in the right we just return
this either if we're in the left we return what was on the left now
notice that this left is of this type and this left is of this type
so we cannot just say this is the same as this they have different
types although the values are just inside this L is the same but this
L is being repackaged into a different type without losing information
so this is how we implement either flat map and map for either the
next example is a type constructor called state now this type constructor
is defined like this it's a curious thing but we will see that it
is useful let's implement pure map and flatmap for this type so pure
is a function that takes a type a sorry it takes a value of type a
and returns a state with parameters si I'm sure the written is as
a superscript in this slide just like I've written it here the map
has a standard signature so the mapping is performed with respect
to the parameter a the parameter S is not changed and there's no way
to implement map with respect to parameter s so with respect to parameter
a we can implement map mapping A to B and we can implement flat map
map in a two state of s B and the result is again this eight of SP
so how do we implement these functions the way to do it is to follow
the types you write down what types you want and you try to implement
so we define the type state this is a little more difficult because
you don't a what this type is doing what is it useful for just follow
the types try to see what is given and how you can return the value
that is required so the pure function is required to return a state
of Si even an a so we return a function here that takes an A and then
it turns in state which is si which is a function that takes an S
and returns a tuple of a s now this is obvious we just have alien
s we can return only that as a tuple and that's what we do is nothing
else we can give to implement this type let's look at map so the map
we have what's called si argument which is of type status a there's
an F argument which is of type A to B and we are supposed to return
state s B so state as B as a function that takes s and returns to
pull of a s so how do we do that well actually we need to return a
tuple of type B and s so we take s and at the end we should be able
to return a tuple of B and perhaps some new value of s we could return
the same value of s here but probably it's not a good idea we'll see
so what can we do how do we get a value of type B the only way is
to use F on some value of type a so let's say B will be computed as
some F of a what is a well we need the value of type a the only thing
we have here is this si which is a function from s 2 to power of a
s so we can apply this si to some value of type s which we have it's
right here so the result of si of s is a tuple with a and some new
base great so we have an a we put it right here we have a B therefore
and then here we use the new s to return that newness Y will be a
shame not to use that thing we got information from using the our
our arguments but if we ignore that will be information loss slightly
this is still an intuition right now we have not formulated criteria
for the map function to be information preserving but just an intuition
at this point which turns out to be correct later but at this point
I just feel that if I ignore this new s and I I could put I could
put s here because I have the value of type s I could put it here
but that will be losing information I'll be not using something that
I have which some something that some that I got as my argument so
this is my implementation and similarly with flat map I have more
complicated situation but first I apply s a to s and I get a and newest
one I apply F to that a I get a new state big no state B is actually
a state of s B which is a function that I can apply to s to get a
tuple of B and s so I apply the state B to this newest one again this
is my intuition I could have applied this to this old s well it doesn't
feel right that would be I'm ignoring this newest I'm ignoring it
I don't like ignoring information if I'm given it and similarly here
I want to return this newest to because if I could I could return
newest one or I could return even this first s in here but that would
be losing information because I am given this and I should be using
it somehow and there's only one way of using it I couldn't exchange
this order I couldn't put s 1 here and s 2 here is at this point I
don't even have s to yet so this is so if I want to not lose any information
there's only one way for me to organize this code the last example
is to define a recursive type non-empty list and this is defined in
the short type notation by this formula so let's see what that is
we need to define it and then we need to implement map and concat
functions which are concatenating the lists and map is just a typical
map for for a collection so what does this non empty list do it's
like a list that we saw in the example except it's never empty it's
either a value a or its value a and another non-empty list so it's
either one value a or it's two values a or it's three values a and
so on so it's like this infinite disjunction that we saw in this slide
this one except it starts here there is no one plus there's it just
starts here so the list is never empty there's always at least one
value of a in it that's the difference between an empty list and traditional
list so how do we do that well we define the sealed trait and so on
so just just like before the formula is given so let me copy this
formula into a comment here to be very clear so there are two case
classes in the disjunction the first one carries just one value okay
let's call this a instead of let's call this T instead of P just so
that it's come it's convenient for us to compare the code so the first
element of the disjunction has the value T is here the second element
has the value T and also it has another non-empty list so how so this
is the entire implementation so how do we now the implement map for
it well the signature start with the type signature let's see what
follows from it first argument is a non-empty list second argument
is a function from T to you and the result must be non-empty list
of you now since the result since the the argument is non-empty list
which is a disjunction we must match on it well to place two two cases
the first case is this second isn't it so what do we do in the first
case it's a list consisting of one element this element this part
of the disjunction well we don't apply F there's nothing else we can
do except apply F to this T get a you and put it into a non-empty
list as ahead not much else we can do we couldn't for example produce
the second one because for this we need already existing non-empty
list and that's not what we want to do we we don't actually have another
non-empty list the only thing we have is this one alright now in the
second case we have a head and a tail head is of type T the tail is
of type non empty waste so now we can actually produce the second
we could produce the first one right we could just ignore this tail
and do the same as we did in the first one it could always return
the same but that would be ignoring information that would be information
loss we don't want to do it so therefore we do not return this we
return the second one the second one requires two values the first
is the application of F to the head and that's the only way to get
a value of type u right so in the any tale of U which we require to
produce there must be two values one is a U and another is a list
non-empty list of you now how do we get a non-empty list of you we're
not giving it the only way to get it is to apply the map function
recursively to this tail that's the only non-empty list of you that
we have that's not trivial and that's what we do therefore so this
implementation works it's not tail recursive because the map now I
can just make intelligent tell me why but it's because the map is
in inside of some expression right away it's read recursive call not
in tail position so this call is in some intermediate position inside
of an expression and so that's not tail recursive that's fine we don't
care about this at this point the second function concat so it takes
two lists and it concatenates them so how do we do that well we have
to match let's say we match from the first list the first list could
be just the head in that case we'll return so that's just a list of
one element so we just prepend this element to l2 which is very easy
to do we return the tail with head given by this and tail given by
l2 and we're done that is directly pretending in the list so basically
this would this takes care of the first case the second case is a
little as wonder so we have a non-trivial list on the left as l1 and
also may be a non-trivial list on the right so what do we do take
the head put it here but then the tail must be the concatenation of
this tail and whatever is left so we use the concat call recursively
here to produce the tail of the list so this is an implementation
that we are looking for so basically that concludes the worked examples
for this tutorial here are some exercises that encourage you to do
in the same way that I was showing the worked examples so what in
these exercises what kind of problems can we solve well this goes
over all the problems that we can now solve using the tools we we
found we whatever yeah I have a slide that we can use the short type
notation for reasoning about types so we can convert short type notation
into case classes and and back now given a fully parametric type we
can decide whether it can be implemented in code and computer scientists
who do theory of types and functional programming say that this type
is inhabited in other words the Curie Harvard's preposition CH of
T is true there exists a value in the in the program that exists a
value has been computed of this type this is what is called inhabited
and if it can be implemented generating the code so this is that algorithm
I am linking here there is a whole overview of these algorithms and
also there is this carry Harvard project which I will demonstrate
right now another thing we can do is what was in the first step and
part of the third chapter is if you take an expression you can infer
the type it can have now there is an algorithm for this tool which
is called adems hindley-milner algorithm and I'm giving the links
here we can decide type isomorphism we can decide whether some types
are equivalent we can simplify type formulas using the arithmetic
carry Harvard respondents well I I call it the arithmetic correct
Harvard this is not accepted terminology in computer science but I
found it very helpful to think about it this way as arithmetic correspondence
as I showed the logic respondent does not give you type isomorphism
information but the arithmetic one does so using these tools you can
compute the necessary types before you start to write code and when
you start writing code you are guided by the types and in many cases
it helps you write code correctly the first time what are the problems
that we cannot with these tools well we can not automatically generate
code that satisfies some complicated properties like for example isomorphisms
this is what I showed you when we were implementing the state code
for flat map for example where there are several possibilities of
what to do there are several implementations and I'm using this intuition
about information loss but so far I have not formulated specific exact
criteria that these functions must satisfy and automatically generate
and code for these functions that satisfies this criteria is something
that we cannot do using these algorithms these algorithms will just
give you some implementation or all implementations but they will
not be able to check equations or for such things as type equivalence
and the second thing you cannot do is Express complicated conditions
for example we defined an A List that is not empty but we could not
define a list that is sorted there is no way to define a type that
automatically sorts the list you can write code that sorts the list
for sure but you cannot have a type that somehow by itself is not
going to compile until unless the array is sorted see that is impossible
types that we have worked with are not powerful enough for that there
are more powerful type systems that are called dependent type systems
and programming languages like Coke agda and Idris these are the languages
that implement dependent types and they can express such conditions
as the list or array is sorted or has a certain length and is sorted
percent - you can have non-trivial conditions enforced by types here
we have for example the condition that the list is not empty we enforced
it by type you could not compile a program using this type and a list
you could not compile and run a compiler program unless the list is
non-empty so this type system of Scala and also Haskell and a comma
and f-sharp and Swift a in such languages the type system of these
languages is powerful enough to express that the list is not empty
but not powerful enough to express that it is sorted let's say another
thing I'd like to talk about well actually let me demonstrate first
the curry Hubbard project I will then talk about this as a conclusion
so in the country Howard project I haven't even started implementing
a scholar function which is called implement and this function looks
like magic so here's how I use it these are tests actually the unit
tests they run and pass so for example I say def F want I define as
if I define a function I specify its type for example from A to B
to unit now this function cannot do very much it's a pure function
so there are no side effects the only the only thing it can do is
ignore a and B and return a unit value and that's what the tests check
that you give its various values and a returns unit you give it all
kinds of different types it ignores them and returns unit another
example is here function with two type parameters a and B it takes
a value of type a it takes a function from A to B and returns a value
of type B so the only way to write code for this function is to take
this parameter which is a function apply it to this parameter which
is a get a B and return that B since this is the only way to write
code for this type in other words the only way to implement this type
I want to do it automatically so this function does it so the result
of writing this is as if I have written the code for this function
and then I have tests to check that it works as if I have written
that code another example here is a more complicated function type
which is also implemented automatically when I have a test that shows
that for example yeah there's another syntax of types that are two
alternatives right now implemented both type which works which works
like this where do I see both type here for example I have off type
so this is the same signature as we had in the pure function for state
take a take be returned to pole a B so this can be automatically generated
this is an alternative syntax that I implemented of type so you say
def F of a B equals of type so a code that is the only one possible
of this type that is what this function does so basically it generates
the code that we had here this code is generated automatically because
of the algorithm that checks the type can be implemented and if so
generates the code another example take a take a tuple of this type
so function of A to B from A to B and C and now here we have to ignore
see there's no way around ignoring see there's nothing you can do
with see we can only take this tuple we take the first part of this
tuple apply that to a get a B return B so C is ignored that's alright
sometimes you ignore arguments this is of course certain information
loss but this is the only way to implement this type another example
from a to this tuple to this tuple now here we don't lose any information
and this type also can be implemented automatically when there are
some more examples like this so this is work in progress I just wanted
to show you that in certain cases these functions can be useful like
for example in the cases we have seen the reader the state that can
be generated automatically flat map and map for them and so on this
is work in progress and I will continue implementing these algorithms
so that we can automatically generate as much as possible so let me
conclude with the discussion on the implications of carry Harbert
respondents for programming languages it is not just for programming
code but also for design of programming languages there's a much more
important consequence of discovering the factory Howard correspondence
so as we have seen the Curie correspondence is a map from a type system
into a certain logical system system of axioms and derivation rules
and such that certain logical propositions are valid or are are Hiram's
and others are not theorems and correspondingly those that are theorems
can be implemented as functions and others cannot be you donated so
one can one consequence of this is that if you have a good logic that
is powerful and can have a lot of theorems a lot of interesting non-trivial
theorems then you can implement a lot of interesting and non-trivial
functions in your programming language and all these functions will
be automatically checked correct if you have a logic that is limited
that cannot derive a lot of theorems you also cannot do a lot on your
type system in your language another consequence is that if your logic
is inconsistent if it can derive a contradiction then your program
will crash it means that you put some type into the wrong function
and it will crash at runtime the compiler won't be able to catch it
so it is very important that you know that we understand and there's
a mathematical principle behind inventing programming languages and
this principle is that the type system of these languages must correspond
to a good consistent non contradicting and fully-featured logic so
to speak with all logical operations that are available so these programming
languages have been designed with this in mind you have been designed
with the idea that we have a certain logic and so this logic has certain
operations such as war and implication and so these operations should
be available in the type system so the type system of these languages
has function types it has disjunction types or or some types as they
are also called and product types of tuples or conjunction whereas
languages such as these do not have for example the disjunction types
they do not have the constant true in the logic which is the unit
type these languages do not have the unit type you could not say let
X be of unit type and then put X as argument into some function you
cannot say that in these languages python and other languages in this
list essentially have only one type which represents any kind of value
that is possible and so these are mapped into these languages are
mapped into incomplete logics these two logics without or operation
without the true or false constants and this is mapped to logic with
only one proposition which is we can compute something of some of
some value and that's it these logics are very limited they're not
a lot of theorems in them and of course more in these than in needs
but type system makes a difference it prevents errors so the mathematical
design principle is that mathematicians have studied logics for a
long time they found interesting logics and they found a minimum set
of axioms for them what's used that choose one of the logics that
mathematicians have found and they found a bunch of them their model
logics temporal logics linear logic are all kinds of variations on
this theme there are different logics choose one of them the one we
have been working with is called intuitionistic propositional logic
well this is a very technical term I prefer to call it constructive
logic but these are also other possible logics for example temporal
logic is a basis of functional reactive programming so the idea is
to implement a language where this is the logic of types and if you
do that and you get the stream type for free and it's it's very interesting
to see that linear logic has been used to model resources such as
memory ownership of pointers one thread has ownership of this pointer
another thread has ownership of that point if you have this in your
language you can do interesting constraints on your program and prevent
errors so the mathematical principle is that you take a logic that
mathematicians have studied it take its axioms and its rules of derivation
mathematicians have found the minimum sets of axioms that was their
game they'd like to find what is the minimum set of axioms for different
given logic such that you still derive the same theorems that's very
interesting for mathematicians so they already did all this work let's
use it take that set of axioms and rules of derivation and for each
of these provide a type constructor of some kind or a language operation
and you get a programming language out of this and put this at the
foundation of your programming language add other features of course
but that should be the foundation it should not remove things like
like this if there's a axiom in the logic do not remove it you will
limit your language fundamentally and irrevocable that is the lesson
of very hard correspondence let's use the centuries of experience
of mathematics it tells us what is actually useful what are the operations
that are useful what are the operations that are not necessary that
what is the minimum set of operations that isn't required do not remove
things from the minimum set both required did not have don't try to
make a logic that doesn't have a true or false constant or doesn't
have an or operation that is unnecessary limitation that is very hard
to lift another illustration of why mathematics is useful is that
I have to implement this exist some as a helper method because the
testing library does not have it let me show you what I found so there
is this link where there is a discussion why there is a for all in
the library but not exists in the library and the reason was that
no user has requested it yet now of course no user has requested that's
not a valid reason for excluding a basic mathematical construct if
you have a for all you must have an exists or the link to help negation
but negation is not there either so logic that has a for all quantifier
but does not have an exists quantifier is fundamentally limited it's
a basic mathematical principle it should not be necessary to wait
until users requested it should be obvious that this is necessary
it should be at the foundation of the design and of course the real
reason there's no exists is that it was hard to implement and the
design was such that it was not easy to implement the design should
have been informed by mathematics and not by what users happen to
want at this point this is among the lessons of mathematics and of
Kurihara correspondence this concludes the third chapter 

this tutorial will explain the Curie Howard correspondence in a more
pedagogical way more easily understandable and more intuitive this
is a compliment to part three of chapter 3 of my functional programming
tutorial where I also talked about peripheral correspondence in that
Chapter three I gave also exercises in this tutorial that won't be
in the exercises this only serves to explain things better and in
more detail and in a more understandable and intuitive way the main
focus of the correct our correspondence is to make a connection between
types in functional programming languages and logic the revisions
in the formal logic and then a goal is to use the knowledge that we
have about a formal logic to make some conclusions about how to write
programs so the goal of my tutorial is to show practical use of this
theoretical knowledge let us begin with what types are available in
functional programming languages because it is that the specific kinds
of type constructions that is the basis of the career for which respondents
without these type constructions it will be impossible to make a connection
with logic and to use the mathematical knowledge in that logicians
have obtained so what are these type constructions there are the tuple
the function type the disjunction or also called the sum type the
unit type and the possibility of having type parameters this is short
notation for these types are these type constructions this is not
syntax of any specific programming language this is just a short notation
I used reason about types now in all the functional programming languages
that are in widely used today such as Oh camel Haskell scholar F sharp
swift and so on including more advanced and more experimental functional
languages such as in recent exam all these languages have the same
type constructions that I just listed up to the differences in syntax
of course their syntax is different but once you understand how these
types work in one of these languages you basically understand how
they work in all of these languages because they work in the same
way here's a Scala syntax for these type constructions in order to
understand how they work well I assume maybe you already know how
they work but even if you don't it is important to see what are the
expressions that are available in the language that have to do with
these types what can we do with these types so let's begin with the
tuple type this is the Scala syntax for the tuple type you can create
a tuple type in other words you can create a value of the tuple type
using this syntax in order to create a value just take let's say two
values one integer one string and put them together in the tuple like
this this is the Scala syntax for the tuple value and this is the
scala syntax for the type expression that is describing this value
so each language has a specific syntax for this and with this syntax
you create a new value called pair which is equal to this and it has
this tuple type integer and string and then the short notation this
is how I denote it once you create a value of the tuple type how can
you use it well though the only way you can really use it is to extract
some parts out of the tuple so you can extract the first part or you
can extract the second part in the Scala syntax this is how you extract
parts of the tuple and here for instance you can now compute a new
value called Y which will be of type string because the second part
of the tuple is of type string in Scala the type annotations are not
required in many cases you can not you can just omit them from your
code but I will write them just to me to be clear what the types are
all the values will create in so these are the two basic things you
can do with tuple type you can create a value of a tuple type and
you can use an already created value we will do the same kind of reasoning
about other like instructions now we'll see how they are created and
how they can be used so the function type considered as an example
function it takes an integer argument and returns a string value this
is the syntax for this kind of function in Scala one of the possible
ways of defining this function is to write this expression this is
the function expression which is the function itself it takes an argument
named X which is of type int as we just said here so you do not have
to repeat the integer type a notation but you could if you wanted
to be more verbose so you say this is a X the name of the argument
of the function then you write this arrow and then you write the expression
which is the body of the function this expression will be computed
and returned when the function is called so this expression uses this
X in some way to compute a string value how do we use the function
so here we created a function value of this type you could actually
also say Val f instead of Def F in Scala but Scala has certain limitations
and sometimes you have to say def especially when the function has
type parameters you cannot say well you have to say def these limitations
are unimportant for the purposes of this tutorial so we will just
consider this as a value of this function type so functions are still
values in Scala and can be used as a values this even though sometimes
you have to say def and at other times you can say well having created
a function of this type how can we use it well we can only do one
thing really with a function we can apply the function to an argument
the argument must be of type string I sorry of type integer because
that's the type of the function the first thing here is integer so
it means that the argument of the function must be of integer type
so we put some integer value here we apply the function to this value
and the result is a value of a string type and so that's how you use
a function disjunction type is another important type construction
in Scala it is defined in a standard library as the either type it
has two type parameters so either with type parameters int and string
and this is the syntax for type parameters in Scala so either is a
type that represents a value that can be integer or string let's see
how we use these types and how we create them to understand how they
work to create a value of this type we can write things like this
so here's X which has this type this is the value that X has in the
standard library the disjunction of integer in string is defined with
either and it has names for the left part of the disjunction which
is left for the first part of the disjunction for the second part
of the disjunction the name for that is right so these are labels
or names that are required in Scala so you cannot have a conjunction
like this without names so the standard library defines the disjunction
called either and its names for the left for the first part and for
the second part or left and right and this is then the syntax that
you use to create values of a disjunction type so X is a value of
this type and it contains an integer inside labeled by this name left
and what Y is a also a value of this type and it is containing a string
inside and it's labeled by the label right so the slave-owning allows
us to distinguish which part of the disjunction it is and labels are
required every time you create a value of this type you must give
the label and once you give the label then the value inside this label
must be of the right type for left it's the integer right is the string
once you have created the value of the disjunction type how can we
use it here isn't it here's what you can do you can match on the disjunction
and the match contains two cases it can be a left and then you have
the value which was on the left and you can write function body that
will use that value in some way and compute some other value let's
say boolean value the second case is that if you have a right label
so in this case our example does not actually need to use this value
so the syntax is to write underscore meaning that we do not need to
use the value what is inside the disjunction part with this label
but we could have a different example where this could be let's say
X and this could be some expression using X to compute a boolean value
so this is a match expression or a case expression sometimes called
and this is the way you can use values of disjunction type so to create
them you have to specify which label you use so you can either create
a left or the right there's no other way to create values of this
type then once you have a value of this type you can use a match expression
with several case so that you decide which party were given and do
appropriate things in each case notice there is no way to create a
value of the either type where you don't know whether it's left or
right when you create it you must know when you are using it and you
don't know because somebody gave you this and they didn't they created
it but they don't tell you which one they created so then you use
the match expression to find out that's how you work with disjunction
types the final construction is the unit type the unit type is denoted
with the syntax it looks like a tuple with no elements inside tuple
with zero parts or an empty tuple because tuples could have 1 part
2 part 3 parts and so on each part having some specific type so it
could be a total of integer integer string boolean whatever but here
we look it looks like a tuple with 0 parts an empty tuple so this
type only has one value the empty to pull value there is nothing that
you can write in there and there's only one way to write that so this
is a very interesting type that only has one value and so you can
create it by just writing this empty tuple expression the result will
be a value X of this type which is called unit in Scala and there
isn't really any way to use it because there is nothing inside it
there's no value that it holds inside it's empty so you could pretend
that you're using it if you have a function that takes this type as
argument but actually there's only one value of this type and you
can always create it if you need it so it is kind of useless to say
that you were requiring an argument of type unit and you're using
that argument you could require it but you don't have to use it there's
no nothing to use there's no content inside it just one empty value
always and so that means we only have one construction with this type
one kind of expression that can be written whereas all other constructions
that have two expressions one for creating or actually one or this
has more expressions for creating and for using we also have here
more than one expression we could take the first element or the second
element so all these constructions have some expressions that create
values of this type and some expressions that use values of this type
are consumed values of this type and give you values of some other
time but for unit there's only creation there's nothing you can deconstruct
here or extract out of it now I give you examples in Scala because
that's the language I'm most familiar with right now but exactly the
same constructions are present in other functional programming languages
for example just for illustration I'll show you the same things in
the o'connell syntax the tuple type in the o'connell syntax has denoted
like this very similarly to my short notation except for this start
symbol instead of Scala's val keyword and now comma we use the Lett
keyword otherwise things are exactly the same in no camel for creating
a tuple and using a tuple is slightly different there isn't underscore
one or underscore two weeks instead there are functions first and
second so the FST and SMD these are defined Kamel standard library
the function type is denoted like this to create a value of the function
type you have to use the keyword fun which is function creating keyword
then you write a variable name or one or more and then you write this
arrow which is different from the Stalin era Scala arrow is double
arrow and the common air is a single here other than that it's very
similar and then you have the body of the function to use a function
he applied to an argument now in no camel applying function to an
argument can look like this in Scala it requires parentheses around
the argument no camel parentheses are optional you can write them
but you can also omit them so if you omit parentheses then you just
write space and that's the syntax and people sometimes prefer also
look at this function body doesn't have any parentheses it applies
this function to this argument and to this argument in Scala the analogous
syntax would be that you have to put parentheses around this will
be one set of parentheses and then they also have to put parentheses
separately around this so the syntax is less verbose than the Scala
syntax but the Scala syntax is more familiar to people use the mathematical
notation where functions are applied to arguments usually with parentheses
although in mathematics there are certain cases when this is not done
for example cosine of X usually is not written with parentheses is
written usually without parentheses like function this for a cosine
and this were some value that would be similar to mathematical notation
but mathematical notation does not use function with more than one
argument were more than one sets of arguments Scala does Kokomo does
functional programming languages usually do and so the syntax becomes
then again unfamiliar anyway this is just syntax it's easy to get
used to syntax a couple of weeks at work and you're not noticing the
syntax anymore syntax is superficial it is the meaning the semantics
that is important and the semantics is the same use function by applying
it to the argument of the correct type and you get the value of the
correct type disjunction type in o'connell is defined using the syntax
Scala also has a syntax for defining the disjunction type but it's
much more verbose than this so I did not write it it's the case classes
syntax sealed trait in and case classes very verbose so I did not
want to write it it will be familiar to Scala programmers in akumal
this is the syntax where you say you define a new type in and this
type is at this Junction it has two parts the first part is labeled
with the word left which is the name of this part of the keyword int
is the type that it contains the second part of the disjunction is
labeled with right as the name then office again the keyword and then
string is the type that it contains you create values of the disjunction
type using this syntax again very similar to Scala except for a keyword
let and except for the absence of parentheses so here it looks like
you're applying a function to the argument just like here it looks
like you're applying a function to the argument and here you do it
without parentheses otherwise it's exactly the same so you can create
a left value or you can create the right value in the rate left well
you must contain an integer and the right value must contain a string
once you have a value of this Junction type you can analyze it by
using a match expression you know comma syntax it looks like this
very similar to the Scala except for slightly fewer parentheses and
let's keep fewer keywords other than that very similar you have a
left case and the right case and these are functions it look like
function from eye to eye greater than zero function from ignored argument
to false so a match statement is basically taking your value of the
disjunction type and two functions depending on which part of the
disjunction this is we use one function where you use the other function
to compute the result value of the type bool and the unit typing of
kamo yeah it's exactly the same in Scala you have two functions and
you match on the disjunction unit type exactly the same hostel syntax
has even less verbose no no comma other than that it's exactly similar
usually in Haskell people do not write types after they define variables
and there is no keyword necessary there's no Val or let usually the
usually necessary hostel other than that it's very similar this is
the pattern matching syntax that extracts the second part of a pair
and why after this definition becomes a string value function type
very similar except for the very short keyword which is this backslash
except that replaces the fun keyword or DEF keyword in Scala in Haskell
this is the backslash key word and then you write your argument name
your arrow and the function body the function body uses notation without
parenthesis and within fixed syntax so plus plus is the operation
of concatenating a string with this string so is a standard library
function that takes an argument and gives you a string out of it similar
to Scala's dot to string you use the syntax to apply functions to
arguments in Haskell just like a camel parentheses are optional this
is the syntax for defining with disjunction type very similar to o'connell
except there are no keyword of no keywords all necessary so very similarly
less verbose the no camel otherwise very similar you create values
of disjunction type and you match them in a case expression which
is exactly similar to the match expressions in the camel and skull
using two functions that compute the boolean value in the two cases
and the unit value is like this so you can see all three languages
kind of very similar types can type constructions and this is why
what I'm going to present right now applies to all of these languages
there is no difference in this level between these languages they
are the same once you understand one of them you understand others
at least you understand how to work with these types and how logic
helps you work with types you have to understand is universal and
I'm pretty sure if they're further language is invented they will
still have the same constructions that I listed and the understanding
will persist because the mathematical value is so great and I will
show why mathematical value arises here it arises if we consider four
positions of correspond to types so how do we do that what what are
these propositions that corresponds to types let us define these propositions
consider in the Scala syntax some variable that you define in your
program in your expression somewhere programs are expressions so I
will say program or expression it's similar same same thing if in
your program somewhere you have this it means assuming of course it
means that you can compute a value of type T it's some part of your
program assuming of course that your program compiles and runs correctly
so this is the proposition that corresponds to types the proposition
is that your code can compute a value of this type any value some
value doesn't matter which value the proposition doesn't know which
value you have computed all it knows that you have computed some value
of the type team for some type T let's denote this proposition is
CH of T so mnemonic Li this is code has a value of type T again we
don't have which value this and at this level we do not express that
we don't say which value we have computed we just know that we have
computed a value of this type this proposition can be true or false
for certain programs depending on the program because some programs
may be unable to compute certain types we given some data from which
to compute these types and other programs are given that necessary
data so these propositions could be true or false first for different
types and second for different programs so let us now see what type
constructions we have and what are the propositions that correspond
to these type construction so for type variable T the proposition
will be denoted as CH of T and in a short notation I will just write
T meaningless at the same time the type and the proposition because
as we will see there is a one-to-one correspondence between types
and propositions what we just defined that we just defined the proposition
that corresponds to any given type so I will use this notation as
a short notation both four types and four propositions but for clarity
I will sometimes write CH of T just to make it clear what we mean
what we mean consider now the tuple type the tuple type means that
somewhere in your program would say you have computed a tuple value
well it means that you have computed the integer and also you have
computed the string there is no other way to compute a tuple value
you have to compute both parts on the tuple or all the parts of the
tuple if there are more than two so if you have a tuple a B and you
have computed the value of this type in your program it means you
have computed a value of type a and also you have computed the value
of type B some value of time B so CH of the tuple a B is ch of a and
CH of B as prepositions where end is the logical and logical conjunction
of prepositions in the short notation the logical conjunction is denoted
like this in the standard logical notation I will also use this notation
reasons I will explain in detail below now let's consider the disjunction
type but either if you computed the value of either it means you computed
the left or you computed a right you must have computed one of these
two so in the logical proposition it means that CH of either a B is
equal to CH of a or CH of Lee where or is the logical or it is a disjunction
operation in the logic which is usually denoted like this in the logic
but I will also use this notation for disjunctions for types especially
function type means that you have a function that computes be given
a it doesn't really mean you have an A or not or that you have a B
you don't necessarily have any of these two but if someone were to
give you a value of type a then you would be able to call this function
and compute a value of type B so the proposition CH of A to B I'm
reading this function type as a to B so the CH of A to B is the logical
formula that is if CH way then C it should be so if I am able to compute
signature a then I'm also able to compute the HMB so this is a logical
implication and a short notation from that will be this the unit type
can always be computed does not need any previous data for to be computed
you can just always write this expression at any time and so the preposition
CH of unit is the proposition that is always true any program can
compute unit type expressions so this crisspoints in the logic to
the proposition that is identically true it is always true in all
programs in the short notation I will denote this as one single one
so this is already very interesting it shows that each of the type
constructions corresponds to a logical construction or logical operation
logic operation if we consider the propositions defined by please
a couple of more remarks for especially since we're going to use type
parameters a lot in Scala the type parameters are denoted like this
and if you have a type parameter you know it means on the logic that
something is asserted for all T so some prepositions are considered
for all types T here's an example if you want to define this function
in Scala duplicate it is parameterize by parameter a which is any
type a so a is a type parameter variable and for any type a the function
takes an argument of type a and returns a tuple of a and a it is clear
how this function could be implemented just take some X of type a
and returns a tuple of X comma X now the type of this function in
the short notation would be written like this and in the logic it
would correspond to this formula for all a from a if a is true then
a is true and a is true because the tuple corresponds to the logical
and click conjunction now this statement that for all a it follows
from a is true that a is true and also a straw that for that is certainly
true in the logic it's not a very interesting statement but it is
certainly correct true statement or in other words a theorem in the
logic so it's very interesting that functions that we have here responds
to theorems of logic how do we understand this so what are the logical
relationships that we have between these ch prepositions these prepositions
show that we can compute certain values of types that are given logical
relationships between these prepositions mean that we can see that
if the program can compute a value of one kind of type then it can
also compute the value of another kind of type so that would be a
logical relation or a relationship of entanglement between propositions
and that's what logic usually studies so if we understand purely on
the logical side how to derive one proposition formula here we will
be able to make conclusions about programs about what kind of types
can be computed if some other types are given or can be computed so
in logic these relationships are studied through purely formal means
you lay down axioms and derivation rules of logic and you follow them
for us know all these logical relationships are direct expressions
of the kinds of code that you can write so that is the second side
were part of the correspondence with a quarter if average code corresponds
to proofs or derivations in the logic so we will explore this now
in detail in logic we are reasoning about what follows from what using
something called a sequence the sequence is device used by logicians
to denote an elementary task of proving something as when assuming
something else to be true this is a notation for a sequence it has
this symbol which is called the turn style to the left of the trend
style are some logical formulas which are the premises to the right
of the turn style is a logical formula which is the goal so sequence
in logic represents a proof task that is the task of proving G the
the formula G or the proposition G assuming that these premises are
already proved and the proofs in logic are achieved by using axioms
and derivation rules that have to be specified in advance axioms in
this notation would mean that we have a list of sequence that are
already true by themselves that do not need any proof and derivation
rules would mean a list of rules saying that a certain sequence would
be true or will be derived if certain other sequins are already derived
previously so these are the rules of derivation what which sequence
can we derive given that some other sequins are already proved so
in order for us to be able to reason like this we need to specify
what are the axioms and what are the derivation rules that represent
the logic of ch propositions or as I call it the logic of types to
make connection with our code we need to translate our code fragments
into sequence somehow what we will do is that we will look at every
construction in the code and we will see what kind of sequence represents
that construction so we will represent expressions for sequence how
how does that work a sequence like this it represents an expression
of type C that uses some variables or parts or expressions of types
a and B so those are assumed as given and out of those will build
a new expression that has type C here are some examples using the
Scala syntax if we consider this expression where I added the type
for clarity in Scala you don't necessarily have to do this so we compute
the string representation of some integer and we append the string
ABC to it just as an example now this entire thing is an expression
of type string but it uses a variable of type int integer so a sequin
that represents this expression would be this sequence the premise
is the integer which is this and the goal is the string so notice
we are representing types of sub expressions so the sequence calls
our attention to the fact that this expression uses an already computed
value of type int so this value should be already computed somehow
previously and then so this becomes the premise of the sequence and
then we can compute a value of type string so that is the goal of
the sequence so I'm not writing CH here just for gravity if I wanted
to that would be here CH event to the left of the turnstile and this
would be CH of string to the right of the turnstile another example
is this expression in the scala syntax this is a function that takes
an argument of integer type and returns this computation which is
a string is a string type so notice this is the same expression as
was here and now it is used as the body of the function now this entire
expression doesn't actually use any variables from outside it has
the variable X which is the argument of the function so this is a
bound variable it is not does not have to be computed before in order
for us to have this function so this entire expression has type integer
to string it as the function type and it is represented by this sequence
to the left of the turnstile there is an empty set of premises because
this expression does not use any variables that are previously computed
so the rights of the turnstile is the function type again more precisely
this will be CH of inste string a very important remark here is that
sequence sequence only describe the types of expressions the types
of parts were variables that were using sequence do not describe the
actual computations entirely so nowhere here does it say that we are
actually taking this integer converting it to string and appending
some other strength to it the sequence does not express this information
it only describes the types it described it is focused on what is
the type of the entire expression which is the goal of the sequence
and what are the types of variables that are assumed to have been
already computed which is the premise of the sequence now we can translate
all the constructions that we had in functional programming languages
into the language of sequence and if we do that which we'll do in
this slide we will obtain all the derivation rules for the logic of
types as well as all the axioms each type construction that we have
seen before corresponds to either a sequence or a derivation role
the expression for creating it an expression for using the type construction
they both gave rise to some sequence because we can just describe
these expressions in terms of sequence and so this sequence for example
would assume that the function f has been already computed and that
this number has been already computed so that would be the sequence
that has least to his premises and this type doesn't as my goal so
if we translate all of those instructions into the language of sequence
here's what we get for the tuple type the expression that creates
a tuple type gives rise to this sequence from a B follows it helps
me because the expression is this and it already uses the previously
computed or available expressions of these types similarly when we
use a tuple we assume that that tuple value has been already constructed
and then we obtain the value of the first part of the tuple or the
second part of the tuple so these sequence directly corresponds to
code fragments or two expressions of the code they do not need the
proof they are axioms the function type we have an interesting situation
that the function type requires a body of the function to be created
so the body of the function needs to be already an expression that
must have been possible to write and so this expression is another
sequence that we must already somehow have established that it can
be written sorry I'm here so if I have this sequence which is the
body of the function that uses some variable of type a then we can
put this variable outside so we have this body we can put this X outside
and make a function that takes X and returns this body this construction
of the language is represented by this derivation if we have this
sequence which is a body if we can write the expression for the body
then we can write the expression for function using the function means
applying a sterile argument so if we have an expression of this type
which is a function already somehow created in computed if we have
a value of the argument type and we can compute a value of the result
type the expression for this is the application of function to an
argument and as I said sequence do not say how this is implemented
in form but we need to keep that in mind we need to keep in mind that
each sequence that is proved really means that we are able somehow
to write code representing these types as the sub expressions or variables
and this is the type of the entire expression for this Junction type
we have two possibilities to create two possibilities of creating
a value of the disjunction type inject into the left or into the right
so these are represented by these two sequence and by writing this
sequence we represent the match expression which takes a value of
the disjunction type it takes two functions from a to some C type
and from B to C and the result is an expression of type C so the match
expression is always of this kind we we have a new type C that is
being computed in each case and each case is a function from the value
that is held by the disjunction to that new type so we have to assume
we already computed this expression and we already have somehow these
two functions and only then we can produce a value of type C so this
is the secret and the unit type is represented by this sequence it
has no premises and it already can give us an expression of unit type
in addition to these constructions have used some specific types we
have constructions that are general not using any specific types and
these constructions are in some sense trivial they are very simple
and obviously reasonable independently of a programming language as
well although we should note that everything we have done here is
common to pretty much every functional programming language this is
really not about a specific language this is about functional programming
paradigm as such so what are these additional constructions well one
trivial construction is that if you have a value of type a let's say
X of type a then you can just write X and that's a valid expression
and that expression obviously has the same type a so that is represented
by this sequence from a follows a or we can compute a if we have an
expression of type a already so that is kind of a trivial expression
but nevertheless we have to include this into the rules of our logic
for completeness this is the simplest expression we can write that
is in some way using a given value another rule is that when we can
compute some value of type J given let's save some data a and so on
then we can also compare J if we are given some more data some additional
data we will just ignore that additional data because we can already
compute J using these these are still here so it's easy for us to
compute this J we just ignored with this extra information that we
will have a lot of situations when we need to talk about sequence
with a certain number of premises which we don't need to use as explicitly
so we will denote them by the uppercase gamma so that is a typical
notation for a sequence of zero or more arbitrary premises that we
will not have to notate one by one finally another rule is that the
order in which data is given does not matter so the same things can
be computed if we have expressions gamma which stands for any number
of expressions and then a and B and also gamma B a so if we change
the order in the sequence of premises we can still compute the single
well this is kind of trivial - it doesn't matter in which order you
give parameters in the function you can still rewrite the code in
a trivial way rename the variables and you get the same computation
these are the additional rules in what follows we will use syntax
conventions that are these so first of all the precedence of operation
so that implication associates to the right which means that this
expression means always this and these parenthesis don't have to be
written if we want first implication from A to B and then implication
from that to C then we will always write parenthesis around this first
implication but but parenthesis around the second implication can
be omitted another syntactic convention I am using is that I will
write types like this what I mean by this is that a conjunction has
the highest precedence then comes disjunction and then comes implication
so I would put parentheses like this if I wanted to completely specify
the order of operations but for brevity I will just write it like
this which agrees with the ordinary rules of school algebra where
plus has lower precedence than multiplication final important remark
here is that we are talking a lot about any B and C and such propositions
or formulas what we actually mean is that these things are valid for
all a for all B for all C so we always implied that there is a for
all in front of the entire expression with every variable that we
use in mathematics this is called the universal quantifier and so
we will say all our variables implicitly by convention here are universally
quantified so when we write a formula like this what we mean is actually
that outside of the entire expression there quantifiers Universal
quantifiers for each variable so our statements are true for all ABC
and so on so now we have all the axioms and all the derivation rules
that govern the lot logic of types as I call it in other words the
logic of propositions of the form CH of some things so those are the
curry Harvard images of types the propositions of form CH of something
what are the theorems that we can derive in this logic here's an example
theorem and an example of how we would have to derive it if we are
using these rules we need to start with an axiom so let's start with
this axiom this is a sequence net does not need a proof this was the
sequent one of our rules now by another rule we can always add an
unused premise so this is this other room so if this sequence is true
then we can always add some new unused premise if we want so let's
add a premise B and we have this sequence so we already proved that
this sequence is valid now we use the create function rule using B
and a so the create function rule says if we have something that has
premise B and the goal a then we could put B on the right hand side
and so this rule good so here's a on the left and B on the right you
can always rename that so whatever's on the left we can pull it to
the right and make it into a function so therefore that premise won't
be removed from the list of services and put it into that goal as
a function so then we do that and we get this sequence remove the
premise B from premises put it to the right and we have this sequence
now we do the same with a and this so finally we get the sequence
where a is pulled to the right and B to a was here before so now we
have this sequence with no premises and a goal is a - B - E and we
have proved the sequence because we have derived it using axioms and
rules of derivation and since this is a formula that is derived from
no premises it is a theorem what is the code let us be find this proof
now recall that every time we use an axiom or a rule of derivation
that corresponds to a certain expression of code that we could write
immediately for example here this axiom represents the expression
X now in my notation I put the type as a superscript just to be shorter
and more readable so this means I have solisten Scala syntax would
be written like this X of type a so I'm writing it here in the short
notation like this this expression uses X and so it is represented
by this sequence the unused premise B means that we have some variable
Y let's say of type B the create function rule means that we just
put this whatever was on the Left we put it to the right of the sequence
will make that into a function argument so we just write an expression
like this you can always do that if we have an expression for the
body that uses some variable we can always write this in front of
the body that the body and we have a functional expression so this
is how we implement the create function rule so that will give us
this expression now the second and this expression still uses X because
this expression doesn't say what X is so X must be already defined
and this is why this expression corresponds to this sequence and has
a premise of type a which corresponds to this X of a that has been
already somehow defined outside the second trade function rule puts
the argument X in front of the function body and now we have an expression
which is this one this expression does not need any variables defined
outside it and it represents the sequence with no premises that's
the same thing so no premises in the sequence means this expression
does not need any values defined outside it so that's it the Scala
code is exactly this this is the entire Scala code of this function
so in this way we see that any expression in the code has a type that
we can translate into the sequence while putting to the left of the
Train style as a premise anything that this expression uses that has
already been computed before any variables of types that are computed
before to the right of the turnstile in a sequence we put the type
of the entire expression so in this way we translate any expression
from code into a sequence and then if we find the proof of the sequence
using the axioms and derivation rules then we can directly translate
every step of the proof into code every step of the proof becomes
some kind of combination of code expressions and we have a table where
we could compile a table that gives us a correspondence between each
axiom or derivation role in terms of sequence and cold expressions
and the ways of combining them that these axioms and derivations rows
came from so by construction the axioms and derivation rules in this
logic are precisely those from which we have some code expression
so there cannot be any proof that is untranslatable into code as long
as we use the right rules and axioms of the logic and it's much easier
to reason about logic than to reason about code expressions because
there is less information in the logic we have discarded all the specific
computations that are in the code and we only keep the types and so
there is less the reason about and more clarity therefore as long
as we only look at type information so we can temporarily forget that
this came from some code you just look at the rules of logic and follow
the derivations and find proofs purely manipulating symbols in the
logic that is faster and that is the hope of getting some results
at the end we will translate that into code and we know we can always
do that because each step in the proof initially came from a specific
type of code expression for construction that we have in our programming
language so and we know this is true for any programming language
that adheres to this paradigm or camo Haskell Swift F sharp and several
others so let us see what we have achieved so basically any theorem
of logic that is any statement that can be derived in the logic corresponds
to some code which we have written so here are some examples this
is a theorem of logic for all hey if it is true then it is true maybe
not a very interesting theorem but this is just a simple example and
then we can write code and Scala like this it takes an argument of
type a and returns the same value take a value of type a return unit
no problem we can do that too just ignore this argument return unit
taken a return a value of the disjunction type that's easy we just
take a left of X we get a value of the disjunction similarly for taking
the first value out of the profit of a tuple that's just using this
and this example we just saw how to derive that in Scala we could
write it like this for example one of the ways of writing it another
way we're showing here this is equivalent in Scala just different
syntax importantly non theorems cannot be implemented in code some
on theorems are statements in logic that cannot be derived statements
that are false or undereye verbal examples of these statements are
these for all a from one follows a now this is certainly suspicious
in terms of logic what if a were false then we would have it from
true false false that's very obviously wrong and we cannot implement
a function of this type to implement it we would have to take a unit
argument and produce a value of type a where a is arbitrary type but
how can we produce a value of type a of the type that we don't even
know what it is and there is no data for us to produce that value
so it is impossible another example of an impossible type is this
type so from a plus B follows a if you wanted to implement this function
you would have to take a value of disjunction type a plus B and return
a value of type a but how can you do that what exodus Junction type
happens to contain B and no a just B it cannot contain a if it contains
a B it's a disjunction so then we don't have an A and then we again
cannot produce any and having a B which is a completely different
arbitrary type doesn't help us to produce me exactly the same reason
shows why we cannot produce an A a and B given a because that requires
a B we cannot produce and also this is not implementable because we
are required to produce an A but all we have is a function from A
to B this function will consume an A if given only this function cannot
possibly produce an A for us but we are required to produce an A as
a result so we cannot and also there is no proof of this formula in
the logic so these examples actually lead us to a natural question
how can we decide given a certain formula whether it is a theorem
in logic and therefore whether it can be implemented in code it is
not obvious consider this example can we write a function with this
type in Scala it is not obvious can we prove this formula it is not
clear not quite obvious right now suppose I were of the opinion that
this cannot be proved but how do I show that this cannot be proved
I certainly cannot just try all possible proofs that would be infinitely
many possible proofs that would give me all kinds of other formulas
and that would give me nothing that I can stand oh how to answer these
questions so it is really a very hard question we are not going to
try to answer it on our own we were going to use the results of mathematicians
they have studied these questions for many many years for centuries
logic has been studied since ancient Greece more than 2,000 years
of study all we need to do is to find out by what name mathematicians
call this logic they are probably already studied it what kind of
logic is this that we are using that follows from the type constructions
remember and the very beginning of our consideration we started with
the type constructions that our programming languages have so that's
set of type constructions specifies the set of rules of derivation
of the logic mathematicians call this logic intuitionistic propositional
logic or IPL also they call it constructive propositional logic but
it is less frequently used most frequently used name is this and mathematicians
also call this a non classical logic because this logic is actually
different from the boolean logic that we are familiar with the logic
of the values true and false and their truth tables I assume that
you are familiar with those computations using truth tables and operations
and or not in the boolean logic so actually this logic the logic of
types as I call it or intuitionistic propositional logic is very different
from boolean logic in certain ways it's similar in other ways disjunction
for instance works very differently here's an example consider this
sequence if it has given that from a follows B plus C then either
from a follows B or from a follows C it sounds right from the common-sense
point of it if if B plus C Falls a B or C if I was I'm using plus
as a logical or so if B or C follows then it kind of makes sense either
B follows or C Falls indeed this is correct in the boolean logic which
we can find out by writing the truth table so we enumerate all the
possibilities for a B and C to be true or false or eight such possibilities
and for each of those possibilities we write the truth value of this
the truth value of this and we see from the table that whenever this
is true then this is also true in the boolean logic but this does
not hold in the intuitionistic logic for the logic of types well why
does it not hold that's counterintuitive well in fact there is very
little that's intuitive about this so-called intuitionistic logic
actually we need to think differently about this logic we need to
think can we implement an expression of this sequent so implementing
it would mean if we're given this expression we can build an expression
of this type so we're given an expression of type A to B plus C let's
say some F of this type can we build an expression of this type we
can this differently by asking can we implement a function that takes
this as an argument and returns this well we know that this is equivalent
one of our derivation rules is that if you have this sequence then
you can also have a sequence that is a function type from this to
this so for the programmer it is easier to reason about a function
taking this as an argument and returning this so how can we implement
this function this function takes F and needs to return a value of
this type so the body of this function if we could implement it and
have to construct a value of type either of something there are only
two ways of constructing a value of type either one is to construct
the left value second is to construct the right value how do we decide
whether to construct the left value or the right value we have to
decide it somehow on the basis of what information can we decide it
we don't actually have any such information what we have here is a
function from a to either BC so given some value of a of type a we
could compute f of that value and then we would have either B or C
we could decide them whether to we could take them that B or that
C but that's not what we need to return we don't need to return either
of BC we need to return either of this function or that function and
that function is not yet applied to any a it is it is too late for
us to ask what is the a we already have to return the left of this
or a right of that in other words this type either of something-something
is not itself a function of a it contains functions away but itself
it cannot be decided on the basis of any assets too late so we need
to supply a left or right so here right away immediately we have to
decide whether this will return a left or a right and we cannot really
decide that if we decide we return the left we must then return a
function from A to B so there's no way for us to construct this function
if we're given this function because this function could sometimes
return C instead of B and then we'll be stuck we cannot do this and
we can also return we cannot also return the right either so it is
impossible to implement a function of this type implication also works
a little differently in the intuitionistic logic here's an example
this holds in boolean logic but not in intuitionistic logic again
let's see why how can we compute this given this this function will
give us an e only when given an argument of this type but how can
we produce a value of this type we cannot we don't have information
that will allow us to produce a value of this type a and B are some
arbitrary types remember there is universal quantifier outside of
all this for all a and for all B we're supposed to produce this and
that is impossible we don't have enough data to produce some values
type a and so we cannot implement this function conjunction works
kind of the same as in boolean logic so here's an example this implemented
and this is also in boolean logic a true theorem now in boolean logic
the usual way of deciding whether something is true or something is
a theorem is to write a truth table unfortunately the intuitionistic
logic cannot have a truth table it cannot have a fixed number of truth
values even if you allow more than two truth values such that the
validity of formulas the truth of theorems can be decided on the basis
of the truth table this was shown by noodle and this means we should
not actually try to reason about this logic using truth values it
is not very useful even an infinite infinite number of truth values
will not help instead however it turns out that this logic has a decision
procedure or an algorithm and this algorithm is guaranteed either
to find the proof for any given formula of the internation intuitionistic
logic or to determine that there is no proof for that formula the
algorithm can also find several in equivalent proofs if there is a
theorem so a theorem could have several in equivalent proofs and since
each proof could be automatically translated into code of that type
it means we could generate several in equivalent expressions of some
type sometimes so that is the situation with this logic which we discover
if we write if we read papers about intuitionistic propositional logic
that are available in the literature and their open source projects
on the web such as the gen GHC which is a compiler plugin for haskell
this is another project doing the same thing and for Scala are implemented
occurred the Clary Howard library both of these Scala and Haskell
all of these color and Haskell projects do the same thing they take
a type of some expression for function and generate code for it automatic
by translating the type into sequence finding a proof in this logic
using the algorithm and translating that proof back into code in the
way that we have seen in an example it is interesting that all these
provers and there's a few others there's one more for the idris language
I did not mention here they all used the same decision procedure or
the same basic algorithm which is called ljt which was explained in
a paper by dick off here they all side the same paper and I believe
this is so because most other papers on this subject are unreadable
to non-specialists they are written in a very complicated way or they
describe algorithms that are too complicated so I will show how this
works in the rest of this tutorial in order to find out how to get
an algorithm we need to ask well first of all do we have the rules
of derivation that allow us to create an algorithm already here is
a summary of the axioms and the rules of derivation that we have found
so far these are direct translations of the cold expressions that
we held in the programming language in the notation of sequence now
there's one other notation for derivation rules which looks like a
fraction like this the numerator is one or more sequins and the denominator
is a sequence and this notation means in order to derive what is in
the denominator you have to present proofs for what is in the numerator
so this is the convention in the literature this fraction like syntax
or notation now we keep in mind that proofs of sequence are actually
just called expressions that have these types as some variables and
this type is the entire expression so these are directly responding
to proofs of this sequence and to the proofs of these derivation rules
and so if we have a proof that operates by combining some of these
axioms and some of these generation rules which directly translate
that back into code now the question is do these rules give us an
algorithm for finding a proof the answer is no how can we use these
rules to obtain an algorithm well suppose we need to prove some sequence
like this in order to prove it we could first see if the sequence
is one of the axioms if so then we have already proved if we know
what expression to write now in this case none of the axioms match
this so much means maybe a is a times B so B here is C and then on
the Left we must have C or you must have a times B now we don't you
don't have C on the left as we have because even that's not the same
we also don't have a times B at the premise we have a but we don't
have a times B so these rules don't match the other rules don't match
the premises and the goal either but also these rules so how can we
use them well when the writer must be an implication we don't have
an application on the right here we could try to delete some of the
premises because it's unused well actually it doesn't look like a
good idea could you read a for example and we end up with an really
hopeless sequence from B plus C we cannot get an A ever and so but
sounds hopeless so this doesn't seem to help and changing the order
doesn't seem to help much either and so we cannot find matching rules
but actually this sequence is provable just a clever combination of
what axiom to start with and what role to use and then again some
axiom and so on it will give us that time sure because I know how
to write code for this this is not difficult you have a function with
two arguments one of them is a the other is B plus C so disjunction
of either B C and we are supposed to produce a disjunction of tuple
a B or C that's easy look at this disjunction if we have a B in this
disjunction then we can produce a left of the tuple a B because we
always have an A anyway if we have a see in this disjunction then
we could return this part of the disjunction in the right of C and
we're done but unfortunately we see that the rules here do not give
us an algorithm for deciding this we need a better formulation of
the logic again mathematicians need to save us from the situation
and they have done so mathematicians have studied this logic for a
long time starting from the early 20th of the last century the first
algorithmic formulation of the logic that was found is due to Jensen
who published what he called the calculus just ignore the word calculus
it means not very much complete and sound calculus means that he came
up with some rules of derivation which are summarized here such that
they are equivalent to these they derive all the same theorems and
only the same theorems so they derive all the stuff that is right
and only that stuff they don't derive any wrong statements it's very
hard to come up with such a system of axioms and derivation rules
that are equivalent to another one in this sense also it's very hard
to prove that these are actually the rules that will give you all
the theorems that could be right in this logic that you can actually
derive all the theorems that are right yet work is already done by
mathematicians so we're not going to try to do it ourselves we're
just going to understand how these rules work now the syntax here
is slightly enhanced compared with this the enhancement is that their
names pretty cool now these are just labels they don't really do anything
in terms of sequence these help us identify which we all have has
been applied to which sequence and that's all we do so other than
that it is the same notation so the fraction such as this one means
that there is a sequence in the denominator which we will prove if
there are proofs given for sequence in the numerator in this rule
there are two sequence of them in the numerator other rules may have
one sequence in the numerator or no sequence in the numerator so these
rules that will have no previous sequence required those are axioms
this axiom means if you have an atomic X in other words it's a variable
it's a type variables not not a complicated expression just attack
variable and you can derive that same variable this is our accion
right here now why is it important that this is atomic that this is
type variable and not a more complicated expression actually not important
but it's the simplest rule that you can come up with and mathematicians
always like the most minimal set of rules so that's why they say let's
only consider this rule for the type variables X not for more complicated
expressions but we can consider this rule for any expression of course
the identity axiom well here is a truth truth axiom net which derives
the truth which is the ste symbol which I denote it by one the format
in logical notation this is the T symbol well let's just call this
one for clarity so that can be derived from any premises with no previous
sequence necessary none of these other rules now what do these other
rules do they do an interesting thing actually each of these rules
is either about something in the sequence on the left to the trans
time or something in the sequence to the right of the transplant which
I here shown in blue so these are the interesting parts of the sequence
that are being worked on or transformed by the rule so here's an example
this rule is actually two rules the eyes the index so I is one or
two another two rules just written for gravity like this with index
I and each of them says you will prove this if you prove one of if
you prove this so for example you will prove C given if you're given
a a one A two if you will prove C given just a one which makes sense
because if you can prove C given a one you don't need a two we can
ignore this a T we can already proved C from anyone so in this way
it would be proved and so all these rules work in this way you can
prove what's on the bottom of the seat of the of the fraction if you're
given proofs for what's on the top so these are eight derivation rules
and two axioms we can use this now to make a proof search how do we
do that I start with a sequence we see which rule matches that sequence
so the sequence must have something on the left and something on the
right well at least one of these it cannot be empty so it must be
something somewhere and there are only four kinds of expressions in
our logic type variables conjunctions implications and disjunctions
now notice I'm using this arithmetic arithmetic all notation for logic
just because I like it better and I will show that it has advantages
later so we take a sequence we see which rule matches one of them
won't match because either in the premise we have one of these expressions
were in the goal we have one of these expressions and then we find
the rule of match that matches we apply that rule so we now have new
sequence one or more that we will need to be proved and if they're
true then we fork the tree and now we have to prove both of them son-in
we continue doing that for each of the sequence until we hit axioms
so the tree will and this leaf or we hit a sequence to which no rule
applies in which case we cannot prove it and the entire thing is unprovable
so in the search tree there will be sequence at the nodes of the tree
and proofs will be at the edges of the tree so each node sends its
proof to the root of the tree this calculus is guaranteed by mathematicians
to be such that indeed if you cannot find a rule that applies that
means the sequence cannot be proved which was not the case here the
sequence can be proved and yet we cannot find a rule that applies
so in this calculus we can use bottom-up approach to make a proof
search as a tree here we cannot that is the advantage capitalizing
on the mathematicians results let us look at an example suppose we
want to prove this formula this theorem so first step we need to write
a sequence and this needs to be proved from no premises so we write
a sequence s0 which has an empty set of premises this is a single
now what rule applies to this sequence with your bottom up so in other
words we look at these rules and they refine which denominator matches
our sequential and our cylinders empty set on the left so all the
rules on the left cannot be applied but on the right we have an expression
which is an implication at the top level of this expression there
is this implies that so this is of the form a implies B so this rule
applies we have a sequence of the form something in our case this
is an empty set and then a implies B so we apply this rule which is
the right implication and we get a new sequence which is that what
was here before the implication is now put on the left to the trans
of the to the left of the trans time and it means that this expression
needs to be now to the left of the turnstile so now this is the sequence
s1 now we need to prove s1 well we see what rule applies to us one
well on the right there is just Q so nothing can be done of these
rules and Q is not truth so we cannot use the axiom either so let's
look at their left rules on the Left we have now an implication so
this is let's say a and this is B so we have a rule which has a implication
B on the left this is the row left implication let's apply it that
law will give us two new sequence so these two new sequence are s2
and s3 no these ones as you can check if you match a location B against
this implication Q so this is a this is B so then you get these two
sequence now we have to prove these two sequence as 2 and s 3 s 3
is easy it is just the axiom of identity it is this now as 2 again
has an implication on the left let's again apply the rule left implication
to that we get two more sequence as foreign s5 as for is this because
5 is this so now actually we are in trouble because as 2 and s 4 is
are the same sequence as 5 actually we could prove with some more
work but that won't help because we are in a situation when to prove
as two we need to prove again s 2 so that's it that's a loop that
will never give us anything it means we applied the wrong rule so
we need to backtrack this step when we apply the rule left implication
to s 2 we erase is 4 in this 5 and try a different rule to apply to
s 2 which rule can apply to s 2 well as to is this it actually has
implication on the right so we can use the right implication rule
and if we do that we get a sequence s 6 which is this and this sequence
immediately follows from the identity axiom because it has promise
are on the left and premise are and goal are on the right and that
is this axiom whatever other premises and the premise X on the left
premise X on the right and that is a type variable so that's perfect
we have done the proof as 6 follows from the axiom and therefore we
have proved s0 no more sequins need to be proved and because sequence
s0 shows this to be derived from no premises than this formula is
the theorem that's what the theorem means in the logic so that is
how we use this calculus to do proof search now we notice that we
were a bit stuck at some point we had a loop now if we are in the
loop we don't know what to do maybe we need to continue applying the
same rule maybe some new sequence come up or maybe we should not continue
it is not clear what to do and just looking at the rule left implication
shows us that it's copying this premise a implication B it is copied
into the premises of the new sequence and so it will generate a loop
assuredly after the second time you apply it however this sequence
might be new so we might need to apply it second time we don't know
that so that is a problem it will do now there have been a lot of
work trying to fix this problem and literally decades from research
by mathematicians the main ones I found were what are the off we published
in the Soviet Union who de Meyer and dick Hoff who published in the
United States over this time discovered gradually a new set of rules
which is called ljt or the calculus ljt which cures this problem of
looping the way it clears this problem is by replacing this rule left
implication through four new rules which are listed here all other
rules are kept the same from this calculus except the rule left implication
which is replaced in what way so left implication was applying it
applied to a sequence when the sequin had an implication among the
premises or on the left to the left of the turnstile the new rules
look in more detail at what is that implication so that implication
could have one of the four expressions as the argument of the implication
it could have an atomic expression as the argument it would have a
conjunction as the argument could have a disjunction as the argument
or it could have an implication as the argument in our logic there
are no more expressions except these four atomic variables conjunctions
disjunction and implications and so we have here enumerated all the
possibilities for what could be to the left of the implication in
this premise which I have here shown in the blue in blue and so for
each of these we do certain things replacing this sequence with one
or more other sequence again it's quite a lot of work to prove that
these rules are equivalent to these and also that the new rules are
somehow better they are not giving loops a lot of work which I am
NOT going to go through because that's far too complicated for the
scope so what we need suffice it to say that we have very smart people
who published on this and it is reasonably sure that this is correct
so the T in the name lgt starts stands for terminating so if we use
these rules in the same way by by creating a proof tree the proof
tree will have no loops and will terminate after a finite number of
steps and there is actually this paper that is also helpful for understanding
how to implement this algorithm and this paper shows explicitly how
to construct an integer function from sequence to integers which is
a measure of the complexity of the sequence and this measure decreases
every time you apply a rule so it strictly decreases and since this
is a strictly decreasing measure on the proof tree it means that all
the next nodes in the proof tree will have a smaller value of this
measure so eventually it will hit zero and the proof tree will terminate
at that leaf either that or you have no more rules to apply and if
you have no more laws to apply then again mathematicians have proved
it means our sequence cannot be proved so this is an important result
that we are going to use and note that this this rule is quite complicated
it does a very interesting thing it takes this expression which has
implication inside an implication and it transforms this expression
in a weird way namely the B here is separated from the C by parenthesis
but here it is not separated so this transformation is highly non-trivial
and unexpected and its validity is based on this theorem that this
in the intuitionistic logic is equivalent to this equivalent means
they're both following from the other so from this promos that and
from there follows this so this key theorem was attributed to rob
you off my dick off in this paper and this is this lemma 2 which says
that if this sorry that the this derivation is if and only if that
derivations will have these two equivalences and the proof is trivial
and the 34 is a reference to to borrow be off now when a mathematician
says that something is trivial doesn't mean that a statement is easy
to understand it doesn't mean that the proof is easy to find or that
it has trees easy to understand it means none of these things it just
means that right now for this mathematician it is not interesting
to talk about how it is done that's all it means could be for any
number of reasons for example mathematicians could just be lazy or
have no time to again explain this and so they say it's trivial don't
be don't be deceived when you see somebody says that something is
trivial in a mathematical text so to prove this one stepping stone
could be to prove this first this is an easier theorem and if you
prove this then clearly from here you can get B to C B to C you can
substitute in here you can get a to B and then you have here a to
B so in this way you can show this equivalence in one direction now
the proof of this statement is obviously trivial in order to show
the expression of this type I will use my short notation so this is
F which has this type the first argument of the function the second
is B which is at this type then we need to produce a see how do we
produce a C we apply F to an argument of this type the argument of
this type is a function that takes a and returns a B so we take some
X of type a and we return a B which was this B so we ignore this X
we just returned that B and that's the argument of F so this expression
is the proof of this sequence in other words this is the code that
has this type and therefore the proof must be available somehow so
the details of proving this theorem are left as an exercise for the
reader again when you see in a mathematical text that something is
left as an exercise for the reader it does not mean that it is easy
to do it does not mean that for you it would be a useful exercise
to do it also does not mean that the author knows how to do it it
means none of these things it just means the author doesn't feel like
doing it right now and showing it to you for whatever reason could
be because they are lazy it could be because I don't know how to do
it could be because they feel that they should know how to do it but
they don't really do know how to do it could be any of these reasons
don't be deceived when you see something like this but of course I
had to actually produce an expression function of this type in order
to implement my curry forward language because as I will show in a
moment we need to be able to implement all these has code in order
to help approver so why is that we believe the mathematicians that
the new rules are equivalent to the old rules which means that if
you find a proof using these rules somehow you should be able to find
the proof also using our initial rules which means that if you found
that proof it would easily translate that to code because each step
here is directly corresponding to a certain code expression as we
have seen at the beginning of this tutorial these cold expressions
from each of these operations so in order to do this with new rules
in other words in order to create code from proof using new rules
we need to show equivalence or we need to show how to get code out
of each of the new rules now proof of a sequence means that we have
some expression let's say T what uses variables a B and C of these
types and expression itself has type G and also as I have shown this
could be conveniently seen as a function the T as a function from
a B and C from these three arguments to the type G so for each sequencing
a proof we should be able to show either that it follows from an axiom
one of these or that it show it follows from a derivation rule and
the derivations all transforms one proof into another the axioms are
just fixed expressions as we had before the axiom that actually didn't
change between our initial formulation of the logic and the new calculus
lgt they actually did not change the derivation rules changed each
new derivation rule means that you're given expressions that prove
the sequence in the numerator one or more and you are out of these
expressions somehow you have to construct an expression that proves
this sequence now when I say an expression proves the sequence what
it means is that expression has the type that is described by the
sequence it's the same thing because we described types of expressions
through sequence and only those sequence that correspond to valid
and existing expressions in the programming language only those sequence
can be proved by the logic this is by construction so now we need
to just find what are these expressions that corresponds to each of
the derivation rules in each rule has a proof transformer function
as I call it and the proof transfer function is explicitly a function
that takes one or more expressions that are in the numerator and converts
that to the expression in the denominator that has this type so it
has an expression as it has an explicit function we need to write
down for each of the derivation rules so let's see how this is done
for these two examples of derivation laws first example have a rule
that says if you want to derive this sequence we need to derive these
two sequence now this sequence represents an expression of type C
which uses an expression of type A plus B so let's represent this
as a function from a plus B to C now we will be able to just ignore
these other premises which are common arguments and all these functions
we just pass them and we don't write them out what is the proof transformer
for this derivation rule the proof transformer for it is a function
that has two arguments t1 which is the proof of this must be a function
of type II to see and t2 which is a proof of this sequence which must
be a function of type B to see now earlier I said that sequence represent
expressions that use certain variables but equivalently we can say
these are functions that take these variables and return these expressions
that's more convenient when you implement this in code so what we
need is a function that takes a to C and B to C and returns a function
from a plus B to C and this is the code that does it we take an argument
of type a plus B and we return a match expression if it's in the left
we applied t1 to that value and we get to see if it's in the right
we apply t2 to that value and we get a C so in any case we get a syllabus
so this is a function from a plus B to C as required another example
is the proof transformer for this rule this rule has one sequence
going to one sequence so in order to transform is proof into this
we need a function that takes argument of type A to B to C to D and
returns a function of type tuple a B going to C to D so here's the
code we take a function f of type A to B to C to D we return a function
that takes a G of this type shown here in blue and return we need
to return a D so how do we get a deal we apply F to a function of
type A to B to C so we create that function out of G X of type a going
to Y of type B going to G of x1 so this is a function of type A to
B to C which is the argument of F as required and the result is of
type D so that is what we write so this kind of code is the proof
transformer for this derivation arrow and we need to produce this
proof transformers for every rule of the calculus lgt and I have done
it because I have implemented the Korea Howard library that uses LG
T so I'll must have done it for each flow this is a bit tedious because
there are many of those rules and you need to implement all this machinery
of passing arguments no matter how many in this gamma which are emitted
from this notation for brevity but in of course in the real code you
have to deal with all that too so let's see how this works on an example
because once the proof tree is found we need to start backwards from
the leaves of the tree back to the root on each step we take the proof
expression apply the proof transformer to ative according to the rule
that was used on that step we get a new proof expression and so on
so for each sequence we will get a proof expression and at the end
we'll have a proof expression for the root sequence and that will
be the answer so I will denote denote by T I the proof expressions
for the sequence s hi so starting from s6 s6 was this sequence in
our proof so I mean yes just just going through the proof example
it was here backwards from a 6 back to a 0 s-six was this it followed
from axiom identity it's proof expression t6 is a function of two
variables these two variables of these two types and this function
just returns the second variable so it's a function of RR q and r
and just denote this by our argued and Garibaldi's types r RQ variable
of this type is hard here so this function is very simple just ignores
the first argument and returns or so that is what the axiom does the
next sequence was as to as to was obtained by rule our implication
or right implication from s 6 so the proof transformer for right implication
let's look at the right implication and see what the proof transformer
must be so we are given this sequence for this expression which is
the function body the function body that uses a variable of type a
somehow out of this we need to produce a function expression that
takes an argument of type a and returns that functional body so this
is the code which is just writing a new argument returning the function
body that was our proof transformer we need to convert function body
into a function so we just write that argument and arrow in the function
body so in our case we need this as a function body and so our t2
is a function of our Q and this function is this the sequence s 3
followed from the axiom and so it was just this function this is just
the identity function then we used the left implication so this was
actually still done in the calculus algae but the same thing works
in the calculus lgt I'm just using algae because it's simpler for
example here proof transformer for the left implication is a little
more complicated and so if you look at it what what does it have to
be it takes these two expressions and returns this expression so it
takes a function from A to B to a and from B to C and it returns a
function from A to B to see how does it do it given a function a to
b you use this to derive a from it then you substitute that a into
the function into B you get a B when you use this to derive see from
that B and that's your C so you use this function a to be twice you
put it in here once and then you get an A and substitute back into
the same function when you get a B then you use that and that's exactly
what the proof transformer does it takes this rrq and it uses it twice
substitutes into it something that was obtained from one of the terms
and then uses the second term on the result so then this is the proof
transformer for the rule left implication the result of the proof
transformation is the proof for the sequence s1 finally we use the
right implication again which is just this function construction and
we get the proof expression for the sequence s0 now this proof expression
is written through these t1 t2 t3 we have to substitute all this back
in order to get the final expression so if we substitute first of
all we find this is our our cubone going to tea one of our cutie one
of our queue is this so we have to put it here now t3 is just identity
so we can just remove that so that gets you riq going to our Q of
T 2 T 2 is less if I have to put it in T 6 is just identity on R so
this is our going to our and so finally you have this expression so
that is the final code that has the required type notice that we have
derived this code completely algorithmic to it there was no guessing
we found which rules applied to the sequence with transformed sequence
according to the rules once we found the proof which was if we use
the calculus ljt the proof will be just a finite tree with no loops
it will terminate you can get an exhaustive depth-first search for
it for example and you find all the possible proofs if you want as
well well you will find many in any case in some for some expressions
and then we use the proof transformers which are fixed functions that
you can upfront compute for each these expressions are proof transformers
applied to the previous proofs so these are completely fixed algorithmically
fixed so we have derived this code completely algorithmically given
this expression this type so it is in this way that the career Howard
correspondence allows us to derive the code of functions from there
type signatures another important application of the correspondence
is to analyze type by some morphisms or type equivalences and I was
led to this by asking the question so in this logic or in the types
are these operations plus and times as I denoted them more like logic
more like the disjunction and conjunction or are they more like arithmetic
plus and times because this is kind of not so clear right away our
logic is this intuitionistic logic it in any case this is different
from boolean logic so what are the properties of these types really
so are the properties such that it is better to think about these
operations as plus and times rather than logical conjunction and disjunction
can answer this question I looked at identities that we have in the
water these are some identities from simple ones obvious ones to less
obvious identities like this the equal sign here stands for implication
in both directions so both this implies that and vice versa because
of this each of the implications means a function so since these are
all identities in logic it means that for example the implication
from here to here is a theorem of logic and so it can be implemented
as we know all our identities in logic can be implemented in code
and we even have an algorithm now that can automatically produce proofs
and automatically produce code so that means for any of these identities
that has some ik some expression X on the left and some Y on the right
so some kind of X equals y we have X implies Y and y implies X if
we convert that to code we will have a pair of functions function
from X to one and the function from Y to X what do these functions
do well they convert values in some ways from type X to type Y and
back so do these functions Express the equivalence of the types x
and y so that any value of type X can be converted to some equivalent
value type while and back without any loss of information is that
so that was the question I asked I looked at some examples well first
what does it mean more rigorously that types are equivalent for as
mathematicians say isomorphic the types are isomorphic and we will
use this notation for that if there is a one-to-one correspondence
between the sets of values of these types and in order to demonstrate
that we need a pair of functions one going from A to B the other going
from B to a such that the composition of these functions in both directions
is equal to identity function so F compose G or F value G will give
you from A to B and then from B to a is back so that would be identity
of a to a this will be identity of B to B if this is true if the composition
is identity it means we indeed did not lose any information let's
consider an example this is an identity in the logic a conjunction
with one is equal to a in Scala the types responding to the left and
the right hand sides of this conjunction all of this are equivalent
are the conjunction of a and unit and a itself now we need functions
with these types indeed we can write functions is having these types
a pair of a and unit we need to produce an a out of that we'll just
take the first element of the pair you are done take an X of type
a will produce tuple of a and unit very easy just put a unit value
in the tuple in here done and it's easy to verify that composition
of these functions will not change any values so it will be identity
in both directions another example this is an identity in logic if
this is understood as a disjunction one or a or true or a is true
that is an identity in logic for theorem in the logic are the types
equivalent though the type for 1 plus a is the option in Scala it
is option in Haskell at is called maybe this type is standard library
type in pretty much every functional programming language now option
of a is a disjunction of one or unit and a it is certainly not equivalent
to just unit because this type could contain a value of a in it but
this could not so there is no way that you could transform this type
to this and then back without losing information you could transform
so since this is a theorem you have functions from this type to this
type and back some functions you have them but these functions do
not compose to identity they cannot because what if you had a here
you must map it into unit from this unit back you must map into this
unit you cannot get an a out of unit and so that will erase this information
and that cannot become isomorphism so we see that some logic identities
do yield isomorphism types but others do not why is that let's look
at some more examples to figure out why in all these examples we can
implement functions F 1 and F 2 between the two sets to two types
in both directions and then we can check we certainly can implement
them because these are logical identities but then we can check if
the compositions are identity functions and if so the types are isomorphic
but we find that in the first three examples we can do it but in this
last example we can note now I have written the logical identities
logical theorems with the arithmetic notation I call this arithmetical
notation because this suggests arithmetic operations plus and times
and if you look at these identities this looks like a well-known algebraic
identity from the school algebra in this too but this certainly seen
your own as an arithmetic as an as an arithmetic identity this is
certainly not true in arithmetic it is true in logical if you replace
this with disjunction and this with conjunction this is an identity
in logic so this suggests an interesting thing if you replace disjunction
by plus and conjunction by x and the result is an identity in arithmetic
then it is an isomorphism of types otherwise it is not let's see why
this is so indeed this is so I call this the arithmetic arithmetic
oh very hard correspondence to see how it works let's consider only
the types without loss of generation of generality that have a finite
set of possible values for example a boolean type has only two possible
true and false integer let's say in the computers all the integers
are fine nights ago so those types have a finite set of possible values
and this does not limit our generality because in the computer everything
is finite all types have a finite set of possible values now let's
consider how many values a given type has so that would be the size
of the type or using the mathematical terminology it's called a cardinality
of the type so let's see what is the cardinality of various type constructions
the sum type for example if the cardinality of types a and B is known
and the cardinality of a plus B the sum type the disjunction of a
and B is the sum of the two cardinalities or sizes this is because
a value of the disjunction type is constructed as either a value of
the first part or a value of the second part and so you cannot have
both together and so obviously the different number of values is just
the sum of the two sizes that the number of different values of the
sum type is just the sum of the numbers of different values of types
a and B for the product type again we have an interesting thing it's
the arithmetic product of the sizes of a and B because for every a
value you could have an arbitrary B value so this is a direct product
or transient product of sets and we have school level identities about
the operations plus and times such as these identities or these all
of these identities are valid for arithmetic and they show if you
translate that into statements about the sizes of types they show
that the size of the type on the left is equal to the size of the
type on the right and that is very suggestive in other words if you
take a identity like this and you compute the size of the type on
the left and the size of the type on the right you get an arithmetic
identity of the sizes but you don't get that identity here because
the earth medical formula is not right this is very suggestive if
the sizes are equal and maybe the types are equivalent or isomorphic
when the sizes are not equal then certainly they cannot be equivalent
the function type very interestingly also is described in the same
way it provides the set of all maps between the two sets of values
so for example from integer to boolean that would be all the functions
that take some integer and return some boolean so that's and a number
of boolean values \textasciicircum{} the number of integer values
that's how many different functions you can have as a combinatorial
number so it's an exponential and so the size of the type of function
a to be is the size of the type of B \textasciicircum{} the size of
type of a and again we have all the school identities about powers
and how to multiply powers and so on and they are directly translated
into these three identities if you take the sizes of the types on
the left and on the right the sizes will be equal due to these three
identities since the sizes are equal it's very likely that the type
our actual equivalent so far haven't seen any counter examples to
this in these constructions so this gives us a meaning of the Curie
Howard correspondence so far we have seen three facets of the curly
Howard correspondence one is the correspondence between types and
logical formulas two is the correspondence between code and proofs
and three the correspondence between the cardinality of a type or
the set size of the type and the arithmetic identities that we have
in the school algebra about these types so arithmetical identities
signify type equivalence or isomorphism while logic identities only
talk about how you create some value of this type out of value of
another type so that does not guarantee that it preserves information
it just guarantees that you can implement some function of that type
it doesn't tell you that the function will be an isomorphism so if
one type is logically equivalent to another it means are equally implementable
if one is implementable another is also implementable but no more
than that whereas arithmetical identities actually tell you about
isomorphism of types therefore if you look at types and write them
using my preferred notation which is using the arithmetic all symbols
instead of logical symbols instead of these I'll use these symbols
if I do that this is very suggestive of a possible isomorphism of
types then it becomes very easy for me to reason about types I can
see right away that these two are isomorphic types or that these two
are isomorphic types because I am used to looking at school algebra
it's very obvious then that this is not an isomorphism of types because
this doesn't make sense in the school algebra so reasoning about isomorphic
types is basically school level algebra involving polynomials and
powers so if you are familiar with all these identities as you should
be it will be very easy for you the reason about what types are equivalent
as long as all these types are made up of constants or primitive types
disjunctions tuples or conjunctions and functions which will then
directly be translated into exponential polynomial expressions constants
sums products and expand powers or Exponential's so I call these exponential
polynomial types that is types built up from these type constructions
so all we have been talking about in this tutorial is what I call
exponential polynomial types these are the basic type constructions
that I started with tuple product function exponential disjunction
some unit constant or 1 now just one comment that in the functional
programming community today there is a terminology algebraic types
so people usually call algebraic types the types that are made from
constant types sums and products excluding Exponential's I do not
find this terminology it's very helpful I find it confusing because
what is particularly an algebraic about these identities these are
identities of school algebra the properties of the function type are
described by algebraic identities like this so it would be strange
to call the function type not algebraic whereas these types are algebraic
they are very similar to each other in terms of their properties being
described by identity is known from school algebra so instead of algebraic
types I would prefer to say polynomial types this is much more descriptive
and precise and if you want to talk about function types as well then
you just can you can just say exponential polynomial types or exfoli
types for short so by way of summarizing what we have done so far
what are the practical implications of the career Howard correspondence
so one set of implications is actually for writing code and reason
and eternal code one thing we can do now is if we're given a function
with some type and usually this will be typed with type parameters
all type trainers fully parametric types such as the function we have
been considering here all these functions do not have any types that
are specific like integer or string all the types are fully parametric
and then there are some constructions some type expressions made out
of these types so these are what I call fully parametric functions
for these functions we have a decision procedure an algorithm that
based on the ljt calculus which decides whether this function can
be implemented in code and computer scientists a type is inhabited
if you can produce a value of this type in your program so CH of T
is this proposition which they call type is inhabited and I prefer
to call it just that you can compute a value of this type or code
has the type O code can create a value of this type and so we have
a algorithm that can also generate the code from type when it is possible
if it is not possible the algorithm will tell you so often not always
but often this algorithm can be used actually to generate the code
you want we can also use what I call the arithmetic of glory Harvard
correspondence to reason about type isomorphisms and to transform
types isomorphic we simplify type expressions just like we simplify
expressions in school level algebra by expanding brackets by permuting
the order of terms like a plus B is equal to B plus a or associativity
a times B all times C can be expanded and so on so this allows us
once we have written types in the short notation in the notation that
I prefer which resembles school algebra because it uses the plus and
times symbols instead of the logic symbols so once we rewrite our
types and this notation which I have been doing consistently in this
tutorial it enables us the reason very easily but which types are
equal or isomorphic because we are all familiar with the school level
algebra what are the problems that we cannot solve using this knowledge
one thing we cannot do is to generate code automatically such that
it will be an isomorphism so for instance in an example here we are
able to generate automatically the code of these functions but it
will not be an isomorphism and the lgt algorithm cannot check that
this is nice a morphism that's the important thing this algorithm
does not know about equations or isomorphisms it only knows that it
found some code that has the type you wanted whether this code is
useful to you or not we don't know the algorithm doesn't know this
also if the algorithm finds several such several proofs of a sequence
it will generate several not in equivalent versions of your code it
doesn't know which one is is useful maybe some of them are useless
maybe not the algorithm cannot automatically decide that in general
another thing we cannot do is to express complicated conditions via
types such as that array is sorted the type system is not powerful
enough in all the languages I listed you need a much more powerful
type system such as that in the programming language interests or
add them or cook those are much more powerful type systems that can
express such complicated conditions but for those type systems there
is no algorithm that will generate code another thing we cannot do
is to generate code that has type constructors such as the map function
here's an example in Scala this is a map function on a list so there's
the list of a a is a type parameter and then we say dot map and map
has another type frame to be it takes a function from A to B for any
B so a is fixed but now from any B we can take a function from A to
B and generate a list of B so if we wrote this formula in the short
type notation this would look something like this I'm writing subscript
a because this is a type parameter so this is like an argument or
a type parameter I'm writing it like this and then from this this
is the first argument of the function and then there is a second argument
which is this F and that is another quantifier for B inside parentheses
so this formula has a quantifier inside so far we have been dealing
with formulas that have all quantifiers outside and so we never write
quantifiers explicitly but here we have to write them inside this
is a more powerful logic which is called first-order logic in other
words this is a logic where you have quantifiers anywhere in the formula
including inside the formula unfortunately this logic is undecidable
so there is no algorithm that we can use either to find the proof
and therefore code freedom type or to show that there is no proof
no code so we're kind of stuck in all these directions some more remarks
about the curry Harvard correspondence first is that only with parameterize
types we can get some interesting information out of it if we take
concrete types like integer then the proposition CH event meaning
that our code can have a value of type int it that's always true can
always write any some integer value we don't need any previous data
for it so for all specific types all these propositions are always
choice completely void of information the only interesting part comes
when we start considering type variables if we start asking can we
make a type which is either of a B going to a going to B in soon for
all a B once we start doing this with type parameters a B and so on
then we get interesting information as we have seen in this tutorial
another remark is that functions like this one are not sufficiently
described by their type so that this is the type of integer going
to integer now looking at this type we can put this into a sequence
but we'll never get enough information to actually get this function
so only certain class of functions which are fully typed biometric
their type signature is informative enough so that we can derive code
automatically only in much more powerful type systems you can have
type information that is enough to specify fully a code like this
another caveat is that I don't know the proof that arithmetic identity
guarantees the type equivalence it is certainly a necessary condition
because if two types have different cardinality or different size
of their sets of values that they cannot be equivalent or they cannot
be isomorphic so this is a necessary condition but it's not a sufficient
condition it looks like I don't know if this is sufficient I haven't
seen any counter examples so far final remarks about type correspondence
the logical constant false did not appear in any of my slides so far
this was on purpose it has extremely limited practical use in programming
languages because actually we have types corresponding to false Scala
has type called nothing Haskell has type usually called void that
corresponds to the logical constant false what does it mean CH of
nothing is false it means your code can never have a value of type
nothing or in Haskell void you can never compute a value of this type
so clearly it has a very limited practical significance you will never
be able to compute any values of this type ever in any program it's
identically falseness this constant so if you want to add it to the
logic it's very easy you just have one rule and you're not done you
can derive things with it if you want but they will have almost never
any use in practical code also we did not talk about negation none
of the calculus calculate that I should have in logical negation as
in operation again for the same reason we do not have a programming
language construction that represents logical negation negation by
definition is like this is an application from 8 to 4 so that's not
a not a means from a follows falsehood now since you cannot ever get
false in a programming language you cannot really implement this function
in any useful sense and so i have seen some haskell library that used
this type void as a type parameter in some way but certainly it's
a very limited and rare use and so it is not really lumen 18 to include
negation it could probably find some very esoteric uses of it but
almost never useful and finally there is another set of important
implications from the Kurihara correspondence these are implications
for people who want to design new programming languages as we have
seen the Karaka with correspondence maps the type system of a programming
language into a certain logical system where prepositions follow from
each other or can be proved from each other and this enables us to
reason about programmed to see what kind of code can be written if
some other kind of code can be written and logical reasoning is very
powerful it's simpler than trying to write code and it gives you algorithms
and all kinds of mathematical results that have been found over the
centuries so languages like those listed here have all the five type
constructions that I wasted in the beginning of this tutorial and
mapping them into logic gives a full constructive logic or full intuitionistic
logic with all logical operations and or so conjunction disjunction
implication and the truth constant whereas languages such as C C++
Java and c-sharp and so on they're mapped to incomplete logics because
they do not have some of these operations for instance they do not
have type constructions of correspond to disjunction we also do not
have the true constant or the false constant so they are mapped to
a logic that lacks some of the foundational logical operation so it
can be only fewer theorems can be proved in that logic and so your
reasoning about theory types is hampered languages called scripting
languages sometimes such as Python or JavaScript will be and so on
also our belongs there in that line those languages only have one
type they actually don't check types at compile time and so they're
mapped to logics with only one proposition those logics are extremely
small in terms of what kind of things you can reason about and so
if you write a program in these languages you are completely unable
to reason at the level of types whereas in these languages you are
able to reason but in a limited way you're not having a complete logic
so this suggests a principle for designing the type system in a new
programming language the first step would be to choose a good and
complete logic that is free of inconsistency mathematicians have studied
all kinds of logics and they are always interested in questions such
as is this logic consistent consistent means you cannot derive false
from true is this logic complete can you derive all things that are
true are there enough axioms and rules of derivation or maybe there
are too many axioms and rules of derivation you can delete some of
them and have fewer mathematicians have always been interested in
such questions they found all kinds of interesting logics where you
can derive a lot of interesting theorems non trivial theorems and
they found the minimum sets of axioms and rules of derivations for
these logics use their results take one of the logics that they do
them and develop such as intuitionistic logic model logic temporal
logic linear logic and so on take one of these logics for each of
the basic operations of this logic provide type constructions in your
programming language that are easy to use for instance your logic
has disjunction implication or something else provide a type constructor
for each of them that's easy to use easy to write down such as provided
by the languages we have seen then every type will be mapped to a
logical form of the OPF logical formula for every type and there will
be a type for every logical formula and then for each rule of the
new logic for each derivation rule there should be a construct in
the code that corresponds to it so that you could transform proofs
in logic into code and code into proofs if you do that your language
will be faithful to the scorecard correspondence you will be able
to use logic to reason about your language and one important result
at this level while we have seen that you can sometimes generate code
that is maybe nice but a very important result is that if your logic
is free of inconsistency it means that no program will ever be able
to derive an inconsistent an inconsistent type means that you had
a function that requires some type a but it was called with a different
type beam which is incompatible and that basically crashes so in languages
like C and C++ we have all kinds of crashes like a segmentation fault
in Java the exceptions nullpointerexception or class cast exception
which happens when you call a function on the wrong type of argument
and that happens if your logic is inconsistent if your logic can derive
incorrect statements from correct premises then if you translate that
derivation into code and the that code will derive incompatible type
at the wrong place and it will crash the crash will happen at runtime
the compiler will not catch this inconsistency because the compiler
only checks the logic of types and the logic checks out you have followed
the rules of derivation of the logic the compiler can check out all
these logical rules but the compiler does not know that your logic
is inconsistent maybe and then it will deep have derived an inconsistent
result falsehood from truth for example and that will crash at runtime
now we know that crashing at runtime is not a good outcome so in fact
languages like Oh camel have been studied and for other languages
some subsets of Haskell I believe called safe Haskell have been studied
and it has been shown that they cannot crash and they're the way to
show it mathematically is to use the fact that they are based on a
complete and consistent logic and then all you need to show is that
your compiler does not have some critical bugs that allow it to oversee
that you have not followed the derivation rules of the logic that
is an extremely valuable feature of functional programming languages
that are based on the Curie habit correspondence you can prove their
safety at compile time or at least exclude a large number of possible
bugs and errors certainly these languages are quite large and they
include features that are not covered by the Carey Hart correspondence
type constructors that I have not considered in this tutorial and
those might may not be safe but at least the foundation of these languages
the foundation of the type system will be safe so that is the final
lesson from the great Howard correspondence this concludes the tutorial 
\end{comment}

