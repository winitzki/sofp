
\chapter{Applicative functors and contrafunctors\label{chap:8-Applicative-functors,-contrafunctors}}

\global\long\def\gunderline#1{\mathunderline{greenunder}{#1}}%
\global\long\def\bef{\forwardcompose}%
\global\long\def\bbnum#1{\custombb{#1}}%
\global\long\def\pplus{{\displaystyle }{+\negmedspace+}}%


\section{Motivation and first examples}

In previous chapters, we generalized the \lstinline!map!, \lstinline!filter!,
and \lstinline!flatMap! methods from sequences to other type constructors
that support such methods obeying suitable laws. Following the same
path, we now turn to the \lstinline!zip! method. Although \lstinline!zip!
is most often used with sequences, many other type constructors can
also have a suitable \lstinline!zip! method. Those type constructors
are known as\textbf{ applicative}.

\subsection{Generalizing the \texttt{zip} method from sequences to other types}

Chapter~\ref{chap:2-Mathematical-induction} showed the use of the
\lstinline!zip! operation for sequences and other collections. For
lists, the \lstinline!zip! method is equivalent to a function with
the following type signature:
\begin{lstlisting}
def zip[A, B](la: List[A], lb: List[B]): List[(A, B)]
\end{lstlisting}
\[
\text{zip}:\text{List}^{A}\times\text{List}^{B}\rightarrow\text{List}^{A\times B}\quad.
\]
It turns out that a broad class of type constructors $L$ can have
a similar \lstinline!zip! method:
\[
\text{zip}:L^{A}\times L^{B}\rightarrow L^{A\times B}\quad.
\]
Using this type signature, the \lstinline!zip! operation may be implemented
for many type constructors, not only for \lstinline!List!-like collections.
In order to ensure that the implementation of \lstinline!zip! is
useful and safe, we will establish and verify the laws of the \lstinline!zip!
operation later in this chapter. For now, let us look at some examples
of implementing a \lstinline!zip! operation.

\subsubsection{Example \label{subsec:Example-applicative-not-monad}\ref{subsec:Example-applicative-not-monad}\index{examples (with code)}}

A \lstinline!zip! method can be implemented for the type constructor
$L^{A}\triangleq\bbnum 1+A\times A$:
\begin{lstlisting}
type L[A] = Option[(A, A)]
def zip[A, B](la: L[A], lb: L[B]): L[(A, B)] = (la, lb) match {
  case (None, _) | (_, None)              => None
  case (Some((a1, a2)), Some((b1, b2)))   => Some(((a1, b1), (a2, b2)))
}

scala> zip(Some((123, 456)), Some(("abc", "def")))
res0: L[(Int, String)] = Some(((123, "abc"), (456, "def")))
\end{lstlisting}


\subsubsection{Example \label{subsec:Example-applicative-tree}\ref{subsec:Example-applicative-tree}}

A \lstinline!zip! method can be implemented for a binary tree. We
define the type constructor \lstinline!BTree[A]! by this code:
\begin{lstlisting}
sealed trait BTree[A]
final case class Leaf[A](a: A) extends BTree[A]
final case class Branch[A](left: BTree[A], right: BTree[A]) extends BTree[A]
\end{lstlisting}
The \lstinline!zip! method should have the following type signature:
\begin{lstlisting}
def zip[A, B](ta: BTree[A], tb: BTree[B]): BTree[(A, B)] = ???
\end{lstlisting}
We would like to preserve the tree structure of \lstinline!ta! and
\lstinline!tb! as much as possible, zipping together subtrees of
equal shape without adding any new branchings. For example, {\tiny{}}zip({\tiny{} \Tree[ [ [ $a$ ] [ $b$ ] ]  [ $c$ ] ] }, {\tiny{} \Tree[ [ [ $d$ ] [ $e$ ] ]  [ $f$ ] ] })
should evaluate to the tree {\tiny{}}{\tiny{} \Tree[ [ [ $a\times d$ ] [ $b\times e$ ] ]  [ $c\times f$ ] ] }.
If a subtree of \lstinline!ta! is a \lstinline!Leaf(x)! while the
corresponding subtree of \lstinline!tb! is a \lstinline!Branch!,
the value \lstinline!x! must be replicated to match the subtree of
\lstinline!tb!. So, the result of evaluating {\tiny{}}zip({\tiny{} \Tree[ [ $b$ ] [ $c$ ] ] }, {\tiny{} \Tree[ [ [ $a$ ] [ [ $d$ ] [ $e$ ] ] ]  [ $f$ ] ] })
should be {\tiny{}}{\tiny{} \Tree[ [ [ $b\times a$ ] [ [ $b\times d$ ] [ $b\times e$ ] ] ]  [ $c\times f$ ] ] }
with replicated $b$.

\subparagraph{Solution}

Begin writing the pattern-matching code: 
\begin{lstlisting}
def zip[A, B](ta: BTree[A], tb: BTree[B]): BTree[(A, B)] = (ta, tb) match {
  case (Leaf(x), Leaf(y))                 => Leaf((x, y))
  case (Branch(lx, rx), Leaf(y))          => ???
  case (Leaf(x), Branch(ly, ry))          => ???
  case (Branch(lx, rx), Branch(ly, ry))   => ???
}
\end{lstlisting}
The type constructor \lstinline!BTree! is a functor and has a \lstinline!map!
method:
\begin{lstlisting}
def map[A, B](ta: BTree[A])(f: A => B): BTree[B] = ta match {
  case Leaf(a)          => Leaf(f(a))
  case Branch(ta, tb)   => Branch(map(ta)(f), map(tb)(f))
}
\end{lstlisting}
When \textsf{``}zipping\textsf{''} a \lstinline!Leaf! with a \lstinline!Branch!,
we use the \lstinline!map! method to replicate the value from the
leaf:
\begin{lstlisting}
  case (Branch(lx, rx), Leaf(y))  => map(Branch(lx, rx))(x => (x, y))
  case (Leaf(x), Branch(ly, ry))  => map(Branch(ly, ry))(y => (x, y)) 
\end{lstlisting}
For the case of two \lstinline!Branch! values, we use two recursive
calls to \lstinline!zip!:
\begin{lstlisting}
  case (Branch(ax, ay), Branch(bx, by))   => Branch(zip(ax, bx), zip(ay, by))
\end{lstlisting}
The final code of the \lstinline!zip! method, after some simplifications,
becomes:

\begin{lstlisting}
def zip[A, B](ta: BTree[A], tb: BTree[B]): BTree[(A, B)] = (ta, tb) match {
  case (Leaf(x), Leaf(y))                 => Leaf((x, y))
  case (xa, Leaf(b))                      => map(xa)(a => (a, b))
  case (Leaf(a), xb)                      => map(xb)(b => (a, b))
  case (Branch(ax, ay), Branch(bx, by))   => Branch(zip(ax, bx), zip(ay, by))
}
\end{lstlisting}
To test our code, let us run the given examples and verify that we
get the required results:
\begin{lstlisting}
val ta: BTree[Int] = Branch(Branch(Leaf(1), Leaf(2)), Leaf(3))
val tb: BTree[String] = Branch(Leaf("b"), Leaf("c"))
val tc: BTree[String] = Branch(Branch(Leaf("a"), Branch(Leaf("d"), Leaf("e"))), Leaf("f")) 

scala> zip(ta, ta)
res0: BTree[(Int, Int)] = Branch(Branch(Leaf((1, 1)), Leaf((2, 2))), Leaf((3, 3)))

scala> zip(tb, tc)
res1: BTree[(String, String)] = Branch(Branch(Leaf(("b", "a")), Branch(Leaf(("b", "d")), Leaf(("b", "e")))), Leaf(("c", "f"))) 
\end{lstlisting}
$\square$

The \lstinline!zip! operation is sometimes defined even for type
constructors that are \emph{not} functors:

\subsubsection{Example \label{subsec:Example-applicative-profunctor}\ref{subsec:Example-applicative-profunctor}}

A pair of monoids is also a monoid (the \textsf{``}products\textsf{''} construction
of Section~\ref{subsec:Monoids-constructions}). A \lstinline!Monoid!
typeclass instance for a type \lstinline!A! can be represented by
a value of type $L^{A}$, where the type constructor $L$ is defined
by:

\begin{wrapfigure}{l}{0.5\columnwidth}%
\vspace{-0.85\baselineskip}
\begin{lstlisting}
type L[A] = (A, (A, A) => A)
\end{lstlisting}

\vspace{-0.25\baselineskip}
\end{wrapfigure}%

\noindent \vspace{-0.5\baselineskip}
\[
L^{A}\triangleq A\times\left(A\times A\rightarrow A\right)\quad.
\]

\noindent We can write a function that creates a monoid typeclass
instance for the type $A\times B$ when $A$ and $B$ are monoids.
That function has the type signature of a \lstinline!zip! method
for the type constructor $L$:
\begin{lstlisting}
def zip[A, B](la: L[A], lb: L[B]): L[(A, B)] = ( (la._1, lb._1),   // The result has type ((A, B),
  { case ((a1, b1), (a2, b2)) => (la._2(a1, a2), lb._2(b1, b2)) }  // ((A, B), (A, B)) => (A, B)).
)
\end{lstlisting}
Applying this function to any two monoid instances, we obtain an instance
value for the pair:
\begin{lstlisting}
def monoidPair[A, B](implicit ma: Monoid[A], mb: Monoid[B]): Monoid[(A, B)] = zip(ma, mb)
\end{lstlisting}

Several typeclasses (such as the \lstinline!Monoid!) have instances
whose type constructors are neither covariant nor contravariant yet
have a \lstinline!zip! method. If we implement a \lstinline!zip!
method for typeclass instances, we will obtain automatic typeclass
derivation for product types (e.g., case classes).

\subsection{Gathering all errors during computations\label{subsec:Programs-that-accumulate-errors}}

A monadic program using pass/fail monads must stop at the first failure:
the code \lstinline!flatMap(x => expr)! cannot start evaluating \lstinline!expr!
if a previous computation failed to produce a value for \lstinline!x!.
However, if some pass/fail computations are independent of each other\textsf{'}s
results, we may wish to run all those computations and gather all
errors.

As an example, consider the task of implementing \textsf{``}safe arithmetic\textsf{''}
where a division by zero or square root of a negative number will
give error messages (see Example~\ref{subsec:disj-Example-resultA}).
To be specific, let us perform the computation \lstinline!(1 / 0) + (2 / 0)!
in the \textsf{``}safe arithmetic\textsf{''}. A monadic implementation (see Section~\ref{subsec:Pass/fail-monads})
will stop the computation after the first error:
\begin{lstlisting}
type Result[A] = Either[String, A]
def add(x: Int, y: Int): Result[Int] = Right(x + y)
def div(x: Int, y: Int): Result[Int] = if (y == 0) Left(s"error: $x / $y") else Right(x / y)

scala> for {
         x <- div(1, 0)
         y <- div(2, 0)
         z <- add(x, y)
       } yield z
res0: Either[String, Int] = Left(error: 1 / 0)
\end{lstlisting}
We notice that the two \lstinline!div! operations do not depend on
each other and may be computed separately. To achieve this, we define
a \lstinline!map2! function for the type constructor \lstinline!Result!:
\begin{lstlisting}
def map2[A, B, C](ra: Result[A], rb: Result[B])(f: (A, B) => C): Result[C] = (ra, rb) match {
  case (Left(e1), Left(e2))     => Left(e1 + "\n" + e2)    // Messages are separated by a newline.
  case (Left(e1), _)            => Left(e1)
  case (_, Left(e2))            => Left(e2)
  case (Right(a), Right(b))     => Right(f(a, b))
}
\end{lstlisting}
 We can now use the \lstinline!map2! function to compute the two
\lstinline!div! operations and gather the errors:
\begin{lstlisting}
scala> for {
         p <- map2(div(1,0), div(2,0)) { (x, y) => (x, y) }    // Create a tuple (x, y).
         z <- add(p._1, p._2)
       } yield z
res1: Either[String, Int] = Left(error: 1 / 0
error: 2 / 0)
\end{lstlisting}
The result of \lstinline!map2! is used in further monadic computations.
In this way, we can combine code that gathers many errors with ordinary
pass/fail monadic code that stops at the first error.

This example can be generalized to the type \lstinline!Either[E, A]!,
where the type \lstinline!E! is a semigroup with a \lstinline!combine!
operation (denoted by \lstinline!|+|!). In other words, error messages
of type \lstinline!E! can be combined in some way. Let us implement
a \lstinline!zip! method that works similarly to \lstinline!map2!
by gathering all error messages:
\begin{lstlisting}
def zipE[A, B, E: Semigroup](x: Either[E, A], y: Either[E, B]): Either[E, (A, B)] = (x, y) match {
  case (Left(e1), Left(e2))   => Left(e1 |+| e2)
  case (Right(_), Left(e2))   => Left(e2)
  case (Left(e1), Right(_))   => Left(e1)
  case (Right(a), Right(b))   => Right((a, b))
}
\end{lstlisting}
In the code notation, the \lstinline!zipE! function is written like
this:
\[
\text{zip}_{E}:(E+A)\times(E+B)\rightarrow E+A\times B\quad,\quad\quad\text{zip}_{E}\triangleq\,\begin{array}{|c||cc|}
 & E & A\times B\\
\hline E\times E & e_{1}\times e_{2}\rightarrow e_{1}\oplus e_{2} & \bbnum 0\\
A\times E & \_\times e_{2}\rightarrow e_{2} & \bbnum 0\\
E\times B & e_{1}\times\_\rightarrow e_{2} & \bbnum 0\\
A\times B & \bbnum 0 & \text{id}
\end{array}\quad.
\]

When both arguments of \lstinline!zipE! have error messages, the
code does not drop one of the errors and return \lstinline!Left(e1)!,
say (although that would still conform to the type signature of \lstinline!zipE!).
Instead, our code combines both error messages, preserving more information.

Comparing the code of \lstinline!map2! and \lstinline!zip!, we find
only one difference: the code of \lstinline!zip! can be obtained
from the code of \lstinline!map2! if we replace \lstinline!f! by
an identity function. We will see later that this correspondence between
\lstinline!zip! and \lstinline!map2! works for all applicative functors.

\subsection{Monadic programs with independent effects\label{subsec:Monadic-programs-with-independent-effects-future-applicative}}

Another motivation for applicative functors comes from considering
monadic programs where some source lines do not depend on previous
results. As an example, consider the following monadic program involving
\lstinline!Future!:
\begin{lstlisting}
val result: Future[Int] = for {
  x <- Future { func1() }  // func1() returns an Int.
  y <- Future { func2() }  // Similarly for func2().
  z <- Future { func3() }  // Similarly for func3().
} yield x + y + z
\end{lstlisting}
Recall that Scala\textsf{'}s \lstinline!Future! values are computations that
have been scheduled to run on other threads (and may be already running).
In this example, the computations in \lstinline!func1()!, \lstinline!func2()!,
and \lstinline!func3()! do not depend on the previously computed
values (\lstinline!x! and \lstinline!y!). So, we would like to run
those computations in parallel. However, the monadic program will
create a second \lstinline!Future! value and schedule the computation
\lstinline!func2()! only after \lstinline!func1()! is done. Similarly,
\lstinline!func3()! will not start until \lstinline!func2()! is
done. This is because the functor block shown above is translated
into \lstinline!flatMap! and \lstinline!map! methods like this:
\begin{lstlisting}
val result: Future[Int] =
  Future { func1() }.flatMap { x =>
    Future { func2() }.flatMap { y =>
      Future { func3() }.map { z => x + y + z } } }
\end{lstlisting}
The implementations of \lstinline!flatMap! and \lstinline!map! cannot
detect whether the next computations actually depend on any previously
computed values. So, the code of \lstinline!flatMap! and \lstinline!map!
\emph{must} wait until all previous values are computed. We need a
new function that can take advantage of the independence of effects
on previous values. A possible type signature for that function is:
\begin{lstlisting}
def map3(f1: Future[Int], f2: Future[Int], f3: Future[Int], f: (Int, Int, Int) => Int): Future[Int] 
\end{lstlisting}
Generalizing this type signature to arbitrary types, we obtain the
following \lstinline!map3! method:
\begin{lstlisting}
def map3[F[_], A, B, C, D](f1: F[A], f2: F[B], f3: F[C])(f: (A, B, C) => D): F[D]
\end{lstlisting}
It is straightforward to implement this method for \lstinline!F = Future!:
\begin{lstlisting}
def map3[A, B, C, D](f1: Future[A], f2: Future[B], f3: Future[C])(f: (A, B, C) => D): Future[D] =
  for { x <- f1; y <- f2; z <- f3 } yield f(x, y, z)
\end{lstlisting}
This will run the three \lstinline!Future! computations in parallel
because the arguments of \lstinline!map3! must be evaluated before
\lstinline!map3! is called, and evaluating a \lstinline!Future!
will schedule its computation immediately. To support different number
of arguments, we can implement similar functions \lstinline!map2!,
\lstinline!map4!, etc.

It is not obvious that the function \lstinline!map3! is related to
\lstinline!zip!. However, once we study the properties of those functions
in more detail, we will find that \lstinline!map3! can be expressed
through \lstinline!map2!, and that defining \lstinline!map2! is
equivalent to defining \lstinline!map! and \lstinline!zip! (assuming
that certain naturality laws hold for these functions). So, applicative
functors can be viewed as functors having either a \lstinline!map2!
or a \lstinline!zip! method in addition to \lstinline!map!. We will
also see that \lstinline!map4!, \lstinline!map5!, etc., can be all
expressed through \lstinline!zip!.

\section{Practical use of applicative functors}

Applicative functors appear whenever it is useful to have a \lstinline!zip!
or a \lstinline!map2! operation. We will now look at a few specific
situations where applicative operations help write better code.

\subsection{Transposing a matrix via \texttt{map2}}

The standard sequence type (\lstinline!Seq!) already has a \lstinline!zip!
method, so implementing a \lstinline!map2! for \lstinline!Seq! is
simple:
\begin{lstlisting}
def map2[A, B, C](as: Seq[A], bs: Seq[B])(f: (A, B) => C): Seq[C] =
  (as zip bs).map { case (a, b) => f(a, b) }

scala> map2(List(1, 2), List(100, 200))(_ + _)
res0: Seq[Int] = List(101, 202)
\end{lstlisting}

\index{matrix transposition}In Example~\ref{subsec:Example-matrix-products}(a),
we implemented matrix transposition using \lstinline!flatMap! and
index-based access to sequences, which is not available for some sequence
types. The index-based access is avoided if we implement \lstinline!transpose!
via \lstinline!map2!. As before, a matrix is represented by a sequence
of sequences:
\begin{lstlisting}
def transpose[A](s: Seq[Seq[A]]): Seq[Seq[A]] = ???

scala> transpose(List(List(1, 2), List(3, 4), List(5, 6)))
res1: Seq[Seq[Int]] = List(List(1, 3, 5), List(2, 4, 6))
\end{lstlisting}
We would like to define \lstinline!transpose! by induction. The base
case is an \textsf{``}empty\textsf{''} matrix:
\begin{lstlisting}
def transpose[A](s: Seq[Seq[A]]): Seq[Seq[A]] = s.headOption match {
  case None          => Seq()
  case Some(heads)   => val tails = s.tail
                        ???
}
\end{lstlisting}
For the inductive step, we assume that the \lstinline!tails! sub-matrix
is already transposed. So, it remains to combine \lstinline!transpose(tails)!
with \lstinline!heads!. Consider the test example shown above:
\[
\left|\begin{array}{cc}
1 & 2\\
3 & 4\\
5 & 6
\end{array}\right|\,\triangleright\text{transpose}=\,\left|\begin{array}{ccc}
1 & 3 & 5\\
2 & 4 & 6
\end{array}\right|\quad,\,\,\quad\left|\begin{array}{cc}
1 & 2\\
3 & 4\\
5 & 6
\end{array}\right|\,\triangleright\text{head}=\,\left|\begin{array}{cc}
1 & 2\end{array}\right|\quad,\,\,\quad\left|\begin{array}{cc}
1 & 2\\
3 & 4\\
5 & 6
\end{array}\right|\,\triangleright\text{tail}=\,\left|\begin{array}{cc}
3 & 4\\
5 & 6
\end{array}\right|\quad.
\]
Splitting this matrix into the first row (\lstinline!heads!) and
the rest (\lstinline!tails!), we will obtain \lstinline!heads == List(1, 2)!
and \lstinline!tails == List(List(3, 4), List(5, 6))!. The inductive
assumption is that \lstinline!transpose(tails)! works correctly and
yields \lstinline!List(List(3, 5), List(4, 6))!. How to combine this
with \lstinline!heads == List(1, 2)! to obtain the desired result
\lstinline!List(List(1, 3, 5), List(2, 4, 6))!? We need to prepend
each element of \lstinline!heads! to the corresponding sub-list in
\lstinline!transpose(tails)!. This is implemented by using \lstinline!map2!:
\begin{lstlisting}
def transpose[A](s: Seq[Seq[A]]): Seq[Seq[A]] = s.headOption match {
  case None          => Seq()
  case Some(heads)   =>
    val tails = s.tail
    map2(heads, transpose(tails)) { (x, y) => x +: y }
}
\end{lstlisting}
This code is still incomplete: it returns an empty sequence for all
arguments. We need to add another base case when the matrix \lstinline!s!
has only one row (so \lstinline!tails! is empty). Here is the final
code:
\begin{lstlisting}
def transpose[A](s: Seq[Seq[A]]): Seq[Seq[A]] = s.headOption match {
  case None          => Seq()
  case Some(heads)   => s.tail match {
    case Seq()   => heads.map(a => Seq(a))
    case tails   => map2(heads, transpose(tails)) { (x, y) => x +: y }
  }
}
\end{lstlisting}


\subsection{Data validation with error reporting}

Suppose we need to create a data structure with \textsf{``}validated\textsf{''} parts
(the data type could be a simple tuple, a case class, a list, a tree,
etc.). While validating any part of the data structure, we may get
an error. This situation happens when validating Web input data, reading
a set of command-line options, or parsing formatted text (such as
JSON). We would like to report all errors to the user, rather than
stopping at the first error. 

To simplify the reasoning, assume that the required data structure
is a case class, such as:
\begin{lstlisting}
final case class MyData(userId: Long, userName: String, userEmails: List[String])
\end{lstlisting}
More generally, consider the type $A\times B\times C$ containing
values of some chosen types $A$, $B$, and $C$:
\begin{lstlisting}
final case class MyData[A, B, C](a: A, b: B, c: C)
\end{lstlisting}

We assume that the validation functions will return values of types
\lstinline!F[A]! defined by:

\begin{wrapfigure}{l}{0.5\columnwidth}%
\vspace{-0.85\baselineskip}
\begin{lstlisting}
type F[A] = Either[E, A]
\end{lstlisting}

\vspace{-0.25\baselineskip}
\end{wrapfigure}%

\noindent \vspace{-0.5\baselineskip}
\[
F^{A}\triangleq E+A\quad.
\]

Here, \lstinline!A! is the type of a successfully validated value,
while the fixed type \lstinline!E! describes the error information.
We will assume that $E$ is a semigroup, so that different pieces
of error information can be combined into a larger value of the same
type $E$.

If some of the validations fail, the result should be a value of type
$E$. So, our task is to create a value of type $E+A\times B\times C$
given values of types $E+A$, $E+B$, and $E+C$. Using the type constructor
$F$ and the type \lstinline!MyData[A, B, C]!, we write the requirements
as the following type signature:
\begin{lstlisting}
def validated[A, B, C](fa: F[A], fb: F[B], fc: F[C]): F[MyData[A, B, C]] = ???
\end{lstlisting}
In the type notation, this is written as:
\[
\text{validated}:F^{A}\times F^{B}\times F^{C}\rightarrow F^{A\times B\times C}\quad.
\]
If we implement this type signature, we will be able to perform data
validation while collecting all errors. Note that the type signature
of \lstinline!validated! is quite similar to that of \lstinline!zip!
except for using \emph{three} different types rather than two. So,
let us define a suitable function \lstinline!zip3!. The code of \lstinline!zip3!
uses the ordinary \lstinline!zip! method twice:
\begin{lstlisting}
def zip3[A, B, C](fa: F[A], fb: F[B], fc: F[C]): F[(A, B, C)] =
  zip(fa, zip(fb, fc)).map { case (a, (b, c)) => (a, b, c) }
\end{lstlisting}
Also, \lstinline!validated! has to insert a \lstinline!MyData! type
constructor, returning a value of type \lstinline!F[MyData[A, B, C]]!
rather than \lstinline!F[(A, B, C)]!. So, we may implement \lstinline!validated!
as:
\begin{lstlisting}
def validated[A, B, C](fa: F[A], fb: F[B], fc: F[C]): F[MyData[A, B, C]] =
  zip(fa, zip(fb, fc)).map { case (a, (b, c)) => MyData(a, b, c) }
\end{lstlisting}

The difference between \lstinline!zip3! and \lstinline!validated!
is only in the function used in the last \lstinline!map! step. So,
we can make that last function a parameter and define the operation
\lstinline!map3! like this:
\begin{lstlisting}
def map3[A, B, C, D](fa: F[A], fb: F[B], fc: F[C])(f: (A, B, C) => D): F[D] =
  zip(fa, zip(fb, fc)).map { case (a, (b, c)) => f(a, b, c) }
\end{lstlisting}
Now the validation function can be implemented via \lstinline!map3!
more concisely:
\begin{lstlisting}
def validated[A, B, C](fa: F[A], fb: F[B], fc: F[C]): F[MyData[A, B, C]] =
  map3(fa, fb, fc)(MyData.apply)
\end{lstlisting}


\subsection{Implementing the functions \texttt{map2}, \texttt{map3}, etc. The
\texttt{ap} method}

Working with applicative functors involves using methods such as \lstinline!map2!,
\lstinline!map3!, and so on. The code of \lstinline!map2! for the
functor $L^{A}\triangleq\text{String}+A$ contains a pattern match
with four cases (see Section~\ref{subsec:Programs-that-accumulate-errors}).
Writing similar code for \lstinline!map3! requires \emph{eight} cases
(to match a triple of \lstinline!Either! values), and \lstinline!map4!
would require $16$ cases. How can we implement all these functions
without writing a lot of code?

One idea is to use the \lstinline!zip! method repeatedly, as we saw
in the previous section. We would then implement \lstinline!map2!
through \lstinline!zip!, \lstinline!map3! through \lstinline!zip3!,
and so on. For the purposes of illustration, let us choose the types
of all elements to be the same, so that the type signatures become:
\[
\text{zip}:L^{A}\times L^{A}\rightarrow L^{A\times A}\quad,\quad\quad\text{zip}_{3}:L^{A}\times L^{A}\times L^{A}\rightarrow L^{A\times A\times A}\quad.
\]
We can now implement a general function ($\text{zip}_{n}$) that uses
a list of values of type $L^{A}$:
\begin{lstlisting}
def zipN[A](xs: List[Either[String, A]]): Either[String, List[A]] = xs match {
  case Nil            => Right(Nil)
  case head :: tail   => zipE(head, zipN(tail)).map { case (h, t) => h :: t }
}
\end{lstlisting}
The corresponding function \lstinline!mapN! is then defined by:
\begin{lstlisting}
def mapN[A, Z](xs: List[Either[String, A]])(f: List[A] => Z): Either[String, Z] = zipN(xs).map(f)
\end{lstlisting}

If we wanted to define general functions \lstinline!zipN! and \lstinline!mapN!
that could take $N$ arguments of arbitrary types (and not all of
type $A$), we would need to use techniques of dependent-type programming,
which is beyond the scope of this book. We will now describe a simpler
solution that implements \lstinline!mapN! via a helper function (\lstinline!ap!)
that performs an inductive step expressing $\text{map}_{N}$ through
$\text{map}_{N-1}$.

If the \lstinline!ap! method can do that, it can also express \lstinline!map2!
via \lstinline!map!. So, let us determine the functionality that
is present in \lstinline!map2! but missing from \lstinline!map!.
That missing functionality is what \lstinline!ap! must implement.

To compare \lstinline!map! and \lstinline!map2! functions more easily,
consider their curried versions \lstinline!fmap! and \lstinline!fmap2!:
\begin{lstlisting}
def fmap[A,B](f: A => B): L[A] => L[B]
def fmap2[A,B,C](f: A => B => C): L[A] => L[B] => L[C]
\end{lstlisting}
\begin{align*}
 & \text{fmap}:\left(A\rightarrow B\right)\rightarrow L^{A}\rightarrow L^{B}\quad,\\
 & \text{fmap}_{2}:\left(A\rightarrow B\rightarrow C\right)\rightarrow L^{A}\rightarrow L^{B}\rightarrow L^{C}\quad.
\end{align*}
If we try implementing \lstinline!fmap2! via \lstinline!map!, we
get stuck:
\begin{lstlisting}
def fmap2[A, B, C](f: A => B => C): L[A] => L[B] => L[C] = { la: L[A] => la.map(f) ??? }
\end{lstlisting}
The value \lstinline!la.map(f)! has type \lstinline!L[B => C]! instead
of the required type \lstinline!L[B] => L[C]!. So, we need a function
that converts between those types. The new function is called \lstinline!ap!
and has the type signature:
\begin{lstlisting}
def ap[B, C](lf: L[B => C]): L[B] => L[C]
\end{lstlisting}
\[
\text{ap}_{L}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{B}\quad.
\]

If we have an implementation of \lstinline!ap! for a functor $L$,
we can write the code for \lstinline!fmap2! as:
\begin{lstlisting}
def fmap2[A, B, C](f: A => B => C): L[A] => L[B] => L[C] = { la: L[A] => ap[B, C](la.map(f)) }
\end{lstlisting}
Written in the point-free style using the code notation, this definition
looks like this:
\[
\text{fmap}_{2}(f)\triangleq f^{\uparrow L}\bef\text{ap}_{L}\quad.
\]
Taking into account the curried type signature of \lstinline!fmap2!,
we may rewrite this as:
\begin{lstlisting}
fmap2(f)(la)(lb) == ap(la.map(f))(lb)
\end{lstlisting}
This expression is made clearer if we implement infix syntax for \lstinline!fmap!
and \lstinline!ap!:
\begin{lstlisting}
implicit class FmapSyntax[A, B](f: A => B) {
  def <@>[F[_] : Functor](fa: F[A]): F[B] = fa.map(f)
} 
implicit class ApSyntax[A, B](lab: L[A => B]) {
  def <*>(la: L[A]): L[B] = ap(lab)(la)
}
\end{lstlisting}
Then we rewrite \lstinline!fmap2(f)(la)(lb)! as \lstinline!f <@> la <*> lb!.
This works because all infix operators group to the left, so \lstinline!f <@> la <*> lb!
means \lstinline!(f <@> la) <*> lb!, which is translated into \lstinline!ap(la.map(f))(lb)!.

The main advantage of this new syntax is that it becomes easier to
implement \lstinline!fmap3!, \lstinline!fmap4!, etc. Let us see
how we can use the \lstinline!ap! method to implement \lstinline!fmap3!:
\begin{lstlisting}
def fmap3[A, B, C, D](f: A => B => C => D): L[A] => L[B] => L[C] => L[D] = { la: L[A] =>
  ap[B, C => D](la.map(f)) andThen ap[C, D] }
\end{lstlisting}
In the point-free style, this definition is written as:
\[
\text{fmap}_{3}(f)\triangleq f^{\uparrow L}\bef\text{ap}_{L}\bef\left(g\rightarrow g\bef\text{ap}_{L}\right)\quad.
\]
It is harder to write and to understand the code of \lstinline!fmap3!
than that of \lstinline!fmap2!. An implementation of \lstinline!fmap4!
via \lstinline!ap! will be even more complicated. But with the infix
syntax shown above, we can write:
\begin{lstlisting}
def fmap3[A, B, C, D](f: A => B => C => D)(a: L[A])(b: L[B])(c: L[C]): L[D] = f <@> a <*> b <*> c
def fmap4[A, B, C, D, E](f: A => B => C => D => E)(a: L[A])(b: L[B])(c: L[C])(d: L[D]): L[E] =
  f <@> a <*> b <*> c <*> d
\end{lstlisting}
These examples show how to implement the function $\text{fmap}_{N}$
for any $N$ and any argument types.

For type constructors that are applicative but not covariant, a method
analogous to \lstinline!map2! must have a different type signature.
For instance, consider the type constructor $L$ defined in Example~\ref{subsec:Example-applicative-profunctor}.
We cannot define \lstinline!map2! as a composition of \lstinline!zip!
and \lstinline!map! because $L^{A}$ does not have a \lstinline!map!
method (as $L$ is not covariant in $A$). Instead, we note that $L$
is a profunctor\index{profunctor} and supports an \lstinline!xmap!
method:
\begin{lstlisting}
def xmap[A, B](la: L[A])(f: A => B, g: B => A): L[B] = (f(la._1), (b1, b2) => f(la._2(g(b1), g(b2))))
\end{lstlisting}
Composing \lstinline!zip! with \lstinline!xmap!, we obtain a new
method we may call \lstinline!xmap2!:
\begin{lstlisting}
def xmap2[A,B,C](la: L[A], lb: L[B])(f: ((A,B)) => C, g: C => (A,B)): L[C] = xmap(zip(la, lb))(f, g)
\end{lstlisting}
Methods such as \lstinline!xmap3!, \lstinline!xmap4!, etc., may
be defined similarly via \lstinline!zip3!, \lstinline!zip4!, etc.
However, a method analogous to \lstinline!ap! does not exist for
applicative profunctors.

\subsection{The applicative \texttt{Reader} functor\label{subsec:The-applicative-Reader-functor}}

The \lstinline!Reader! functor, $L^{A}\triangleq E\rightarrow A$,
is applicative and supports a \lstinline!zip! method as well:
\begin{lstlisting}
type Reader[A] = E => A    // The fixed type E must be already defined.
def zip[A, B](ra: Reader[A], rb: Reader[B]): Reader[(A, B)] = { e => (ra(e), rb(e)) }
\end{lstlisting}
The \lstinline!map2! method is implemented similarly:
\begin{lstlisting}
def map2[A, B, C](ra: Reader[A], rb: Reader[B])(f: A => B => C): Reader[C] = { e => f(ra(e))(rb(e)) }
\end{lstlisting}
These are the \emph{only} fully parametric implementations of the
type signatures of \lstinline!zip! and \lstinline!map2! for \lstinline!Reader!.

Since \lstinline!Reader! is also a monad (see Section~\ref{subsec:The-Reader-monad}),
we may implement the type signature of \lstinline!map2! as:
\begin{lstlisting}
def map2[A, B, C](ra: Reader[A], rb: Reader[B])(f: A => B => C): Reader[C] = for {
  x <- ra
  y <- rb
} yield f(x)(y)
\end{lstlisting}
This code is fully parametric. So, this \lstinline!map2! must be
equal to the direct implementation shown above.

It is noteworthy that the \lstinline!Reader!\textsf{'}s applicative and monad
instances agree on the implementation of \lstinline!map2!. We can
understand this intuitively if we consider that \lstinline!Reader!\textsf{'}s
effect is a dependency on a constant \textsf{``}environment\textsf{''} (a value of
type $E$). All \lstinline!Reader! computations in a given functor
block will read the same value of the \textsf{``}environment\textsf{''}. So, the \lstinline!Reader!
effects are always independent, and a \lstinline!map2! function does
not need to be implemented separately (it can be expressed via \lstinline!flatMap!).

\subsection{Single-traversal \texttt{fold} operations. I. Applicative \textquotedblleft fold
fusion\textquotedblright\label{subsec:Single-traversal-fold-operations-applicative-fold-fusion}}

We have seen various \textsf{``}\lstinline!fold!-like\textsf{''} operations (such
as \lstinline!foldLeft!, \lstinline!foldRight!, or \lstinline!reduce!)
that iterate over a sequence and accumulate a result value. An example
is the computation of the average of a list of numbers (see Example~\ref{subsec:ch1-aggr-Example-4}):
\begin{lstlisting}
def average(s: List[Double]): Double = s.sum / s.size
\end{lstlisting}
Both operations \lstinline!s.sum! and \lstinline!s.size! will iterate
over the list \lstinline!s!. So, \lstinline!average(s)! iterates
\emph{twice} over \lstinline!s!.

It is sometimes undesirable (or impossible) to iterate over a sequence
more than once. An example of that situation is a data stream whose
data elements arrive over a network at high speed. Usually, the stream
data cannot be stored because the data volume would grow too quickly.
So, the program needs to be implemented as an aggregation\index{aggregation}
operation that uses a single traversal of the stream. 

Another example is the computation of word distribution statistics
in a large text corpus.\index{corpus}\footnote{Such as the \textsf{``}Common Crawl\textsf{''} corpus, see \texttt{\href{https://commoncrawl.org/}{https://commoncrawl.org/}}}
For our purposes, a \textbf{corpus} is a sequence of large chunks
of text. Each chunk takes a long time to download, and storing the
entire sequence in memory is impossible. It is necessary to minimize
the number of traversals of the corpus. Ideally, all computations
need to be implemented in a single traversal.

Note that the \lstinline!sum! and \lstinline!size! methods are particular
cases of a \lstinline!foldLeft! operation. We could avoid a double
traversal in the code of \lstinline!average! if we implemented it
as a single \lstinline!foldLeft! call:
\begin{lstlisting}
def average(s: List[Double]): Double = {
  val (sum, size) = s.foldLeft((0.0, 0)) { case ((sum, size), x) => (sum + x, size + 1) }
  sum / size
}
\end{lstlisting}
This maintains a combined accumulated state of type \lstinline!(Double, Int)!
for the \lstinline!sum! and \lstinline!size! aggregations. 

We could always rewrite any number of aggregations as a single \lstinline!foldLeft!
that combines the aggregation codes into a more complicated updater
function operating on a combined state value. How to avoid writing
that code manually? The trick known as \textbf{fold fusion}\index{fold fusion}
allows us to merge any number of \lstinline!fold!-like operations
into a single operation. The code will automatically build and update
the accumulated state correctly, traversing the sequence only once.

In this section, we will implement fold fusion in two ways: as an
\textsf{``}applicative\textsf{''} fusion and as a \textsf{``}monadic\textsf{''} fusion. The contrast
between these approaches will illustrate the difference between applicative
and monadic functors.

The idea of fold fusion is to represent \lstinline!fold!-like operations
by values of a certain type, and to implement functions that combine
and \textsf{``}run\textsf{''} such operations.

We begin by defining a type constructor that represents a \lstinline!fold!-like
operation as a value. Let us look at the type signature of \lstinline!foldLeft!:
\begin{lstlisting}
def foldLeft[Z, R](zs: Seq[Z])(init: R)(update: (R, Z) => R): R
\end{lstlisting}
We should be able to apply a given \lstinline!fold!-like operation
to many different sequences \lstinline!zs!. So, we translate the
type signature of \lstinline!foldLeft! into a value by omitting the
argument \lstinline!zs!:
\begin{lstlisting}
final case class Fold[Z, R](init: R, update: (R, Z) => R)
\end{lstlisting}
To run the \lstinline!fold!-like operation represented by a value
of type \lstinline!Fold[Z, R]!, we implement a \textsf{``}runner\textsf{''}:\index{runner!for fold operations@for \texttt{fold} operations}
\begin{lstlisting}
def run[Z, R](zs: Seq[Z], fold: Fold[Z, R]): R = zs.foldLeft(fold.init)(fold.update)
\end{lstlisting}

Two \lstinline!fold!-like operations can be combined only if they
have the same type \lstinline!Z!. If we combine an \lstinline!op1: Fold[Z, R]!
with an \lstinline!op2: Fold[Z, S]!, we should obtain a new \lstinline!fold!-like
operation of type \lstinline!Fold[Z, (R, S)]!. So, the operation
of combining \lstinline!op1! and \lstinline!op2! is equivalent to
a \lstinline!zip! operation for the type constructor \lstinline!Fold[Z, R]!
with respect to the type parameter \lstinline!R!, while the type
\lstinline!Z! is kept fixed:
\begin{lstlisting}
def zipFold[Z, R, S](op1: Fold[Z, R], op2: Fold[Z, S]): Fold[Z, (R, S)] =
  Fold((op1.init, op2.init), (r, z) => (op1.update(r._1, z), op2.update(r._2, z)))
\end{lstlisting}

The types of \lstinline!Fold! and \lstinline!zipFold! are expressed
in the short type notation as:
\begin{align*}
 & \text{Fold}^{Z,R}\triangleq R\times\left(R\times Z\rightarrow R\right)\quad,\\
 & \text{zip}_{\text{Fold}}:\text{Fold}^{Z,R}\times\text{Fold}^{Z,S}\rightarrow\text{Fold}^{Z,R\times S}\quad,\quad\quad\text{or equivalently:}\\
 & \text{zip}_{\text{Fold}}:R\times\left(R\times Z\rightarrow R\right)\times S\times\left(S\times Z\rightarrow S\right)\rightarrow\left(R\times S\right)\times(R\times S\times Z\rightarrow R\times S)\quad.
\end{align*}
A fully parametric, information-preserving implementation of \lstinline!zipFold!
follows unambiguously from its type signature. So, let us use the
\index{curryhoward library@\texttt{curryhoward} library}\lstinline!curryhoward!
library\footnote{See \texttt{\href{https://github.com/Chymyst/curryhoward}{https://github.com/Chymyst/curryhoward}}}
to generate the code automatically:
\begin{lstlisting}
import io.chymyst.ch._       // Import some symbols from the `curryhoward` library.
def zipFold[Z, R, S](op1: Fold[Z, R], op2: Fold[Z, S]): Fold[Z, (R, S)] = implement
\end{lstlisting}

With these definitions, we can rewrite \lstinline!average! as a single-traversal
calculation:
\begin{lstlisting}
val sum = Fold[Double, Double](0, _ + _)
val length = Fold[Double, Int](0, (n, _) => n + 1)
val sumLength: Fold[Double, (Double, Int)] = zipFold(sum, length)
val res: (Double, Int) = run(Seq(1.0, 2.0, 3.0), sumLength)

scala> val average = res._1 / res._2
average: Double = 2.0
\end{lstlisting}
In this way, we may combine any number of \lstinline!fold!-like operations
into a single traversal. 

While this simple implementation of applicative fold fusion works,
it is not easy to use. The result of \textsf{``}running\textsf{''} a combined \lstinline!Fold!
operation will be a tuple structure that must be transformed in just
the right way (do we need \lstinline!res._1 / res._2! or \lstinline!res._2 / res._1!?)
to obtain the final result. A better approach is to implement a syntax
that combines some \lstinline!Fold! operations and at the same time
defines a final transformation to be applied to the results. We would
like to write code that directly manipulates \lstinline!fold!-like
operations and produces the final result:
\begin{lstlisting}
val sum = ...            // Define `sum` and `length` in a suitable way.
val length = ...
Seq(1.0, 2.0, 3.0).runFold(sum / length)    // Should return `2.0` here.
\end{lstlisting}

To enable this kind of code,\footnote{A more fully featured library using this approach is \texttt{scala-fold},
see \texttt{\href{https://github.com/amarpotghan/scala-fold}{https://github.com/amarpotghan/scala-fold}} } we need to add a final transformation to the \lstinline!Fold! type:
\begin{lstlisting}
final case class FoldOp[Z, R, A](init: R, update: (R, Z) => R, transform: R => A)
\end{lstlisting}
A value of type \lstinline!FoldOp[Z, R, A]! represents a \lstinline!fold!-like
operation for sequences of type \lstinline!Seq[Z]!. The operation
accumulates an intermediate state of type \lstinline!R! and computes
a final result of type \lstinline!A!.

The runner is implemented as an extension method on \lstinline!Seq!:
\begin{lstlisting}
implicit class FoldOpSyntax[Z](zs: Seq[Z]) {
  def runFold[R, A](op: FoldOp[Z, R, A]): A = op.transform(zs.foldLeft(op.init)(op.update))
}
\end{lstlisting}

How can we combine two \lstinline!fold!-like operations of types
$\text{FoldOp}^{Z,R,A}$ and $\text{FoldOp}^{Z,S,B}$? The combined
operation must maintain intermediate states of types $R$ and $S$.
Also, we have two result values of types $A$ and $B$. It seems that
the combined operation needs an intermediate state of type $R\times S$
and a final result of type $A\times B$. So, let us implement a \lstinline!zip!
extension method by using those types:
\begin{lstlisting}
implicit class FoldOpZip[Z, R, A](op: FoldOp[Z, R, A]) {
  def zip[S, B](other: FoldOp[Z, S, B]): FoldOp[Z, (R, S), (A, B)] = implement
  def map[B](f: A => B): FoldOp[Z, R, B] = implement
  def map2[S, B, C](other: FoldOp[Z, S, B])(f: (A, B) => C): FoldOp[Z, (R, S), C] = implement
} // The type signatures unambiguously determine the implementations.
\end{lstlisting}
The \lstinline!map! and \lstinline!map2! methods exist because \lstinline!FoldOp[Z, R, A]!
is covariant with respect to \lstinline!A!.

We can now implement extension methods allowing us to do arithmetic
on \lstinline!fold!-like operations:
\begin{lstlisting}
implicit class FoldOpMath[Z, R](op: FoldOp[Z, R, Double]) {
  def binaryOp[S](other: FoldOp[Z, S, Double])(f: (Double, Double) => Double): FoldOp[Z, (R, S), Double] = op.map2(other) { case (x, y) => f(x, y) }
  def +[S](other: FoldOp[Z, S, Double]): FoldOp[Z, (R, S), Double] = op.binaryOp(other)(_ + _) 
  def /[S](other: FoldOp[Z, S, Double]): FoldOp[Z, (R, S), Double] = op.binaryOp(other)(_ / _) 
} // May need to define more operations here.
\end{lstlisting}

After these definitions, the following code will work:
\begin{lstlisting}
val sum = FoldOp[Double, Double, Double](0, (s, i) => s + i, identity)
val length = FoldOp[Double, Double, Double](0, (s, _) => s + 1, identity)

scala> Seq(1.0, 2.0, 3.0).runFold(sum / length)
res0: Double = 2.0
\end{lstlisting}

We note that the type signatures of \lstinline!zip! and \lstinline!map2!
are different from the usual ones: the accumulated state\textsf{'}s type (\lstinline!R!)
is transformed into a pair type \lstinline!(R, S)!. However, the
programmer\textsf{'}s code does not need to annotate those types because they
are automatically inferred by the Scala compiler.

The standard \lstinline!scanLeft! method is similar to \lstinline!foldLeft!
except it outputs all intermediate accumulated states (while \lstinline!foldLeft!
outputs only the last one). Can we merge several \lstinline!scanLeft!
operations into a single traversal? Since the type signature of \lstinline!scanLeft!
is the same as that of \lstinline!foldLeft! except for the return
type, the \lstinline!FoldOp! data structure already stores the information
needed to run \lstinline!scanLeft!. The code for a corresponding
runner (called \lstinline!runScan!) is:
\begin{lstlisting}
implicit class FoldOpScan[Z](zs: Seq[Z]) {
 def runScan[R,A](op: FoldOp[Z,R,A]): Seq[A] = zs.scanLeft(op.init)(op.update).map(op.transform).tail
}
\end{lstlisting}

As an example of using this functionality, consider the task of computing
the running average of a sequence. A running average could be computed
from the beginning of the sequence:
\[
\text{ave}_{k}\triangleq\frac{1}{k}\sum_{i=0}^{k-1}s_{i}\quad.
\]
The code \lstinline!val average = sum / length! implements a \lstinline!fold!-like
operation computing this average.

Another task is to compute the average over a sliding window of a
fixed size $n$:
\[
\text{ave}_{n,k}\triangleq\frac{1}{n}\sum_{i=k-n}^{k-1}s_{i}\quad.
\]
To implement this, we first create a sliding window as a \lstinline!fold!-like
operation and then do averaging:
\begin{lstlisting}
def window[A](n: Int): FoldOp[A, IndexedSeq[A], IndexedSeq[A]] = FoldOp(init = Vector(),
  update = { (window, x) => (if (window.size < n) window else window.drop(1)) :+ x }, identity)

def window_average(n: Int) = window[Double](n).map(_.sum / n)
\end{lstlisting}
To test that the averaging code works as expected, we run some fold
and scan operations:
\begin{lstlisting}
scala> (0 to 10).map(_.toDouble).runFold(average)
res0: Double = 5.0

scala> (0 to 10).map(_.toDouble).runScan(average)
res1: Vector[Double] = Vector(0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0)

scala> (0 to 10).map(_.toDouble).runScan(window_average(3))
res2: Vector[Double] = Vector(0.0, 0.3333333333333333, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0)
\end{lstlisting}


\subsection{Single-traversal \texttt{fold} operations. II. Monadic \textquotedblleft fold
fusion\textquotedblright}

The applicative fold fusion as defined in the previous section works
by combining the results of independent \lstinline!fold!-like operations.
For instance, the definition \lstinline!average = sum / length! uses
\lstinline!sum! and \lstinline!length! as independent \lstinline!fold!-like
operations (the results computed by \lstinline!sum! do not depend
on the results of \lstinline!length!). The independence of effects
is a defining feature of applicative composition. In contrast, monadic
composition supports effects that may depend on previously computed
values. To illustrate this contrast, let us implement the monadic
version of fold fusion.

The type constructor \lstinline!FoldOp[Z, R, A]! can be a monad only
if it is covariant. So, a \lstinline!flatMap! method must operate
on the type parameter \lstinline!A!. We begin writing the type signature
as:
\begin{lstlisting}
def flatMap[Z, R, A, S, B](op: FoldOp[Z, R, A])(f: A => FoldOp[Z, S, B]): FoldOp[Z, ???]
\end{lstlisting}
The first \lstinline!fold!-like operation (\lstinline!op!) will
compute a value of type \lstinline!A! at each step. We will then
apply the given function \lstinline!f! to that value and obtain a
new \lstinline!fold!-like operation (of type \lstinline!FoldOp[Z, S, B]!)
that also needs to be evaluated. Since \lstinline!flatMap! will need
to run two \lstinline!fold!-like operations in one traversal, it
must maintain a combined accumulated state of type \lstinline!(R, S)!.
So, the type signature of \lstinline!flatMap! needs to accommodate
the change of that type (just as the type signature of \lstinline!map2!
did in the previous section). For convenience, we define \lstinline!flatMap!
as an extension method: 
\begin{lstlisting}
implicit class FoldFlatMap[Z, R, A](op: FoldOp[Z, R, A]) {
  def flatMap[S, B](f: A => FoldOp[Z, S, B]): FoldOp[Z, (R, S), B] = ???
}
\end{lstlisting}
The result of \lstinline!op1.flatMap(x => op2(x))! must be a \lstinline!fold!-like
operation that combines \lstinline!op1! and \lstinline!op2! and
supports arbitrary dependency in \lstinline!op2(x)! on the intermediate
result (\lstinline!x: A!), which is obtained by running \lstinline!op1!.
To achieve that, we implement \lstinline!flatMap! via the following
code:
\begin{lstlisting}[numbers=left]
implicit class FoldFlatMap[Z, R, A](op: FoldOp[Z, R, A]) {
   def flatMap[S, B](f: A => FoldOp[Z, S, B]): FoldOp[Z, (R, S), B] = {
      // To create a new `FoldOp()` value, we need `init`, `update`, and `transform`.
      val init: (R, S) = (op.init, f(op.transform(op.init)).init) // Use `init` from both operations.
      val update: ((R, S), Z) => (R, S) = { case ((r, s), z) =>   // Run both `update` functions:
          val newR = op.update(r, z)                              // First `update` function.
          val newOp: FoldOp[Z, S, B] = f(op.transform(newR))      // We may use `newR` or `r` here!
          val newS = newOp.update(s, z)                           // Second `update` function.
          (newR, newS)
      }
      val transform: ((R, S)) => B = { case (r, s) =>
          val newOp: FoldOp[Z, S, B] = f(op.transform(r))
          newOp.transform(s)
      }
      FoldOp(init, update, transform)
  }
}
\end{lstlisting}

In line 7 of the code above, we have a choice of applying \lstinline!op.transform(...)!
to the old accumulated value (\lstinline!r!) or to the updated value
(\lstinline!newR!). It appears to be better to use the updated value
(\lstinline!newR!). 

To motivate this choice, consider a running average operation applied
twice:
\begin{lstlisting}
scala> (0 to 10).toList.map(_.toDouble).runScan(average).runScan(average)
res3: List[Double] = List(1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25)
\end{lstlisting}
Using \lstinline!flatMap!, we can express the same computation as
a single \lstinline!fold!-like operation:
\begin{lstlisting}
def add(x: Double) = FoldOp[Double, Double, Double](0, (a, _) => a + x, identity)

scala> (0 to 10).toList.map(_.toDouble).runScan(average.flatMap(x => add(x) / length))
res4: List[Double] = List(1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25)
\end{lstlisting}
The code can be rewritten as a functor block, making it more visually
clear:
\begin{lstlisting}
val average2a = for {
      x <- average
      accum <- add(x)
      n <- length
} yield accum / n

scala> (0 to 10).toList.map(_.toDouble).runScan(average2a)
res5: List[Double] = List(1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25)
\end{lstlisting}
We get the same result as when applying \lstinline!runScan(average)!
twice. The result would not be obtained if we used the old value (\lstinline!r!)
in line 7 in the definition of \lstinline!flatMap! above.

The \lstinline!flatMap! function implements a monadic fusion of \lstinline!fold!-like
operations.\footnote{The \textsf{``}\texttt{origami}\textsf{''} library (see \texttt{\href{https://github.com/atnos-org/origami}{https://github.com/atnos-org/origami}})
implements a more general functionality: values computed by its \lstinline!fold!-like
operations are themselves of monadic type. This is similar to using
\lstinline!FoldOp[Z, R, M[A]]! where \lstinline!M! is a monad and
the updater function has type \lstinline!(R, Z) => M[R]!.} It is interesting to note that the monadic fold fusion is compatible
with the applicative fold fusion. For instance, one could define \lstinline!average!
equivalently as:
\begin{lstlisting}
val average = for {
  acc <- sum
  n <- length
} yield acc / n
\end{lstlisting}
This equivalence comes from the fact that the case class \lstinline!FoldOp[Z, R, A]!
depends on \lstinline!A! only through the \lstinline!transform!
value, which is of type \lstinline!R => A!. So, the monadic behavior
of \lstinline!FoldOp[Z, R, A]! with respect to \lstinline!A! is
similar to that of the \lstinline!Reader! monad of type \lstinline!R => A!.
Similarly to the \lstinline!Reader! monad, the effects in the \lstinline!FoldOp!
monad are independent.

\subsection{Parsing with applicative and monadic combinators\label{subsec:Parsing-with-applicative-and-monadic-parsers}}

Applicative functors are used for implementing parsers via the \textsf{``}parser
combinator\textsf{''} technique. In this technique, the programmer builds
a large parser out of smaller ones by using different functions (\textsf{``}combinators\textsf{''}).
We will now look at both applicative and monadic parser combinators.

The parsing techniques will be illustrated on some toy \textsf{``}languages\textsf{''}.
The first language encodes square roots of positive integers in an
XML-like format, as, for example, in the strings \lstinline!<sqrt>121</sqrt>!,
\lstinline!<sqrt><sqrt>10000</sqrt></sqrt>!, and \lstinline!123!.
Examples of \emph{invalid} strings are \lstinline!<sqrt>100</sqrt></sqrt>!
(junk at end of text), \lstinline!<sqrt>121<sqrt>! (tag not closed),
\lstinline!</sqrt>10</sqrt>! (tag not opened), and \lstinline!1.0!
(not an integer). The parser should output an integer result or an
error message.

To simplify the code, we define a parser as a function taking an input
string and returning either a value of some type \lstinline!A! (the
value \textsf{``}parsed out\textsf{''} of the input) or some error information. The
parser also returns the unused part of the input string. So, we define
the type constructor \lstinline!P[A]! by:
\begin{lstlisting}
final case class P[A](run: String => (Either[Err, A], String)) // A parser returns a result or fails.
type Err = List[String]           // A list of error messages.
\end{lstlisting}

In order to parse the toy language, we need to be able to parse integers,
opening tags, closing tags, and detect whether the end of input occurs
at a point where all tags are balanced. The parser combinator technique
begins by defining the simplest necessary parsers. We will need a
parser for integers and a parser that expects the input to start with
a given, fixed string:
\begin{lstlisting}
val intP: P[Int] = P { 
  val numRegex = "^([0-9]+)(.*)$".r
  s => s match {
    case numRegex(num, rest)   => (Right(num.toInt), rest)
    case s                     => (Left(List("no number")), s)
  } 
}
def constP(prefix: String, error: String = "no prefix"): P[String] = P { s =>
  if (s startsWith prefix) (Right(prefix), s.stripPrefix(prefix)) else (Left(List(error)), s)
}
\end{lstlisting}
Let us test that these parsers work as expected:
\begin{lstlisting}
scala> intP.run("123xyz")
res0: (Either[Err, Int], String) = (Right(123), "xyz")

scala> constP("<sqrt>").run("<sqrt>1</sqrt>")
res1: (Either[Err, String], String) = (Right("<sqrt>"), "1</sqrt>")
\end{lstlisting}

The next step is to define combinators that produce larger parsers
from smaller ones. We will define four different combinators corresponding
to a functor\textsf{'}s \lstinline!map!, an applicative \lstinline!zip!,
a monoid\textsf{'}s \lstinline!combine!, and a monad\textsf{'}s \lstinline!flatMap!
methods. For convenience, we implement the combinators as extension
methods on the type constructor \lstinline!P!. Start with \lstinline!zip!
and use the code from Section~\ref{subsec:Programs-that-accumulate-errors}:
\begin{lstlisting}
implicit class ParserZipOps[A](parserA: P[A]) {
   def zip[B](parserB: P[B]): P[(A, B)] = P { s =>
      val (resultA, rest) = parserA.run(s)
      val (resultB, restB) = parserB.run(rest)
      val result = (resultA, resultB) match { // Use the `zip` operation for Either[List[String], A].
        case (Left(x), Left(y))     => Left(x ++ y)
        case (Left(x), Right(_))    => Left(x)
        case (Right(_), Left(y))    => Left(y)
        case (Right(x), Right(y))   => Right((x, y))
      }
      (result, restB)
    }
}
\end{lstlisting}
To test this, let us define a parser \lstinline!p1! for strings of
the form \lstinline!<sqrt>123</sqrt>!:
\begin{lstlisting}
val openTag = constP("<sqrt>", "tag must be open")
val closeTag = constP("</sqrt>", "tag must be closed")
val p1 = openTag zip intP zip closeTag

scala> p1.run("<sqrt>123</sqrt>")
res4: (Either[Err, ((String, Int), String)], String) = (Right((("<sqrt>", 123), "</sqrt>")), "")

scala> p1.run("<sqrt></sqrt>")
res5: (Either[Err, ((String, Int), String)], String) = (Left(List("no number")), "")

scala> p1.run("abc")
res6: (Either[Err, ((String, Int), String)], String) = (Left(List("tag must be open", "no number", "tag must be closed")), "abc")
\end{lstlisting}
Because the parser \lstinline!p1! is defined via the applicative
\lstinline!zip! method, the code can report several errors at once.
With suitable definitions, parsers combined via \lstinline!zip! could
skip some invalid text and attempt to parse further data, possibly
reporting more errors to the user.

It is inconvenient that a \lstinline!zip!-combined parser returns
a deeply nested tuple structure as shown above; we are often interested
in reading only one value inside that structure. We would like \lstinline!p1.run("<sqrt>123</sqrt>")!
to return just \lstinline!Right(123, "")!. To achieve this, we define
the helper methods \lstinline!zipLeft! and \lstinline!zipRight!,
whose names indicate the part of the tuple that is being returned:
\begin{lstlisting}
implicit class ParserMoreZipOps[A](parserA: P[A]) {
   def map[B](f: A => B): P[B] = P { s =>
      val (result, rest) = parserA.run(s)
      (result.map(f), rest)
   } // 
   def zipLeft [B](parserB: P[B]): P[A] = (parserA zip parserB).map(_._1)
   def zipRight[B](parserB: P[B]): P[B] = (parserA zip parserB).map(_._2)
}

scala> (openTag zipRight intP zipLeft closeTag).run("<sqrt>123</sqrt>")
res7: (Either[Err, Int], String) = (Right(123), "")
\end{lstlisting}

The next parser combinator corresponds to a monoid\textsf{'}s \lstinline!combine!,
but we will call it \textsf{``}\lstinline!or!\textsf{''} for convenience. The result
of \lstinline!(p or q)! is a parser that succeeds if either \lstinline!p!
or \lstinline!q! succeed and returns the corresponding result (the
parser \lstinline!p! is tried first):
\begin{lstlisting}
implicit class ParserCombineOps[A](p: P[A]) {
   def or(q: => P[A]): P[A] = P { s =>       // Important: the argument `q` must be lazy.
   val (result, rest) = p.run(s)
   result match {                            // If `p` failed to parse the string `s`,
     case Left(err)  => q.run(s)             // ignore the error from `p` and run `q`.
     case Right(x)   => (Right(x), rest)     // Otherwise, use the result of running `p`.
   }
}    
\end{lstlisting}
We implement \lstinline!q! as a lazy argument because we should not
evaluate \lstinline!q! when the parser \lstinline!p! succeeds. This
will help us write recursively defined parsers, as we will see next.

Using these combinators, we write a first attempt at parsing the toy
language. Since the language allows us to have arbitrarily deep nesting
of the tags \lstinline!<sqrt>...</sqrt>!, we need to define the parser
as a recursive function (called \lstinline!p2!):
\begin{lstlisting}
def p2: P[Int] = intP or (openTag zipRight p2 zipLeft closeTag).map { x => math.sqrt(x).toInt }

scala> p2.run("121")._1.right.get
res8: Int = 121

scala> p2.run("<sqrt>121</sqrt>")._1.right.get
res9: Int = 11
\end{lstlisting}
The recursion stops only because the operation \textsf{``}\lstinline!or!\textsf{''}
does not evaluate its second parser when the first parser succeeds.
Otherwise the code would go into an infinite loop.

We are able to parse the toy language using only applicative combinators.
Let us now consider a more complicated language that \emph{requires}
monadic combinators to be parsed correctly. The language represents
equations between integers, such as \lstinline!1=1! or \lstinline!123=123!,
but does not admit \lstinline!1=2!. To parse this language, we need
to get the first integer, skip the equals sign, and then check that
the second integer is equal to the first one. So, we need to use a
parser that depends on the result of parsing a previous substring.
We would like to implement the parser using this straightforward code:
\begin{lstlisting}
val emptyP: P[Unit] = P { s => if (s.isEmpty) (Right(()), s) else (Left(List("junk at end")), s) }

val p3: P[Int] = for {
  x <- intP                // Expect an integer.
  _ <- constP("=")         // Expect an equals sign.
  _ <- constP(x.toString)  // Expect a string corresponding to the previously parsed integer `x`.
  _ <- emptyP              // Expect end of input.
} yield x 
\end{lstlisting}
To make this code work, we define \lstinline!flatMap! as an extension
method on \lstinline!P!:
\begin{lstlisting}
implicit class ParserMonadOps[A](parserA: P[A]) {
   def flatMap[B](f: A => P[B]): P[B] = P { s =>
      val (result, rest) = parserA.run(s)
      result match {
        case Left(err)   => (Left(err), rest)
        case Right(x)    => f(x).run(rest)
      }
   }
}
\end{lstlisting}
The result is a parser \lstinline!p3! that that stops at the first
error:
\begin{lstlisting}
scala> p3.run("123=123")
res10: (Either[Err, Int], String) = (Right(123), "")

scala> p3.run("1=2")
res11: (Either[Err, Int], String) = (Left(List("integer 1 not found")), "2")
\end{lstlisting}

The error reporting in \lstinline!p3! can be improved if we use both
monadic and applicative combinators in the same code. Let us implement
and use a special parser (called \textsf{``}\lstinline!expect!\textsf{''}) that will
fail with an informative message when a parsed integer is not equal
to a given value:
\begin{lstlisting}
val success: P[Unit] = P { s => (Right(()), s) }  // Always-succeeding parser that consumes no input.
def failure(message: String): P[Unit] = P { s => (Left(List(message)), s) } // Always-failing parser.

def expect(n: Int): P[Unit] = intP.flatMap { x =>
  if (x == n) success else failure(s"got $x but expected $n")
}

val p4: P[Int] = for {
   x <- intP zipLeft constP("=")
   _ <- expect(x) zip emptyP
} yield x

scala> p4.run("1=2mumbo-jumbo gobbledy-gook")
res12: (Either[Err, Int], String) = (Left(List("got 2 but expected 1", "junk at end")), "mumbo-jumbo gobbledy-gook")
\end{lstlisting}

Several parser combinator libraries\footnote{See, for example, the \textsf{``}\texttt{fastparse}\textsf{''} library: \texttt{\href{https://com-lihaoyi.github.io/fastparse}{https://com-lihaoyi.github.io/fastparse}}}
are designed after these principles and provide applicative, monadic,
filterable, monoidal, and other functionality for building up large
parsers from parts.

\subsection{Functor block syntax for applicative functors}

It is often desirable to combine applicative and monadic computations
in a single functor block. This can be done by using \lstinline!zip!,
\lstinline!zipLeft!, and \lstinline!zipRight! within source lines,
as shown in the previous section.
\begin{lstlisting}
for {
  x       <-   p
  (y, z)  <-   q(x) zip r(x)
  t       <-   s(x, y, z)
  ...
\end{lstlisting}
In the code shown above, \lstinline!q! and \lstinline!r! cannot
depend on each other\textsf{'}s returned values (\lstinline!y! and \lstinline!z!).
However, \lstinline!q! and \lstinline!r! may depend on any of the
previously computed values (such as \lstinline!x!). Later computations
(such as \lstinline!s!) may also depend on the previous values (\lstinline!x!,
\lstinline!y!, \lstinline!z!).

There have been several proposals\footnote{See, for example, \texttt{\href{https://contributors.scala-lang.org/t/4474}{https://contributors.scala-lang.org/t/4474}}}
to add a special block syntax for applicative functors in Scala. So
far, these proposals have \emph{not} been accepted as a part of the
Scala language. Some experimental projects implement a \lstinline!for!/\lstinline!yield!
syntax for applicative functors.\footnote{See, for example, \texttt{\href{https://github.com/bkirwi/applicative-syntax}{https://github.com/bkirwi/applicative-syntax}}
where a special macro is used, which may not work with Scala 3. The
blog post \texttt{\href{https://lptk.github.io/programming/2018/03/02/a-dual-to-iterator.html}{https://lptk.github.io/programming/2018/03/02/a-dual-to-iterator.html}}
proposes a \lstinline!for!/\lstinline!yield! syntax for applicative
functors and implements single-traversal applicative folds without
using macros.}

\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-applicative-I}\ref{subsec:Exercise-applicative-I}}

Implement \lstinline!map2! (or \lstinline!xmap2! if appropriate)
for the following type constructors $F^{A}$:

\textbf{(a)} $F^{A}\triangleq\bbnum 1+A+A\times A$.

\textbf{(b)} $F^{A}\triangleq E\rightarrow A\times A$.

\textbf{(c)} $F^{A}\triangleq Z\times A\rightarrow A$.

\textbf{(d)} $F^{A}\triangleq A\rightarrow A\times Z$ where $Z$
is a \lstinline!Monoid!.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-1-1}\ref{subsec:Exercise-applicative-I-1-1}}

Implement a \lstinline!zip! method for a ternary tree \lstinline!T3[A]!
with extra data on branches:
\begin{lstlisting}
sealed trait T3[A]
case class Leaf[A](a: A) extends T3[A]
case class Branch[A](label: A, left: T3[A], center: T3[A], right: T3[A]) extends T3[A]

def zip[A, B](ta: T3[A], tb: T3[B]): T3[(A, B)] = ???
\end{lstlisting}
Aim to preserve information and avoid changing the tree structures
unnecessarily.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-1}\ref{subsec:Exercise-applicative-I-1}}

Write a \lstinline!zip! function that defines an instance of the
\lstinline!Semigroup! type class for a tuple \lstinline!(A, B)!
where the types \lstinline!A! and \lstinline!B! are assumed to already
have a \lstinline!Semigroup! instance. Test on an example with \lstinline!Semigroup!
instances for \lstinline!A = Int! and \lstinline!B = String!.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-2}\ref{subsec:Exercise-applicative-I-2}}

Define a \lstinline!Monoid! instance for the type $F^{S}$ where
$F$ is an applicative functor that has \lstinline!map2! and \lstinline!pure!,
while $S$ is itself a monoid type.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-3}\ref{subsec:Exercise-applicative-I-3}}

Define a \textsf{``}regexp extractor\textsf{''} as a type constructor $R^{A}$ describing
extraction of various data from strings; the extracted data has type
\lstinline!Option[A]!. Implement \lstinline!zip! and \lstinline!map2!
for $R^{A}$.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-5}\ref{subsec:Exercise-applicative-I-5}}

Use fold fusion (Section~\ref{subsec:Single-traversal-fold-operations-applicative-fold-fusion})
to implement a \lstinline!FoldOp! that computes the standard deviation
of a sequence of type \lstinline!Seq[Double]! in one traversal.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-5-1}\ref{subsec:Exercise-applicative-I-5-1}}

Use parser combinators defined in Section~\ref{subsec:Parsing-with-applicative-and-monadic-parsers}
(alternatively, use a parser combinator library of your choice) to
implement a parser for a toy language that contains a single integer
in a sequence of arbitrary nested tags, such as \lstinline!<a><b>123</b></a>!.
Tag names are restricted to lowercase Latin letters. The parser should
return that integer or fail if tags are incorrectly nested.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-4}\ref{subsec:Exercise-applicative-I-4}}

\textbf{(a)} Use parser combinators to parse a simple arithmetic language
containing only decimal digits and \textsf{``}\lstinline!+!\textsf{''} symbols. The
parser should return the resulting integer. For example, parsing \lstinline!1+321+20!
should return \lstinline!342!. 

\textbf{(b)} Extend the parser to support parentheses and \textsf{``}\lstinline!-!\textsf{''}
symbols. For example, parsing \lstinline!10-(2+3)! should return
$5$.

\section{Laws and structure of applicative functors}

\subsection{Equivalence of \texttt{map2}, \texttt{zip}, and \texttt{ap}\label{subsec:Equivalence-of-map2-zip-ap}}

We have seen in Section~\ref{subsec:Programs-that-accumulate-errors}
that the implementations of \lstinline!map2! and \lstinline!zip!
for \lstinline!Either[E, A]! are quite similar. This similarity holds
in the general case: \lstinline!map2! and \lstinline!zip! are equivalent
assuming a suitable naturality law of \lstinline!map2!. (Naturality
laws will always hold if the code is fully parametric.)

\subsubsection{Statement \label{subsec:Statement-map2-zip-equivalence}\ref{subsec:Statement-map2-zip-equivalence}
(equivalence of \lstinline!map2! and \lstinline!zip!)}

For any functor $L$ for which \lstinline!map2! or \lstinline!zip!
can be implemented, the type of functions \lstinline!zip! (type signature
$\text{zip}:L^{A}\times L^{B}\rightarrow L^{A\times B}$) is equivalent
to the type of functions \lstinline!map2! (type signature $L^{A}\times L^{B}\rightarrow\left(A\times B\rightarrow C\right)\rightarrow L^{C}$),
assuming that the functions \lstinline!map2! satisfy the naturality
law with respect to the type parameter $C$.

\subparagraph{Proof}

We need to show equivalence in two directions (from \lstinline!map2!
to \lstinline!zip! and back).

\textbf{(1)} Given any function $\text{map}_{2}:L^{A}\times L^{B}\rightarrow\left(A\times B\rightarrow C\right)\rightarrow L^{C}$
satisfying the naturality law:
\[
\text{map}_{2}\,(p^{:L^{A}}\times q^{:L^{B}})(f^{:A\times B\rightarrow C})\triangleright(g^{:C\rightarrow D})^{\uparrow L}=\text{map}_{2}\,(p\times q)(f\bef g)\quad,
\]
we first define a \lstinline!zip! method:
\[
\text{zip}:L^{A}\times L^{B}\rightarrow L^{A\times B}\quad,\quad\quad\text{zip}\,(p^{:L^{A}}\times q^{:L^{B}})\triangleq\text{map}_{2}\,(p\times q)(\text{id}^{:A\times B\rightarrow A\times B})\quad,
\]
and then define a new \lstinline!map2!$^{\prime}$ function through
that \lstinline!zip! method:
\[
\text{map}_{2}^{\prime}\,(p^{:L^{A}}\times q^{:L^{B}})(f^{:A\times B\rightarrow C})\triangleq(p\times q)\triangleright\text{zip}\triangleright f^{\uparrow L}\quad.
\]
Then we need to show that $\text{map}_{2}^{\prime}=\text{map}_{2}$.
We apply $\text{map}_{2}^{\prime}$ to arbitrary arguments and write:
\begin{align*}
 & \text{map}_{2}^{\prime}\,(p\times q)(f)=(p\times q)\triangleright\text{zip}\triangleright f^{\uparrow L}=\text{map}_{2}\,(p\times q)(\text{id})\,\gunderline{\triangleright\,f^{\uparrow L}}\\
{\color{greenunder}\text{naturality law of }\text{map}_{2}:}\quad & =\text{map}_{2}\,(p\times q)(\text{id}\bef f)=\text{map}_{2}\,(p\times q)(f)\quad.
\end{align*}

\textbf{(2)} Given any function $\text{zip}:L^{A}\times L^{B}\rightarrow L^{A\times B}$,
we first define a \lstinline!map2! method:
\[
\text{map}_{2}\,(p^{:L^{A}}\times q^{:L^{B}})(f^{:A\times B\rightarrow C})\triangleq(p\times q)\triangleright\text{zip}\triangleright f^{\uparrow L}\quad,
\]
and then define a new \lstinline!zip!$^{\prime}$ function through
that \lstinline!map2! method:
\[
\text{zip}^{\prime}\,(p^{:L^{A}}\times q^{:L^{B}})\triangleq\text{map}_{2}\,(p\times q)(\text{id}^{:A\times B\rightarrow A\times B})\quad.
\]
Then we need to show that $\text{zip}^{\prime}=\text{zip}$. We apply
$\text{zip}^{\prime}$ to arbitrary arguments $p$, $q$ and write:
\[
\text{zip}^{\prime}\,(p^{:L^{A}}\times q^{:L^{B}})=\text{map}_{2}\,(p\times q)(\text{id}^{:A\times B\rightarrow A\times B})=(p\times q)\triangleright\text{zip}\triangleright\text{id}^{\uparrow L}=(p\times q)\triangleright\text{zip}=\text{zip}\,(p\times q)\quad.
\]

It remains to show that the \lstinline!map2! method will satisfy
the naturality law if defined via \lstinline!zip!:
\begin{align*}
 & \gunderline{\text{map}_{2}\,(p\times q)(f)}\triangleright g^{\uparrow L}=(p\times q)\triangleright\text{zip}\triangleright f^{\uparrow L}\triangleright g^{\uparrow L}=(p\times q)\triangleright\text{zip}\triangleright(f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{definition of }\text{map}_{2}\text{ via }\text{zip}:}\quad & =\text{map}_{2}\,(p\times q)(f\bef g)\quad.
\end{align*}
$\square$

The equivalence of \lstinline!map2! and \lstinline!zip! follows
a pattern similar to one shown in Section~\ref{subsec:Yoneda-identities}:
a function with one type parameter (a natural transformation) is equivalent
to a function with two type parameters if a naturality law holds with
respect to one of those type parameters. Here \lstinline!zip! is
playing the role of the natural transformation, and \lstinline!map2!
is a function obeying a naturality law. The proof of Statement~\ref{subsec:Statement-map2-zip-equivalence}
is similar to the proof of Statement~\ref{subsec:Statement-tr-equivalent-to-ftr}.

However, \lstinline!map2! is not a \textsf{``}lifting\textsf{''} in the sense of
Section~\ref{subsec:Yoneda-identities}. The method \textsf{``}\lstinline!ap!\textsf{''}
better fits the role of a lifting since its type signature ($\text{ap}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{B}$)
transforms functions wrapped under $L$ (i.e., functions of type $L^{A\rightarrow B}$)
to functions of type $L^{A}\rightarrow L^{B}$. So, let us prove that
\lstinline!ap! is equivalent to \lstinline!map2!. Since the type
signature of \lstinline!ap! is curried, it is more convenient to
use the curried version (\lstinline!fmap2!) instead of \lstinline!map2!.
Let us write the relationship between \lstinline!ap! and \lstinline!fmap2!
in the code notation:
\[
\xymatrix{\xyScaleY{0.4pc}\xyScaleX{2pc} & F^{B\rightarrow C}\ar[rd]\sp(0.5){\text{ap}^{B,C}}\\
F^{A}\ar[ru]\sp(0.45){f^{\uparrow L}}\ar[rr]\sb(0.43){\text{fmap}_{2}\,(f^{:A\rightarrow B\rightarrow C})} &  & \left(F^{B}\rightarrow F^{C}\right)
}
\]
\begin{align*}
 & \text{fmap}_{2}\,(f^{:A\rightarrow B\rightarrow C})=f^{\uparrow L}\bef\text{ap}^{B,C}\quad,\\
 & \text{ap}^{A,B}=\text{fmap}_{2}\,(\text{id}^{:\left(A\rightarrow B\right)\rightarrow A\rightarrow B})\quad.
\end{align*}

We are now ready to prove the equivalence of \lstinline!ap! and \lstinline!fmap2!:

\subsubsection{Statement \label{subsec:Statement-fmap2-equivalence-to-ap}\ref{subsec:Statement-fmap2-equivalence-to-ap}
(equivalence of \lstinline!ap! and \lstinline!fmap2!)}

For any functor $L$ for which \lstinline!fmap2! or \lstinline!ap!
can be implemented, the type of functions \lstinline!ap! (type signature
$\text{ap}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{B}$)
is equivalent to the type of functions \lstinline!fmap2! (type signature
$\left(A\rightarrow B\rightarrow C\right)\rightarrow L^{A}\rightarrow L^{B}\rightarrow L^{C}$),
assuming that the functions \lstinline!fmap2! satisfy the naturality
law with respect to the type parameter $C$.

\subparagraph{Proof}

We need to show equivalence in two directions (from \lstinline!fmap2!
to \lstinline!ap! and back).

\textbf{(1)} Given any function $\text{fmap}_{2}:\left(A\rightarrow B\rightarrow C\right)\rightarrow L^{A}\rightarrow L^{B}\rightarrow L^{C}$
satisfying the naturality law,
\[
\text{fmap}_{2}\,(g^{:X\rightarrow A}\bef f^{:A\rightarrow B\rightarrow C})(p^{:L^{X}})=\text{fmap}_{2}\,(f)(p\triangleright g^{\uparrow L})\quad,
\]
we first define an \lstinline!ap! method:
\[
\text{ap}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{B}\quad,\quad\quad\text{ap}\,(r^{:L^{A\rightarrow B}})\triangleq\text{fmap}_{2}\,(\text{id}^{:(A\rightarrow B)\rightarrow A\rightarrow B})(r)\quad,
\]
and then define a new \lstinline!fmap2!$^{\prime}$ function through
that \lstinline!ap! method:
\[
\text{fmap}_{2}^{\prime}\,(f^{:A\rightarrow B\rightarrow C})(p^{:L^{A}})\triangleq p\triangleright f^{\uparrow L}\triangleright\text{ap}\quad.
\]
Then we need to show that $\text{fmap}_{2}^{\prime}=\text{fmap}_{2}$.
We apply $\text{fmap}_{2}^{\prime}$ to arbitrary arguments and write:
\begin{align*}
 & \text{fmap}_{2}^{\prime}\,(f^{:A\rightarrow B\rightarrow C})(p^{:L^{A}})=p\triangleright f^{\uparrow L}\triangleright\text{ap}=\text{fmap}_{2}\,(\text{id})(p\triangleright f^{\uparrow L})\\
{\color{greenunder}\text{naturality law of }\text{fmap}_{2}:}\quad & =\text{fmap}_{2}\,(f\bef\text{id})(p)=\text{fmap}_{2}\,(f)(p)\quad.
\end{align*}

\textbf{(2)} Given any function $\text{ap}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{A}$,
we first define \lstinline!fmap2!:
\[
\text{fmap}_{2}\,(f^{:A\rightarrow B\rightarrow C})\triangleq f^{\uparrow L}\bef\text{ap}\quad,
\]
and then define a new \lstinline!ap!$^{\prime}$ function through
that \lstinline!fmap2! method:
\[
\text{ap}^{\prime}\triangleq\text{fmap}_{2}\,(\text{id})\quad.
\]
Then we need to show that $\text{ap}^{\prime}=\text{ap}$. We write:
\[
\text{ap}^{\prime}=\text{fmap}_{2}\,(\text{id})=\text{id}^{\uparrow L}\bef\text{ap}=\text{ap}\quad.
\]

It remains to show that the \lstinline!fmap2! method will satisfy
the naturality law if defined via \lstinline!ap!:
\begin{align*}
 & \text{fmap}_{2}\,(g\bef f)(p)=p\triangleright(g\bef f)^{\uparrow L}\bef\text{ap}=p\triangleright g^{\uparrow L}\triangleright\gunderline{f^{\uparrow L}\bef\text{ap}}\\
{\color{greenunder}\text{definition of }\text{fmap}_{2}\text{ via }\text{ap}:}\quad & =(p\triangleright g^{\uparrow L})\triangleright\text{fmap}_{2}\,(f)=\text{fmap}_{2}\,(f)(p\triangleright g^{\uparrow L})\quad.
\end{align*}
$\square$

Since \lstinline!zip! is equivalent to \lstinline!map2! and \lstinline!map2!
is equivalent to \lstinline!ap!, it follows (assuming suitable naturality
laws) that \lstinline!zip! is equivalent to \lstinline!ap!. In Scala,
the relationship between \lstinline!zip! and \lstinline!ap! is coded
like this:
\begin{lstlisting}
def zip[A, B](la: L[A], lb: L[B]): L[(A, B)] =    // Assuming that `ap` is already defined.
  ap[B, (A, B)](la.map {a => (b: B) => (a, b)} )(lb)

def ap[A, B](lf: L[A => B])(la: L[A]): L[B] =     // Assuming that `zip` is already defined.
  zip(lf, la).map { case (f, a) => f(a) }
\end{lstlisting}


\subsubsection{Statement \label{subsec:Statement-zip-ap-equivalence}\ref{subsec:Statement-zip-ap-equivalence}}

The functions \lstinline!zip! and \lstinline!ap! are equivalent
assuming the following naturality laws:
\[
(f^{\uparrow L}\boxtimes\text{id})\bef\text{zip}=\text{zip}\bef(f\boxtimes\text{id})^{\uparrow L}\quad,\quad\big(\text{ap}\,(r^{:L^{A\rightarrow B}})(p^{:L^{A}})\big)\triangleright(f^{:B\rightarrow C})^{\uparrow L}=\text{ap}\big(r\triangleright(x^{:A\rightarrow B}\rightarrow x\bef f)^{\uparrow L}\big)(p)\quad.
\]


\subparagraph{Proof}

We need to show equivalence in two directions (from \lstinline!zip!
to \lstinline!ap! and back). To write the relationships between \lstinline!zip!
and \lstinline!ap! more easily, let us denote by \lstinline!pair!
and \lstinline!eval! the following two functions:
\[
\text{pair}^{:A\rightarrow B\rightarrow A\times B}\triangleq a\rightarrow b\rightarrow a\times b\quad,\quad\quad\text{eval}^{:\left(A\rightarrow B\right)\times A\rightarrow B}\triangleq g^{:A\rightarrow B}\times a^{:A}\rightarrow g(a)\quad.
\]
The code of these fully parametric functions follows from their types.
Then we can write:
\[
\text{zip}\,(p^{:L^{A}}\times q^{:L^{B}})=\text{ap}\,(p\triangleright\text{pair}^{\uparrow L})(q)\quad,\quad\quad\text{ap}\,(r^{:L^{A\rightarrow B}})(p^{:L^{A}})=(r\times p)\triangleright\text{zip}\triangleright\text{eval}^{\uparrow L}\quad.
\]

The functions \lstinline!pair! and \lstinline!eval! satisfy the
following property that we will use in the proof:
\begin{equation}
(\text{pair}^{:A\rightarrow B\rightarrow A\times B}\boxtimes\text{id}^{:B\rightarrow B})\bef\text{eval}^{:\left(B\rightarrow A\times B\right)\times B\rightarrow A\times B}=\text{id}^{:A\times B\rightarrow A\times B}\quad.\label{eq:pair-and-eval-property-derivation1}
\end{equation}
To prove this property, apply both sides to arbitrary $a^{:A}$ and
$b^{:B}$:
\[
(a^{:A}\times b^{:B})\triangleright(\text{pair}\boxtimes\text{id})\bef\text{eval}=\big((z^{:B}\rightarrow a\times z)\times b\big)\triangleright\text{eval}=(z^{:B}\rightarrow a\times z)(b)=a\times b\quad.
\]

\textbf{(1)} Given any function $\text{zip}:L^{A}\times L^{B}\rightarrow L^{A\times B}$
satisfying the naturality law shown above, we define an \lstinline!ap!
method by $\text{ap}\,(r)(p)\triangleq(r\times p)\triangleright\text{zip}\triangleright\text{eval}^{\uparrow L}$
and then define a new \lstinline!zip!$^{\prime}$ function through
that \lstinline!ap!:
\[
\text{zip}^{\prime}\,(p^{:L^{A}}\times q^{:L^{B}})\triangleq\text{ap}\,(p\triangleright\text{pair}^{\uparrow L})(q)\quad.
\]
Then we need to show that $\text{zip}^{\prime}=\text{zip}$. We apply
$\text{zip}^{\prime}$ to arbitrary arguments and write:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{zip}\,(p\times q):}\quad & \text{zip}^{\prime}\,(p^{:L^{A}}\times q^{:L^{B}})=\text{ap}\,(p\triangleright\text{pair}^{\uparrow L})(q)\\
{\color{greenunder}\text{definition of }\text{ap}:}\quad & =\big((p\triangleright\text{pair}^{\uparrow L})\times q\big)\triangleright\text{zip}\bef\text{eval}^{\uparrow L}=(p\times q)\triangleright\gunderline{(\text{pair}^{\uparrow L}\times\text{id})\bef\text{zip}}\bef\text{eval}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =(p\times q)\triangleright\text{zip}\bef\gunderline{(\text{pair}\boxtimes\text{id})^{\uparrow L}\bef\text{eval}^{\uparrow L}}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:pair-and-eval-property-derivation1})}:}\quad & =(p\times q)\triangleright\text{zip}\bef\text{id}^{\uparrow L}=(p\times q)\triangleright\text{zip}\quad.
\end{align*}
It remains to show that the \lstinline!ap! method defined via \lstinline!zip!
will satisfy its required naturality law:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \big(\text{ap}\,(r)(p)\big)\triangleright f^{\uparrow L}=(r\times p)\triangleright\text{zip}\bef\text{eval}^{\uparrow L}\bef f^{\uparrow L}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{ap}\big(r\triangleright(x\rightarrow x\bef f)^{\uparrow L}\big)(p)=\big((r\triangleright(x\rightarrow x\bef f)^{\uparrow L})\times p\big)\triangleright\text{zip}\bef\text{eval}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & \quad=(r\times p)\triangleright\text{zip}\bef\big((x\rightarrow x\bef f)\boxtimes\text{id}\big)^{\uparrow L}\bef\text{eval}^{\uparrow L}\quad.
\end{align*}
The difference between the two sides of the law will disappear if
we show that:
\[
\text{eval}\bef f\overset{?}{=}\big((x\rightarrow x\bef f)\boxtimes\text{id}\big)\bef\text{eval}\quad.
\]
Apply both sides to an arbitrary pair $g\times a$ of type $\left(A\rightarrow B\right)\times A$
and obtain equal results:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (g^{:A\rightarrow B}\times a^{:A})\triangleright\text{eval}\bef f=(g\times a)\triangleright\text{eval}\triangleright f=g(a)\triangleright f=a\triangleright g\triangleright f\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & (g\times a)\triangleright\big((x\rightarrow x\bef f)\boxtimes\text{id}\big)\bef\text{eval}=(g\times a)\triangleright\big((x\rightarrow x\bef f)\boxtimes\text{id}\big)\triangleright\text{eval}\\
 & \quad=\big((g\bef f)\times a\big)\triangleright\text{eval}=a\triangleright g\bef f=a\triangleright g\bef f\quad.
\end{align*}

\textbf{(2)} Given any function $\text{ap}:L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{A}$,
we first define \lstinline!zip!:
\[
\text{zip}\,(p^{:L^{A}}\times q^{:L^{B}})\triangleq\text{ap}\big(p\triangleright\text{pair}^{\uparrow L}\big)(q)\quad,
\]
and then define a new \lstinline!ap!$^{\prime}$ function through
that \lstinline!zip! method:
\[
\text{ap}^{\prime}\,(r^{:L^{A\rightarrow B}})(p^{:L^{A}})\triangleq(r\times p)\triangleright\text{zip}\triangleright\text{eval}^{\uparrow L}\quad.
\]
Then we need to show that $\text{ap}^{\prime}=\text{ap}$. We write:
\begin{align*}
 & \text{ap}^{\prime}\,(r)(p)=(r\times p)\triangleright\text{zip}\triangleright\text{eval}^{\uparrow L}=\big(\text{ap}\,(r\triangleright\text{pair}^{\uparrow L})(p)\gunderline{\big)\triangleright\text{eval}^{\uparrow L}}\quad.\\
{\color{greenunder}\text{naturality law of }\text{ap}:}\quad & =\text{ap}\big(r\,\gunderline{\triangleright\,\text{pair}^{\uparrow L}\triangleright(x^{:A\rightarrow\left(A\rightarrow B\right)\times A}\rightarrow x\bef\text{eval})^{\uparrow L}}\big)(p)\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{ap}\big(r\triangleright(\text{pair}\bef(x\rightarrow x\bef\text{eval}))^{\uparrow L}\big)(p)\quad.
\end{align*}
The remaining difference between $\text{ap}^{\prime}(r)(p)$ and $\text{ap}\,(r)(p)$
will disappear if we show that: 
\[
\text{pair}^{:\left(A\rightarrow B\right)\rightarrow A\rightarrow\left(A\rightarrow B\right)\times A}\bef(x^{:A\rightarrow\left(A\rightarrow B\right)\times A}\rightarrow x\bef\text{eval})\overset{?}{=}\text{id}\quad.
\]
To see this, apply both sides to an arbitrary value $f$ of type $A\rightarrow B$:
\begin{align*}
{\color{greenunder}\text{expect to equal }f:}\quad & f^{:A\rightarrow B}\triangleright\text{pair}^{:\left(A\rightarrow B\right)\rightarrow A\rightarrow\left(A\rightarrow B\right)\times A}\bef(x^{:A\rightarrow\left(A\rightarrow B\right)\times A}\rightarrow x\bef\text{eval})\\
 & =(a^{:A}\rightarrow f\times a)\bef(x\rightarrow x\bef\text{eval})=a^{:A}\rightarrow(f\times a)\triangleright\text{eval}=a^{:A}\rightarrow f(a)=f\quad.
\end{align*}

Finally, we show that the \lstinline!zip! method will satisfy its
naturality law if defined via \lstinline!ap!:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (p^{:L^{A}}\times q^{:L^{B}})\triangleright(f^{\uparrow L}\boxtimes\text{id})\bef\text{zip}=\text{zip}\big((p\triangleright f^{\uparrow L})\times q\big)\\
{\color{greenunder}\text{express }\text{zip}\text{ via }\text{ap}:}\quad & \quad=\text{ap}\big(p\triangleright f^{\uparrow L}\triangleright\text{pair}^{\uparrow L})(q)=\text{ap}\big(p\triangleright(f\bef\text{pair})^{\uparrow L})(q)\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & (p^{:L^{A}}\times q^{:L^{B}})\triangleright\text{zip}\triangleright(f\boxtimes\text{id})^{\uparrow L}=\big(\text{ap}\,(p\triangleright\text{pair}^{\uparrow L})(q)\gunderline{\big)\triangleright(f\boxtimes\text{id})^{\uparrow L}}\\
{\color{greenunder}\text{naturality law of }\text{ap}:}\quad & \quad=\text{ap}\big(p\triangleright\text{pair}^{\uparrow L}\triangleright(x\rightarrow x\bef(f\boxtimes\text{id}))^{\uparrow L}\big)(q)\quad.
\end{align*}
The remaining difference will disappear if for any $f^{:Z\rightarrow A}$,
the following equation holds:
\[
f^{:Z\rightarrow A}\bef\text{pair}^{:A\rightarrow B\rightarrow A\times B}\overset{?}{=}\text{pair}^{:Z\rightarrow B\rightarrow Z\times B}\bef(x^{:B\rightarrow Z\times B}\rightarrow x\bef(f\boxtimes\text{id}))\quad.
\]
To verify this equation, substitute the definition of \lstinline!pair!
into both sides:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & f\bef\text{pair}=(z^{:Z}\rightarrow f(z))\bef(a^{:A}\rightarrow b^{:B}\rightarrow a\times b)=z\rightarrow b\rightarrow f(z)\times b\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{pair}\bef(x\rightarrow x\bef(f\boxtimes\text{id}))=(z^{:Z}\rightarrow b^{:B}\rightarrow z\times b)\bef(x^{:B\rightarrow Z\times B}\rightarrow x\bef(f\boxtimes\text{id}))\\
{\color{greenunder}\text{compute composition}:}\quad & \quad=z\rightarrow(b\rightarrow z\times b)\bef(f\boxtimes\text{id})=z\rightarrow b\rightarrow f(z)\times b\quad.
\end{align*}
Both sides are now equal. $\square$

\subsection{The \texttt{Zippable} and \texttt{Applicative} typeclasses\label{subsec:The-Zippable-and-Applicative-typeclass}}

The previous section showed that \lstinline!map2!, \lstinline!zip!,
and \lstinline!ap! are equivalent to each other, assuming that certain
naturality laws hold. (In practice, implementations of these methods
always satisfy the naturality laws with respect to each type parameter.)
We can encapsulate the functionality of these methods into a \textsf{``}zippable
functor\textsf{''} typeclass\footnote{A similar typeclass is called \textsf{``}\lstinline!Zip!\textsf{''} in \texttt{scalaz}
and \textsf{``}\lstinline!Semigroupal!\textsf{''} in \texttt{cats}.} whose \lstinline!map2! and \lstinline!ap! methods are defined via
\lstinline!zip!:\index{typeclass!Zippable@\texttt{Zippable}}
\begin{lstlisting}
abstract class Zippable[L[_]: Functor] {
  def zip[A, B](la: L[A], lb: L[B]): L[(A, B)]
  def map2[A, B, C](la: L[A], lb: L[B])(f: (A, B) => C): L[C] = zip(la, lb).map(Function.untupled(f))
  def ap[A, B](lf: L[A => B], la: L[A]): L[B] = zip(lf, la).map { case (f, a) => f(a) }
}
\end{lstlisting}
Instead of using \lstinline!zip!, either \lstinline!map2! or \lstinline!ap!
could be used to implement the other methods.

In addition to these methods, it is helpful to require a \lstinline!pure!
method for the functor \lstinline!L!. The resulting typeclass\index{typeclass!Applicative@\texttt{Applicative}}
is known as \lstinline!Applicative!. The \lstinline!pure! method
is equivalent to a \textsf{``}wrapped unit\textsf{''} (denoted \lstinline!wu!, see
Section~\ref{subsec:Pointed-functors-motivation-equivalence}). So,
the simplest definition of the \lstinline!Applicative! typeclass
contains just these two methods:
\begin{lstlisting}
trait Applicative[L[_]] {
  def zip[A, B](la: L[A], lb: L[B]): L[(A, B)]
  def wu: L[Unit]
}
\end{lstlisting}
Other methods (\lstinline!map2!, \lstinline!ap!, \lstinline!pure!)
can be defined separately (as extension methods) using the functor
instance for \lstinline!L!. However, this definition of the \lstinline!Applicative!
typeclass can be used also with type constructors \lstinline!L[A]!
that are not covariant in \lstinline!A!. So, we will use this definition
later in this chapter.

\subsection{Motivation for the laws of \texttt{map2}\label{subsec:Motivation-for-the-laws-of-map2}}

We now turn to laws that an applicative functor must satisfy. The
motivation for the laws comes from treating \lstinline!map2(la, lb)(f)!
as a replacement for the following code (assuming \lstinline!L! is
a monad):
\begin{lstlisting}
def monadicMap2(la: L[A], lb: L[B])(f: (A, B) => C): L[C] = for {
  x <- la
  y <- lb
} yield f(x, y)
\end{lstlisting}
Unlike the code of \lstinline!monadicMap2!, which uses the monad\textsf{'}s
methods, \lstinline!map2(la, lb)! assumes that the effects in \lstinline!la!
and \lstinline!lb! are independent of the wrapped values (such as
\lstinline!x! and \lstinline!y!). But the type signatures of \lstinline!monadicMap2!
and \lstinline!map2! are the same, and for some type constructors
(e.g., the \lstinline!Reader! monad) the results are also the same.
So, we will require that \lstinline!map2! should obey the same laws
as the code in \lstinline!monadicMap2!.

To derive the laws of \lstinline!map2!, we will specialize the laws
of monads (which are imposed on \lstinline!flatMap! and \lstinline!pure!)
to the code of the form of \lstinline!monadicMap2! and then express
those laws in terms of \lstinline!map2! and \lstinline!pure!.

We begin with the associativity law of monads, which says that these
three expressions are equal:

\vspace{0.2\baselineskip}

\noindent %
\begin{minipage}[c][1\totalheight][t]{0.3\columnwidth}%
\begin{lstlisting}
for {
  x <- la
  y <- lb
  z <- lc
} yield g(x, y, z)
\end{lstlisting}
%
\end{minipage}\hfill{}%
\begin{minipage}[c][1\totalheight][t]{0.3\columnwidth}%
\begin{lstlisting}
for {
  x <- la
  (y, z) <- for {
         yy <- lb
         zz <- lc
       } yield (yy, zz)
} yield g(x, y, z)
\end{lstlisting}
%
\end{minipage}\hfill{}%
\begin{minipage}[c][1\totalheight][t]{0.3\columnwidth}%
\begin{lstlisting}
for {
  (x, y) <- for {
              xx <- la
              yy <- lb
            } yield (xx, yy)
   z <- lc
} yield g(x, y, z)
\end{lstlisting}
%
\end{minipage}\\
Expressed via \lstinline!map2! and \lstinline!map3!, that law gives
an equation between the following function calls:
\begin{lstlisting}
map3(la, lb, lc) { (a, b, c) => g(a, b, c) }
  == map2(la, map2(lb, lc) { (b, c) => (b, c) } { case (x, (y, z)) => g(x, y, z) }
  == map2(map2(la, lb) { (a, b) => (a, b) }, lc) { case ((x, y), z)) => g(x, y, z) } 
\end{lstlisting}
In the code notation, this law is written as:
\begin{align*}
 & \text{map}_{3}\,(p^{:L^{A}}\times q^{:L^{B}}\times r^{:L^{C}})(g^{:A\times B\times C\rightarrow D})\\
 & =\text{map}_{2}\,(p\times\text{map}_{2}\,(q\times r)(\text{id}^{:B\times C\rightarrow B\times C}))\big(a^{:A}\times(b^{:B}\times c^{:C})\rightarrow g(a\times b\times c)\big)\\
 & =\text{map}_{2}\,(\text{map}_{2}\,(p\times q)(\text{id})\times r)\big((a^{:A}\times b^{:B})\times c^{:C}\rightarrow g(a\times b\times c)\big)\quad.
\end{align*}
This law guarantees that \lstinline!map3! can be expressed unambiguously
through \lstinline!map2! regardless of the order in which we group
the arguments. This is the \index{associativity law!of map2@of \texttt{map2}}\textbf{associativity
law} of \lstinline!map2!.

Next, we consider the monadic left identity law. That law says that
the following codes are equal:

\vspace{0.2\baselineskip}

\noindent %
\begin{minipage}[c][1\totalheight][t]{0.4\columnwidth}%
\begin{lstlisting}
for {
  x <- pure(a)
  y <- lb
} yield g(x, y)
\end{lstlisting}
%
\end{minipage}\hfill{}%
\begin{minipage}[c][1\totalheight][t]{0.4\columnwidth}%
\begin{lstlisting}
for {
  // No need to define x. 
  y <- lb
} yield g(a, y)
\end{lstlisting}
%
\end{minipage}

Writing this in terms of \lstinline!map2!, we obtain the equation:
\begin{lstlisting}
map2(pure(a), lb)(g) == lb.map { y => g(a, y) }
\end{lstlisting}

The monadic right identity law says that the following two expressions
are equal:

\vspace{0.2\baselineskip}

\noindent %
\begin{minipage}[c][1\totalheight][t]{0.4\columnwidth}%
\begin{lstlisting}
for {
  x <- la
  y <- pure(b)
} yield g(x, y)
\end{lstlisting}
%
\end{minipage}\hfill{}%
\begin{minipage}[c][1\totalheight][t]{0.4\columnwidth}%
\begin{lstlisting}
for {
  x <- la
  // No need to define y. 
} yield g(x, b)
\end{lstlisting}
%
\end{minipage}\\
Writing this in terms of \lstinline!map2!, we obtain the equation:
\begin{lstlisting}
map2(la, pure(b))(g) == la.map { x => g(x, b) }
\end{lstlisting}

In the code notation, the two \index{identity laws!of map2@of \texttt{map2}}\textbf{identity
laws} of \lstinline!map2! are written as:
\begin{align*}
{\color{greenunder}\text{left identity law}:}\quad & \text{map}_{2}\,(\text{pu}_{L}(a^{:A})\times q^{:L^{B}})(g^{:A\times B\rightarrow C})=q\triangleright(b^{:B}\rightarrow g(a\times b))^{\uparrow L}\quad,\\
{\color{greenunder}\text{right identity law}:}\quad & \text{map}_{2}\,(p^{:L^{A}}\times\text{pu}_{L}(b^{:B}))(g^{:A\times B\rightarrow C})=p\triangleright(a^{:A}\rightarrow g(a\times b))^{\uparrow L}\quad.
\end{align*}

To simplify and analyze the laws of \lstinline!map2!, we will now
derive the laws of \lstinline!zip! that will follow once we express
\lstinline!map2! via \lstinline!zip!.

\subsection{Deriving the laws of \texttt{zip} from the laws of \texttt{map2}\label{subsec:Deriving-the-laws-of-zip}}

To derive the laws of \lstinline!zip! that follow from the identity
and associativity laws of \lstinline!map2!, we express \lstinline!map2!
via \lstinline!zip! and then substitute into those laws:
\begin{equation}
\text{map}_{2}\,(p^{:L^{A}}\times q^{:L^{B}})(f^{:A\times B\rightarrow C})=\text{zip}\,(p\times q)\triangleright f^{\uparrow L}\quad.\label{eq:express-map2-via-zip}
\end{equation}

Begin with the associativity law and write its two sides separately:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{map}_{2}\,(p^{:L^{A}}\times\text{map}_{2}\,(q^{:L^{B}}\times r^{:L^{C}})(\text{id}^{:B\times C\rightarrow B\times C}))\big(a^{:A}\times(b^{:B}\times c^{:C})\rightarrow g(a\times b\times c)\big)\\
 & \quad=\text{zip}\,(p\times\text{zip}\,(q\times r))\triangleright(a\times(b\times c)\rightarrow g(a\times b\times c))^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & \overset{!}{=}\text{map}_{2}\,(\text{map}_{2}\,(p\times q)(\text{id})\times r)\big((a^{:A}\times b^{:B})\times c^{:C}\rightarrow g(a\times b\times c)\big)\\
 & \quad=\text{zip}\,(\text{zip}\,(p\times q)\times r)\triangleright((a\times b)\times c)\rightarrow g(a\times b\times c))^{\uparrow L}\quad.
\end{align*}
Both sides now depend on an arbitrary function $g^{:A\times B\times C\rightarrow D}$
in almost the same way, except for a rearrangement of the nested products.
We can refactor the code to remove the dependency on $g$. Define
two functions $\varepsilon_{1,23}$ and $\varepsilon_{12,3}$ that
rearrange the tuples:
\[
\varepsilon_{1,23}\triangleq a^{:A}\times(b^{:B}\times c^{:C})\rightarrow a\times b\times c\quad,\quad\quad\varepsilon_{12,3}\triangleq(a^{:A}\times b^{:B})\times c\rightarrow a\times b\times c\quad.
\]
Then we can rewrite the two sides of the associativity law as:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}\,(p\times\text{zip}\,(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}\bef g^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & \overset{!}{=}\text{zip}\,(\text{zip}\,(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}\bef g^{\uparrow L}\quad.
\end{align*}
We can now omit $g^{\uparrow L}$ (see Exercise~\ref{subsec:Exercise-simplify-law-omit-lifted-function})
and obtain an equivalent law:
\begin{equation}
\text{zip}\big(p\times\text{zip}\left(q\times r\right)\big)\triangleright\varepsilon_{1,23}^{\uparrow L}\overset{!}{=}\text{zip}\big(\text{zip}\left(p\times q\right)\times r\big)\triangleright\varepsilon_{12,3}^{\uparrow L}\quad.\label{eq:zip-associativity-law-with-epsilons}
\end{equation}
A type diagram illustrating this law is shown below:
\[
\xymatrix{\xyScaleY{1.4pc}\xyScaleX{1.7pc} & L^{A\times B}\times L^{C}\ar[r]\sp(0.5){\text{zip}} & L^{(A\times B)\times C}\ar[rd]\sp(0.5){\varepsilon_{12,3}^{\uparrow L}} &  & L^{A\times(B\times C)}\ar[ld]\sb(0.5){\varepsilon_{1,23}^{\uparrow L}} & L^{A}\times L^{B\times C}\ar[l]\sb(0.5){\text{zip}}\\
L^{A}\times L^{B}\ar[r]\sp(0.55){\text{zip}} & L^{A\times B}\ar[u] &  & L^{A\times B\times C} &  & L^{B\times C}\ar[u] & L^{B}\times L^{C}\ar[l]\sb(0.5){\text{zip}}\\
 &  & p:L^{A}\ar[llu]\ar[rrruu] & q:L^{B}\ar[lllu]\ar[rrru] & r:L^{C}\ar[rru]\ar[llluu]
}
\]

The functions $\varepsilon_{1,23}$ and $\varepsilon_{12,3}$ implement
the conversion of the equivalent types $(A\times B)\times C$ and
$A\times(B\times C)$ to $A\times B\times C$. Since $L$ is a functor,
the lifted functions $\varepsilon_{1,23}^{\uparrow L}$ and $\varepsilon_{12,3}^{\uparrow L}$
produce equivalences between the types $L^{(A\times B)\times C}$,
$L^{A\times(B\times C)}$, and $L^{A\times B\times C}$. With these
equivalences in mind, we rewrite the associativity law in a simpler
form:\index{associativity law!of zip@of \texttt{zip}}
\[
\text{zip}\big(p\times\text{zip}\left(q\times r\right)\big)\cong\text{zip}\big(\text{zip}\left(p\times q\right)\times r\big)\quad.
\]
To make this law more visually clear, let us temporarily use the infix
syntax for \lstinline!zip! and write:
\[
p\,\,\text{zip}\,\,q\triangleq\text{zip}\big(p\times q\big)\quad.
\]
In this notation, the associativity law is:
\begin{equation}
p\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,r)\cong(p\,\,\text{zip}\,\,q)\,\,\text{zip}\,\,r\quad.\label{eq:zip-associativity-law}
\end{equation}
In Eq.~(\ref{eq:zip-associativity-law}), the symbol $\cong$ denotes
equality up to the type equivalence. To obtain a real equation, one
would need to apply $\varepsilon_{1,23}^{\uparrow L}$ and $\varepsilon_{12,3}^{\uparrow L}$
at appropriate places. Apart from that, the law~(\ref{eq:zip-associativity-law})
has the usual form of an associativity law for a binary operation.

When writing laws with implied type equivalences, as in the formulation~(\ref{eq:zip-associativity-law}),
we help build the intuition about applicative laws. We also save time
because we do not write out a number of tuple-swapping functions.
To avoid errors, derivations using this technique must first check
that all types match up to tuple-swapping isomorphisms.

We now turn to the identity laws. Substitute Eq.~(\ref{eq:express-map2-via-zip})
into \lstinline!map2!\textsf{'}s left identity law:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{map}_{2}\,(\text{pu}_{L}(a^{:A})\times q^{:L^{B}})(g)=\text{zip}\,(\text{pu}_{L}(a)\times q)\triangleright g^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & \overset{!}{=}q\triangleright(b^{:B}\rightarrow g(a\times b))^{\uparrow L}=q\triangleright(b^{:B}\rightarrow a\times b)^{\uparrow L}\triangleright g^{\uparrow L}\quad.
\end{align*}
We may remove the common function $g^{\uparrow L}$ from both sides
and get an equivalent law:
\[
\text{zip}\,(\text{pu}_{L}(a)\times q)\overset{!}{=}q\triangleright(b\rightarrow a\times b)^{\uparrow L}\quad.
\]
We express \lstinline!pure! through the \textsf{``}wrapped unit\textsf{''} value\index{wrapped@\textsf{``}wrapped unit\textsf{''} value}
(\lstinline!wu!) as we did in Section~\ref{subsec:Pointed-functors-motivation-equivalence}:
\[
\text{wu}:L^{\bbnum 1}\quad,\quad\quad\text{pu}_{L}(a)=\text{wu}\triangleright(1\rightarrow a)^{\uparrow L}\quad.
\]
Then we get:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}\,(\text{pu}_{L}(a)\times q)=\text{zip}\big((\text{wu}\triangleright(1\rightarrow a)^{\uparrow L})\times q\big)\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & \quad=\text{zip}\,(\text{wu}\times q)\triangleright(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & \overset{!}{=}q\triangleright(b\rightarrow a\times b)^{\uparrow L}\quad.
\end{align*}
To simplify this equation further, we note that the function $(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}$
in the left-hand side and the function $(b^{:B}\rightarrow a\times b)^{\uparrow L}$
in the right-hand side are similar. The only difference is between
the arguments $1\times b$ and $b$. But the types $\bbnum 1\times B$
and $B$ are equivalent. Let us define a function called \lstinline!ilu!
(\textsf{``}insert left unit\textsf{''}) implementing this equivalence:
\[
\text{ilu}:B\rightarrow\bbnum 1\times B\quad,\quad\quad\text{ilu}\triangleq b^{:B}\rightarrow1\times b\quad.
\]
Now we can express both sides of the law as:
\begin{equation}
\text{zip}\,(\text{wu}\times q)\triangleright(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}\overset{!}{=}q\triangleright\text{ilu}^{\uparrow L}\triangleright(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}\quad.\label{eq:left-identity-zip-derivation1}
\end{equation}
Both sides contain the function $(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}$
with an arbitrary value $a^{:A}$ of arbitrary type $A$. Is it correct
to simplify the law by discarding that function? To show that it is,
we first set $A\triangleq\bbnum 1$ (since the type $A$ may be chosen
arbitrarily) and obtain the function:
\[
(1\times b^{:B}\rightarrow1\times b)^{\uparrow L}=\text{id}^{\uparrow L}=\text{id}^{:L^{\bbnum 1\times B}\rightarrow L^{\bbnum 1\times B}}\quad.
\]
Applying an identity function to both sides of Eq.~(\ref{eq:left-identity-zip-derivation1}),
we find:\index{identity laws!of zip@of \texttt{zip}}
\begin{equation}
\text{zip}\,(\text{wu}\times q)=q\triangleright\text{ilu}^{\uparrow L}\quad.\label{eq:zip-left-identity-law}
\end{equation}
This equation is a consequence of Eq.~(\ref{eq:left-identity-zip-derivation1}).
At the same time, we may apply the function $(1\times b^{:B}\rightarrow a\times b)^{\uparrow L}$,
this time with an arbitrary $a^{:A}$, to both sides of Eq.~(\ref{eq:zip-left-identity-law})
and recover Eq.~(\ref{eq:left-identity-zip-derivation1}). So, we
have justified the simplification of the left identity law to Eq.~(\ref{eq:zip-left-identity-law}).

Since $L$ is a functor, the conversion function $\text{ilu}^{\uparrow L}$
implements the type equivalence $L^{B}\cong L^{\bbnum 1\times B}$.
Denoting this equivalence by $\cong$ and using the infix syntax for
\lstinline!zip!, we rewrite the left identity law as:
\[
\text{wu}\,\,\text{zip}\,\,q\cong q\quad.
\]
Apart from the implied conversion function, this is a familiar form
of the left identity law: a value $q^{:L^{B}}$ remains unchanged
(up to type equivalence) when \textsf{``}zipping\textsf{''} it with the special value
\lstinline!wu!.

A similar derivation shows that the right identity law of \lstinline!zip!
is:
\[
\text{zip}\,(p^{:L^{A}}\times\text{wu})=p\triangleright\text{iru}^{\uparrow L}\quad,\quad\quad\text{iru}\triangleq a^{:A}\rightarrow a\times1\quad,
\]
or using the infix syntax and the implied equivalence between the
types $L^{A}$ and $L^{A\times\bbnum 1}$:
\[
p\,\,\text{zip}\,\,\text{wu}\cong p\quad.
\]
So, \lstinline!wu! is the \textsf{``}empty value\textsf{''} for the binary operation
\lstinline!zip!.

We have shown that the laws of \lstinline!map2! can be simplified
when formulated via \lstinline!zip!. In that formulation, the laws
of applicative functors are similar to the laws of a \emph{monoid}\index{monoid}
(see Example~\ref{subsec:tc-Example-Monoids}): the binary operation
is \lstinline!zip! and the empty value is \lstinline!wu!. 

\subsection{Commutative applicative functors and parallel computation\label{subsec:Commutative-applicative-functors}}

For some applicative functors, the function call \lstinline!zip(p, q)!
may compute the effects of \lstinline!p! and \lstinline!q! \emph{in
parallel} without changing the results. For instance, assume that
\lstinline!p! and \lstinline!q! represent processes that compute
one value each. Then \lstinline!zip(p, q)! is a combined process
that may compute two values in parallel. We have seen an implementation
of this idea in Section~\ref{subsec:Monadic-programs-with-independent-effects-future-applicative}
where the \lstinline!Future! class is used an applicative functor
that parallelizes computations automatically.

However, not every applicative functor is compatible with parallel
computation. The main distinguishing feature of applicative functors
is that \lstinline!zip(p, q)! combines the effects expressed by \lstinline!p!
and \lstinline!q!, and those effects cannot depend on the wrapped
values. However, the effects in \lstinline!q! may depend on the \emph{effects}
in \lstinline!p!, so that combining \lstinline!p! with \lstinline!q!
is not the same as combining \lstinline!q! with \lstinline!p!. We
have seen an example of this behavior in Section~\ref{subsec:Parsing-with-applicative-and-monadic-parsers}:
an applicative parser may succeed or fail depending on the effect
of a previous parser. For this reason, applicative parsing cannot
be automatically parallelized.

The \lstinline!zip! function can be parallelized if the value \lstinline!zip(p, q)!
does not depend on the order in which the effects of \lstinline!p!
and \lstinline!q! are combined. A precise condition for parallelizability
is the \textsf{``}commutativity\textsf{''} property of applicative functors:

\subsubsection{Definition \label{subsec:Definition-commutative-applicative}\ref{subsec:Definition-commutative-applicative}}

An applicative functor $L^{\bullet}$ is \textbf{commutative}\index{applicative functor!commutative}
if its \lstinline!zip! operation satisfies the commutativity law\index{commutativity law!of zip@of \texttt{zip}}
(in addition to the standard applicative laws):
\begin{equation}
\text{zip}\,(p^{:L^{A}}\times q^{:L^{B}})=\text{zip}\,(q\times p)\triangleright(b^{:B}\times a^{:A}\rightarrow a\times b)^{\uparrow L}\quad\text{or equivalently}:\quad\text{swap}\bef\text{zip}=\text{zip}\bef\text{swap}^{\uparrow L}\quad.\label{eq:commutativity-law-of-zip}
\end{equation}
Here we used the standard Scala function \lstinline!swap! defined
by $\text{swap}\triangleq a\times b\rightarrow b\times a$. 

The commutativity law says that \lstinline!zip! commutes with \lstinline!swap!.
If we use \lstinline!swap! as a type isomorphism between $A\times B$
and $B\times A$, we may write the commutativity law more concisely
and suggestively:
\[
p\,\,\text{zip}\,\,q\cong q\,\,\text{zip}\,\,p\quad.
\]

This definition agrees with that of the commutative monad (see Exercise~\ref{subsec:Exercise-1-monads-9-1}).
A monad\textsf{'}s commutativity is defined via code that is equivalent to
the \lstinline!map2! operation:\texttt{\textcolor{blue}{\footnotesize{}}}
\begin{lstlisting}
def map2[A, B, C](p: M[A], q: M[B])(f: (A, B) => C): M[C] = for {
    x <- p               // Effects of p and q are independent of
    y <- q               // the values x or y.
} yield f(x, y)
\end{lstlisting}
 So, the definition of a commutative monad is that the corresponding
\lstinline!map2! operation (defined via \lstinline!map! and \lstinline!flatMap!)
must satisfy the following commutativity property: 
\[
\text{map}_{2}(p\times q)(f)=\text{map}_{2}(q\times p)\big(b\times a\rightarrow f(a\times b)\big)\quad.
\]
Expressing \lstinline!map2! through \lstinline!zip! as $\text{map}_{2}(p\times q)(f)=\text{zip}\,(p\times q)\triangleright f^{\uparrow L}$,
we obtain Eq.~(\ref{eq:commutativity-law-of-zip}).

Most monads are not commutative (\lstinline!Option! and \lstinline!Reader!
are among the few exceptions). So, even when the effects are independent
of the previous values, it is not possible to parallelize monadic
code automatically. In contrast, most applicative functors turn out
to be commutative, so their \lstinline!zip! methods could (in principle)
have a parallel implementation. An exception is the parsing functor
from Section~\ref{subsec:Parsing-with-applicative-and-monadic-parsers}.
We cannot expect to be able to parse different parts of a file in
parallel, because correct parsing often depends on the success or
failure of parsing of previous portions of the data.

An example of a data structure that can automatically parallelize
computations is Apache Spark\textsf{'}s \lstinline!RDD! class.\index{Spark\textsf{'}s RDD data type@\texttt{Spark}\textsf{'}s \texttt{RDD} data type}
It is important that \lstinline!RDD[_]! is a commutative applicative
functor but \emph{not} a monad.\footnote{The \texttt{Spark} library does not support values of type \lstinline!RDD[RDD[A]]!.
The \lstinline!RDD! class\textsf{'}s \lstinline!cartesian! method has the
type signature corresponding to \lstinline!zip!. A method called
\textsf{``}\lstinline!flatMap!\textsf{''} exists but does not have the type signature
of a monad\textsf{'}s \lstinline!flatMap!. See \texttt{\href{https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html}{https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html}}} This agrees with the intuition that monadic programs are not automatically
parallelizable.

In many cases, a commutative applicative functor will also be a \emph{non-commutative}
monad. Some examples are the \lstinline!Either! and \lstinline!List!
functors. For these and other functors, the \lstinline!map2! and
\lstinline!zip! methods must be defined separately and not derived
from the monadic methods. If we \emph{did} choose to define the \lstinline!map2!
method via the monadic methods, there are two possible implementations:

\noindent %
\begin{minipage}[c][1\totalheight][t]{0.45\columnwidth}%
\begin{lstlisting}
map2(p, q)(f) == for { // Direct order.
    a <- p             // p: L[A]
    b <- q             // q: L[B]
} yield f(a, b)
\end{lstlisting}
%
\end{minipage}\hfill{}%
\begin{minipage}[c][1\totalheight][t]{0.45\columnwidth}%
\begin{lstlisting}
map2(p, q)(f) == for { // Inverse order.
    b <- q             // q: L[B]
    a <- p             // p: L[A]
} yield f(a, b)
\end{lstlisting}
%
\end{minipage}

\noindent The difference is only in the order in which the effects
of \lstinline!p! and \lstinline!q! are concatenated. The two corresponding
\lstinline!zip! functions differ by the permutation of the effects:
\[
\text{zip}\left(p\times q\right)\triangleq p\triangleright\text{flm}_{L}(a\rightarrow q\triangleright(b\rightarrow a\times b)^{\uparrow L})\quad,\quad\quad\text{zip}^{\prime}\left(p\times q\right)=\text{zip}\left(q\times p\right)\triangleright\text{swap}^{\uparrow L}\quad.
\]
For commutative monads, there will be no difference between $\text{zip}^{\prime}$
and $\text{zip}$. 

When an applicative functor is commutative, its associativity law
has a simplified form:

\subsubsection{Statement \label{subsec:Statement-associativity-law-of-zip-with-commutative}\ref{subsec:Statement-associativity-law-of-zip-with-commutative}}

The associativity law of a \emph{commutative} applicative functor
$L$ is equivalent to:
\begin{equation}
\text{zip}\big(p^{:L^{A}}\times\text{zip}\big(q^{:L^{B}}\times r^{:L^{C}}\big)\big)=\text{zip}\left(r\times\text{zip}\left(q\times p\right)\right)\triangleright(c^{:C}\times(b^{:B}\times a^{:A})\rightarrow a\times(b\times c))^{\uparrow L}\quad.\label{eq:associativity-law-of-zip-commutative}
\end{equation}
Up to the tuple-swapping isomorphism, Eq.~(\ref{eq:associativity-law-of-zip-commutative})
has the form where only $p$ and $r$ are swapped:
\begin{equation}
p\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,r)\cong r\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,p)\quad.\label{eq:associativity-law-of-zip-commutative-short}
\end{equation}


\subparagraph{Proof}

We need to demonstrate two directions of the equivalence: \textbf{(1)}
Derive Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})
assuming that the associativity law holds. \textbf{(2)} Derive the
associativity law assuming that Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})
holds.

To save time, we will write $\cong$ to imply tuple-swapping isomorphisms
wherever necessary.

\textbf{(1)} Begin with the left-hand side of Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})
and apply Eq.~(\ref{eq:zip-associativity-law}):
\begin{align*}
 & p\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,r)\cong\gunderline{(p\,\,\text{zip}\,\,q)}\,\,\text{zip}\,\,\gunderline r\\
{\color{greenunder}\text{twice use the commutativity law of }\text{zip}:}\quad & \cong r\,\,\text{zip}\,\,(\gunderline p\,\,\text{zip}\,\,\gunderline q)\cong r\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,p)\quad.
\end{align*}
This is the right-hand side of Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})

\textbf{(2)} Begin with the right-hand side of Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})
and apply the commutativity law:
\begin{align*}
 & r\,\,\text{zip}\,\,(\gunderline q\,\,\text{zip}\,\,\gunderline p)=\gunderline r\,\,\text{zip}\,\,\gunderline{(p\,\,\text{zip}\,\,q)}=(p\,\,\text{zip}\,\,q)\,\,\text{zip}\,\,r\\
{\color{greenunder}\text{use Eq.~(\ref{eq:associativity-law-of-zip-commutative-short})}:}\quad & \overset{!}{=}p\,\,\text{zip}\,\,(q\,\,\text{zip}\,\,r)\quad.
\end{align*}
We have derived Eq.~(\ref{eq:zip-associativity-law}). $\square$

This statement makes it easier to check the associativity law of \lstinline!zip!
if we know that the commutativity law holds. We just need to write
the code for $\text{zip}\left(p\times\text{zip}\left(q\times r\right)\right)$
and swap $p^{:L^{A}}$ and $r^{:L^{C}}$ in that code. The result
must be the same up to swapping the result values of types $A$ and
$C$.

For commutative applicative functors, it is also sufficient to check
only \emph{one} of the identity laws:

\subsubsection{Statement \label{subsec:Statement-identity-law-of-zip-with-commutative-applicative}\ref{subsec:Statement-identity-law-of-zip-with-commutative-applicative}}

The two identity laws of a commutative applicative functor $L$ are
equivalent.

\subparagraph{Proof}

We need to show the two directions of the isomorphism: 

\textbf{(1)} Derive $p\,\,\text{zip}\,\,\text{wu}\cong p$ from $\text{wu}\,\,\text{zip}\,\,q\cong q$.
Use the commutativity law:
\[
\gunderline p\,\,\text{zip}\,\,\gunderline{\text{wu}}\cong\text{wu}\,\,\text{zip}\,\,p\cong p\quad,
\]
where we use the left identity law with $q\triangleq p$.

\textbf{(2)} Derive $\text{wu}\,\,\text{zip}\,\,q\cong q$ from $p\,\,\text{zip}\,\,\text{wu}\cong p$.
Use the commutativity law:
\[
\gunderline{\text{wu}}\,\,\text{zip}\,\,\gunderline q\cong q\,\,\text{zip}\,\,\text{wu}\cong q\quad,
\]
where we use the right identity law with $p\triangleq q$. $\square$

\subsection{Constructions of applicative functors\label{subsec:Constructions-of-applicative-functors}}

We will now perform structural analysis of applicative functors, going
through all possible type constructions. In each case, we will implement
the \lstinline!zip! and \lstinline!wu! methods and verify their
laws, indicating whether the commutativity law holds. We will also
compare the applicative functor constructions to the corresponding
constructions for monads (Section~\ref{subsec:Structural-analysis-of-monads}).
As before, we say \textsf{``}$F$ is applicative\textsf{''} when there is at least
one lawful implementation of \lstinline!zip! and \lstinline!wu!.

Note that any monad will already have a lawful implementation of \lstinline!zip!
and \lstinline!wu! since the laws of \lstinline!map2! and \lstinline!zip!
were derived directly from the monad laws. If the monad is non-commutative,
there will be two inequivalent implementations of \lstinline!zip!.
So, each monad construction gives at least one applicative construction.
We will focus on constructions that generate applicative functors
and have no analogous monad constructions.

\paragraph{Type parameters}

Three type constructions are based on using just type parameters:
a constant functor, $L^{A}\triangleq Z$, the identity functor $L^{A}\triangleq A$,
and the functor composition, $L^{A}\triangleq F^{G^{A}}$.

Given a fixed monoid type $Z$, the constant functor $L^{A}\triangleq Z$
is applicative. To see this, consider the operation \lstinline!zip!
of type $L^{A}\times L^{B}\rightarrow L^{A\times B}$, which in this
case becomes just $Z\times Z\rightarrow Z$, the monoid $Z$\textsf{'}s \lstinline!combine!
function. The value \lstinline!wu! of type $L^{\bbnum 1}=Z$ is just
the monoid $Z$\textsf{'}s empty value ($e_{Z}$). The laws of \lstinline!zip!
then reduce to the monoid laws of $Z$. For the same reason, the applicative
functor $L^{A}\triangleq Z$ will be commutative if the monoid $Z$
is commutative.

Comparing this with the corresponding monad construction, we note
that $L^{A}\triangleq Z$ is not a monad unless $Z=\bbnum 1$. For
an arbitrary monoid type $Z$, the functor $L^{A}\triangleq Z$ is
only a semimonad.

The identity functor $L^{A}\triangleq A$ is applicative and commutative:
the \lstinline!zip! operation is the identity function of type $A\times B\rightarrow A\times B$,
and the wrapped unit (\lstinline!wu!) is just the unit value. All
required laws hold for the identity function.

The functor composition $L\triangleq F\circ G$ of two applicative
functors $F$ and $G$ is again applicative:

\subsubsection{Statement \label{subsec:Statement-applicative-composition}\ref{subsec:Statement-applicative-composition}}

For any two applicative functors $F$ and $G$ with lawful \lstinline!zip!
and \lstinline!pure! methods, the functor $L^{A}\triangleq F^{G^{A}}$
is also applicative. Its \lstinline!zip! and \lstinline!pure! methods
are defined by:
\begin{align*}
 & \text{zip}_{L}:F^{G^{A}}\times F^{G^{B}}\rightarrow F^{G^{A\times B}}\quad,\quad\quad\text{zip}_{L}\triangleq\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad,\\
 & \text{pu}_{L}:A\rightarrow F^{G^{A}}\quad,\quad\quad\text{pu}_{L}\triangleq\text{pu}_{G}\bef\text{pu}_{F}\quad,\quad\quad\text{wu}_{L}\triangleq\text{pu}_{F}(\text{wu}_{G})\quad.
\end{align*}
If $F$ and $G$ are commutative applicative functors, then so is
$L$.

\subparagraph{Proof}

We note that the lifting to $L$ is defined by $f^{\uparrow L}\triangleq f^{\uparrow G\uparrow F}=(f^{\uparrow G})^{\uparrow F}$.

To verify the left identity law~(\ref{eq:zip-left-identity-law}):
\begin{align*}
{\color{greenunder}\text{expect to equal }p\triangleright\text{ilu}^{\uparrow L}:}\quad & \text{zip}_{L}(\text{wu}_{L}\times p^{:F^{G^{A}}})=(\gunderline{\text{pu}_{F}(\text{wu}_{G})}\times p)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\\
{\color{greenunder}\text{left identity law of }\text{zip}_{F}:}\quad & =p\triangleright(g^{:G^{A}}\rightarrow\gunderline{\text{wu}_{G}\times g)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =p\triangleright\big(g^{:G^{A}}\rightarrow\gunderline{\text{zip}_{G}(\text{wu}_{G}\times g)}\big)^{\uparrow F}\\
{\color{greenunder}\text{left identity law of }\text{zip}_{G}:}\quad & =p\triangleright(\gunderline{g\rightarrow g}\triangleright\text{ilu}^{\uparrow G})^{\uparrow F}=p\triangleright(\text{ilu}^{\uparrow G})^{\uparrow F}=p\triangleright\text{ilu}^{\uparrow L}\quad.
\end{align*}

To verify the right identity law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p\triangleright\text{iru}^{\uparrow L}:}\quad & \text{zip}_{L}(p^{:F^{G^{A}}}\times\text{wu}_{L})=(p\times\gunderline{\text{pu}_{F}(\text{wu}_{G})})\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\\
{\color{greenunder}\text{right identity law of }\text{zip}_{F}:}\quad & =p\triangleright(g^{:G^{A}}\rightarrow\gunderline{g\times\text{wu}_{G})^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}}=p\triangleright\big(g^{:G^{A}}\rightarrow\gunderline{\text{zip}_{G}(g\times\text{wu}_{G})}\big)^{\uparrow F}\\
{\color{greenunder}\text{right identity law of }\text{zip}_{G}:}\quad & =p\triangleright(\gunderline{g\rightarrow g\,\triangleright}\,\text{iru}^{\uparrow G})^{\uparrow F}=p\triangleright(\text{iru}^{\uparrow G})^{\uparrow F}=p\triangleright\text{iru}^{\uparrow L}\quad.
\end{align*}

To verify the associativity law, first substitute the definition of
$\text{zip}_{L}$ into one side:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}\big(p\times\text{zip}_{L}(q\times r)\big)\triangleright\varepsilon_{1,23}^{\uparrow L}=\big(p\times\text{zip}_{L}(q\times r)\big)\triangleright\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\gunderline{\varepsilon_{1,23}^{\uparrow L}}\\
{\color{greenunder}\text{definition of }^{\uparrow L}:}\quad & =\big(p\times\big(\text{zip}_{F}(q\times r)\triangleright\gunderline{\text{zip}_{G}^{\uparrow F}}\big)\big)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\varepsilon_{1,23}^{\uparrow G\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{F}:}\quad & =\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\text{zip}_{F}\bef\big(g\times k^{:G^{B}\times G^{C}}\!\rightarrow\gunderline{g\times\text{zip}_{G}(k)\big)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\varepsilon_{1,23}^{\uparrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =\text{zip}_{F}\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\big(g\times(h\times j)\rightarrow\text{zip}_{G}(g\times\text{zip}_{G}(h\times j))\triangleright\varepsilon_{1,23}^{\uparrow G}\big)^{\uparrow F}\quad.
\end{align*}
Now rewrite the right-hand side in a similar way:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}\big(\text{zip}_{L}(p\times q)\times r\big)\triangleright\varepsilon_{12,3}^{\uparrow L}=\big(\text{zip}_{L}(p\times q)\times r\big)\triangleright\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\varepsilon_{12,3}^{\uparrow L}\\
{\color{greenunder}\text{definition of }^{\uparrow L}:}\quad & =\big(\big(\text{zip}_{F}(p\times q)\triangleright\gunderline{\text{zip}_{G}^{\uparrow F}}\big)\times r\big)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\varepsilon_{12,3}^{\uparrow G\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{F}:}\quad & =\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\text{zip}_{F}\bef\big(k^{:G^{A}\times G^{B}}\!\times j\rightarrow\gunderline{\text{zip}_{G}(k)\times j\big)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\varepsilon_{12,3}^{\uparrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =\text{zip}_{F}\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\big((g\times h)\times j\rightarrow\text{zip}_{G}(\text{zip}_{G}(g\times h)\times j)\triangleright\varepsilon_{12,3}^{\uparrow G}\big)^{\uparrow F}\quad.
\end{align*}
The two sides become equal after using the associativity laws of $\text{zip}_{F}$
and $\text{zip}_{G}$:
\begin{align*}
 & \text{zip}_{F}\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\varepsilon_{1,23}^{\uparrow F}=\text{zip}_{F}\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\varepsilon_{12,3}^{\uparrow F}\quad,\\
 & \text{zip}_{G}(g\times\text{zip}_{G}(h\times j))\triangleright\varepsilon_{1,23}^{\uparrow G}=\text{zip}_{G}(\text{zip}_{G}(g\times h)\times j)\triangleright\varepsilon_{12,3}^{\uparrow G}\quad.
\end{align*}

To verify the commutativity law~(\ref{eq:commutativity-law-of-zip})
for $L$, we assume that the law holds for $F$ and $G$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(\text{zip}_{L}\bef\text{swap}^{\uparrow L}):}\quad & \text{swap}\bef\text{zip}_{L}=\gunderline{\text{swap}\bef\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\\
{\color{greenunder}\text{commutativity law of }F:}\quad & =\text{zip}_{F}\bef\gunderline{\text{swap}^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}}=\text{zip}_{F}\bef(\gunderline{\text{swap}\bef\text{zip}_{G}})^{\uparrow F}\\
{\color{greenunder}\text{commutativity law of }G\text{ under }^{\uparrow F}:}\quad & =\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\text{swap}^{\uparrow G\uparrow F}=\text{zip}_{L}\bef\text{swap}^{\uparrow L}\quad.
\end{align*}
$\square$

\paragraph{Products}

Similarly to most other typeclasses, the product of applicative functors
is applicative:

\subsubsection{Statement \label{subsec:Statement-applicative-product}\ref{subsec:Statement-applicative-product}}

For any applicative functors $F$ and $G$, the functor $L^{A}\triangleq F^{A}\times G^{A}$
is also applicative. Its \lstinline!zip! and \lstinline!wu! methods
are defined by:
\begin{align*}
 & \text{zip}_{L}:(F^{A}\times G^{A})\times(F^{B}\times G^{B})\rightarrow F^{A\times B}\times G^{A\times B}\quad,\\
 & \text{zip}_{L}\big((m^{:F^{A}}\times n^{:G^{A}})\times(p^{:F^{B}}\times q^{:G^{B}})\big)\triangleq\text{zip}_{F}(m\times p)\times\text{zip}_{G}(n\times q)\quad,\\
 & \text{wu}_{L}:F^{\bbnum 1}\times G^{\bbnum 1}\quad,\quad\quad\text{wu}_{L}\triangleq\text{wu}_{F}\times\text{wu}_{G}\quad.
\end{align*}
If $F$ and $G$ are commutative applicative functors, then so is
$L$.

\subparagraph{Proof}

We note that the lifting to $L$ is defined by $f^{\uparrow L}\triangleq f^{\uparrow F}\boxtimes f^{\uparrow G}$.

To verify the left identity law of $\text{zip}_{L}$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(p\times q)\triangleright\text{ilu}^{\uparrow L}:}\quad & \text{zip}_{L}\big(\gunderline{\text{wu}_{L}}\times(p^{:F^{A}}\times q^{:G^{A}})\big)=\gunderline{\text{zip}_{L}}((\text{wu}_{F}\times\text{wu}_{G})\times(p\times q))\\
{\color{greenunder}\text{definition of }\text{zip}_{L}:}\quad & =\text{zip}_{F}(\text{wu}_{F}\times p)\times\text{zip}_{G}(\text{wu}_{G}\times q)\\
{\color{greenunder}\text{left identity laws of }\text{zip}_{F}\text{ and }\text{zip}_{G}:}\quad & =(p\triangleright\text{ilu}^{\uparrow F})\times(q\triangleright\text{ilu}^{\uparrow G})=(p\times q)\triangleright(\text{ilu}^{\uparrow F}\boxtimes\text{ilu}^{\uparrow G})\\
{\color{greenunder}\text{definition of }^{\uparrow L}:}\quad & =(p\times q)\triangleright\text{ilu}^{\uparrow L}\quad.
\end{align*}

To verify the right identity law of $\text{zip}_{L}$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(p\times q)\triangleright\text{iru}^{\uparrow L}:}\quad & \text{zip}_{L}\big((p^{:F^{A}}\times q^{:G^{A}})\times\gunderline{\text{wu}_{L}}\big)=\gunderline{\text{zip}_{L}}((p\times q)\times(\text{wu}_{F}\times\text{wu}_{G}))\\
{\color{greenunder}\text{definition of }\text{zip}_{L}:}\quad & =\text{zip}_{F}(p\times\text{wu}_{F})\times\text{zip}_{G}(q\times\text{wu}_{G})\\
{\color{greenunder}\text{right identity laws of }\text{zip}_{F}\text{ and }\text{zip}_{G}:}\quad & =(p\triangleright\text{iru}^{\uparrow F})\times(q\triangleright\text{iru}^{\uparrow G})=(p\times q)\triangleright(\text{iru}^{\uparrow F}\boxtimes\text{iru}^{\uparrow G})\\
{\color{greenunder}\text{definition of }^{\uparrow L}:}\quad & =(p\times q)\triangleright\text{iru}^{\uparrow L}\quad.
\end{align*}

To verify the associativity law, begin with its left-hand side and
use the definition of $\text{zip}_{L}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}\big((p_{1}^{:F^{A}}\times p_{2}^{:G^{A}})\times\text{zip}_{L}\big((q_{1}^{:F^{B}}\times q_{2}^{:G^{B}})\times(r_{1}^{:F^{C}}\times r_{2}^{:G^{C}})\big)\big)\triangleright\gunderline{\varepsilon_{1,23}^{\uparrow L}}\\
 & =\text{zip}_{L}\big((p_{1}\times p_{2})\times\big(\text{zip}_{F}(q_{1}\times r_{1})\times\text{zip}_{G}(q_{2}\times r_{2})\big)\big)\triangleright\big(\varepsilon_{1,23}^{\uparrow F}\boxtimes\varepsilon_{1,23}^{\uparrow G}\big)\\
 & =\big(\gunderline{\text{zip}_{F}\big(p_{1}\times\text{zip}_{F}(q_{1}\times r_{1})\big)\triangleright\varepsilon_{1,23}^{\uparrow F}}\big)\times\big(\gunderline{\text{zip}_{G}\big(p_{2}\times\text{zip}_{G}(q_{2}\times r_{2})\big)\triangleright\varepsilon_{1,23}^{\uparrow G}}\big)\quad.
\end{align*}
The right-hand side is rewritten in a similar way:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}\big(\text{zip}_{L}\big((p_{1}^{:F^{A}}\times p_{2}^{:G^{A}})\times(q_{1}^{:F^{B}}\times q_{2}^{:G^{B}})\big)\times(r_{1}^{:F^{C}}\times r_{2}^{:G^{C}})\big)\triangleright\gunderline{\varepsilon_{12,3}^{\uparrow L}}\\
 & =\text{zip}_{L}\big(\big(\text{zip}_{F}(p_{1}\times q_{1})\times\text{zip}_{G}(p_{2}\times q_{2})\big)\times(r_{1}\times r_{2})\big)\big)\triangleright\big(\varepsilon_{12,3}^{\uparrow F}\boxtimes\varepsilon_{12,3}^{\uparrow G}\big)\\
 & =\big(\gunderline{\text{zip}_{F}\big(\text{zip}_{F}(p_{1}\times q_{1})\times r_{1}\big)\triangleright\varepsilon_{12,3}^{\uparrow F}}\big)\times\big(\gunderline{\text{zip}_{G}\big(\text{zip}_{G}(p_{2}\times q_{2})\times r_{2}\big)\triangleright\varepsilon_{12,3}^{\uparrow G}}\big)\quad.
\end{align*}
The underlined expressions in both sides are equal due to associativity
laws of $\text{zip}_{F}$ and $\text{zip}_{G}$.

To verify the commutativity law of $L$ assuming it holds for $F$
and $G$:
\begin{align*}
{\color{greenunder}\text{expect to equal }\text{zip}_{L}\big((p\times q)\times(m\times n)\big):}\quad & \text{zip}_{L}\big((m\times n)\times(p\times q)\big)\triangleright\text{swap}^{\uparrow L}\\
{\color{greenunder}\text{definitions of }\text{zip}_{L}\text{ and }^{\uparrow L}:}\quad & =\big(\text{zip}_{F}(m\times p)\times\text{zip}_{G}(n\times q)\big)\triangleright(\text{swap}^{\uparrow F}\boxtimes\text{swap}^{\uparrow G})\\
{\color{greenunder}\text{definition of }\boxtimes:}\quad & =\big(\text{zip}_{F}(m\times p)\triangleright\text{swap}^{\uparrow F}\big)\times\big(\text{zip}_{G}(n\times q)\triangleright\text{swap}^{\uparrow G}\big)\\
{\color{greenunder}\text{commutativity laws of }F\text{ and }G:}\quad & =\text{zip}_{F}(p\times m)\times\text{zip}_{G}(q\times n)=\text{zip}_{L}\big((p\times q)\times(m\times n)\big)\quad.
\end{align*}
$\square$

\paragraph{Co-products}

The co-product $F^{A}+G^{A}$ of two arbitrary applicative functors
$F$ and $G$ is not always applicative (just as with monads). One
case where the \lstinline!zip! method cannot be implemented for $F^{A}+G^{A}$
was shown in Example~\ref{subsec:tc-Example-10}(b). However, the
following statements demonstrate that the type constructors $L^{A}\triangleq Z+F^{A}$
and $L^{A}\triangleq A+F^{A}$ are applicative functors.

\subsubsection{Statement \label{subsec:Statement-co-product-with-constant-functor-applicative}\ref{subsec:Statement-co-product-with-constant-functor-applicative}}

If $F^{\bullet}$ is applicative and $Z$ is a fixed monoid type then
$L^{A}\triangleq Z+F^{A}$ is applicative:
\begin{align*}
 & \text{zip}_{L}:(Z+F^{A})\times(Z+F^{B})\rightarrow Z+F^{A\times B}\quad,\quad\quad\text{zip}_{L}\triangleq\,\begin{array}{|c||cc|}
 & Z & F^{A\times B}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{A}\times Z & \_^{:F^{A}}\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_^{:F^{B}}\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\quad.
\end{align*}
The \textsf{``}wrapped unit\textsf{''} ($\text{wu}_{L}:Z+F^{\bbnum 1}$) is defined
as $\text{wu}_{L}\triangleq\bbnum 0^{:Z}+\text{wu}_{F}$. If $Z$
is a commutative monoid and $F^{\bullet}$ is commutative then $L^{\bullet}$
is also commutative.

\subparagraph{Proof}

We will verify the laws of $\text{zip}_{L}$ and $\text{wu}_{L}$,
assuming that $F$ is a lawful applicative functor.

The lifting to $L$ is defined in the standard way:
\[
(f^{:A\rightarrow B})^{\uparrow L}\triangleq\,\begin{array}{|c||cc|}
 & Z & F^{B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & f^{\uparrow F}
\end{array}\quad.
\]

To verify the left identity law, we use the left identity law of $\text{zip}_{F}$:
\begin{align*}
 & \text{zip}_{L}(\text{wu}_{L}\times p^{:Z+F^{B}})=\text{zip}_{L}((\bbnum 0+\text{wu}_{F})\times p)=(\text{wu}_{F}\times p)\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline F^{\bbnum 1}\times Z & \_^{:F^{\bbnum 1}}\times z\rightarrow z & \bbnum 0\\
F^{\bbnum 1}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{B} & \bbnum 0 & k^{:F^{B}}\rightarrow\gunderline{\text{zip}_{F}(\text{wu}_{F}\times k)}
\end{array}\,=p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{B} & \bbnum 0 & k\rightarrow k\triangleright\text{ilu}^{\uparrow F}
\end{array}\,=p\triangleright\text{ilu}^{\uparrow L}\quad.
\end{align*}

To verify the right identity law, we write a similar calculation:
\begin{align*}
 & \text{zip}_{L}(p^{:Z+F^{A}}\times\text{wu}_{L})=\text{zip}_{L}(p\times(\bbnum 0+\text{wu}_{F}))=(p\times\text{wu}_{F})\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z\times F^{\bbnum 1} & z\times\_^{:F^{\bbnum 1}}\rightarrow z & \bbnum 0\\
F^{A}\times F^{\bbnum 1} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & k^{:F^{A}}\rightarrow\gunderline{\text{zip}_{F}(k\times\text{wu}_{F})}
\end{array}\,=p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & k\rightarrow k\triangleright\text{iru}^{\uparrow F}
\end{array}\,=p\triangleright\text{iru}^{\uparrow L}\quad.
\end{align*}

To verify the associativity law, we use a trick to avoid long derivations.
The two sides of the associativity law are expressions of type $Z+F^{A\times B\times C}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}\big(p^{:Z+F^{A}}\times\text{zip}_{L}(q^{:Z+F^{B}}\times r^{:Z+F^{C}})\big)\triangleright\varepsilon_{1,23}^{\uparrow L}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}\big(\text{zip}_{L}(p^{:Z+F^{A}}\times q^{:Z+F^{B}})\times r^{:Z+F^{C}}\big)\triangleright\varepsilon_{12,3}^{\uparrow L}\quad.
\end{align*}
Since each of the arguments $p$, $q$, $r$ may be in one of the
two parts of the disjunction type $Z+F^{\bullet}$, we have 8 cases.
We note, however, that the code of $\text{zip}_{L}(p\times q)$ will
return a value of type $Z+\bbnum 0$ whenever at least one of the
arguments ($p$, $q$) is of type $Z+\bbnum 0$. So, a composition
of two \lstinline!zip! operations will also return a value of type
$Z+\bbnum 0$ whenever at least one of the arguments ($p$, $q$,
$r$) is of type $Z+\bbnum 0$. So, we need to consider the following
two cases: 

\textbf{(1)} At least one of $p$, $q$, $r$ is of type $Z+\bbnum 0$.
In this case, any arguments of type $\bbnum 0+F^{\bullet}$ are ignored
by $\text{zip}_{L}$, while the arguments of type $Z+\bbnum 0$ are
combined using the monoid $Z$\textsf{'}s binary operation ($\oplus$). So,
the result of the \lstinline!zip! operation is the same if we replace
any arguments ($p$, $q$, $r$) of type $\bbnum 0+F^{\bullet}$ by
the empty value $e_{Z}$. For example:
\[
\text{zip}_{L}\big((z+\bbnum 0)\times(\bbnum 0+k^{:F^{A}})\big)=z+\bbnum 0=\text{zip}_{L}\big((z+\bbnum 0)\times(e_{Z}+\bbnum 0)\big)\quad.
\]
After this replacement, we have three arguments ($z_{1}+\bbnum 0$,
$z_{2}+\bbnum 0$, $z_{3}+\bbnum 0$) instead of $p$, $q$, $r$,
and the function $\text{zip}_{L}$ reduces to the operation $\oplus$,
for which the associativity law holds by assumption.%
\begin{comment}
{*}{*}{*} do we need this?
\begin{align*}
 & \text{zip}_{L}\big(p\times\text{zip}_{L}(q\times r)\big)\triangleright\varepsilon_{1,23}^{\uparrow L}=\big((z_{1}\oplus z_{2}\oplus z_{3})+\bbnum 0\big)\triangleright\varepsilon_{1,23}^{\uparrow L}=(z_{1}\oplus z_{2}\oplus z_{3})+\bbnum 0\quad,\\
 & \text{zip}_{L}\big(\text{zip}_{L}(p\times q)\times r\big)\triangleright\varepsilon_{12,3}^{\uparrow L}=\big((z_{1}\oplus z_{2}\oplus z_{3})+\bbnum 0\big)\triangleright\varepsilon_{12,3}^{\uparrow L}=(z_{1}\oplus z_{2}\oplus z_{3})+\bbnum 0\quad.
\end{align*}
\end{comment}

\textbf{(2)} All of $p$, $q$, $r$ are of type $\bbnum 0+F^{\bullet}$.
In this case, $\text{zip}_{L}$ reduces to $\text{zip}_{F}$, which
satisfies the associativity law by assumption.

To verify the commutativity law of $L$, use the code matrix for \lstinline!swap!
with the relevant types:
\begin{align*}
 & \text{swap}\bef\text{zip}_{L}=\,\begin{array}{|c||cccc|}
 & Z\times Z & F^{B}\times Z & Z\times F^{A} & F^{B}\times F^{A}\\
\hline Z\times Z & \text{swap} & \bbnum 0 & \bbnum 0 & \bbnum 0\\
F^{A}\times Z & \bbnum 0 & \bbnum 0 & \text{swap} & \bbnum 0\\
Z\times F^{B} & \bbnum 0 & \text{swap} & \bbnum 0 & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \bbnum 0 & \bbnum 0 & \text{swap}
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{B}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{A} & z\times\_\rightarrow z & \bbnum 0\\
F^{B}\times F^{A} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{2}\oplus z_{1} & \bbnum 0\\
F^{A}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\end{align*}
By assumption, $\text{swap}\bef\text{zip}_{F}=\text{zip}_{F}\bef\text{swap}^{\uparrow F}$.
Next, we need the code for the lifted $\text{swap}^{\uparrow L}$:
\[
\text{swap}^{\uparrow L}=\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A\times B} &  & \text{swap}^{\uparrow F}
\end{array}\quad.
\]
We can now transform the right-hand side of the commutativity law:
\[
\text{zip}_{L}\bef\text{swap}^{\uparrow L}=\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{A}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}\bef\text{swap}^{\uparrow F}
\end{array}\quad.
\]
The difference between the sides disappears if $Z$ is a commutative
monoid ($z_{1}\oplus z_{2}=z_{2}\oplus z_{1}$). $\square$

\subsubsection{Statement \label{subsec:Statement-co-product-with-identity-applicative}\ref{subsec:Statement-co-product-with-identity-applicative}}

If $F^{\bullet}$ is applicative then $L^{A}\triangleq A+F^{A}$ is
also applicative:
\begin{align*}
 & \text{zip}_{L}:(A+F^{A})\times(B+F^{B})\rightarrow A\times B+F^{A\times B}\quad,\quad\quad\text{zip}_{L}\triangleq\,\begin{array}{|c||cc|}
 & A\times B & F^{A\times B}\\
\hline A\times B & \text{id} & \bbnum 0\\
F^{A}\times B & \bbnum 0 & (\text{id}\boxtimes\text{pu}_{F})\bef\text{zip}_{F}\\
A\times F^{B} & \bbnum 0 & (\text{pu}_{F}\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\quad.
\end{align*}
The \lstinline!wu! method is defined by $\text{wu}_{L}\triangleq1+\bbnum 0^{:F^{\bbnum 1}}$.
If $F^{\bullet}$ is commutative then $L^{\bullet}$ is also commutative.

\subparagraph{Proof}

We will use Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative},
where the same properties are demonstrated for a more general functor
$L^{A}\triangleq H^{A}+F^{A}$. We will set $H^{A}\triangleq A$ in
Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative}
and obtain the present statement because the compatibility law holds
automatically for $\text{ex}_{H}\triangleq\text{id}$ and $\text{zip}_{H}\triangleq\text{id}$.
 $\square$

It is important that Statements~\ref{subsec:Statement-co-product-with-constant-functor-applicative}\textendash \ref{subsec:Statement-co-product-with-identity-applicative}
define the methods \lstinline!zip! and \lstinline!wu! differently
for $L^{A}\triangleq Z+F^{A}$ and for $L^{A}\triangleq A+F^{A}$.
Only then the applicative laws of \lstinline!zip! and \lstinline!wu!
will hold for those functors. Also, those definitions agree with the
code of \lstinline!zip! and \lstinline!wu! for the \lstinline!Either!
functor (Section~\ref{subsec:Programs-that-accumulate-errors}).
The \lstinline!Either! functor ($L^{A}\triangleq Z+A$ with a monoid
type $Z$) is constructed as $L^{A}\triangleq Z+F^{A}$ with $F^{A}\triangleq A$
(via Statement~\ref{subsec:Statement-co-product-with-constant-functor-applicative})
or as $L^{A}\triangleq A+F^{A}$ with $F^{A}\triangleq Z$ (via Statement~\ref{subsec:Statement-co-product-with-identity-applicative}).

The following statement generalizes the construction $L^{A}\triangleq A+F^{A}$
to $L^{A}\triangleq H^{A}+F^{A}$ where $H^{\bullet}$ is applicative
and at the same time co-pointed (see Section~\ref{subsec:Co-pointed-functors}).
The code for $\text{zip}_{L}$ will use the co-pointed functor\textsf{'}s \lstinline!extract!
method. The next statement shows that $\text{zip}_{L}$ will obey
the applicative laws if a special \textbf{compatibility law}\index{compatibility law!of extract and zip@of \texttt{extract} and \texttt{zip}}
holds for \lstinline!extract! and \lstinline!zip!:
\begin{equation}
\text{ex}_{H}(\text{zip}_{H}(p^{:H^{A}}\times q^{:H^{B}}))=\text{ex}_{H}(p)\times\text{ex}_{H}(q)\quad,\quad\text{or equivalently}:\quad\text{zip}_{H}\bef\text{ex}_{H}=\text{ex}_{H}\boxtimes\text{ex}_{H}\quad.\label{eq:compatibility-law-of-extract-and-zip}
\end{equation}

A simple example of a co-pointed applicative functor is $H^{A}\triangleq A\times G^{A}$:

\subsubsection{Statement \label{subsec:Statement-co-pointed-applicative-example}\ref{subsec:Statement-co-pointed-applicative-example}}

Given any applicative functor $G^{\bullet}$, the functor $H^{A}\triangleq A\times G^{A}$
is applicative and co-pointed. The compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
holds if the \lstinline!extract! method is defined by $\text{ex}_{H}\triangleq\pi_{1}=a^{:A}\times\_^{:G^{A}}\rightarrow a$.

\subparagraph{Proof}

The applicative and co-pointed properties of $H$ follow from the
product constructions of applicative functors (Statement~\ref{subsec:Statement-applicative-product})
and co-pointed functors (Section~\ref{subsec:Co-pointed-functors}).
To verify the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip}),
we write the definition of $\text{zip}_{H}$: 
\[
\text{zip}_{H}\big((a^{:A}\times g_{1}^{:G^{A}})\times(b^{:B}\times g_{2}^{:G^{B}})\big)=(a\times b)\times\text{zip}_{G}(g_{1}\times g_{2})\quad,
\]
and apply \lstinline!extract! to the last expression:
\begin{align*}
 & \text{ex}_{H}\big(\text{zip}_{H}\big((a^{:A}\times g_{1}^{:G^{A}})\times(b^{:B}\times g_{2}^{:G^{B}})\big)\big)=\text{ex}_{H}\big((a\times b)\times\text{zip}_{G}(g_{1}\times g_{2})\big)=a\times b\quad,\\
 & \text{ex}_{H}(a\times g_{1})\times\text{ex}_{H}(b\times g_{2})=a\times b\quad.
\end{align*}
$\square$

It is not known (Problem~\ref{subsec:Problem-co-pointed-applicative})
whether there exist co-pointed applicative functors that satisfy the
compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
but are \emph{not} of the form $A\times G^{A}$. Here is an example
of an applicative functor not of the form $A\times G^{A}$ that violates
the compatibility law:

\subsubsection{Statement \label{subsec:Statement-co-pointed-applicative-example-failing-compatibility-law}\ref{subsec:Statement-co-pointed-applicative-example-failing-compatibility-law}}

Given any monoid type $Z$, the functor $F^{A}\triangleq Z\times\left(Z\rightarrow A\right)$
is applicative and co-pointed. However, the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
does not hold. 

\subparagraph{Proof}

The product construction (Statement~\ref{subsec:Statement-applicative-product})
shows that $F^{\bullet}$ is applicative: it is a product of a constant
functor ($Z$) and a \lstinline!Reader! functor ($Z\rightarrow A$).
The \lstinline!zip! method is:
\[
\text{zip}_{F}\big((z_{1}^{:Z}\times r_{1}^{:Z\rightarrow A})\times(z_{2}^{:Z}\times r_{2}^{:Z\rightarrow B})\big)\triangleq(z_{1}\oplus z_{2})\times(z^{:Z}\rightarrow r_{1}(z)\times r_{2}(z))\quad.
\]
 The functor $F^{\bullet}$ is co-pointed because it has a fully parametric
\lstinline!extract! method defined by: 
\[
\text{ex}_{F}\triangleq z^{:Z}\times r^{:Z\rightarrow A}\rightarrow r(z)\quad.
\]
Let us see whether the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
holds:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{ex}_{F}\big(\text{zip}_{F}\big((z_{1}\times r_{1})\times(z_{2}\times r_{2})\big)\big)=\text{ex}_{F}\big((z_{1}\oplus z_{2})\times(z^{:Z}\rightarrow r_{1}(z)\times r_{2}(z))\big)\\
 & \quad=r_{1}(z_{1}\oplus z_{2})\times r_{2}(z_{1}\oplus z_{2})\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{ex}_{F}(z_{1}\times r_{1})\times\text{ex}_{F}(z_{2}\times r_{2})=r_{1}(z_{1})\times r_{2}(z_{2})\quad.
\end{align*}
The two sides are not equal (when $Z\neq\bbnum 1$). So, the compatibility
law does not hold. $\square$

We are now ready to prove the co-pointed co-product construction:

\subsubsection{Statement \label{subsec:Statement-co-product-with-co-pointed-applicative}\ref{subsec:Statement-co-product-with-co-pointed-applicative}}

The functor $L^{A}\triangleq H^{A}+F^{A}$ is applicative if $F^{\bullet}$
and $H^{\bullet}$ are applicative and in addition $H^{\bullet}$
is co-pointed with a method $\text{ex}_{H}:H^{A}\rightarrow A$ that
satisfies the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip}).
The applicative methods of $L^{\bullet}$ are defined by:
\begin{align*}
 & \text{zip}_{L}:(H^{A}+F^{A})\times(H^{B}+F^{B})\rightarrow H^{A\times B}+F^{A\times B}\quad,\\
 & \text{zip}_{L}\triangleq\,\begin{array}{|c||cc|}
 & H^{A\times B} & F^{A\times B}\\
\hline H^{A}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\quad.
\end{align*}
The method $\text{wu}_{L}:H^{\bbnum 1}+F^{\bbnum 1}$ is defined by
$\text{wu}_{L}\triangleq\text{wu}_{H}+\bbnum 0$. If $F^{\bullet}$
and $H^{\bullet}$ are commutative applicative functors then $L^{\bullet}$
is also commutative.

\subparagraph{Proof}

The lifting to $L^{\bullet}$ is defined by:
\[
(f^{:A\rightarrow B})^{\uparrow L}\triangleq\,\begin{array}{|c||cc|}
 & H^{B} & F^{B}\\
\hline H^{A} & f^{\uparrow H} & \bbnum 0\\
F^{A} & \bbnum 0 & f^{\uparrow F}
\end{array}\quad.
\]

To verify the left identity law, we begin with its left-hand side:
\begin{align*}
 & \text{zip}_{L}(\text{wu}_{L}\times p)=\big((\text{wu}_{H}^{:H^{\bbnum 1}}+\bbnum 0^{:F^{\bbnum 1}})\times p^{:H^{B}+F^{B}}\big)\triangleright\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{\bbnum 1}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
H^{\bbnum 1}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{\bbnum 1}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{\bbnum 1}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & h\rightarrow\text{zip}_{H}(\text{wu}_{H}\times h) & \bbnum 0\\
F^{B} & \bbnum 0 & f\rightarrow\text{zip}_{F}((\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F})\times f)
\end{array}\quad.
\end{align*}
Using Eq.~(\ref{eq:co-pointed-nondegeneracy-law-wu}) and the definition
of \lstinline!wu! through \lstinline!pure!, we find:
\[
\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F}=1\triangleright\text{pu}_{F}=\text{wu}_{F}\quad.
\]
Since the identity laws of $F$ and $H$ are assumed to hold, we can
transform the last matrix as:
\[
\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & h\rightarrow\text{zip}_{H}(\text{wu}_{H}\times h) & \bbnum 0\\
F^{B} & \bbnum 0 & f\rightarrow\text{zip}_{F}((\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F})\times f)
\end{array}\,=\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & \text{ilu}^{\uparrow H} & \bbnum 0\\
F^{B} & \bbnum 0 & \text{ilu}^{\uparrow F}
\end{array}\,=\text{ilu}^{\uparrow L}\quad.
\]
After this simplification, the left-hand side equals $p\triangleright\text{ilu}^{\uparrow L}$,
i.e., the right-hand side of the law.

The right identity law is verified in a similar way:
\begin{align*}
 & \text{zip}_{L}(p\times\text{wu}_{L})=\text{zip}_{L}\big(p^{:H^{A}+F^{A}}\times(\text{wu}_{H}^{:H^{\bbnum 1}}+\bbnum 0^{:F^{\bbnum 1}})\big)\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & H^{A\times\bbnum 1} & F^{A\times\bbnum 1}\\
\hline H^{A} & h\rightarrow\text{zip}_{H}(h\times\text{wu}_{H}) & \bbnum 0\\
F^{A} & \bbnum 0 & f\rightarrow\text{zip}_{F}(f\times(\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F}))
\end{array}
\end{align*}
\begin{align*}
 & =p\triangleright\,\,\begin{array}{|c||cc|}
 & H^{A\times\bbnum 1} & F^{A\times\bbnum 1}\\
\hline H^{A} & \text{iru}^{\uparrow H} & \bbnum 0\\
F^{A} & \bbnum 0 & \text{iru}^{\uparrow F}
\end{array}\,=p\triangleright\text{iru}^{\uparrow L}\quad.
\end{align*}

The associativity law is an equation between values of type $H^{A\times B\times C}+F^{A\times B\times C}$:
\[
\text{zip}_{L}(p^{:H^{A}+F^{A}}\times\text{zip}_{L}(q^{:H^{B}+F^{B}}\times r^{:H^{C}+F^{C}}))\triangleright\varepsilon_{1,23}^{\uparrow L}=\text{zip}_{L}(\text{zip}_{L}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}\quad.
\]
The operation $\text{zip}_{L}(p\times q)$ is defined in such a way
that it returns a value of type $H^{A\times B}+\bbnum 0$ only when
both $p$ and $q$ are in the left part of the disjunction:
\[
\text{zip}_{L}\big((a^{:H^{A}}+\bbnum 0^{:F^{A}})\times(b^{:H^{B}}+\bbnum 0^{:F^{B}})\big)=\text{zip}_{H}(a\times b)+\bbnum 0^{:F^{A\times B}}\quad.
\]
Otherwise, $\text{zip}_{L}(p\times q)$ returns a value of type $\bbnum 0^{:H^{A\times B}}+F^{A\times B}$.
So, we need to consider three cases:

\textbf{(1)} The arguments are $p=a^{:H^{A}}+\bbnum 0$, $q=b^{:H^{B}}+\bbnum 0$,
$r=c^{:H^{C}}+\bbnum 0$. In this case, $\text{zip}_{L}$ reduces
to $\text{zip}_{H}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\text{zip}_{L}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}=\text{zip}_{H}\big(a\times\text{zip}_{H}(b\times c)\big)\triangleright\varepsilon_{1,23}^{\uparrow H}+\bbnum 0\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}(\text{zip}_{L}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}=\text{zip}_{H}\big(\text{zip}_{H}(a\times b)\times c\big)\triangleright\varepsilon_{12,3}^{\uparrow H}+\bbnum 0\quad.
\end{align*}
The two sides are equal due to the associativity law of $\text{zip}_{H}$.

\textbf{(2)} The argument $q$ has type $\bbnum 0+F^{B}$. In this
case, $\text{zip}_{L}$ reduces to $\text{zip}_{F}$ after converting
arguments of type $H^{\bullet}+0$ to type $F^{\bullet}$ when needed.
We may define this conversion as a helper function \lstinline!toF!:
\begin{align*}
 & \text{toF}:H^{A}+F^{A}\rightarrow F^{A}\quad,\quad\quad\text{toF}\triangleq\,\begin{array}{|c||c|}
 & F^{A}\\
\hline H^{A} & \text{ex}_{H}\bef\text{pu}_{F}\\
F^{A} & \text{id}
\end{array}\quad.
\end{align*}
If $q$ has type $\bbnum 0+F^{B}$ then we have:
\[
\text{zip}_{L}(p\times q)=\bbnum 0+\text{zip}_{F}(\text{toF}\left(p\right)\times\text{toF}\left(q\right))\quad,\quad\quad\text{zip}_{L}(q\times r)=\bbnum 0+\text{zip}_{F}(\text{toF}\left(q\right)\times\text{toF}\left(r\right))\quad.
\]
Now the associativity law of $\text{zip}_{L}$ is reduced to the same
law of $\text{zip}_{F}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\text{zip}_{L}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}=\bbnum 0+\text{zip}_{F}\big(\text{toF}\left(p\right)\times\text{zip}_{F}(\text{toF}\left(q\right)\times\text{toF}\left(r\right))\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}(\text{zip}_{L}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}=\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(\text{toF}\left(p\right)\times\text{toF}\left(q\right))\times\text{toF}\left(r\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\quad.
\end{align*}
The two sides are equal due to the associativity law of $\text{zip}_{F}$.

\textbf{(3)} Either $p=\bbnum 0+a^{:F^{A}}$ while $q=b^{:H^{B}}+\bbnum 0$
and $r=c^{:H^{C}}+\bbnum 0$; or $r=\bbnum 0+c^{:F^{C}}$ while $p=a^{:H^{A}}+\bbnum 0$
and $q=b^{:H^{B}}+\bbnum 0$. The two situations are symmetric, so
let us consider the first one:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\text{zip}_{L}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}=\bbnum 0+\text{zip}_{F}\big(\text{toF}\,(p)\times\text{toF}\,(\text{zip}_{H}(b\times c)+\bbnum 0)\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\quad.
\end{align*}
Simplify the sub-expressions involving \lstinline!toF! separately:
\begin{align*}
 & \text{toF}\,(p)=\text{toF}\,(\bbnum 0+a)=a\quad,\\
 & \text{toF}\,(\text{zip}_{H}(b\times c)+\bbnum 0)=\text{pu}_{F}(\gunderline{\text{ex}_{H}(\text{zip}_{H}}(b\times c))\\
{\color{greenunder}\text{use Eq.~(\ref{eq:compatibility-law-of-extract-and-zip})}:}\quad & \quad=\text{pu}_{F}(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\quad.
\end{align*}
So, we can rewrite the left-hand side as:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\text{zip}_{L}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}=\bbnum 0+\gunderline{\text{zip}_{F}}\big(a\times\gunderline{\text{pu}_{F}}(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\\
{\color{greenunder}\text{identity law of }\text{zip}_{F}:}\quad & =\bbnum 0+a\triangleright\big(k^{:A}\rightarrow k\times(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\big)^{\uparrow F}\triangleright\varepsilon_{1,23}^{\uparrow F}\\
 & =\bbnum 0+a\triangleright\big(k^{:A}\rightarrow k\times\text{ex}_{H}(b)\times\text{ex}_{H}(c)\big)^{\uparrow F}\quad.
\end{align*}
The right-hand side can be transformed by using \lstinline!toF! on
all arguments:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}(\text{zip}_{L}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}=\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(\text{toF}\left(p\right)\times\text{toF}\left(q\right))\times\text{toF}\left(r\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\\
 & =\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))\times\text{toF}\left(r+\bbnum 0\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\quad.
\end{align*}
Simplify the sub-expressions of the form $\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))$:
\begin{equation}
\text{zip}_{F}\big(a^{:F^{A}}\times\text{toF}\,(b^{:H^{B}}+\bbnum 0)\big)=\text{zip}_{F}(a\times\text{pu}_{F}(\text{ex}_{H}(b))=a\triangleright(k^{:A}\rightarrow k\times\text{ex}_{H}(b))^{\uparrow F}\quad.\label{eq:zip-copointed-construction-derivation1}
\end{equation}
Using this formula, we continue to transform the right-hand side:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \bbnum 0+\gunderline{\text{zip}_{F}}(\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))\times\gunderline{\text{toF}\left(c+\bbnum 0\right)})\triangleright\varepsilon_{12,3}^{\uparrow F}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-copointed-construction-derivation1})}:}\quad & =\bbnum 0+\gunderline{\text{zip}_{F}}(a\times\gunderline{\text{toF}\left(b+\bbnum 0\right)})\triangleright\big(k\rightarrow k\times\text{ex}_{H}(c)\big)^{\uparrow F}\triangleright\varepsilon_{12,3}^{\uparrow F}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-copointed-construction-derivation1})}:}\quad & =\bbnum 0+a\triangleright\big(k\rightarrow k\times\text{ex}_{H}(b)\big)^{\uparrow F}\triangleright\big(k\rightarrow k\times\text{ex}_{H}(c)\big)^{\uparrow F}\triangleright\varepsilon_{12,3}^{\uparrow F}\\
{\color{greenunder}\text{compute composition}:}\quad & =\bbnum 0+a\triangleright\big(k\rightarrow k\times\text{ex}_{H}(b)\times\text{ex}_{H}(c)\big)^{\uparrow F}\quad.
\end{align*}
The two sides are now equal. 

It remains to verify the commutativity law in case that law holds
for $F$ and $H$:
\[
\text{swap}\bef\text{zip}_{F}\overset{!}{=}\text{zip}_{F}\bef\text{swap}^{\uparrow F}\quad,\quad\quad\text{swap}\bef\text{zip}_{H}\overset{!}{=}\text{zip}_{H}\bef\text{swap}^{\uparrow H}\quad\quad.
\]
Begin with the left-hand side of the commutativity law for $\text{zip}_{L}$:
\[
\text{swap}\bef\text{zip}_{L}=\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A}\times H^{B} & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & \text{swap}\bef((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\]
Writing out the compositions of \lstinline!swap! and the pair product
functions, we get:
\begin{align*}
 & \text{swap}\bef((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})=(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\quad,\\
 & \text{swap}\bef(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))=(\text{pu}_{F}\boxtimes\text{id})\bef\text{swap}\quad.
\end{align*}
Using these simplifications, we rewrite the left-hand side as:
\[
\text{swap}\bef\text{zip}_{L}=\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A}\times H^{B} & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{swap}\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\]
The right-hand side is rewritten to the same code after using the
laws of $\text{zip}_{F}$ and $\text{zip}_{H}$:
\begin{align*}
 & \text{zip}_{L}\bef\text{swap}^{\uparrow L}=\,\begin{array}{|c||cc|}
 & H^{A\times B} & F^{A\times B}\\
\hline H^{A}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A\times B} & \text{swap}^{\uparrow H} & \bbnum 0\\
F^{A\times B} & \bbnum 0 & \text{swap}^{\uparrow F}
\end{array}\\
 & =\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline A\times B & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times B & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\bef\text{zip}_{F}\\
A\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{swap}\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\end{align*}
The two sides are now equal. $\square$

The constructions shown so far define applicative methods for all
polynomial functors.

\subsubsection{Statement \label{subsec:Statement-polynomial-functor-applicative}\ref{subsec:Statement-polynomial-functor-applicative}}

\textbf{(a)} Any polynomial functor $L^{A}$ whose fixed types are
monoids can be made into an applicative functor. 

\textbf{(b)} If $P^{A}$ and $Q^{A}$ are polynomial functors with
monoidal fixed types and $R^{A}$ is any applicative functor then
$L^{A}\triangleq P^{A}+Q^{A}\times R^{A}$ is applicative.

\textbf{(c)} A recursive polynomial functor $L^{\bullet}$ defined
by $L^{A}\triangleq S^{A,L^{F^{A}}}$ is applicative if $S^{\bullet,\bullet}$
and $F^{\bullet}$ are polynomial (bi)functors with monoidal fixed
types.

\textbf{(d)} If all fixed types used in parts \textbf{(a)}\textendash \textbf{(c)}
are commutative monoids, $L^{\bullet}$ will be also commutative.

\subparagraph{Proof}

\textbf{(a)} Any polynomial functor $L^{A}$ is built (in at least
one way) by combining fixed types and the type parameter $A$ using
products and co-products. By rearranging the type expression $L^{A}$,
we may bring it to the following equivalent form:
\[
L^{A}\cong Z_{0}+A\times(Z_{1}+A\times(...\times(Z_{n-1}+A\times Z_{n})...))\quad.
\]
Here, the fixed types $Z_{i}$ are all monoids by assumption. Statements~\ref{subsec:Statement-applicative-product}
and~\ref{subsec:Statement-co-product-with-constant-functor-applicative}
are then sufficient to make $L$ into a lawful applicative functor. 

\textbf{(b)} We can express $P^{A}$ and $Q^{A}$ equivalently as:
\[
P^{A}=Z_{1}+A\times S_{1}^{A}\quad,\quad\quad Q^{A}=Z_{2}+A\times S_{2}^{A}\quad,
\]
with some fixed types $Z_{1}$, $Z_{2}$ and some polynomial functors
$S_{1}^{A}$, $S_{2}^{A}$. We can then rewrite the type $L^{A}$
equivalently as:
\[
P^{A}+Q^{A}\times R^{A}=Z_{1}+A\times S_{1}^{A}+(Z_{2}+A\times S_{2}^{A})\times R^{A}\cong(Z_{1}+Z_{2}\times R^{A})+A\times(S_{1}^{A}+S_{2}^{A}\times R^{A})\quad.
\]
Since $S_{1}^{A}$ and $S_{2}^{A}$ are polynomial functors of smaller
degree than $P^{A}$ and $Q^{A}$, we may assume by induction that
the property we are proving will already hold for the functor $G^{A}\triangleq S_{1}^{A}+S_{2}^{A}\times R^{A}$.
Statements~\ref{subsec:Statement-applicative-product}, \ref{subsec:Statement-co-product-with-constant-functor-applicative},
\ref{subsec:Statement-co-product-with-identity-applicative}, and~\ref{subsec:Statement-co-product-with-co-pointed-applicative}
then show that $F^{A}\triangleq Z_{1}+Z_{2}\times R^{A}$ is applicative
and $F^{A}+A\times G^{A}\cong L^{A}$ is applicative.

\textbf{(c)} Write the recursive definition $L^{A}\triangleq S^{A,L^{F^{A}}}$.
To prove that $S^{A,L^{F^{A}}}$ is an applicative functor, we may
use the inductive assumption that $L^{A}$ is applicative when used
in the recursive position (i.e., as the second argument of $S^{\bullet,\bullet}$).
By Statement~\ref{subsec:Statement-applicative-composition}, $L^{F^{A}}$
is applicative when used in that position. Denoting $N^{A}\triangleq L^{F^{A}}$,
we now rewrite $L^{A}$ as: 
\[
L^{A}=S^{A,L^{F^{A}}}=S^{A,N^{A}}\quad.
\]
It remains to prove that $S^{A,N^{A}}$ is applicative given that
$N$ is applicative. 

The polynomial bifunctor $S^{\bullet,\bullet}$ can be expressed as:
\[
S^{A,R}=P_{0}^{A}+R\times(P_{1}^{A}+R\times(...\times(P_{n-1}^{A}+R\times P_{n}^{A})...))\quad,
\]
where $P_{0}^{A}$, ..., $P_{n}^{A}$ are some polynomial functors
(with respect to the type parameter $A$) with monoidal fixed types.
We need to set $R=N^{A}$ in the type expression above. Then it follows
from part \textbf{(b)} that $P_{n-1}^{A}+N^{A}\times P_{n}^{A}$ is
applicative. In the same way, it follows that $P_{n-2}^{A}+N^{A}\times(P_{n-1}^{A}+N^{A}\times P_{n}^{A})$
is applicative, and so on, until we show that $S^{A,N^{A}}$ is applicative.

\textbf{(d)} The proofs of parts \textbf{(a)}\textendash \textbf{(c)}
use only Statements~\ref{subsec:Statement-applicative-composition},
\ref{subsec:Statement-applicative-product}, \ref{subsec:Statement-co-product-with-constant-functor-applicative},
\ref{subsec:Statement-co-product-with-identity-applicative}, and~\ref{subsec:Statement-co-product-with-co-pointed-applicative}.
These statements guarantee that $L$ will be commutative if all relevant
fixed types are commutative monoids. $\square$

\paragraph{Function types}

We have seen in Section~\ref{subsec:The-applicative-Reader-functor}
that the \lstinline!Reader! functor ($L^{A}\triangleq R\rightarrow A$)
has a \lstinline!zip! operation. That \lstinline!zip! operation
can be derived from the monadic methods of the \lstinline!Reader!
monad (which is commutative). Statement~\ref{subsec:Statement-monad-construction-2}
generalized the \lstinline!Reader! monad to a wider class of monads
with type $L^{A}\triangleq H^{A}\rightarrow A$, where $H^{\bullet}$
is an arbitrary contrafunctor. The lawful monad gives up to two definitions
of a \lstinline!zip! method for the functors $L$ of this type. However,
commutativity is not guaranteed for arbitrary $H^{\bullet}$. One
can implement a \lstinline!zip! method with type signature:
\[
\text{zip}:(H^{A}\rightarrow A)\times(H^{B}\rightarrow B)\rightarrow H^{A\times B}\rightarrow A\times B\quad,
\]
such that the commutativity law~(\ref{eq:associativity-law-of-zip-commutative})
always holds. That definition of \lstinline!zip! will, however, fail
the associativity law (we omit the proof for brevity). So, there are
no new constructions of lawful applicative functors of function type.

\paragraph{Recursive types}

There exist two different constructions of applicative functors based
on recursive types. These constructions generalize the tree-like \textsf{``}free
monad\textsf{''} (see Statement~\ref{subsec:Statement-monad-construction-4-free-monad})
and the standard \lstinline!zip! implementation for the \lstinline!List!
functor.

The first construction generalizes tree-like functors. A simple example
is the binary tree:
\[
T^{A}\triangleq A+T^{A}\times T^{A}\quad.
\]
This data structure describes trees with leaves of type $A$ and branches
that contain two sub-trees. We could generalize this type by adding
extra data of type $A$ for each branch:
\[
T^{A}\triangleq A+A\times T^{A}\times T^{A}\quad.
\]
The following statement generalizes this type further by using two
arbitrary applicative functors to describe the branch shapes and any
extra data on branches.

\subsubsection{Statement \label{subsec:Statement-applicative-recursive-type}\ref{subsec:Statement-applicative-recursive-type}}

For any applicative functors $F$ and $H$, the functor $L$ defined
recursively by $L^{A}\triangleq A+H^{A}\times F^{L^{A}}$ is applicative.
If $F$ and $H$ are commutative then $L$ is also commutative.

\subparagraph{Proof}

It turns out that we can verify the required laws without long derivations
by using type constructions whose validity we already proved. Let
us define $N^{A}\triangleq H^{A}\times F^{L^{A}}$ for brevity. Then
the definition of $L$ is equivalently written as $L^{A}=A+N^{A}=A+H^{A}\times F^{L^{A}}$.
This type is built up from applicative functors $H$, $F$, and (recursively)
$L$ itself. The functor $A+N^{A}$ is applicative due to Statement~\ref{subsec:Statement-co-product-with-identity-applicative},
while $N^{A}=H^{A}\times F^{L^{A}}$ is a functor product (Statement~\ref{subsec:Statement-applicative-product}).
Finally, $F^{L^{A}}$ is applicative because it is a functor composition
(Statement~\ref{subsec:Statement-applicative-composition}), and
because we may assume that $L^{A}$ is already a lawful applicative
functor when we use its methods in recursive calls.

Since all these constructions preserve commutative applicative functors,
we conclude that $L$ will be commutative if $F$ and $H$ are.

The constructions also give us the code for the applicative methods
\lstinline!zip! and \lstinline!wu! of $L$. This code is similar
to the code for a tree-like applicative shown in Example~\ref{subsec:Example-applicative-tree}.
When combining a leaf value ($A+\bbnum 0$) with a sub-tree ($\bbnum 0+N^{A}$),
we duplicate the leaf value as many times as needed to cover the sub-tree:
\begin{align*}
 & \text{zip}_{L}:(A+N^{A})\times(B+N^{B})\rightarrow A\times B+N^{A\times B}\quad,\\
 & \text{zip}_{L}\triangleq\,\begin{array}{|c||cc|}
 & A\times B & N^{A\times B}\\
\hline A\times B & \text{id} & \bbnum 0\\
N^{A}\times B & \bbnum 0 & n\times b\rightarrow n\triangleright(a^{:A}\rightarrow a\times b)^{\uparrow N}\\
A\times N^{B} & \bbnum 0 & a\times n\rightarrow n\triangleright(b^{:B}\rightarrow a\times b)^{\uparrow N}\\
N^{A}\times N^{B} & \bbnum 0 & \overline{\text{zip}_{N}}
\end{array}\quad.
\end{align*}
The \textsf{``}wrapped unit\textsf{''} is $\text{wu}_{L}\triangleq1+\bbnum 0^{:N^{\bbnum 1}}$.
The liftings to $L$ and $N$ are defined by:
\[
(f^{:A\rightarrow B})^{\uparrow L}=\,\begin{array}{|c||cc|}
 & B & N^{B}\\
\hline A & \text{id} & \bbnum 0\\
N^{A} & \bbnum 0 & f^{\uparrow N}
\end{array}\quad,\quad\quad f^{\uparrow N}=f^{\uparrow H}\times f^{\overline{\uparrow L}\uparrow F}=h^{:H^{A}}\times k^{:F^{L^{A}}}\rightarrow(h\triangleright f^{\uparrow H})\times(k\triangleright f^{\overline{\uparrow L}\uparrow F})\quad.
\]
For brevity, we denoted by $\overline{\text{zip}_{N}}$ the following
function:
\[
\overline{\text{zip}_{N}}\triangleq(h_{1}^{:H^{A}}\times k_{1}^{:F^{L^{A}}})\times(h_{2}^{:H^{B}}\times k_{2}^{:F^{L^{B}}})\rightarrow\text{zip}_{H}(h_{1}\times h_{2})\times\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}_{L}}^{\uparrow F}\big)\quad.
\]
The overline reminds us that $\overline{\text{zip}_{N}}$ uses a recursive
call to $\overline{\text{zip}_{L}}$. %
\begin{comment}
$\text{zip}\left(p\times q\right)$ is simplified when one its arguments
($p$, $q$) are in the left part of the disjunction. If $p\triangleq a^{:A}+\bbnum 0^{:N^{A}}$
and $q^{:L^{B}}$ is arbitrary, we have:
\begin{align}
 & \text{zip}_{L}(p\times q)=(p\times q)\triangleright\text{zip}_{L}=(p\times q)\triangleright\,\begin{array}{|c||cc|}
 & A\times B & N^{A\times B}\\
\hline A\times B & \text{id} & \bbnum 0\\
A\times N^{B} & \bbnum 0 & a\times n\rightarrow n\triangleright(b^{:B}\rightarrow a\times b)^{\uparrow N}
\end{array}\nonumber \\
 & =q\triangleright\,\begin{array}{|c||cc|}
 & A\times B & N^{A\times B}\\
\hline B & b^{:B}\rightarrow a\times b & \bbnum 0\\
N^{B} & \bbnum 0 & n\rightarrow n\triangleright(b^{:B}\rightarrow a\times b)^{\uparrow N}
\end{array}\,=q\triangleright(b^{:B}\rightarrow a\times b)^{\uparrow N}\quad.\label{eq:zip-p-q-case1-derivation1}
\end{align}
If $p^{:L^{A}}$ is arbitrary and $q\triangleq b^{:B}+\bbnum 0^{:N^{B}}$,
we have:
\begin{align}
 & \text{zip}_{L}(p\times q)=(p\times q)\triangleright\text{zip}_{L}=(p\times q)\triangleright\,\begin{array}{|c||cc|}
 & A\times B & N^{A\times B}\\
\hline A\times B & \text{id} & \bbnum 0\\
N^{A}\times B & \bbnum 0 & n\times b\rightarrow n\triangleright(a^{:A}\rightarrow a\times b)^{\uparrow N}
\end{array}\nonumber \\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & A\times B & N^{A\times B}\\
\hline A & a^{:A}\rightarrow a\times b & \bbnum 0\\
N^{A} & \bbnum 0 & n\rightarrow n\triangleright(a^{:A}\rightarrow a\times b)^{\uparrow N}
\end{array}\,=p\triangleright(a^{:A}\rightarrow a\times b)^{\uparrow N}\quad.\label{eq:zip-p-q-case2-derivation1}
\end{align}

To verify the left identity law:
\begin{align*}
 & \text{zip}_{L}(\text{wu}_{L}\times p)=\text{zip}_{L}\\
\end{align*}
To verify the associativity law~(\ref{eq:zip-associativity-law-with-epsilons})
by making use of Eqs.~(\ref{eq:zip-p-q-case1-derivation1})\textendash (\ref{eq:zip-p-q-case2-derivation1}),
we will consider separately the three cases when one of $p$, $q$,
$r$ is in the left part of their disjunctive types, and the remaining
case where all of $p$, $q$, $r$ are in the right parts.

We begin with the case $p=a^{:A}+\bbnum 0$ and write the left-hand
side of the associativity law:
\begin{align*}
 & \gunderline{\text{zip}_{L}(p}\times\text{zip}_{L}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow L}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-p-q-case1-derivation1})}:}\quad & =\text{zip}_{L}(q\times r)\triangleright(b^{:B}\times c^{:C}\rightarrow a\times(b\times c))^{\uparrow L}\bef\varepsilon_{1,23}^{\uparrow L}\\
 & =(q\times r)\triangleright\text{zip}_{L}\bef(b^{:B}\times c^{:C}\rightarrow a\times b\times c)^{\uparrow L}\quad.
\end{align*}
Compare this with the right-hand side of the associativity law:
\begin{align*}
 & \text{zip}_{L}(\gunderline{\text{zip}_{L}(p\times q)}\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-p-q-case1-derivation1})}:}\quad & =\big((q\triangleright(b^{:B}\rightarrow a\times b))\times r\big)\triangleright\text{zip}_{L}\bef\varepsilon_{12,3}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{L}:}\quad & =(q\times r)\triangleright\text{zip}_{L}\bef(b^{:B}\times c^{:C}\rightarrow a\times b\times c)^{\uparrow L}\quad.
\end{align*}
The two sides are now equal. We are allowed to use the naturality
law of $\text{zip}_{L}$ since it is a fully parametric function (and
assuming that naturality laws hold for $\text{zip}_{F}$ and $\text{zip}_{H}$).

In the second case, $q=b^{:B}+\bbnum 0$. The two sides of the associativity
law become equal after simplification:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\gunderline{\text{zip}_{L}(q\times r)})\triangleright\varepsilon_{1,23}^{\uparrow L}=(p\times(r\triangleright(c^{:C}\rightarrow b\times c)^{\uparrow L}))\triangleright\text{zip}_{L}\bef\varepsilon_{1,23}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{L}:}\quad & \quad=(p\times r)\triangleright\text{zip}_{L}\bef(a^{:A}\times c^{:C}\rightarrow a\times b\times c)^{\uparrow L}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{L}(\gunderline{\text{zip}_{L}(p\times q)}\times r)\triangleright\varepsilon_{12,3}^{\uparrow L}=\big((p\triangleright(a^{:A}\rightarrow a\times b)^{\uparrow L})\times r\big)\triangleright\text{zip}_{L}\bef\varepsilon_{12,3}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{L}:}\quad & \quad=(p\times r)\triangleright\text{zip}_{L}\bef(a^{:A}\times c^{:C}\rightarrow a\times b\times c)^{\uparrow L}\quad.
\end{align*}

In the third case, $r=c^{:C}+\bbnum 0$. Write the two sides of the
associativity law using Eq.~(\ref{eq:zip-p-q-case2-derivation1}):
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{L}(p\times\gunderline{\text{zip}_{L}(q\times r)})\triangleright\varepsilon_{1,23}^{\uparrow L}=(p\times(q\triangleright(b^{:B}\rightarrow b\times c)^{\uparrow L}))\triangleright\text{zip}_{L}\bef\varepsilon_{1,23}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{L}:}\quad & \quad=(p\times q)\triangleright\text{zip}_{L}\bef(a^{:A}\times b^{:B}\rightarrow a\times b\times c)^{\uparrow L}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \gunderline{\text{zip}_{L}}(\text{zip}_{L}(p\times q)\times\gunderline r)\triangleright\varepsilon_{12,3}^{\uparrow L}=\text{zip}_{L}(p\times q)\triangleright(a^{:A}\times b^{:B}\rightarrow(a\times b)\times c)^{\uparrow L}\bef\varepsilon_{12,3}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{L}:}\quad & \quad=(p\times r)\triangleright\text{zip}_{L}\bef(a^{:A}\times c^{:C}\rightarrow a\times b\times c)^{\uparrow L}\quad.
\end{align*}
 

It remains to consider the case when the three arguments are of the
form 
\[
p\triangleq\bbnum 0+h_{1}^{:H^{A}}\times k_{1}^{:F^{L^{A}}}\quad,\quad\quad q\triangleq\bbnum 0+h_{2}^{:H^{B}}\times k_{2}^{:F^{L^{B}}}\quad,\quad\quad r\triangleq\bbnum 0+h_{3}^{:H^{C}}\times k_{3}^{:F^{L^{C}}}\quad.
\]
Simplify some sub-expressions used in the associativity law:
\begin{align*}
 & \text{zip}_{L}(p\times q)=\bbnum 0+\text{zip}_{H}(h_{1}\times h_{2})\times\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\big)\quad,\\
 & \text{zip}_{L}(q\times r)=\bbnum 0+\text{zip}_{H}(h_{2}\times h_{3})\times\big(\text{zip}_{F}(k_{2}\times k_{3})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\big)\quad.
\end{align*}
Write the two sides of the associativity law separately, omitting
$\varepsilon_{1,23}$ and $\varepsilon_{12,3}$ for now:
\begin{align*}
 & \text{zip}_{L}(p\times\gunderline{\text{zip}_{L}(q\times r)})=\bbnum 0+\text{zip}_{L}\big((h_{1}\times k_{1})\times\big(\text{zip}_{H}(h_{2}\times h_{3})\times(\text{zip}_{F}(k_{2}\times k_{3})\triangleright\overline{\text{zip}}_{L}^{\uparrow F})\big)\big)\\
 & \quad=\bbnum 0+\text{zip}_{H}(h_{1}\times\text{zip}_{H}(h_{2}\times h_{3}))\times\gunderline{\text{zip}_{F}}(k_{1}\times(\text{zip}_{F}(k_{2}\times k_{3})\gunderline{\triangleright\overline{\text{zip}}_{L}^{\uparrow F}}))\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\quad,\\
 & \text{zip}_{L}(\text{zip}_{L}(p\times q)\times r)=\bbnum 0+\text{zip}_{L}\big(\big(\text{zip}_{H}(h_{1}\times h_{2})\times\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\big)\big)\times(h_{3}\times k_{3})\big)\\
 & \quad=\bbnum 0+\text{zip}_{H}(\text{zip}_{H}(h_{1}\times h_{2})\times h_{3})\times\text{zip}_{F}\big(\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\big)\times k_{3}\big)\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\quad.
\end{align*}
We assume that the associativity laws of $\text{zip}_{H}$, $\text{zip}_{F}$,
and $\overline{\text{zip}_{L}}$ already hold. The sub-expressions
involving $\text{zip}_{H}$ become equal after applying $\varepsilon_{1,23}^{\uparrow H}$
and $\varepsilon_{12,3}^{\uparrow H}$. The remaining difference between
the two parts is:
\[
\text{zip}_{F}(k_{1}\times(\text{zip}_{F}(k_{2}\times k_{3})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}))\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\bef\varepsilon_{1,23}^{\uparrow L\uparrow F}\overset{?}{=}\text{zip}_{F}\big(\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\big)\times k_{3}\big)\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\bef\varepsilon_{12,3}^{\uparrow L\uparrow F}\quad.
\]
To show that this equation holds, we use the naturality law of $\text{zip}_{F}$
and write for one side:
\begin{align*}
 & \gunderline{\text{zip}_{F}}(k_{1}\times(\text{zip}_{F}(k_{2}\times k_{3})\gunderline{\triangleright\overline{\text{zip}}_{L}^{\uparrow F}}))\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\bef\varepsilon_{1,23}^{\uparrow L\uparrow F}=\text{zip}_{F}(k_{1}\times\text{zip}_{F}(k_{2}\times k_{3}))\triangleright(\text{id}\boxtimes\overline{\text{zip}}_{L})^{\uparrow F}\bef\overline{\text{zip}}_{L}^{\uparrow F}\bef\varepsilon_{1,23}^{\uparrow L\uparrow F}\\
 & =\text{zip}_{F}(k_{1}\times\text{zip}_{F}(k_{2}\times k_{3}))\triangleright\varepsilon_{1,23}^{\uparrow F}\bef\big(l_{1}\times l_{2}\times l_{3}\rightarrow\overline{\text{zip}}_{L}(l_{1}\times\overline{\text{zip}}_{L}(l_{2}\times l_{3}))\triangleright\varepsilon_{1,23}^{\uparrow L}\big)^{\uparrow F}\quad.
\end{align*}
For the other side:
\begin{align*}
 & \gunderline{\text{zip}_{F}}\big(\big(\text{zip}_{F}(k_{1}\times k_{2})\gunderline{\triangleright\overline{\text{zip}}_{L}^{\uparrow F}}\big)\times k_{3}\big)\triangleright\overline{\text{zip}}_{L}^{\uparrow F}\bef\varepsilon_{12,3}^{\uparrow L\uparrow F}=\text{zip}_{F}\big(\text{zip}_{F}(k_{1}\times k_{2})\times k_{3}\big)\triangleright\big((\overline{\text{zip}}_{L}\boxtimes\text{id})\bef\overline{\text{zip}}_{L}\bef\varepsilon_{12,3}^{\uparrow L}\big)^{\uparrow F}\\
 & =\text{zip}_{F}\big(\text{zip}_{F}(k_{1}\times k_{2})\times k_{3}\big)\triangleright\varepsilon_{12,3}^{\uparrow F}\bef\big(l_{1}\times l_{2}\times l_{3}\rightarrow\overline{\text{zip}}_{L}(\overline{\text{zip}}_{L}(l_{1}\times l_{2})\times l_{3})\triangleright\varepsilon_{12,3}^{\uparrow L}\big)^{\uparrow F}\quad.
\end{align*}
The two sides are now equal because of the associativity laws of $\text{zip}_{F}$
and $\text{zip}_{H}$.

Finally, we check that the commutativity law holds under appropriate
assumptions:
\begin{align*}
\\
\end{align*}
\end{comment}
$\square$

Statement~\ref{subsec:Statement-applicative-recursive-type} covers
tree-like functors $L^{A}\triangleq A+H^{A}\times F^{L^{A}}$ with
leaf values of type $A$ and branch shapes described by a functor
$F$. In addition, each branch may carry a value of type $H^{A}$.

When $H^{A}$ is a constant functor, the resulting $L^{\bullet}$
is a monad (Statement~\ref{subsec:Statement-monad-construction-4-free-monad}).
It is important that the applicative implementations of \lstinline!map2!
and \lstinline!zip! for this tree-like monad are \emph{not} compatible
with its monad methods. To see this, it is sufficient to note that
even the simplest tree-like monad (the binary tree, $L^{A}\triangleq A+L^{A}\times L^{A}$)
is not commutative. The applicative functor $L$, however, is commutative
because it is built from $H^{A}\triangleq1$ and $F^{A}\triangleq A\times A$;
those are polynomial functors with commutative monoidal coefficients.

The next statement generalizes the \lstinline!List! functor, 
\[
\text{List}^{A}\triangleq\bbnum 1+A\times\text{List}^{A}\quad,
\]
by introducing \emph{three} arbitrary applicative functors into the
above formula for $\text{List}^{A}$.

\subsubsection{Statement \label{subsec:Statement-applicative-recursive-type-1}\ref{subsec:Statement-applicative-recursive-type-1}}

\textbf{(a)} For any applicative functors $F$, $G$, and $H$, the
functor $L$ defined recursively by $L^{A}\triangleq G^{A}+H^{A}\times F^{L^{A}}$
is applicative as long as $H$ is co-pointed and the compatibility
law~(\ref{eq:compatibility-law-of-extract-and-zip}) holds. If $F$,
$G$, and $H$ are commutative then $L$ is also commutative.

\textbf{(b)} The same properties hold for the functor $P$ defined
by $P^{A}\triangleq F^{G^{A}+H^{A}\times P^{A}}$ instead.

\subparagraph{Proof}

\textbf{(a)} We will avoid long derivations if we show that the functor
$L$ is built via known type constructions. At the top level, $L^{A}$
is the co-pointed co-product construction (Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative})
with functors $G^{\bullet}$ and $H^{\bullet}\times F^{L^{\bullet}}$.
The functor $H^{\bullet}\times F^{L^{\bullet}}$ is co-pointed because
$H$ is (Section~\ref{subsec:Co-pointed-functors}). The compatibility
law holds for $H^{\bullet}\times F^{L^{\bullet}}$ due to Exercise~\ref{subsec:Exercise-applicative-II-4-1}.
The functor $F^{L^{\bullet}}$ is applicative because it is a composition
of $F$ and the recursively used $L$. As usual, we may assume that
recursive uses of $L$\textsf{'}s methods will satisfy all required laws.

The constructions give us the code of $L$\textsf{'}s methods \lstinline!zip!
and \lstinline!wu!. Define $N^{A}\triangleq H^{A}\times F^{L^{A}}$
and write: 
\begin{align*}
 & \text{ex}_{N}\triangleq\pi_{1}\bef\text{ex}_{H}=\big(h^{:H^{A}}\times k^{:F^{L^{A}}}\rightarrow\text{ex}_{H}(h)\big)\quad,\\
 & \overline{\text{zip}_{N}}\triangleq(h_{1}^{:H^{A}}\times k_{1}^{:F^{L^{A}}})\times(h_{2}^{:H^{B}}\times k_{2}^{:F^{L^{B}}})\rightarrow\text{zip}_{H}(h_{1}\times h_{2})\times\big(\text{zip}_{F}(k_{1}\times k_{2})\triangleright\overline{\text{zip}_{L}}^{\uparrow F}\big)\quad.
\end{align*}
The function \lstinline!toG! converts values of type $N^{A}$ to
values of type $G^{A}$:
\[
\text{toG}:N^{A}\rightarrow G^{A}\quad,\quad\quad\text{toG}\triangleq\text{ex}_{N}\bef\text{pu}_{G}\quad.
\]
Then we have the following code:
\begin{align*}
 & \text{zip}_{L}:(G^{A}+N^{A})\times(G^{B}+N^{B})\rightarrow G^{A\times B}+N^{A\times B}\quad,\\
 & \text{zip}_{L}\triangleq\,\begin{array}{|c||cc|}
 & G^{A\times B} & N^{A\times B}\\
\hline G^{A}\times G^{B} & \text{zip}_{G} & \bbnum 0\\
N^{A}\times G^{B} & (\text{toG}\boxtimes\text{id})\bef\text{zip}_{G} & \bbnum 0\\
G^{A}\times N^{B} & (\text{id}\boxtimes\text{toG})\bef\text{zip}_{G} & \bbnum 0\\
N^{A}\times N^{B} & \bbnum 0 & \overline{\text{zip}_{N}}
\end{array}\quad,\\
 & \text{wu}_{L}:G^{\bbnum 1}+H^{\bbnum 1}\times F^{L^{\bbnum 1}}\quad,\quad\quad\text{wu}_{L}\triangleq\bbnum 0^{:G^{\bbnum 1}}+\text{wu}_{H}\times\text{pu}_{F}(\text{wu}_{L})\quad.
\end{align*}

\textbf{(b)} Let us use the \index{recursive types!unrolling trick}\index{unrolling trick for recursive types}unrolling
trick to compare the recursive types $L$ and $P$:
\[
L^{A}=G^{A}+H^{A}\times F^{G^{A}+H^{A}\times F^{G^{A}+...}}\quad,\quad\quad P^{A}=F^{G^{A}+H^{A}\times F^{G^{A}+H^{A}\times F^{G^{A}+...}}}\quad.
\]
It suggests that $P^{A}\cong F^{L^{A}}$. Then the required properties
hold for $P$ due to the functor composition construction (Statement~\ref{subsec:Statement-applicative-composition}).
To establish the type equivalence $P^{A}\cong F^{L^{A}}$ rigorously,
we use Statement~\ref{subsec:Statement-unrolling-trick} below, where
we need to set $R^{\bullet}\triangleq F^{\bullet}$, $S^{T}\triangleq G^{A}+H^{A}\times T$,
$U\triangleq P^{A}$, and $V\triangleq L^{A}$. $\square$

Note that the construction shown in Statement~\ref{subsec:Statement-applicative-recursive-type-1}
needs to define \emph{both} \lstinline!zip! and \lstinline!wu! recursively.
This leads to a problem in case \lstinline!wu! does not have the
type of a function. A simple example when this problem occurs is with
the \lstinline!List! functor. The value $\text{wu}_{\text{List}}$
is then defined as:
\[
\text{List}^{A}\triangleq\bbnum 1+A\times\text{List}^{A}\quad,\quad\quad\text{wu}_{\text{List}}\triangleq\bbnum 0+1\times\text{wu}_{\text{List}}\quad.
\]
This value apparently represents an \emph{infinite} list of unit values:
\[
\text{wu}_{L}=\bbnum 0+1\times\left(\bbnum 0+1\times\left(\bbnum 0+1\times\left(...\right)\right)\right)\quad\quad???
\]
We need a \textsf{``}lazy list\index{lazy collection}\textsf{''} (or another non-eager
collection) if we want to define \lstinline!wu! recursively by the
code shown above. However, the data structure $\text{List}^{A}$ is
eagerly evaluated and can only hold a finite number of values. So,
we must conclude that \lstinline!wu! is undefined for the functor
\lstinline!List!, which then cannot be considered a fully lawful
applicative functor. Nevertheless, the associativity and the commutativity
laws hold for the \lstinline!zip! method of \lstinline!List!. The
absence of a well-defined value \lstinline!wu! does not lead to practical
disadvantages when working with finite lists. 

For certain choices of $F$, $G$, and $H$, the functor $L$ will
be a monad. It is important that the implementation of applicative
methods \lstinline!pure!, \lstinline!map2!, and \lstinline!zip!
for this functor will be, in general, incompatible with its monad
methods. The \lstinline!List! functor again gives an example where
the monad is not commutative while the applicative functor is. So,
the commutative \lstinline!zip! method of \lstinline!List! cannot
be defined via its \lstinline!map! and \lstinline!flatMap! methods.
Also, the \lstinline!List! monad\textsf{'}s \lstinline!pure! method (returning
a list with a single value) differs from the applicative \lstinline!pure!
method (returning an infinite list of values, as we just saw).

There could exist other recursive constructions that produce lawful
applicative functors. For instance, one could assume an \textsf{``}biapplicative
bifunctor\textsf{''} $P^{A,R}$ having a \lstinline!bizip! method with this
type signature: 
\[
\text{bizip}_{P}^{A,B,F^{\bullet}}:P^{A,F^{A}}\times P^{B,F^{B}}\rightarrow P^{A\times B,F^{A}\times F^{B}}\quad(\text{for all functors }F)\quad.
\]
The bifunctor $P$ should also have a designated \textsf{``}wrapped unit\textsf{''}
value, $\text{wu}_{P}:P^{\bbnum 1,\bbnum 1}$. Then the functor $L$
defined via the recursive type equation $L^{A}\triangleq P^{A,L^{A}}$
will be applicative when suitable laws are imposed on \lstinline!bizip!.
The methods $\text{wu}_{L}$ and $\text{zip}_{L}$ will be defined
recursively by:
\begin{align*}
\text{wu}_{L}:L^{\bbnum 1}\cong P^{\bbnum 1,L^{\bbnum 1}}\quad, & \quad\quad\text{wu}_{L}\triangleq\text{wu}_{P}\triangleright(\_\rightarrow\overline{\text{wu}_{L}})^{\uparrow P^{\bbnum 1,\bullet}}\quad,\\
\text{zip}_{L}:P^{A,L^{A}}\times P^{B,L^{B}}\rightarrow P^{A\times B,L^{A\times B}}\quad, & \quad\quad\text{zip}_{L}=\text{bizip}_{P}^{A,B,L^{\bullet}}\bef\overline{\text{zip}_{L}}^{\uparrow P^{A\times B,\bullet}}\quad.
\end{align*}

To find what biapplicative bifunctors $P$ exist, one could continue
with structural analysis (considering products $P_{1}^{\bullet,\bullet}\times P_{2}^{\bullet,\bullet}$,
co-products $P_{1}^{\bullet,\bullet}+P_{2}^{\bullet,\bullet}$, and
so on). This book will not pursue that analysis further, because we
already found sufficiently many type constructions required for practical
applications.

We conclude this section with a proof of one version of the \textsf{``}unrolling
trick\textsf{''} for recursive types:\index{unrolling trick for recursive types!proof}\index{recursive types!unrolling trick!proof}

\subsubsection{Statement \label{subsec:Statement-unrolling-trick}\ref{subsec:Statement-unrolling-trick}}

Given two functors $R^{\bullet}$ and $S^{\bullet}$, define two recursive
types $U$ and $V$ by $U\triangleq R^{S^{U}}$ and $V\triangleq S^{R^{V}}$.
The \textsf{``}unrolling trick\textsf{''} writes (non-rigorously) $U=R^{S^{R^{S^{\iddots}}}}\!$
and $V=S^{R^{S^{R^{\iddots}}}}\!$, which suggests that $U$ and $R^{V}$
are the same type. In fact, the type $U$ is rigorously equivalent
to the type $R^{V}$.

\subparagraph{Proof}

We will show that $U\cong R^{V}$ by implementing the isomorphisms
in two directions. 

By definition of the type $U$, we must have some isomorphisms (called
$\text{fix}_{U}$ and $\text{unfix}_{U}$) between types $U$ and
$R^{S^{U}}$, and similarly for the type $V$. So, we assume that
the following functions are known and satisfy the properties of isomorphisms:
\begin{align*}
 & \text{fix}_{U}:R^{S^{U}}\rightarrow U\quad,\quad\text{unfix}_{U}:U\rightarrow R^{S^{U}}\quad,\quad\text{fix}_{U}\bef\text{unfix}_{U}=\text{id}\quad,\quad\text{unfix}_{U}\bef\text{fix}_{U}=\text{id}\quad;\\
 & \text{fix}_{V}:S^{R^{V}}\rightarrow V\quad,\quad\text{unfix}_{V}:V\rightarrow S^{R^{V}}\quad,\quad\text{fix}_{V}\bef\text{unfix}_{V}=\text{id}\quad,\quad\text{unfix}_{V}\bef\text{fix}_{V}=\text{id}\quad.
\end{align*}

We implement the isomorphism functions (called \lstinline!toU! and
\lstinline!toRV!) recursively:
\begin{align*}
 & \text{toU}:R^{V}\rightarrow U\quad,\quad\quad\text{toU}\triangleq\text{unfix}_{V}^{\uparrow R}\bef\overline{\text{toU}}^{\uparrow S\uparrow R}\bef\text{fix}_{U}\quad;\\
 & \text{toRV}:U\rightarrow R^{V}\quad,\quad\quad\text{toRV}\triangleq\text{unfix}_{U}\bef\overline{\text{toRV}}^{\uparrow S\uparrow R}\bef\text{fix}_{V}^{\uparrow R}\quad.
\end{align*}

To verify the isomorphism properties ($\text{toU}\bef\text{toRV}=\text{id}$
and $\text{toRV}\bef\text{toU}=\text{id}$), we use the inductive
assumption that those properties already hold for any recursive calls
of these functions:
\begin{align*}
 & \text{toU}\bef\text{toRV}=\text{unfix}_{V}^{\uparrow R}\bef\overline{\text{toU}}^{\uparrow S\uparrow R}\bef\gunderline{\text{fix}_{U}\bef\text{unfix}_{U}}\bef\overline{\text{toRV}}^{\uparrow S\uparrow R}\bef\text{fix}_{V}^{\uparrow R}\\
 & \quad=\text{unfix}_{V}^{\uparrow R}\bef\gunderline{\overline{\text{toU}}^{\uparrow S\uparrow R}\bef\overline{\text{toRV}}^{\uparrow S\uparrow R}}\bef\text{fix}_{V}^{\uparrow R}=\text{unfix}_{V}^{\uparrow R}\bef\text{fix}_{V}^{\uparrow R}=\text{id}\quad,\\
 & \text{toRV}\bef\text{toU}=\text{unfix}_{U}\bef\overline{\text{toRV}}^{\uparrow S\uparrow R}\bef\gunderline{\text{fix}_{V}^{\uparrow R}\bef\text{unfix}_{V}^{\uparrow R}}\bef\overline{\text{toU}}^{\uparrow S\uparrow R}\bef\text{fix}_{U}\\
 & \quad=\text{unfix}_{U}\bef\gunderline{\overline{\text{toRV}}^{\uparrow S\uparrow R}\bef\overline{\text{toU}}^{\uparrow S\uparrow R}}\bef\text{fix}_{U}=\text{unfix}_{U}\bef\text{fix}_{U}=\text{id}\quad.
\end{align*}


\section{Applicative contrafunctors and profunctors\label{sec:Applicative-contrafunctors-and-profunctors}}

We have seen in Example~\ref{subsec:Example-applicative-profunctor}
that a \lstinline!zip! method can be implemented for some type constructors
that are not covariant. In this section, we will apply structural
analysis systematically to discover the non-covariant type constructors
(contrafunctors and profunctors) that admit a lawful \lstinline!zip!
method.

\subsection{Applicative contrafunctors: Laws and constructions}

Contrafunctors (see Section~\ref{subsec:Contrafunctors}) support
a \lstinline!contramap! method instead of \lstinline!map!. So, contrafunctors
cannot have the \lstinline!map2! or \lstinline!ap! methods. Nevertheless,
the applicative laws can be formulated via \lstinline!zip! and \lstinline!wu!
methods using \lstinline!contramap!. That is the formulation we will
use for contrafunctors.

\subsubsection{Definition \label{subsec:Definition-applicative-contrafunctor}\ref{subsec:Definition-applicative-contrafunctor}}

A contrafunctor $C^{\bullet}$ is \textbf{applicative} if there exist
methods \lstinline!zip! and \lstinline!wu! such that:
\begin{align}
 & \text{zip}_{C}:C^{A}\times C^{B}\rightarrow C^{A\times B}\quad,\quad\quad\text{wu}_{C}:C^{\bbnum 1}\quad,\nonumber \\
{\color{greenunder}\text{associativity law}:}\quad & \text{zip}_{C}(p\times\text{zip}_{C}(q\times r))\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow C}=\text{zip}_{C}(\text{zip}_{C}(p\times q)\times r)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow C}\quad,\label{eq:applicative-contrafunctor-associativity-law}\\
{\color{greenunder}\text{left and right identity laws}:}\quad & \text{zip}_{C}(\text{wu}_{C}\times p)\triangleright\text{ilu}^{\downarrow C}=p\quad,\quad\quad\text{zip}_{C}(p\times\text{wu}_{C})\triangleright\text{iru}^{\downarrow C}=p\quad.\label{eq:applicative-contrafunctor-identity-laws}
\end{align}
Here the tuple-rearranging isomorphisms $\tilde{\varepsilon}_{1,23}$,
$\tilde{\varepsilon}_{12,3}$, \lstinline!ilu!, and \lstinline!iru!
are defined by:
\begin{align*}
 & \tilde{\varepsilon}_{1,23}\triangleq a\times b\times c\rightarrow a\times\left(b\times c\right)\quad,\quad\quad\tilde{\varepsilon}_{12,3}\triangleq a\times b\times c\rightarrow\left(a\times b\right)\times c\quad,\\
 & \text{ilu}\triangleq a\rightarrow1\times a\quad,\quad\quad\text{iru}=a\rightarrow a\times1\quad.
\end{align*}

It is important to require the laws to hold. Otherwise, a function
with the type signature of \lstinline!zip! could be implemented for
any contrafunctor $C$:
\[
\text{badZip}:C^{A}\times C^{B}\rightarrow C^{A\times B}\quad,\quad\quad\text{badZip}\triangleq p^{:C^{A}}\times\_^{:C^{B}}\rightarrow p\triangleright(a\times b\rightarrow a)^{\downarrow C}\quad.
\]
This function loses information because it ignores one of its arguments.
Intuitively, we expect that some laws will fail for this function.
Indeed, the left identity law cannot hold: \lstinline!badZip(wu, p)!
ignores \lstinline!p!, so \lstinline!badZip(wu, p).contramap(ilu)!
cannot be equal to \lstinline!p!. 

Once the \lstinline!zip! and \lstinline!wu! methods are known, we
can define \lstinline!cpure! and \lstinline!cmap2!:
\begin{align*}
 & \text{cpu}_{C}:\forall A.\,C^{A}\quad,\quad\quad\text{cpu}_{C}\triangleq\text{wu}_{C}\triangleright(\_^{:A}\rightarrow1)^{\downarrow C}\quad,\\
 & \text{cmap}_{2}:(D\rightarrow A\times B)\rightarrow C^{A}\times C^{B}\rightarrow C^{D}\quad,\quad\quad\text{cmap}_{2}(f^{:D\rightarrow A\times B})\triangleq\text{zip}_{C}\bef f^{\downarrow C}\quad.
\end{align*}

The commutativity law is formulated for applicative contrafunctors
like this:
\[
\text{zip}_{C}(q\times p)=\text{zip}_{C}(p\times q)\triangleright\text{swap}^{\downarrow C}\quad,\quad\quad\text{or equivalently}:\quad\text{swap}\bef\text{zip}_{C}=\text{zip}_{C}\bef\text{swap}^{\downarrow C}\quad.
\]

The rest of this section proves some constructions that produce lawful
applicative contrafunctors.

\paragraph{Type parameters}

A constant contrafunctor $C^{A}\triangleq Z$ (where $Z$ is a fixed
type) is at the same time a constant functor. We already showed that
a constant functor is applicative when $Z$ is a monoid. 

Another construction for applicative contrafunctors is the composition
with functors:

\subsubsection{Statement \label{subsec:Statement-applicative-contrafunctor-composition}\ref{subsec:Statement-applicative-contrafunctor-composition}}

If $F^{\bullet}$ is an applicative \emph{functor} and $G^{\bullet}$
is an applicative contrafunctor then the contrafunctor $C^{A}\triangleq F^{G^{A}}$
(equivalently written as $C\triangleq F\circ G$) is applicative.

\subparagraph{Proof}

We follow the proof of Statement~\ref{subsec:Statement-applicative-composition}.
The methods \lstinline!zip! and \lstinline!wu! are defined by:
\begin{align*}
 & \text{zip}_{C}:F^{G^{A}}\times F^{G^{B}}\rightarrow F^{G^{A\times B}}\quad,\quad\quad\text{zip}_{C}\triangleq\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\quad,\\
 & \text{wu}_{C}\triangleq\text{pu}_{F}(\text{wu}_{G})\quad.
\end{align*}
Note that the lifting to $C$ is defined by $f^{\downarrow C}\triangleq f^{\downarrow G\uparrow F}=(f^{\downarrow G})^{\uparrow F}$. 

To verify the left identity law in Eq.~(\ref{eq:applicative-contrafunctor-identity-laws}):
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{zip}_{C}(\text{wu}_{C}\times p^{:F^{G^{A}}})\triangleright\text{ilu}^{\downarrow C}=(\gunderline{\text{pu}_{F}(\text{wu}_{G})}\times p)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\text{ilu}^{\downarrow G\uparrow F}\\
{\color{greenunder}\text{left identity law of }\text{zip}_{F}:}\quad & =p\triangleright(g^{:G^{A}}\rightarrow\gunderline{\text{wu}_{G}\times g)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\text{ilu}^{\downarrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =p\triangleright\big(g^{:G^{A}}\rightarrow\gunderline{\text{zip}_{G}(\text{wu}_{G}\times g)\triangleright\text{ilu}^{\downarrow G}}\big)^{\uparrow F}\\
{\color{greenunder}\text{left identity law of }\text{zip}_{G}:}\quad & =p\triangleright(\gunderline{g\rightarrow g})^{\uparrow F}=p\triangleright\text{id}^{\uparrow F}=p\quad.
\end{align*}

To verify the right identity law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{zip}_{C}(p^{:F^{G^{A}}}\times\text{wu}_{C})\triangleright\text{iru}^{\downarrow C}=(p\times\gunderline{\text{pu}_{F}(\text{wu}_{G})})\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\text{iru}^{\downarrow G\uparrow F}\\
{\color{greenunder}\text{right identity law of }\text{zip}_{F}:}\quad & =p\triangleright(g^{:G^{A}}\rightarrow\gunderline{g\times\text{wu}_{G})^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\text{iru}^{\downarrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =p\triangleright\big(g^{:G^{A}}\rightarrow\gunderline{\text{zip}_{G}(g\times\text{wu}_{G})\triangleright\text{iru}^{\downarrow G}}\big)^{\uparrow F}\\
{\color{greenunder}\text{right identity law of }\text{zip}_{G}:}\quad & =p\triangleright(\gunderline{g\rightarrow g})^{\uparrow F}=p\triangleright\text{id}^{\uparrow F}=p\quad.
\end{align*}

To verify the associativity law, first substitute the definition of
$\text{zip}_{C}$ into one side:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{C}\big(p\times\text{zip}_{C}(q\times r)\big)\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow C}=\big(p\times\gunderline{\text{zip}_{C}(q\times r)}\big)\triangleright\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{1,23}^{\downarrow C}\\
 & =\big(p\times\big(\text{zip}_{F}(q\times r)\triangleright\gunderline{\text{zip}_{G}^{\uparrow F}}\big)\big)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{1,23}^{\downarrow G\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{F}:}\quad & =\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\text{zip}_{F}\bef\big(g\times k^{:G^{B}\times G^{C}}\!\rightarrow\gunderline{g\times\text{zip}_{G}(k)\big)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{1,23}^{\downarrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =\text{zip}_{F}\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\big(g\times(h\times j)\rightarrow\text{zip}_{G}(g\times\text{zip}_{G}(h\times j))\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow G}\big)^{\uparrow F}\quad.
\end{align*}
Rewrite the right-hand side of the associativity law in a similar
way:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{C}\big(\text{zip}_{C}(p\times q)\times r\big)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow C}=\big(\gunderline{\text{zip}_{C}(p\times q)}\times r\big)\triangleright\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{12,3}^{\downarrow C}\\
 & =\big(\big(\text{zip}_{F}(p\times q)\triangleright\gunderline{\text{zip}_{G}^{\uparrow F}}\big)\times r\big)\triangleright\gunderline{\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{12,3}^{\downarrow G\uparrow F}\\
{\color{greenunder}\text{naturality law of }\text{zip}_{F}:}\quad & =\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\text{zip}_{F}\bef\big(k^{:G^{A}\times G^{B}}\!\times j\rightarrow\gunderline{\text{zip}_{G}(k)\times j\big)^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}\bef\tilde{\varepsilon}_{12,3}^{\downarrow G\uparrow F}}\\
{\color{greenunder}\text{composition under }^{\uparrow F}:}\quad & =\text{zip}_{F}\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\big((g\times h)\times j\rightarrow\text{zip}_{G}(\text{zip}_{G}(g\times h)\times j)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow G}\big)^{\uparrow F}\quad.
\end{align*}
The two sides become equal after using the associativity laws of $\text{zip}_{F}$
and $\text{zip}_{G}$:
\begin{align*}
 & \text{zip}_{F}\big(p\times\text{zip}_{F}(q\times r)\big)\triangleright\varepsilon_{1,23}^{\uparrow F}=\text{zip}_{F}\big(\text{zip}_{F}(p\times q)\times r\big)\triangleright\varepsilon_{12,3}^{\uparrow F}\quad,\\
 & \text{zip}_{G}(g\times\text{zip}_{G}(h\times j))\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow G}=\text{zip}_{G}(\text{zip}_{G}(g\times h)\times j)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow G}\quad.
\end{align*}

To verify the commutativity law for $C$, we assume that the law holds
for $F$ and $G$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(\text{zip}_{C}\bef\text{swap}^{\downarrow C}):}\quad & \text{swap}\bef\text{zip}_{C}=\gunderline{\text{swap}\bef\text{zip}_{F}}\bef\text{zip}_{G}^{\uparrow F}\\
{\color{greenunder}\text{commutativity law of }F:}\quad & =\text{zip}_{F}\bef\gunderline{\text{swap}^{\uparrow F}\bef\text{zip}_{G}^{\uparrow F}}=\text{zip}_{F}\bef(\gunderline{\text{swap}\bef\text{zip}_{G}})^{\uparrow F}\\
{\color{greenunder}\text{commutativity law of }G\text{ under }^{\uparrow F}:}\quad & =\text{zip}_{F}\bef\text{zip}_{G}^{\uparrow F}\bef\text{swap}^{\downarrow G\uparrow F}=\text{zip}_{C}\bef\text{swap}^{\downarrow C}\quad.
\end{align*}
$\square$

\paragraph{Products}

This construction for applicative contrafunctors is similar to Statement~\ref{subsec:Statement-applicative-contrafunctor-product}:

\subsubsection{Statement \label{subsec:Statement-applicative-contrafunctor-product}\ref{subsec:Statement-applicative-contrafunctor-product}}

If $F^{\bullet}$ and $G^{\bullet}$ are applicative contrafunctors
then the contrafunctor $C^{A}\triangleq F^{A}\times G^{A}$ is also
applicative. If $F$ and $G$ are commutative then $C$ is also commutative.

\subparagraph{Proof}

Exercise~\ref{subsec:Exercise-applicative-II-5}. $\square$

\paragraph{Co-products}

The co-product of applicative contrafunctors is always applicative: 

\subsubsection{Statement \label{subsec:Statement-applicative-contrafunctor-co-product}\ref{subsec:Statement-applicative-contrafunctor-co-product}}

If $F^{\bullet}$ and $G^{\bullet}$ are applicative contrafunctors
then the contrafunctor $C^{A}\triangleq F^{A}+G^{A}$ is also applicative.
If $F$ and $G$ are commutative then $C$ is also commutative.

\subparagraph{Proof}

We need to implement the methods $\text{zip}_{C}$ and $\text{wu}_{C}$.
Since the type $F^{A}+G^{A}$ is completely symmetric in $F$ and
$G$ and the requirements for $F$ and $G$ are the same, there are
two ways of defining the applicative methods for $F^{A}+G^{A}$ where
either $F$ or $G$ is in some sense \textsf{``}preferred\textsf{''}.

To implement $\text{zip}_{C}$, we first transform its type signature
to an equivalent type:
\begin{align*}
 & C^{A}\times C^{B}\rightarrow C^{A\times B}\cong(F^{A}+G^{A})\times(F^{B}+G^{B})\rightarrow F^{A\times B}+G^{A\times B}\\
 & \cong F^{A}\times F^{B}+F^{A}\times G^{B}+G^{A}\times F^{B}+G^{A}\times G^{B}\rightarrow F^{A\times B}+G^{A\times B}\quad.
\end{align*}
The code must fill a matrix of the following form:
\[
\text{zip}_{C}\triangleq\,\begin{array}{|c||cc|}
 & F^{A\times B} & G^{A\times B}\\
\hline F^{A}\times F^{B} & \text{zip}_{F} & \bbnum 0\\
F^{A}\times G^{B} & \text{???} & \text{???}\\
G^{A}\times F^{B} & \text{???} & \text{???}\\
G^{A}\times G^{B} & \bbnum 0 & \text{zip}_{G}
\end{array}\quad.
\]
The line with type $F^{A}\times G^{B}\rightarrow F^{A\times B}+G^{A\times B}$
must hard-code the decision of whether to return $F^{A\times B}+\bbnum 0$
or $\bbnum 0+G^{A\times B}$. Choosing arbitrarily to \textsf{``}prefer\textsf{''}
$F$ over $G$, we decide to always return $F^{A\times B}+\bbnum 0$
in that line. To convert $F^{A}$ into $F^{A\times B}$, we use \lstinline!contramap!
with the projection function $\pi_{1}:A\times B\rightarrow A$, obtaining
$\pi_{1}^{\downarrow F}:F^{A}\rightarrow F^{A\times B}$. The line
with type $G^{A}\times F^{B}\rightarrow F^{A\times B}+G^{A\times B}$
is handled similarly:
\[
\text{zip}_{C}\triangleq\,\begin{array}{|c||cc|}
 & F^{A\times B} & G^{A\times B}\\
\hline F^{A}\times F^{B} & \text{zip}_{F} & \bbnum 0\\
F^{A}\times G^{B} & p^{:F^{A}}\times\_^{:G^{B}}\rightarrow p\triangleright\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B} & \_^{:G^{A}}\times q^{:F^{B}}\rightarrow q\triangleright\pi_{2}^{\downarrow F} & \bbnum 0\\
G^{A}\times G^{B} & \bbnum 0 & \text{zip}_{G}
\end{array}\quad.
\]
This function will sometimes ignore its argument when that argument
has type $\bbnum 0+G^{\bullet}$.

Looking at the possible implementations of $\text{wu}_{C}$ (of type
$F^{\bbnum 1}+G^{\bbnum 1}$), we find two choices:
\[
\text{wu}_{C}\triangleq\text{wu}_{F}+\bbnum 0\quad,\quad\quad\text{or alternatively}:\quad\text{wu}_{C}\triangleq\bbnum 0+\text{wu}_{G}\quad.
\]
Let us check whether the identity laws hold with any of these choices.
The left identity law is:
\[
\text{zip}_{C}(\text{wu}_{C}\times p)\triangleright\text{ilu}^{\downarrow C}\overset{?}{=}p\quad.
\]
We know that $\text{zip}_{C}$ will sometimes ignore its argument
of type $\bbnum 0+G^{\bullet}$, and yet we need to guarantee that
no information is lost from the argument $p$. At the same time, it
is acceptable if $\text{zip}_{C}(\text{wu}_{C}\times p)$ ignored
the argument $\text{wu}_{C}$. So, we need to choose $\text{wu}_{C}\triangleq\bbnum 0+\text{wu}_{G}$. 

With this choice, we can now verify the left identity law:
\begin{align*}
 & \text{zip}_{C}(\text{wu}_{C}\times p)\triangleright\text{ilu}^{\downarrow C}=\big((\bbnum 0^{:F^{\bbnum 1}}+\text{wu}_{G})\times p\big)\triangleright\,\begin{array}{|c||cc|}
 & F^{\bbnum 1\times B} & G^{\bbnum 1\times B}\\
\hline F^{\bbnum 1}\times F^{B} & \text{zip}_{F} & \bbnum 0\\
F^{\bbnum 1}\times G^{B} & p\times\_^{:G^{B}}\rightarrow p\triangleright\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{\bbnum 1}\times F^{B} & \_^{:G^{\bbnum 1}}\times q^{:F^{B}}\rightarrow q\triangleright\pi_{2}^{\downarrow F} & \bbnum 0\\
G^{\bbnum 1}\times G^{B} & \bbnum 0 & \text{zip}_{G}
\end{array}\bef\text{ilu}^{\downarrow C}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & F^{\bbnum 1\times B} & G^{\bbnum 1\times B}\\
\hline F^{B} & \pi_{2}^{\downarrow F} & \bbnum 0\\
G^{B} & \bbnum 0 & g\rightarrow\text{zip}_{G}(\text{wu}_{G}\times g)
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & F^{B} & G^{B}\\
\hline F^{\bbnum 1\times B} & \text{ilu}^{\downarrow F} & \bbnum 0\\
G^{\bbnum 1\times B} & \bbnum 0 & \text{ilu}^{\downarrow G}
\end{array}\\
 & =p\triangleright\,\,\begin{array}{|c||cc|}
 & F^{B} & G^{B}\\
\hline F^{B} & \gunderline{\pi_{2}^{\downarrow F}\bef\text{ilu}^{\downarrow F}} & \bbnum 0\\
G^{B} & \bbnum 0 & g\rightarrow\gunderline{\text{zip}_{G}(\text{wu}_{G}\times g)\triangleright\text{ilu}^{\downarrow F}}
\end{array}\,=p\triangleright\,\begin{array}{|c||cc|}
 & F^{B} & G^{B}\\
\hline F^{B} & \text{id} & \bbnum 0\\
G^{B} & \bbnum 0 & g\rightarrow g
\end{array}\,=p\triangleright\text{id}=p\quad.
\end{align*}

The right identity law is verified in a similar way:
\begin{align*}
 & \text{zip}_{C}(p\times\text{wu}_{C})\triangleright\text{iru}^{\downarrow C}=p\triangleright\,\begin{array}{|c||cc|}
 & F^{A} & G^{A}\\
\hline F^{A} & \gunderline{\pi_{1}^{\downarrow F}\bef\text{iru}^{\downarrow F}} & \bbnum 0\\
G^{A} & \bbnum 0 & g\rightarrow\gunderline{\text{zip}_{G}(g\times\text{wu}_{G})\triangleright\text{iru}^{\downarrow F}}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & F^{A} & G^{A}\\
\hline F^{A} & \text{id} & \bbnum 0\\
G^{A} & \bbnum 0 & g\rightarrow g
\end{array}\,=p\triangleright\text{id}=p\quad.
\end{align*}

If we defined $\text{wu}_{C}\triangleq\text{wu}_{F}+\bbnum 0$, the
identity laws would have failed to hold. It follows that the choice
of \textsf{``}preferred\textsf{''} $F$ in the code of $\text{zip}_{C}$ needs to
be accompanied by the definition $\text{wu}_{C}\triangleq\bbnum 0+\text{wu}_{G}$.
The other choice (\textsf{``}prefer $G$\textsf{''}) in $\text{zip}_{C}$ would require
us to define $\text{wu}_{C}\triangleq\text{wu}_{F}+\bbnum 0$. Both
definitions yield lawful \lstinline!zip! and \lstinline!wu! methods
for the contrafunctor $C$ and differ only by swapping $F$ and $G$.
In the rest of this proof, we stick to the definition that \textsf{``}prefers\textsf{''}
$F$.

Next, we verify the associativity law~(\ref{eq:applicative-contrafunctor-associativity-law})
of $\text{zip}_{C}$. The expression $\text{zip}_{C}(p\times\text{zip}_{C}(q\times r))$
has eight cases depending on whether $p$, $q$, and $r$ are in the
$F$ or in the $G$ parts of their disjunctive types. Let us look
at the conditions for the result of a \lstinline!zip! operation to
be of type $F^{\bullet}+\bbnum 0$ or $\bbnum 0+G^{\bullet}$. According
to the code matrix of $\text{zip}_{C}$, the result of computing $\text{zip}_{C}(p\times q)$
is type $\bbnum 0+G^{\bullet}$ only when both $p$ and $q$ are in
their $G$ parts. In that case, we find that $\text{zip}_{C}$ is
reduced to $\text{zip}_{G}$:
\[
\text{zip}_{C}\big(p^{:\bbnum 0+G^{A}}\times q^{:\bbnum 0+G^{B}}\big)=\text{zip}_{C}\big((\bbnum 0+g^{:G^{A}})\times(\bbnum 0+h^{:G^{B}})\big)=\bbnum 0^{:F^{A\times B}}+\text{zip}_{G}(g\times h)\quad.
\]
So, if all of $p$, $q$, $r$ are of type $\bbnum 0+G^{\bullet}$,
the associativity law of $\text{zip}_{C}$ is reduced to the associativity
law of $\text{zip}_{G}$. Similarly, if all of $p$, $q$, $r$ are
of type $F^{\bullet}+\bbnum 0$, the associativity law of $\text{zip}_{C}$
is reduced to the associativity law of $\text{zip}_{F}$. Since the
laws of $\text{zip}_{F}$ and $\text{zip}_{G}$ hold by assumption,
we will not need to consider these two cases any further. 

To verify the associativity law in the remaining cases, write $p\times\text{zip}_{C}(q\times r)$
separately:
\begin{align*}
 & p\times\text{zip}_{C}(q\times r)=(q\times r)\triangleright\,\begin{array}{|c||cc|}
 & C^{A}\times F^{B\times C} & C^{A}\times G^{B\times C}\\
\hline F^{B}\times F^{C} & g\times h\rightarrow p\times\text{zip}_{F}(g\times h) & \bbnum 0\\
F^{B}\times G^{C} & f\times\_\rightarrow p\times(f\triangleright\pi_{1}^{\downarrow F}) & \bbnum 0\\
G^{B}\times F^{C} & \_\times f\rightarrow p\times(f\triangleright\pi_{2}^{\downarrow F}) & \bbnum 0\\
G^{B}\times G^{C} & \bbnum 0 & g\times h\rightarrow p\times\text{zip}_{G}(g\times h)
\end{array}\quad.
\end{align*}
We can now compute $\text{zip}_{C}(p\times\text{zip}_{C}(q\times r))$,
which always returns values of type $F^{\bullet}+\bbnum 0$:
\begin{align*}
 & \text{zip}_{C}(p\times\text{zip}_{C}(q\times r))=(p\times q\times r)\triangleright\,\begin{array}{|c||cc|}
 & F^{A\times(B\times C)} & G^{A\times(B\times C)}\\
\hline F^{A}\times F^{B}\times G^{C} & f\times g\times\_\rightarrow\text{zip}_{F}(f\times(g\triangleright\pi_{1}^{\downarrow F})) & \bbnum 0\\
F^{A}\times G^{B}\times F^{C} & f\times\_\times h\rightarrow\text{zip}_{F}(f\times(h\triangleright\pi_{2}^{\downarrow F})) & \bbnum 0\\
F^{A}\times G^{B}\times G^{C} & f\times\_\times\_\rightarrow f\triangleright\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B}\times F^{C} & \_\times g\times h\rightarrow\text{zip}_{F}(g\times h)\triangleright\pi_{2}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B}\times G^{C} & \_\times g\times\_\rightarrow g\triangleright\pi_{1}^{\downarrow F}\bef\pi_{2}^{\downarrow F} & \bbnum 0\\
G^{A}\times G^{B}\times F^{C} & \_\times\_\times h\rightarrow h\triangleright\pi_{2}^{\downarrow F}\bef\pi_{2}^{\downarrow F} & \bbnum 0
\end{array}\quad.
\end{align*}
The expression $\text{zip}_{C}(\text{zip}_{C}(p\times q)\times r)$
in the right-hand side of the associativity law is written as:
\begin{align*}
 & \text{zip}_{C}(\text{zip}_{C}(p\times q)\times r)=(p\times q\times r)\triangleright\,\begin{array}{|c||cc|}
 & F^{(A\times B)\times C} & G^{(A\times B)\times C}\\
\hline F^{A}\times F^{B}\times G^{C} & f\times g\times\_\rightarrow\text{zip}_{F}(f\times g)\triangleright\pi_{1}^{\downarrow F} & \bbnum 0\\
F^{A}\times G^{B}\times F^{C} & f\times\_\times h\rightarrow\text{zip}_{F}((f\triangleright\pi_{1}^{\downarrow F})\times h) & \bbnum 0\\
F^{A}\times G^{B}\times G^{C} & f\times\_\times\_\rightarrow f\triangleright\pi_{1}^{\downarrow F}\bef\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B}\times F^{C} & \_\times g\times h\rightarrow\text{zip}_{F}((g\triangleright\pi_{2}^{\downarrow F})\times h) & \bbnum 0\\
G^{A}\times F^{B}\times G^{C} & \_\times g\times\_\rightarrow g\triangleright\pi_{2}^{\downarrow F}\bef\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{A}\times G^{B}\times F^{C} & \_\times\_\times h\rightarrow h\triangleright\pi_{2}^{\downarrow F} & \bbnum 0
\end{array}\quad.
\end{align*}
 To show that the two sides are equal, it remains to use the naturality
law of $\text{zip}_{F}$:
\[
\text{zip}_{F}\big((p\triangleright s^{\downarrow F})\times q\big)=\text{zip}_{F}(p\times q)\triangleright(a\times b\rightarrow s(a)\times b)^{\downarrow F}\quad,
\]
and to apply the tuple-rearranging isomorphisms. For instance, in
the row for $F^{A}\times G^{B}\times F^{C}$ we get:
\begin{align*}
 & \text{zip}_{F}(f\times(h\triangleright\pi_{2}^{\downarrow F}))\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow F}=\text{zip}_{F}(f\times h)\triangleright(a\times(b\times c)\rightarrow a\times c)^{\downarrow F}\bef(a\times b\times c\rightarrow a\times(b\times c))^{\downarrow F}\\
 & \quad=\text{zip}_{F}(f\times h)\triangleright(a\times b\times c\rightarrow a\times c)^{\downarrow F}\quad.\\
 & \text{zip}_{F}((f\triangleright\pi_{1}^{\downarrow F})\times h)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow F}=\text{zip}_{F}(f\times h)\triangleright((a\times b)\times c\rightarrow a\times c)^{\downarrow F}\bef(a\times b\times c\rightarrow(a\times b)\times c)^{\downarrow F}\\
 & \quad=\text{zip}_{F}(f\times h)\triangleright(a\times b\times c\rightarrow a\times c)^{\downarrow F}\quad.
\end{align*}
In a similar way, we can show that the two sides are equal for all
other rows of the matrices.

To verify the commutativity law of $\text{zip}_{C}$ when that law
holds for $\text{zip}_{F}$ and $\text{zip}_{G}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{swap}\bef\text{zip}_{C}=\,\begin{array}{|c||cc|}
 & F^{B\times A} & G^{B\times A}\\
\hline F^{A}\times F^{B} & \text{swap}\bef\text{zip}_{F} & \bbnum 0\\
F^{A}\times G^{B} & p^{:F^{A}}\times\_^{:G^{B}}\rightarrow p\triangleright\pi_{2}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B} & \_^{:G^{A}}\times q^{:F^{B}}\rightarrow q\triangleright\pi_{1}^{\downarrow F} & \bbnum 0\\
G^{A}\times G^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{G}
\end{array}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{C}\bef\text{swap}^{\downarrow C}=\,\begin{array}{|c||cc|}
 & F^{B\times A} & G^{B\times A}\\
\hline F^{A}\times F^{B} & \text{zip}_{F}\bef\text{swap}^{\downarrow F} & \bbnum 0\\
F^{A}\times G^{B} & p^{:F^{A}}\times\_^{:G^{B}}\rightarrow p\triangleright\pi_{1}^{\downarrow F}\bef\text{swap}^{\downarrow F} & \bbnum 0\\
G^{A}\times F^{B} & \_^{:G^{A}}\times q^{:F^{B}}\rightarrow q\triangleright\pi_{2}^{\downarrow F}\bef\text{swap}^{\downarrow F} & \bbnum 0\\
G^{A}\times G^{B} & \bbnum 0 & \text{zip}_{G}\bef\text{swap}^{\downarrow G}
\end{array}\quad.
\end{align*}
The two sides are equal due to the commutativity laws of $\text{zip}_{F}$
and $\text{zip}_{G}$, and due to the properties
\[
\text{swap}\bef\pi_{1}=(a\times b\rightarrow b\times a)\bef(c\times d\rightarrow c)=a\times b\rightarrow b=\pi_{2}\quad,\quad\quad\text{swap}\bef\pi_{2}=\pi_{1}\quad.
\]
$\square$

\paragraph{Function types}

The construction $P^{A}\triangleq H^{A}\rightarrow G^{A}$ for applicative
contrafunctors has no analog for applicative \emph{functors}. Exercise~\ref{subsec:Exercise-function-type-construction-not-applicative}
shows simple examples where a function type construction fails to
produce applicative functors. However, the function type construction
works for a wide class of applicative contrafunctors:

\subsubsection{Statement \label{subsec:Statement-applicative-contrafunctor-exponential}\ref{subsec:Statement-applicative-contrafunctor-exponential}}

If $G^{\bullet}$ is an applicative contrafunctor and $H^{\bullet}$
is \emph{any functor} then the contrafunctor $P^{A}\triangleq H^{A}\rightarrow G^{A}$
is applicative. If $G^{\bullet}$ is commutative then $P^{\bullet}$
is also commutative.

\subparagraph{Proof}

We implement the \lstinline!zip! and \lstinline!wu! methods like
this:
\begin{align*}
 & \text{zip}_{P}:(H^{A}\rightarrow G^{A})\times(H^{B}\rightarrow G^{B})\rightarrow H^{A\times B}\rightarrow G^{A\times B}\quad,\\
 & \text{zip}_{P}\big(p^{:H^{A}\rightarrow G^{A}}\times q^{:H^{B}\rightarrow G^{B}}\big)\triangleq h^{:H^{A\times B}}\rightarrow\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times q(h\triangleright\pi_{2}^{\uparrow H})\big)\quad,\\
 & \text{wu}_{P}\triangleq\_^{:H^{\bbnum 1}}\rightarrow\text{wu}_{G}\quad.
\end{align*}
We can equivalently write the definition of $\text{zip}_{P}$ in a
point-free form, omitting the argument $h$:
\[
\text{zip}_{P}(p\times q)\triangleq\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(p\boxtimes q)\bef\text{zip}_{G}\quad.
\]

The code for lifting to $P$ is standard:
\[
f^{\downarrow P}=p^{:H^{A}\rightarrow G^{A}}\rightarrow f^{\uparrow H}\bef p\bef f^{\downarrow G}\quad.
\]

To verify the left identity law, we need to show that:
\[
h^{:H^{A}}\triangleright\big(\text{zip}_{P}(\text{wu}_{P}\times p^{:H^{A}\rightarrow G^{A}})\triangleright\text{ilu}^{\downarrow P}\big)\overset{?}{=}h\triangleright p=p(h)\quad.
\]
We use the properties such as $\text{ilu}\bef\pi_{2}=\text{id}$ and
compute:
\begin{align*}
 & h\triangleright\text{ilu}^{\uparrow H}\triangleright\text{zip}_{P}(\text{wu}_{P}\times p)\triangleright\text{ilu}^{\downarrow G}=\text{zip}_{G}\big(\text{wu}_{P}(h\triangleright\text{ilu}^{\uparrow H}\triangleright\pi_{1}^{\uparrow H})\times p(h\triangleright\text{ilu}^{\uparrow H}\triangleright\pi_{2}^{\uparrow H})\big)\triangleright\text{ilu}^{\downarrow G}\\
 & =\text{zip}_{G}(\text{wu}_{G}\times p(h))\triangleright\text{ilu}^{\downarrow G}=p(h)\quad.
\end{align*}

The right identity law is verified by a similar calculation:
\begin{align*}
 & h\triangleright\text{iru}^{\uparrow H}\triangleright\text{zip}_{P}(p\times\text{wu}_{P})\triangleright\text{iru}^{\downarrow G}=\text{zip}_{G}(p(h\triangleright\text{iru}^{\uparrow H}\triangleright\pi_{1}^{\uparrow H})\times\text{wu}_{P}(h\triangleright\text{iru}^{\uparrow H}\triangleright\pi_{2}^{\uparrow H}))\triangleright\text{iru}^{\downarrow G}\\
 & =\text{zip}_{G}(p(h)\times\text{wu}_{G})\triangleright\text{iru}^{\downarrow G}=p(h)\quad.
\end{align*}

To verify the associativity law, we use properties such as $\varepsilon_{12,3}\bef\pi_{2}=\pi_{3}$
and so on:
\begin{align*}
 & h^{:H^{A\times B\times C}}\triangleright\tilde{\varepsilon}_{1,23}^{\uparrow H}\triangleright\text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow G}=\text{zip}_{G}\big(p(h\triangleright\gunderline{\tilde{\varepsilon}_{1,23}^{\uparrow H}\bef\pi_{1}^{\uparrow H}})\times\text{zip}_{P}(q\times r)(h\triangleright\gunderline{\tilde{\varepsilon}_{1,23}^{\uparrow H}\bef\pi_{2}^{\uparrow H}})\big)\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times\text{zip}_{G}\big(q(h\triangleright\pi_{2}^{\uparrow H})\times r(h\triangleright\pi_{3}^{\uparrow H})\big)\big)\triangleright\tilde{\varepsilon}_{1,23}^{\downarrow G}\quad,\\
 & h^{:H^{A\times B\times C}}\triangleright\tilde{\varepsilon}_{12,3}^{\uparrow H}\triangleright\text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow G}=\text{zip}_{G}\big(\text{zip}_{P}(p\times q)(h\triangleright\gunderline{\tilde{\varepsilon}_{12,3}^{\uparrow H}\bef\pi_{1}^{\uparrow H}})\times r(h\triangleright\gunderline{\tilde{\varepsilon}_{12,3}^{\uparrow H}\bef\pi_{2}^{\uparrow H}})\big)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times q(h\triangleright\pi_{2}^{\uparrow H})\big)\times r(h\triangleright\pi_{3}^{\uparrow H})\big)\triangleright\tilde{\varepsilon}_{12,3}^{\downarrow G}\quad.
\end{align*}
The two sides are now equal due to the assumed associativity law of
$\text{zip}_{G}$.

It remains to verify the commutativity law, assuming that $\text{zip}_{G}$
satisfies that law:
\begin{align*}
 & \text{zip}_{P}(q\times p)=\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(q\boxtimes p)\bef\text{zip}_{G}\quad,\\
 & \text{zip}_{P}(p\times q)\triangleright\text{swap}^{\downarrow P}=\text{swap}^{\uparrow H}\bef\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(p\boxtimes q)\bef\gunderline{\text{zip}_{G}\bef\text{swap}^{\downarrow G}}\\
 & \quad=\Delta\bef\gunderline{(\text{swap}^{\uparrow H}\boxtimes\text{swap}^{\uparrow H})\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})}\bef\gunderline{(p\boxtimes q)\bef\text{swap}}\bef\text{zip}_{G}\\
 & \quad=\gunderline{\Delta\bef(\pi_{2}^{\uparrow H}\boxtimes\pi_{1}^{\uparrow H})\bef\text{swap}}\bef(q\boxtimes p)\bef\text{zip}_{G}=\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(q\boxtimes p)\bef\text{zip}_{G}\quad.
\end{align*}
$\square$

The constructions shown in this section cover all exponential-polynomial
contrafunctors built up from monoidal fixed types. It follows that
\emph{all} exponential-polynomial contrafunctors with monoidal fixed
types are applicative. The applicative instance will not be unique
if the contrafunctor involves a co-product, because the construction
in Statement~\ref{subsec:Statement-applicative-contrafunctor-co-product}
admits two alternative implementations of the applicative methods.

\subsection{Applicative profunctors: Laws and constructions}

The word \textsf{``}profunctor\index{profunctor}\textsf{''} is used in two ways in
this book (see Section~\ref{subsec:f-Profunctors}). We call a type
constructor $P^{X,Y}$ a profunctor when it is contravariant in $X$
and covariant in $Y$. Given such a profunctor $P^{X,Y}$, we can
set $X=Y$ and obtain a type constructor $Q^{A}\triangleq P^{A,A}$,
which is neither covariant nor contravariant in $A$. This new type
constructor ($Q$) is also called a \textsf{``}profunctor\textsf{''} for brevity. 

When a given type constructor $Q$ is fully parametric, we can always
separate the covariant and the contravariant occurrences of the type
parameter $A$ in $Q^{A}$. We can then rename the contravariant occurrences
to $X$ and the covariant ones to $Y$, obtaining a type constructor
$P^{X,Y}$ that fits the definition of a profunctor. In this sense,
all fully parametric type constructors are profunctors.

An example of a type constructor that \emph{cannot} be a profunctor
is a \textsf{``}GADT\index{GADT}\textsf{''}:

\begin{lstlisting}
sealed trait U[A]                // Unfunctor.
final case class U1(s: Double) extends U[Unit]
final case class U2(b: String) extends U[Int]
final case class U3(b: String) extends U[Long]
\end{lstlisting}
\[
U^{A}\triangleq\text{Double}^{:U^{\bbnum 1}}+\text{String}^{:U^{\text{Int}}}+\text{String}^{:U^{\text{Long}}}\quad.
\]
The definition of this type constructor is \emph{not} fully parametric:
values of type $U^{A}$ cannot be created for arbitrary type parameter
$A$ (only for $A=\text{Int}$ or $A=\text{Long}$ or $A=\bbnum 1$).
This prevents us from implementing any \lstinline!map!-like methods
for $U^{\bullet}$.

Profunctors have an \lstinline!xmap! method instead of a \lstinline!map!
method:
\begin{lstlisting}
def xmap[A, B](f: A => B, g: B => A): P[A] => P[B]
\end{lstlisting}
 For brevity, in this section we will denote Scala code such as \lstinline!xmap(f, g)(p)!
or \lstinline!p.xmap(f, g)! by:
\[
p\triangleright f^{\uparrow P}g^{\downarrow P}\quad,\quad\text{or equivalently}:\quad p\triangleright g^{\downarrow P}f^{\uparrow P}\quad.
\]

The applicative laws for profunctors are formulated via \lstinline!zip!
and \lstinline!wu! methods using \lstinline!xmap!:

\subsubsection{Definition \label{subsec:Definition-applicative-profunctor}\ref{subsec:Definition-applicative-profunctor}}

A profunctor $P^{\bullet}$ is \textbf{applicative} if there exist
methods \lstinline!zip! and \lstinline!wu! such that:
\begin{align*}
 & \text{zip}_{P}:P^{A}\times P^{B}\rightarrow P^{A\times B}\quad,\quad\quad\text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\cong\text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\quad,\\
 & \text{wu}_{P}:P^{\bbnum 1}\quad,\quad\text{zip}_{P}(\text{wu}_{P}\times p)\cong p\quad,\quad\quad\text{zip}_{P}(p\times\text{wu}_{P})\cong p\quad.
\end{align*}
Here we imply tuple-rearranging isomorphisms $P^{(A\times B)\times C}\cong P^{A\times(B\times C)}$
and $P^{\bbnum 1\times A}\cong P^{A}\cong P^{A\times\bbnum 1}$, which
are implemented via the functions $\varepsilon_{1,23}$, $\varepsilon_{12,3}$,
$\tilde{\varepsilon}_{1,23}$, $\tilde{\varepsilon}_{12,3}$, \lstinline!ilu!,
and \lstinline!iru!:
\begin{align*}
 & \text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}=\text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}\quad,\\
 & \text{zip}_{P}(\text{wu}_{P}\times p)=p\triangleright\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}\quad,\quad\quad\text{zip}_{P}(p\times\text{wu}_{P})=p\triangleright\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P}\quad.
\end{align*}

Once the \lstinline!zip! and \lstinline!wu! methods are known, we
can define \lstinline!pure! and \lstinline!xmap2!:
\begin{align*}
 & \text{pu}_{P}:A\rightarrow P^{A}\quad,\quad\quad\text{pu}_{P}(a^{:A})\triangleq\text{wu}_{P}\triangleright(\_^{:A}\rightarrow1)^{\downarrow P}(\_^{:\bbnum 1}\rightarrow a)^{\uparrow P}\quad,\\
 & \text{xmap}_{2}:(A\times B\rightarrow D)\times(D\rightarrow A\times B)\rightarrow P^{A}\times P^{B}\rightarrow P^{D}\quad,\\
 & \text{xmap}_{2}(f^{:A\times B\rightarrow D}\times g^{:D\rightarrow A\times B})\triangleq\text{zip}_{P}\bef(g^{\downarrow C}f^{\uparrow C})\quad.
\end{align*}

It is important that the \lstinline!pure! method ($\text{pu}_{P}$)
for profunctors is defined via the wrapped unit ($\text{wu}_{P}$).
The presence of a value $\text{wu}_{P}:P^{\bbnum 1}$ means that the
profunctor $P^{\bullet}$ is pointed\index{profunctor!pointed} (see
Section~\ref{subsec:Pointed-functors-motivation-equivalence}). For
functors and contrafunctors, the naturality law of \lstinline!pure!
is enough to enforce the equivalence of \lstinline!pure! and \lstinline!wu!.
For profunctors, however, the value $\text{wu}_{P}:P^{\bbnum 1}$
is \emph{not} equivalent to the type of fully parametric functions
with the type signature of \lstinline!pure!. The following example
illustrates this (see also Exercise~\ref{subsec:Exercise-profunctor-pure-not-equivalent-1}).

\subsubsection{Example \label{subsec:Example-profunctor-pure-not-equivalent}\ref{subsec:Example-profunctor-pure-not-equivalent}\index{examples (with code)}}

For the profunctor $P^{A}\triangleq\left(A\rightarrow A\right)\rightarrow A$,
show that the type $P^{\bbnum 1}$ is \emph{not} equivalent to the
type of fully parametric functions $\text{pu}_{P}:A\rightarrow P^{A}$.

\subparagraph{Solution}

We can rewrite the type of $\text{wu}_{P}$ equivalently as $P^{\bbnum 1}=\left(\bbnum 1\rightarrow\bbnum 1\right)\rightarrow\bbnum 1\cong\bbnum 1$.
So, there is only one value of this type (a function that ignores
its argument and always returns the unit value $1$). The corresponding
\lstinline!pure! method is a function that ignores its argument always
returns the given value:
\[
\text{pu}_{P}\triangleq a^{:A}\rightarrow\_^{:A\rightarrow A}\rightarrow a\quad.
\]
But there are many more functions with the same type signature as
$\text{pu}_{P}$. To see this, it is convenient swap the curried arguments
of $\text{pu}_{P}$ and obtain an equivalent type: 
\[
A\rightarrow P^{A}=A\rightarrow\left(A\rightarrow A\right)\rightarrow A\cong\left(A\rightarrow A\right)\rightarrow A\rightarrow A\quad.
\]
Examples of functions of this type are
\[
f_{1}\triangleq k^{:A\rightarrow A}\rightarrow k\quad,\quad\quad f_{2}\triangleq k^{:A\rightarrow A}\rightarrow(k\bef k)\quad,
\]
and so on. For any non-negative integer $n=0,1,2,...$, we can define
the function $f_{n}$ that applies its argument $n$ times (similar
functions were defined in Example~\ref{subsec:Example-hof-derive-types-2}
and Exercise~\ref{subsec:Exercise-hof-simple-4}). The function \lstinline!pure!
defined above is the same as $f_{0}$. All functions $f_{n}$ are
fully parametric. So, the type of fully parametric functions with
type signature $A\rightarrow P^{A}$ contains many more values than
the type $P^{\bbnum 1}$. $\square$

The commutativity law of applicative profunctors is written like this:
\[
\text{zip}_{P}(q\times p)\triangleright\text{swap}^{\downarrow P}\text{swap}^{\uparrow P}=\text{zip}_{P}(p\times q)\quad.
\]

The rest of this section proves some constructions that produce lawful
applicative profunctors.

\subsubsection{Statement \label{subsec:Statement-applicative-profunctor-composition}\ref{subsec:Statement-applicative-profunctor-composition}}

If $F^{\bullet}$ is an applicative \emph{functor} and $G^{\bullet}$
is an applicative profunctor then the profunctor $P^{A}\triangleq F^{G^{A}}$
(equivalently written as $P\triangleq F\circ G$) is applicative.
If both $F$ and $G$ are commutative then so is $P$.

\subparagraph{Proof}

Exercise~\ref{subsec:Exercise-applicative-profunctor-composition}.

\subsubsection{Statement \label{subsec:Statement-applicative-profunctor-product}\ref{subsec:Statement-applicative-profunctor-product}}

If $G^{\bullet}$ and $H^{\bullet}$ are applicative profunctors then
so is $P^{A}\triangleq G^{A}\times H^{A}$. If both $G$ and $H$
are commutative then so is $P$.

\subparagraph{Proof}

We follow the proof of Statement~\ref{subsec:Statement-applicative-product}.
Begin by implementing the \lstinline!zip! and \lstinline!wu! methods
for $P$:
\begin{align*}
 & \text{zip}_{P}:(F^{A}\times G^{A})\times(F^{B}\times G^{B})\rightarrow F^{A\times B}\times G^{A\times B}\quad,\\
 & \text{zip}_{P}\big((m^{:F^{A}}\times n^{:G^{A}})\times(p^{:F^{B}}\times q^{:G^{B}})\big)\triangleq\text{zip}_{F}(m\times p)\times\text{zip}_{G}(n\times q)\quad,\\
 & \text{wu}_{P}:F^{\bbnum 1}\times G^{\bbnum 1}\quad,\quad\quad\text{wu}_{P}\triangleq\text{wu}_{F}\times\text{wu}_{G}\quad.
\end{align*}

The lifting to $P$ is defined by $(f^{\uparrow P}g^{\downarrow P})\triangleq(f^{\uparrow F}g^{\downarrow F})\boxtimes(f^{\uparrow G}g^{\downarrow G})$.

To verify the left identity law of $\text{zip}_{P}$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(p\times q)\triangleright\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}:}\quad & \text{zip}_{P}\big(\gunderline{\text{wu}_{L}}\times(p^{:F^{A}}\times q^{:G^{A}})\big)=\gunderline{\text{zip}_{P}}((\text{wu}_{F}\times\text{wu}_{G})\times(p\times q))\\
{\color{greenunder}\text{definition of }\text{zip}_{P}:}\quad & =\text{zip}_{F}(\text{wu}_{F}\times p)\times\text{zip}_{G}(\text{wu}_{G}\times q)\\
{\color{greenunder}\text{left identity laws of }\text{zip}_{F}\text{ and }\text{zip}_{G}:}\quad & =(p\triangleright\text{ilu}^{\uparrow F}\pi_{2}^{\downarrow F})\times(q\triangleright\text{ilu}^{\uparrow G}\pi_{2}^{\downarrow G})\\
{\color{greenunder}\text{definition of }\boxtimes:}\quad & =(p\times q)\triangleright\big((\text{ilu}^{\uparrow F}\pi_{2}^{\downarrow F})\boxtimes(\text{ilu}^{\uparrow G}\pi_{2}^{\downarrow G})\big)=(p\times q)\triangleright\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}\quad.
\end{align*}

To verify the right identity law of $\text{zip}_{P}$:
\begin{align*}
{\color{greenunder}\text{expect to equal }(p\times q)\triangleright\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P}:}\quad & \text{zip}_{P}\big((p^{:F^{A}}\times q^{:G^{A}})\times\gunderline{\text{wu}_{L}}\big)=\gunderline{\text{zip}_{P}}((p\times q)\times(\text{wu}_{F}\times\text{wu}_{G}))\\
{\color{greenunder}\text{definition of }\text{zip}_{P}:}\quad & =\text{zip}_{F}(p\times\text{wu}_{F})\times\text{zip}_{G}(q\times\text{wu}_{G})\\
{\color{greenunder}\text{right identity laws of }\text{zip}_{F}\text{ and }\text{zip}_{G}:}\quad & =(p\triangleright\text{iru}^{\uparrow F}\pi_{1}^{\downarrow F})\times(q\triangleright\text{iru}^{\uparrow G}\pi_{1}^{\downarrow G})\\
{\color{greenunder}\text{definition of }\boxtimes:}\quad & =(p\times q)\triangleright\big((\text{iru}^{\uparrow F}\pi_{1}^{\downarrow F})\boxtimes(\text{iru}^{\uparrow G}\pi_{1}^{\downarrow G})\big)=(p\times q)\triangleright\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P}\quad.
\end{align*}

To verify the associativity law, begin with its left-hand side and
use the definition of $\text{zip}_{P}$:
\begin{align*}
 & \text{zip}_{P}\big((p_{1}^{:F^{A}}\times p_{2}^{:G^{A}})\times\text{zip}_{P}\big((q_{1}^{:F^{B}}\times q_{2}^{:G^{B}})\times(r_{1}^{:F^{C}}\times r_{2}^{:G^{C}})\big)\big)\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}\\
 & =\text{zip}_{P}\big((p_{1}\times p_{2})\times\big(\text{zip}_{F}(q_{1}\times r_{1})\times\text{zip}_{G}(q_{2}\times r_{2})\big)\big)\triangleright\big((\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F})\boxtimes(\varepsilon_{1,23}^{\uparrow G}\tilde{\varepsilon}_{1,23}^{\downarrow G})\big)\\
 & =\big(\gunderline{\text{zip}_{F}\big(p_{1}\times\text{zip}_{F}(q_{1}\times r_{1})\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F}}\big)\times\big(\gunderline{\text{zip}_{G}\big(p_{2}\times\text{zip}_{G}(q_{2}\times r_{2})\big)\triangleright\varepsilon_{1,23}^{\uparrow G}\tilde{\varepsilon}_{1,23}^{\downarrow G}}\big)\quad.
\end{align*}
The right-hand side is rewritten in a similar way:
\begin{align*}
 & \text{zip}_{P}\big(\text{zip}_{P}\big((p_{1}^{:F^{A}}\times p_{2}^{:G^{A}})\times(q_{1}^{:F^{B}}\times q_{2}^{:G^{B}})\big)\times(r_{1}^{:F^{C}}\times r_{2}^{:G^{C}})\big)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}\\
 & =\text{zip}_{P}\big(\big(\text{zip}_{F}(p_{1}\times q_{1})\times\text{zip}_{G}(p_{2}\times q_{2})\big)\times(r_{1}\times r_{2})\big)\big)\triangleright\big((\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F})\boxtimes(\varepsilon_{12,3}^{\uparrow G}\tilde{\varepsilon}_{12,3}^{\downarrow G})\big)\\
 & =\big(\gunderline{\text{zip}_{F}\big(\text{zip}_{F}(p_{1}\times q_{1})\times r_{1}\big)\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}}\big)\times\big(\gunderline{\text{zip}_{G}\big(\text{zip}_{G}(p_{2}\times q_{2})\times r_{2}\big)\triangleright\varepsilon_{12,3}^{\uparrow G}\tilde{\varepsilon}_{12,3}^{\downarrow G}}\big)\quad.
\end{align*}
The underlined expressions in both sides are equal due to associativity
laws of $\text{zip}_{F}$ and $\text{zip}_{G}$.

To verify the commutativity law of $P$ assuming it holds for $F$
and $G$:
\begin{align*}
{\color{greenunder}\text{expect }\text{zip}_{P}\big((p\times q)\times(m\times n)\big):}\quad & \text{zip}_{P}\big((m\times n)\times(p\times q)\big)\triangleright\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}\\
{\color{greenunder}\text{definitions of }\text{zip}_{P}\text{ and }^{\uparrow P}:}\quad & =\big(\text{zip}_{F}(m\times p)\times\text{zip}_{G}(n\times q)\big)\triangleright\big((\text{swap}^{\uparrow F}\text{swap}^{\downarrow F})\boxtimes(\text{swap}^{\uparrow G}\text{swap}^{\downarrow G})\big)\\
{\color{greenunder}\text{definition of }\boxtimes:}\quad & =\big(\text{zip}_{F}(m\times p)\triangleright\text{swap}^{\uparrow F}\text{swap}^{\downarrow F}\big)\times\big(\text{zip}_{G}(n\times q)\triangleright\text{swap}^{\uparrow G}\text{swap}^{\downarrow G}\big)\\
{\color{greenunder}\text{commutativity of }F\text{ and }G:}\quad & =\text{zip}_{F}(p\times m)\times\text{zip}_{G}(q\times n)=\text{zip}_{P}\big((p\times q)\times(m\times n)\big)\quad.
\end{align*}
$\square$

\subsubsection{Statement \label{subsec:Statement-applicative-profunctor-co-product-1}\ref{subsec:Statement-applicative-profunctor-co-product-1}}

If $F^{\bullet}$ is an applicative profunctor and $Z$ is a fixed
monoid type then the profunctor $P^{A}\triangleq Z+F^{A}$ is also
applicative:
\begin{align*}
 & \text{zip}_{P}:(Z+F^{A})\times(Z+F^{B})\rightarrow Z+F^{A\times B}\quad,\quad\quad\text{zip}_{P}\triangleq\,\begin{array}{|c||cc|}
 & Z & F^{A\times B}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{A}\times Z & \_^{:F^{A}}\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_^{:F^{B}}\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\quad.
\end{align*}
The method $\text{wu}_{P}:Z+F^{\bbnum 1}$ is defined by $\text{wu}_{P}\triangleq\bbnum 0^{:Z}+\text{wu}_{F}$.
If $Z$ is a commutative monoid and $F^{\bullet}$ is commutative
then $P^{\bullet}$ is also commutative.

\subparagraph{Proof}

We follow the proof of Statement~\ref{subsec:Statement-co-product-with-constant-functor-applicative}
\emph{mutatis mutandis}.

The lifting to $P$ is defined in the standard way:
\[
(f^{:A\rightarrow B})^{\uparrow P}(g^{:B\rightarrow A})^{\downarrow P}\triangleq\,\begin{array}{|c||cc|}
 & Z & F^{B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & f^{\uparrow F}g^{\downarrow F}
\end{array}\quad.
\]

To verify the left identity law, we use the left identity law of $\text{zip}_{F}$:
\begin{align*}
 & \text{zip}_{P}(\text{wu}_{P}\times p^{:Z+F^{B}})=\text{zip}_{P}((\bbnum 0+\text{wu}_{F})\times p)=(\text{wu}_{F}\times p)\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline F^{\bbnum 1}\times Z & \_^{:F^{\bbnum 1}}\times z\rightarrow z & \bbnum 0\\
F^{\bbnum 1}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{B} & \bbnum 0 & k^{:F^{B}}\rightarrow\gunderline{\text{zip}_{F}(\text{wu}_{F}\times k)}
\end{array}\,=p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{\bbnum 1\times B}\\
\hline Z & \text{id} & \bbnum 0\\
F^{B} & \bbnum 0 & k\rightarrow k\triangleright\text{ilu}^{\uparrow F}\pi_{2}^{\downarrow F}
\end{array}\,=p\triangleright\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}\quad.
\end{align*}

To verify the right identity law, we write a similar calculation:
\begin{align*}
 & \text{zip}_{P}(p^{:Z+F^{A}}\times\text{wu}_{P})=\text{zip}_{P}(p\times(\bbnum 0+\text{wu}_{F}))=(p\times\text{wu}_{F})\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z\times F^{\bbnum 1} & z\times\_^{:F^{\bbnum 1}}\rightarrow z & \bbnum 0\\
F^{A}\times F^{\bbnum 1} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & k^{:F^{A}}\rightarrow\gunderline{\text{zip}_{F}(k\times\text{wu}_{F})}
\end{array}\,=p\triangleright\,\begin{array}{|c||cc|}
 & Z & F^{A\times\bbnum 1}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A} & \bbnum 0 & k\rightarrow k\triangleright\text{iru}^{\uparrow F}\pi_{1}^{\downarrow F}
\end{array}\,=p\triangleright\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P}\quad.
\end{align*}

To verify the associativity law, we use a trick to avoid long derivations.
The two sides of the associativity law are expressions of type $Z+F^{A\times B\times C}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{P}\big(p^{:Z+F^{A}}\times\text{zip}_{P}(q^{:Z+F^{B}}\times r^{:Z+F^{C}})\big)\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{P}\big(\text{zip}_{P}(p^{:Z+F^{A}}\times q^{:Z+F^{B}})\times r^{:Z+F^{C}}\big)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}\quad.
\end{align*}
Since each of the arguments $p$, $q$, $r$ may be in one of the
two parts of the disjunction type $Z+F^{\bullet}$, we have 8 cases.
We note, however, that the code of $\text{zip}_{P}(p\times q)$ will
return a value of type $Z+\bbnum 0$ whenever at least one of the
arguments ($p$, $q$) is of type $Z+\bbnum 0$. So, a composition
of two \lstinline!zip! operations will also return a value of type
$Z+\bbnum 0$ whenever at least one of the arguments ($p$, $q$,
$r$) is of type $Z+\bbnum 0$. It remains to consider only two cases: 

\textbf{(1)} At least one of $p$, $q$, $r$ is of type $Z+\bbnum 0$.
In this case, any arguments of type $\bbnum 0+F^{\bullet}$ are ignored
by $\text{zip}_{P}$, while the arguments of type $Z+\bbnum 0$ are
combined using the monoid $Z$\textsf{'}s binary operation ($\oplus$). So,
the result of the \lstinline!zip! operation is the same if we replace
any arguments ($p$, $q$, $r$) of type $\bbnum 0+F^{\bullet}$ by
the empty value $e_{Z}$. For example:
\[
\text{zip}_{P}\big((z+\bbnum 0)\times(\bbnum 0+k^{:F^{A}})\big)=z+\bbnum 0=\text{zip}_{P}\big((z+\bbnum 0)\times(e_{Z}+\bbnum 0)\big)\quad.
\]
After this replacement, we have three arguments ($z_{1}+\bbnum 0$,
$z_{2}+\bbnum 0$, $z_{3}+\bbnum 0$) instead of $p$, $q$, $r$,
and the function $\text{zip}_{P}$ reduces to the operation $\oplus$,
for which the associativity law holds by assumption.

\textbf{(2)} All of $p$, $q$, $r$ are of type $\bbnum 0+F^{\bullet}$.
In this case, $\text{zip}_{P}$ reduces to $\text{zip}_{F}$, which
satisfies the associativity law by assumption.

To verify the commutativity law of $P$, use the code matrix for \lstinline!swap!
with the relevant types:
\begin{align*}
 & \text{swap}\bef\text{zip}_{P}=\,\begin{array}{|c||cccc|}
 & Z\times Z & F^{B}\times Z & Z\times F^{A} & F^{B}\times F^{A}\\
\hline Z\times Z & \text{swap} & \bbnum 0 & \bbnum 0 & \bbnum 0\\
F^{A}\times Z & \bbnum 0 & \bbnum 0 & \text{swap} & \bbnum 0\\
Z\times F^{B} & \bbnum 0 & \text{swap} & \bbnum 0 & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \bbnum 0 & \bbnum 0 & \text{swap}
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{B}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{A} & z\times\_\rightarrow z & \bbnum 0\\
F^{B}\times F^{A} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{2}\oplus z_{1} & \bbnum 0\\
F^{A}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\end{align*}
By assumption, $\text{swap}\bef\text{zip}_{F}=\text{zip}_{F}\bef\text{swap}^{\uparrow F}\text{swap}^{\downarrow F}$.
The code for $\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}$
is:
\[
\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}=\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z & \text{id} & \bbnum 0\\
F^{A\times B} &  & \text{swap}^{\uparrow F}\text{swap}^{\downarrow F}
\end{array}\quad.
\]
We can now transform the right-hand side of the commutativity law:
\[
\text{zip}_{L}\bef\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}=\,\begin{array}{|c||cc|}
 & Z & F^{B\times A}\\
\hline Z\times Z & z_{1}\times z_{2}\rightarrow z_{1}\oplus z_{2} & \bbnum 0\\
F^{A}\times Z & \_\times z\rightarrow z & \bbnum 0\\
Z\times F^{B} & z\times\_\rightarrow z & \bbnum 0\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}\bef\text{swap}^{\uparrow F}\text{swap}^{\downarrow F}
\end{array}\quad.
\]
The difference between the sides disappears if $Z$ is a commutative
monoid ($z_{1}\oplus z_{2}=z_{2}\oplus z_{1}$). $\square$

\subsubsection{Statement \label{subsec:Statement-applicative-profunctor-co-product-2}\ref{subsec:Statement-applicative-profunctor-co-product-2}}

If $F^{\bullet}$ and $H^{\bullet}$ are applicative profunctors and
$H^{\bullet}$ is co-pointed with the method $\text{ex}_{F}:H^{A}\rightarrow A$
such that the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
holds, then $P^{A}\triangleq H^{A}+F^{A}$ is also applicative:
\begin{align*}
 & \text{zip}_{P}:(H^{A}+F^{A})\times(H^{B}+F^{B})\rightarrow H^{A\times B}+F^{A\times B}\quad,\\
 & \text{zip}_{P}\triangleq\,\begin{array}{|c||cc|}
 & H^{A\times B} & F^{A\times B}\\
\hline H^{A}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\quad;
\end{align*}
The method $\text{wu}_{P}:H^{\bbnum 1}+F^{\bbnum 1}$ is defined by
$\text{wu}_{P}\triangleq\text{wu}_{H}+\bbnum 0^{:F^{\bbnum 1}}$.
If $F^{\bullet}$ and $H^{\bullet}$ are commutative then $P^{\bullet}$
is also commutative.

\subparagraph{Proof}

We follow the proof of Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative}.
The lifting to $P^{\bullet}$ is defined by:
\[
(f^{:A\rightarrow B})^{\uparrow P}(g^{:B\rightarrow A})^{\downarrow P}\triangleq\,\begin{array}{|c||cc|}
 & H^{B} & F^{B}\\
\hline H^{A} & f^{\uparrow H}g^{\downarrow H} & \bbnum 0\\
F^{A} & \bbnum 0 & f^{\uparrow F}g^{\downarrow F}
\end{array}\quad.
\]

To verify the left identity law, we begin with its left-hand side:
\begin{align*}
 & \text{zip}_{P}(\text{wu}_{P}\times p)=\big((\text{wu}_{H}^{:H^{\bbnum 1}}+\bbnum 0^{:F^{\bbnum 1}})\times p^{:H^{B}+F^{B}}\big)\triangleright\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{\bbnum 1}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
H^{\bbnum 1}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{\bbnum 1}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{\bbnum 1}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & h\rightarrow\text{zip}_{H}(\text{wu}_{H}\times h) & \bbnum 0\\
F^{B} & \bbnum 0 & f\rightarrow\text{zip}_{F}((\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F})\times f)
\end{array}\quad.
\end{align*}
Using Eq.~(\ref{eq:co-pointed-nondegeneracy-law-wu}) and the definition
of \lstinline!wu! through \lstinline!pure!, we find:
\[
\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F}=1\triangleright\text{pu}_{F}=\text{wu}_{F}\quad.
\]
Since the identity laws of $F$ and $H$ are assumed to hold, we can
transform the last matrix as:
\begin{align*}
 & \begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & h\rightarrow\text{zip}_{H}(\text{wu}_{H}\times h) & \bbnum 0\\
F^{B} & \bbnum 0 & f\rightarrow\text{zip}_{F}((\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F})\times f)
\end{array}\,=\,\begin{array}{|c||cc|}
 & H^{\bbnum 1\times B} & F^{\bbnum 1\times B}\\
\hline H^{B} & \text{ilu}^{\uparrow H}\pi_{2}^{\downarrow H} & \bbnum 0\\
F^{B} & \bbnum 0 & \text{ilu}^{\uparrow F}\pi_{2}^{\downarrow F}
\end{array}\\
 & =\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}\quad.
\end{align*}
After this simplification, the left-hand side equals $p\triangleright\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P}$
(the right-hand side of the law).

The right identity law is verified in a similar way:
\begin{align*}
 & \text{zip}_{P}(p\times\text{wu}_{L})=\text{zip}_{P}\big(p^{:H^{A}+F^{A}}\times(\text{wu}_{H}^{:H^{\bbnum 1}}+\bbnum 0^{:F^{\bbnum 1}})\big)\\
 & =p\triangleright\,\begin{array}{|c||cc|}
 & H^{A\times\bbnum 1} & F^{A\times\bbnum 1}\\
\hline H^{A} & h\rightarrow\text{zip}_{H}(h\times\text{wu}_{H}) & \bbnum 0\\
F^{A} & \bbnum 0 & f\rightarrow\text{zip}_{F}(f\times(\text{wu}_{H}\triangleright\text{ex}_{H}\triangleright\text{pu}_{F}))
\end{array}
\end{align*}
\begin{align*}
 & =p\triangleright\,\,\begin{array}{|c||cc|}
 & H^{A\times\bbnum 1} & F^{A\times\bbnum 1}\\
\hline H^{A} & \text{iru}^{\uparrow H}\pi_{1}^{\downarrow H} & \bbnum 0\\
F^{A} & \bbnum 0 & \text{iru}^{\uparrow F}\pi_{1}^{\downarrow F}
\end{array}\,=p\triangleright\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P}\quad.
\end{align*}

The associativity law is an equation between values of type $H^{A\times B\times C}+F^{A\times B\times C}$:
\[
\text{zip}_{P}(p^{:H^{A}+F^{A}}\times\text{zip}_{P}(q^{:H^{B}+F^{B}}\times r^{:H^{C}+F^{C}}))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}=\text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}\quad.
\]
The operation $\text{zip}_{P}(p\times q)$ is defined in such a way
that it returns a value of type $H^{A\times B}+\bbnum 0$ only when
both $p$ and $q$ are in the left part of the disjunction:
\[
\text{zip}_{P}\big((a^{:H^{A}}+\bbnum 0^{:F^{A}})\times(b^{:H^{B}}+\bbnum 0^{:F^{B}})\big)=\text{zip}_{H}(a\times b)+\bbnum 0^{:F^{A\times B}}\quad.
\]
Otherwise, $\text{zip}_{P}(p\times q)$ returns a value of type $\bbnum 0^{:H^{A\times B}}+F^{A\times B}$.
So, let us consider three cases:

\textbf{(1)} The arguments are $p=a^{:H^{A}}+\bbnum 0$, $q=b^{:H^{B}}+\bbnum 0$,
$r=c^{:H^{C}}+\bbnum 0$. In this case, $\text{zip}_{P}$ reduces
to $\text{zip}_{H}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}=\text{zip}_{H}\big(a\times\text{zip}_{H}(b\times c)\big)\triangleright\varepsilon_{1,23}^{\uparrow H}\tilde{\varepsilon}_{1,23}^{\downarrow H}+\bbnum 0\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}=\text{zip}_{H}\big(\text{zip}_{H}(a\times b)\times c\big)\triangleright\varepsilon_{12,3}^{\uparrow H}\tilde{\varepsilon}_{12,3}^{\downarrow H}+\bbnum 0\quad.
\end{align*}
The two sides are equal due to the associativity law of $\text{zip}_{H}$.

\textbf{(2)} The argument $q$ has type $\bbnum 0+F^{B}$. In this
case, $\text{zip}_{P}$ reduces to $\text{zip}_{F}$ after converting
arguments of type $H^{\bullet}+0$ to type $F^{\bullet}$ when needed.
We may define this conversion as a helper function \lstinline!toF!
in the same way as in the proof of Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative}.
The associativity law of $\text{zip}_{P}$ is then reduced to the
same law of $\text{zip}_{F}$:
\begin{align*}
 & \text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}=\bbnum 0+\text{zip}_{F}\big(\text{toF}\left(p\right)\times\text{zip}_{F}(\text{toF}\left(q\right)\times\text{toF}\left(r\right))\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F}\quad,\\
 & \text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}=\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(\text{toF}\left(p\right)\times\text{toF}\left(q\right))\times\text{toF}\left(r\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\quad.
\end{align*}
The two sides are equal due to the associativity law of $\text{zip}_{F}$.

\textbf{(3)} Either $p=\bbnum 0+a^{:F^{A}}$ while $q=b^{:H^{B}}+\bbnum 0$
and $r=c^{:H^{C}}+\bbnum 0$; or $r=\bbnum 0+c^{:F^{C}}$ while $p=a^{:H^{A}}+\bbnum 0$
and $q=b^{:H^{B}}+\bbnum 0$. The two situations are symmetric, so
let us consider the first one:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}\\
 & =\bbnum 0+\text{zip}_{F}\big(\text{toF}\,(p)\times\text{toF}\,(\text{zip}_{H}(b\times c)+\bbnum 0)\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F}\quad.
\end{align*}
Simplify the sub-expressions involving \lstinline!toF! separately:
\begin{align*}
 & \text{toF}\,(p)=\text{toF}\,(\bbnum 0+a)=a\quad,\\
 & \text{toF}\,(\text{zip}_{H}(b\times c)+\bbnum 0)=\text{pu}_{F}(\gunderline{\text{ex}_{H}(\text{zip}_{H}}(b\times c))\\
{\color{greenunder}\text{use Eq.~(\ref{eq:compatibility-law-of-extract-and-zip})}:}\quad & \quad=\text{pu}_{F}(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\quad.
\end{align*}
For profunctors, the right identity law of \lstinline!zip! and \lstinline!pure!
has the form:
\[
\text{zip}_{F}(a^{:F^{A}}\times\text{pu}_{F}(b^{:B}))=a\triangleright(k^{:A}\rightarrow k\times b)^{\uparrow F}\pi_{1}^{\downarrow F}\quad.
\]
So, we can rewrite the left-hand side of the associativity law like
this:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}=\bbnum 0+\gunderline{\text{zip}_{F}}\big(a\times\gunderline{\text{pu}_{F}}(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\big)\triangleright\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F}\\
{\color{greenunder}\text{identity law of }\text{zip}_{F}:}\quad & =\bbnum 0+a\triangleright\big(k^{:A}\rightarrow k\times(\text{ex}_{H}(b)\times\text{ex}_{H}(c))\big)^{\uparrow F}\pi_{1}^{\downarrow F}\triangleright\varepsilon_{1,23}^{\uparrow F}\tilde{\varepsilon}_{1,23}^{\downarrow F}\\
 & =\bbnum 0+a\triangleright\big(k^{:A}\rightarrow k\times\text{ex}_{H}(b)\times\text{ex}_{H}(c)\big)^{\uparrow F}\pi_{1}^{\downarrow F}\quad.
\end{align*}
The right-hand side can be transformed by using \lstinline!toF! on
all arguments:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow P}\tilde{\varepsilon}_{12,3}^{\downarrow P}\\
 & =\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(\text{toF}\left(p\right)\times\text{toF}\left(q\right))\times\text{toF}\left(r\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\\
 & =\bbnum 0+\text{zip}_{F}(\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))\times\text{toF}\left(r+\bbnum 0\right))\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\quad.
\end{align*}
Simplify the sub-expressions of the form $\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))$:
\begin{equation}
\text{zip}_{F}\big(a^{:F^{A}}\times\text{toF}\,(b^{:H^{B}}+\bbnum 0)\big)=\text{zip}_{F}(a\times\text{pu}_{F}(\text{ex}_{H}(b))=a\triangleright(k^{:A}\rightarrow k\times\text{ex}_{H}(b))^{\uparrow F}\pi_{1}^{\downarrow F}\quad.\label{eq:zip-copointed-construction-derivation1-1}
\end{equation}
Using this formula, we continue to transform the right-hand side:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & \bbnum 0+\gunderline{\text{zip}_{F}}(\text{zip}_{F}(a\times\text{toF}\left(b+\bbnum 0\right))\times\gunderline{\text{toF}\left(c+\bbnum 0\right)})\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-copointed-construction-derivation1-1})}:}\quad & =\bbnum 0+\gunderline{\text{zip}_{F}}(a\times\gunderline{\text{toF}\left(b+\bbnum 0\right)})\triangleright\big(k\rightarrow k\times\text{ex}_{H}(c)\big)^{\uparrow F}\pi_{1}^{\downarrow F}\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\\
{\color{greenunder}\text{use Eq.~(\ref{eq:zip-copointed-construction-derivation1-1})}:}\quad & =\bbnum 0+a\triangleright\big(k\rightarrow k\times\text{ex}_{H}(b)\big)^{\uparrow F}\pi_{1}^{\downarrow F}\triangleright\big(k\rightarrow k\times\text{ex}_{H}(c)\big)^{\uparrow F}\pi_{1}^{\downarrow F}\triangleright\varepsilon_{12,3}^{\uparrow F}\tilde{\varepsilon}_{12,3}^{\downarrow F}\\
{\color{greenunder}\text{compute composition}:}\quad & =\bbnum 0+a\triangleright\big(k\rightarrow k\times\text{ex}_{H}(b)\times\text{ex}_{H}(c)\big)^{\uparrow F}\pi_{1}^{\downarrow F}\quad.
\end{align*}
The two sides are now equal. 

It remains to verify the commutativity law in case that law holds
for $F$ and $H$:
\[
\text{swap}\bef\text{zip}_{F}\overset{!}{=}\text{zip}_{F}\bef\text{swap}^{\uparrow F}\text{swap}^{\downarrow F}\quad,\quad\text{swap}\bef\text{zip}_{H}\overset{!}{=}\text{zip}_{H}\bef\text{swap}^{\uparrow H}\text{swap}^{\downarrow H}\quad\quad.
\]
Begin with the left-hand side of the commutativity law for $\text{zip}_{P}$:
\[
\text{swap}\bef\text{zip}_{P}=\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A}\times H^{B} & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & \text{swap}\bef((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\]
Writing out the compositions of \lstinline!swap! and the pair product
functions, we get:
\begin{align*}
 & \text{swap}\bef((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})=(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\quad,\\
 & \text{swap}\bef(\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))=(\text{pu}_{F}\boxtimes\text{id})\bef\text{swap}\quad.
\end{align*}
Using these simplifications, we rewrite the left-hand side as:
\[
\text{swap}\bef\text{zip}_{P}=\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A}\times H^{B} & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{swap}\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\]
The right-hand side is rewritten to the same code after using the
laws of $\text{zip}_{F}$ and $\text{zip}_{H}$:
\begin{align*}
 & \text{zip}_{P}\bef\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}\\
 & =\,\begin{array}{|c||cc|}
 & H^{A\times B} & F^{A\times B}\\
\hline H^{A}\times H^{B} & \text{zip}_{H} & \bbnum 0\\
F^{A}\times H^{B} & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{zip}_{F}\\
H^{A}\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{zip}_{F}
\end{array}\,\bef\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline H^{A\times B} & \text{swap}^{\uparrow H}\text{swap}^{\downarrow H} & \bbnum 0\\
F^{A\times B} & \bbnum 0 & \text{swap}^{\uparrow F}\text{swap}^{\downarrow F}
\end{array}\\
 & =\,\,\begin{array}{|c||cc|}
 & H^{B\times A} & F^{B\times A}\\
\hline A\times B & \text{swap}\bef\text{zip}_{H} & \bbnum 0\\
F^{A}\times B & \bbnum 0 & (\text{id}\boxtimes(\text{ex}_{H}\bef\text{pu}_{F}))\bef\text{swap}\bef\text{zip}_{F}\\
A\times F^{B} & \bbnum 0 & ((\text{ex}_{H}\bef\text{pu}_{F})\boxtimes\text{id})\bef\text{swap}\bef\text{zip}_{F}\\
F^{A}\times F^{B} & \bbnum 0 & \text{swap}\bef\text{zip}_{F}
\end{array}\quad.
\end{align*}
The two sides are now equal. $\square$

\subsubsection{Statement \label{subsec:Statement-applicative-profunctor-exponential}\ref{subsec:Statement-applicative-profunctor-exponential}}

If $G^{\bullet}$ is an applicative profunctor and $H^{\bullet}$
is \emph{any functor} then the profunctor $P^{A}\triangleq H^{A}\rightarrow G^{A}$
is applicative.

\subparagraph{Proof}

We follow the proof of Statement~\ref{subsec:Statement-applicative-contrafunctor-exponential}.
Implement the \lstinline!zip! and \lstinline!wu! methods for $P$:
\begin{align*}
 & \text{zip}_{P}:(H^{A}\rightarrow G^{A})\times(H^{B}\rightarrow G^{B})\rightarrow H^{A\times B}\rightarrow G^{A\times B}\quad,\\
 & \text{zip}_{P}\big(p^{:H^{A}\rightarrow G^{A}}\times q^{:H^{B}\rightarrow G^{B}}\big)\triangleq h^{:H^{A\times B}}\rightarrow\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times q(h\triangleright\pi_{2}^{\uparrow H})\big)\quad,\\
 & \text{wu}_{P}\triangleq\_^{:H^{\bbnum 1}}\rightarrow\text{wu}_{G}\quad.
\end{align*}
We will also use the definition of $\text{zip}_{P}$ in a point-free
form, which omits the argument $h$:
\[
\text{zip}_{P}(p\times q)\triangleq\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(p\boxtimes q)\bef\text{zip}_{G}\quad.
\]

The code for lifting to $P$ is standard:
\[
(p^{:H^{A}\rightarrow G^{A}})\triangleright f^{\downarrow P}g^{\uparrow P}=f^{\uparrow H}\bef p\bef(f^{\downarrow G}g^{\uparrow G})\quad.
\]

To verify the left identity law of $P$, we use the left identity
law of $G$:
\begin{align*}
 & h^{:H^{\bbnum 1\times A}}\triangleright\text{zip}_{P}(\text{wu}_{P}\times p^{:H^{A}\rightarrow G^{A}})=\text{zip}_{G}\big(\gunderline{\text{wu}_{P}(h\triangleright\pi_{1}^{\uparrow H})}\times p(h\triangleright\pi_{2}^{\uparrow H})\big)\\
{\color{greenunder}\text{definition of }\text{wu}_{P}:}\quad & =\text{zip}_{G}\big(\text{wu}_{G}\times p(h\triangleright\pi_{2}^{\uparrow H})\big)\\
{\color{greenunder}\text{left identity law of }G:}\quad & =p(h\triangleright\pi_{2}^{\uparrow H})\triangleright\text{ilu}^{\uparrow G}\pi_{2}^{\downarrow G}=h\triangleright\gunderline{\pi_{2}^{\uparrow H}\bef p\bef\text{ilu}^{\uparrow G}\pi_{2}^{\downarrow G}}=h\triangleright(p\bef\text{ilu}^{\uparrow P}\pi_{2}^{\downarrow P})\quad.
\end{align*}

To verify the right identity law of $P$:
\begin{align*}
 & h^{:H^{A\times\bbnum 1}}\triangleright\text{zip}_{P}(p^{:H^{A}\rightarrow G^{A}}\times\text{wu}_{P})=\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times\text{wu}_{P}(h\triangleright\pi_{2}^{\uparrow H})\big)=\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times\text{wu}_{G}\big)\\
 & =p(h\triangleright\pi_{1}^{\uparrow H})\triangleright\text{iru}^{\uparrow G}\pi_{1}^{\downarrow G}=h\triangleright\gunderline{\pi_{1}^{\uparrow H}\bef p\bef\text{iru}^{\uparrow G}\pi_{1}^{\downarrow G}}=h\triangleright(p\bef\text{iru}^{\uparrow P}\pi_{1}^{\downarrow P})\quad.
\end{align*}

To verify the associativity law, we use the definition of $^{\uparrow P}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & h^{:H^{A\times B\times C}}\triangleright\big(\text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\,\gunderline{\triangleright\,\varepsilon_{1,23}^{\uparrow P}\tilde{\varepsilon}_{1,23}^{\downarrow P}}\big)\\
 & \quad=h^{:H^{A\times B\times C}}\triangleright\tilde{\varepsilon}_{1,23}^{\uparrow H}\triangleright\text{zip}_{P}(p\times\text{zip}_{P}(q\times r))\triangleright\varepsilon_{1,23}^{\uparrow G}\tilde{\varepsilon}_{1,23}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(p(h\triangleright\gunderline{\tilde{\varepsilon}_{1,23}^{\uparrow H}\bef\pi_{1}^{\uparrow H}})\times\text{zip}_{P}(q\times r)(h\triangleright\tilde{\varepsilon}_{1,23}^{\uparrow H}\bef\pi_{2}^{\uparrow H})\big)\triangleright\varepsilon_{1,23}^{\uparrow G}\tilde{\varepsilon}_{1,23}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times\text{zip}_{G}\big(q(h\triangleright\pi_{2}^{\uparrow H})\times r(h\triangleright\pi_{3}^{\uparrow H})\big)\big)\triangleright\varepsilon_{1,23}^{\uparrow G}\tilde{\varepsilon}_{1,23}^{\downarrow G}\quad,\\
{\color{greenunder}\text{right-hand side}:}\quad & h^{:H^{A\times B\times C}}\triangleright\tilde{\varepsilon}_{12,3}^{\uparrow H}\triangleright\text{zip}_{P}(\text{zip}_{P}(p\times q)\times r)\triangleright\varepsilon_{12,3}^{\uparrow G}\tilde{\varepsilon}_{12,3}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(\text{zip}_{P}(p\times q)(h\triangleright\tilde{\varepsilon}_{12,3}^{\uparrow H}\bef\pi_{1}^{\uparrow H})\times r(h\triangleright\tilde{\varepsilon}_{12,3}^{\uparrow H}\bef\pi_{2}^{\uparrow H})\big)\triangleright\varepsilon_{12,3}^{\uparrow G}\tilde{\varepsilon}_{12,3}^{\downarrow G}\\
 & \quad=\text{zip}_{G}\big(\text{zip}_{G}\big(p(h\triangleright\pi_{1}^{\uparrow H})\times q(h\triangleright\pi_{2}^{\uparrow H})\big)\times r(h\triangleright\pi_{3}^{\uparrow H})\big)\triangleright\varepsilon_{12,3}^{\uparrow G}\tilde{\varepsilon}_{12,3}^{\downarrow G}\quad.
\end{align*}
The two sides are now equal due to the associativity law of $\text{zip}_{G}$.

It remains to verify the commutativity law, assuming that $\text{zip}_{G}$
satisfies that law:
\begin{align*}
 & \text{zip}_{P}(q\times p)=\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(q\boxtimes p)\bef\text{zip}_{G}\quad,\\
 & \text{zip}_{P}(p\times q)\triangleright\text{swap}^{\uparrow P}\text{swap}^{\downarrow P}=\text{swap}^{\uparrow H}\bef\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(p\boxtimes q)\bef\gunderline{\text{zip}_{G}\bef\text{swap}^{\uparrow G}\text{swap}^{\downarrow G}}\\
 & \quad=\Delta\bef\gunderline{(\text{swap}^{\uparrow H}\boxtimes\text{swap}^{\uparrow H})\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})}\bef\gunderline{(p\boxtimes q)\bef\text{swap}}\bef\text{zip}_{G}\\
 & \quad=\gunderline{\Delta\bef(\pi_{2}^{\uparrow H}\boxtimes\pi_{1}^{\uparrow H})\bef\text{swap}}\bef(q\boxtimes p)\bef\text{zip}_{G}=\Delta\bef(\pi_{1}^{\uparrow H}\boxtimes\pi_{2}^{\uparrow H})\bef(q\boxtimes p)\bef\text{zip}_{G}\quad.
\end{align*}
$\square$

Note that the functor $H$ must be \emph{covariant} in the last construction
($P^{A}\triangleq H^{A}\rightarrow G^{A}$). Otherwise, the profunctor
$P$ will not be applicative even the simple cases such as $P^{A}\triangleq H^{A}\rightarrow A$
where $H$ is an arbitrary profunctor. (We omit the proof of that
statement.)

\section{Discussion and further developments}

\subsection{Equivalence of typeclass methods with laws}

In this and the previous chapters, we have seen that certain typeclass
methods are equivalent. The equivalence of \lstinline!map2!, \lstinline!zip!,
and \lstinline!ap! was proved in Section~\ref{subsec:Equivalence-of-map2-zip-ap}.
For filterable functors, we have shown that the methods \lstinline!filter!,
\lstinline!deflate!, and \lstinline!liftOpt! are equivalent if some
naturality laws hold (Sections~\ref{subsec:Equivalence-of-filter-and-deflate}
and~\ref{subsec:Motivation-and-laws-for-liftopt-and-equivalence}).
For monads, we proved that \lstinline!flatMap! is equivalent to \lstinline!flatten!
(Statement~\ref{subsec:Statement-flatten-equivalent-to-flatMap}).
Another case of equivalence between certain function types was proved
in Section~\ref{subsec:Yoneda-identities}. 

Perhaps the simplest example of this sort of equivalence is that between
the \lstinline!pure! method and the \textsf{``}wrapped unit\textsf{''} (\lstinline!wu!),
as we saw in Section~\ref{subsec:Pointed-functors-motivation-equivalence}.
A function $\text{pu}_{F}:A\rightarrow F^{A}$ satisfying a naturality
law is equivalent to a value $\text{wu}_{F}:F^{\bbnum 1}$. 

After seeing those detailed proofs, we can now clarify the meaning
of \textsf{``}equivalence under laws\textsf{''}. The goal of this subsection is to
find a rigorous formulation of that equivalence.

In each case seen so far, we have two functions with two different
type signatures (usually with type parameters), and we assume that
certain naturality laws hold. It is important to keep in mind that
the naturality laws are equations whose form \emph{automatically}
follows from type signatures. If a function has more than one type
parameter, there is one naturality law per type parameter (the full
details are in Section~\ref{subsec:Naturality-laws-and-natural-transformations}
and Appendix~\ref{app:Proofs-of-naturality-parametricity}). For
example, the type signature \lstinline!pure[A]: A => F[A]! (in the
type notation, $\text{pu}_{F}:\forall A.\,A\rightarrow F^{A}$) gives
rise to the naturality law $f\bef\text{pu}_{F}=\text{pu}_{F}\bef f^{\uparrow F}$.
In Scala code, this law is written as \lstinline!pure(f(x)) == pure(x).map(f)!.

What we have proved in Section~\ref{subsec:Pointed-functors-motivation-equivalence}
is a one-to-one correspondence between two \emph{sets}: the set of
all values \lstinline!wu! of type \lstinline!F[Unit]!, and the set
of all functions with type \lstinline!pure[A]: A => F[A]! satisfying
the naturality law \lstinline!pure(f(x)) == pure(x).map(f)!. The
proof defines explicit mappings between \lstinline!wu! and \lstinline!pure!
in both directions and verifies that their compositions are identity
maps. We also made sure to exclude all functions of type \lstinline!A => F[A]!
that fail to satisfy the naturality law. For instance, we proved that
if \lstinline!pure! is defined via \lstinline!wu! then the naturality
law will hold for \lstinline!pure!.

So, if \lstinline!wu: F[Unit]! is defined in some way for a functor
\lstinline!F!, we will always have a corresponding definition of
\lstinline!pure! that satisfies the naturality law. Conversely, for
any given lawful definition of \lstinline!pure! we will have a corresponding
value \lstinline!wu!.

The same pattern was used in the proofs of equivalence in other chapters.
Let us formulate this pattern in a more abstract way. We would like
to prove that some function $p$ (of type $P^{A,B,...}$) is equivalent
to some $q$ (of type $Q^{A,B,...}$) when suitable naturality laws
hold. These functions may have any number of type parameters ($A$,
$B$, etc.). More explicitly, the types of $p$ and $q$ are:
\[
p:\forall(A,B,...).\,P^{A,B,...}\quad,\quad\quad q:\forall(A,B,...).\,Q^{A,B,...}\quad.
\]
We view the type $\forall(A,B,...).\,P^{A,B,...}$ as a set of all
functions $p$ (implementable in Scala) with this code:
\begin{lstlisting}
def p[A, B, ...]: P[A, B, ...] = { ... }
\end{lstlisting}
and similarly for $q$. To establish the equivalence, we first write
some code that expresses $p$ through $q$ and back. This requires
two functions, $f:P^{A,B,...}\rightarrow Q^{A,B,...}$ and $g:Q^{A,B,...}\rightarrow P^{A,B,...}$.
These functions are mappings (in both directions) between the set
of all functions $p$ and the set of all functions $q$ of the corresponding
types. Then we prove that the mappings are one-to-one: for any $p$
and $q$ we have $g(f(p))=p$ and $f(g(q))=q$. Finally, we check
that the required naturality laws still hold after applying the mappings
$f$ and $g$. In other words, for any $p$ satisfying its naturality
law, we prove that the function $f(p)$ of type $Q^{A,B,...}$ will
satisfy \emph{its} naturality law; and similarly for $q$ and $g(q)$.

Without the naturality laws, the types $\forall(A,B,...).\,P^{A,B,...}$
and $\forall(A,B,...).\,Q^{A,B,...}$ are usually \emph{not} equivalent:
there is no one-to-one correspondence between functions $p$ and functions
$q$ of those types. Imposing a naturality law means that we exclude
functions $p$ of type $\forall(A,B,...).\,P^{A,B,...}$ that do not
satisfy the naturality law. This gives us a smaller set of all lawful
functions $p$ (and similarly for $q$). The equivalence holds only
for those smaller sets of lawful functions.

To see this in a simple example, consider the equivalence of $p$=\lstinline!pure!
and $q$=\lstinline!wu! for the identity functor ($L^{A}\triangleq A$).
The method \lstinline!pure! has type $\forall A.\,A\rightarrow A$,
while \lstinline!wu! has the \lstinline!Unit! type ($L^{\bbnum 1}=\bbnum 1$).
There is only one value \lstinline!wu!, but there are many possible
functions $p$ of type $\forall A.\,A\rightarrow A$. For instance,
we could define a function $p$ like this:
\begin{lstlisting}
def p[A]: A => A = {
  case a: Int   => (a + 123)
            .asInstanceOf[A]
  case a        => a
}
\end{lstlisting}
This function is defined for all types $A$, so it fits the type $\forall A.\,A\rightarrow A$.
The set of all functions of type $\forall A.\,A\rightarrow A$ contains
a large number of similarly defined functions. For example, we may
replace \lstinline!123! by another number or select another type
instead of \lstinline!Int!. However, all those functions fail to
satisfy the naturality law, which has the form $f^{:A\rightarrow B}\bef p^{B}=p^{A}\bef f$.
The only function $p$ that satisfies this naturality law is the identity
function, $p^{A}=\text{id}^{A}\triangleq a^{:A}\rightarrow a$ (see
Exercise~\ref{subsec:Exercise-hof-composition-1}). We find that
imposing the naturality law removes all functions $p$ except one
($p=\text{id}$) from the set of functions $p:\text{\ensuremath{\forall A.\,A\rightarrow A}}$.
There remains a set containing a single element ($\text{id}$), which
is in a one-to-one correspondence with the set of values of type $\bbnum 1$.

So, the rigorous meaning of an \textsf{``}equivalence between functions $p$
and $q$ assuming some laws\textsf{''} is a one-to-one correspondence between
the set of all functions $p:\forall(A,B,...).\,P^{A,B,...}$ that
obey the given laws of $p$, and the set of all functions $q:\forall(A,B,...).\,Q^{A,B,...}$
that obey the given laws of $q$.

Note that the Scala compiler is unable to check automatically that
the naturality laws hold for a function $p$ with type parameters.
Indeed, those laws are equations for $p$ that involve arbitrary functions
$f^{:A\rightarrow B}$ with arbitrary types $A$ and $B$. Such equations
cannot be enforced by the type signature of $p$. Neither can we prove
naturality laws by running tests: each test will have to use specific
types as the type parameters, while the purpose of naturality laws
is to verify that the function works in the same way for \emph{all}
types. Naturality laws can be verified only through a proof that uses
symbolic reasoning. (The proof can be omitted if the code is fully
parametric, as shown in Appendix~\ref{app:Proofs-of-naturality-parametricity}.)

The Scala compiler also cannot verify the laws specific to a given
typeclass (e.g., the identity and composition laws of filterable functors,
or the identity and associativity laws of monads and applicative functors).
These laws must be also verified by symbolic reasoning. It is often
easier to verify laws if the type signature of the method has fewer
type parameters. For instance, the \lstinline!zip! method has two
type parameters while \lstinline!map2! has three; \lstinline!flatten!
has one type parameter while \lstinline!flatMap! has two. For this
reason, we have systematically derived the laws of all the equivalent
typeclass methods. In many cases, we found a formulation of the laws
that was either conceptually simpler or more straightforward to verify. 

\subsection{Relationship between monads and applicative functors}

Any lawful monad gives its type constructor at least one applicative
functor definition: as we showed in Section~\ref{subsec:Commutative-applicative-functors},
we may define the \lstinline!map2! method via \lstinline!map! and
\lstinline!flatMap! in two ways (which will give the same code if
the monad is commutative). The \lstinline!map2! methods defined in
this way will have the right type signature and will satisfy the laws
of applicative functors. This is due to the fact that the laws of
\lstinline!map2! are derived (as shown in Section~\ref{subsec:Motivation-for-the-laws-of-map2})
from the monad laws precisely by considering the \lstinline!map2!
method defined via the monad\textsf{'}s \lstinline!flatMap!.

However, in many cases we need to define the \lstinline!map2! method
in a different way because expressing \lstinline!map2! via \lstinline!flatMap!
does not give us the required functionality. We have seen several
examples of this. For instance, the standard behavior of \lstinline!map2!
for sequences, trees, and \lstinline!Either! is not compatible with
the standard \lstinline!flatMap! methods of those functors.

So, it is rarely useful to define an \lstinline!Applicative! typeclass
instance automatically for all monads. In most cases, we need to define
the applicative instance separately from the monad instance. (Automatic
derivation of \lstinline!Applicative! instances is made difficult
also by the fact that many type constructors will admit more than
one lawful implementation of \lstinline!map2!.)

In addition, some applicative functors are not monads. We have shown
in Section~\ref{subsec:Constructions-of-applicative-functors} that
a lawful \lstinline!Applicative! instance exists for all polynomial
functors with monoidal fixed types. (Accordingly, all our examples
of non-applicative functors involve non-polynomial functors.) This
does not hold for monads; not all polynomial functors are monadic.\footnote{It is unknown how to characterize or enumerate all polynomial functors
that are monads (see Problems~\ref{par:Problem-monads}\textendash \ref{par:Problem-monads-1}).} Example~\ref{subsec:Example-applicative-not-monad} shows a simple
applicative functor ($L^{A}\triangleq\bbnum 1+A\times A$) that \emph{cannot}
have a lawful monad implementation (Exercise~\ref{subsec:Exercise-1-monads-7-not-a-monad}).

We have also seen that the \lstinline!zip! and \lstinline!wu! operations
exist for some type constructors that are not covariant. We conclude
that the \lstinline!Applicative! typeclass is larger than the \lstinline!Monad!
typeclass.

\subsection{Applicative morphisms}

One of the applicative constructions (Statement~\ref{subsec:Statement-co-product-with-co-pointed-applicative})
needs a compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
between the methods $\text{zip}_{H}$ and $\text{ex}_{H}$. This law
is understood better if we rewrite the type signature $\text{ex}_{H}^{A}:H^{A}\rightarrow A$
as $\text{ex}_{H}^{A}:H^{A}\rightarrow\text{Id}^{A}$, where $\text{Id}^{A}\triangleq A$
is the identity functor (which is also applicative). Viewed in this
way, the \lstinline!extract! method is an example of a mapping between
two applicative functors ($H$ and $\text{Id}$). So, the compatibility
law requires that the operation $\text{zip}_{H}$ be mapped to the
tupling operation, which is the same as the \lstinline!zip! operation
of the identity functor: $\text{zip}_{\text{Id}}(a\times b)=a\times b$.

This suggests considering a more general mapping $\phi^{A}:H^{A}\rightarrow K^{A}$
where $H$ and $K$ are two applicative functors, and formulating
a \textbf{composition law}\index{composition law!of applicative morphisms}
of $\phi$ by analogy with Eq.~(\ref{eq:compatibility-law-of-extract-and-zip}):
\[
\phi\big(\text{zip}_{H}(p\times q)\big)=\text{zip}_{K}(\phi(p)\times\phi(q))\quad,\quad\text{or equivalently}:\quad\text{zip}_{H}\bef\phi=(\phi\boxtimes\phi)\bef\text{zip}_{K}\quad.
\]
This law reproduces Eq.~(\ref{eq:compatibility-law-of-extract-and-zip})
when $\phi=\text{ex}_{H}$ and $K=\text{Id}$ because we will then
have $\text{zip}_{K}=\text{id}$. 

It also seems reasonable to require that $\phi$ should map the method
$\text{pu}_{H}$ to $\text{pu}_{K}$. This gives the \textbf{identity
law}: $\text{pu}_{H}\bef\phi=\text{pu}_{K}$.\index{identity laws!of applicative morphisms}
Functions $\phi$ satisfying these two laws are called \textbf{applicative
morphisms}\index{applicative morphism} between two applicative functors
$H$ and $K$. The two laws make applicative morphisms fully analogous
to monad morphisms (Section~\ref{subsec:Monads-in-category-theory-monad-morphisms}).

\subsection{Deriving the laws of \texttt{ap} using category theory}

Statement~\ref{subsec:Statement-fmap2-equivalence-to-ap} shows that
the additional capabilities of \lstinline!map2! compared with \lstinline!map!
can be expressed via the function \lstinline!ap!:
\[
\text{map}_{2}\,(p^{:L^{A}}\times q^{:L^{B}})(f^{:A\times B\rightarrow C})=\text{ap}\,(p\triangleright(a^{:A}\rightarrow b^{:B}\rightarrow f(a\times b))^{\uparrow L})(q)\quad.
\]
How can we derive the laws of \lstinline!ap! that correspond to the
laws of \lstinline!map2!? If we substitute the equation above into
the laws of \lstinline!map2!, we will obtain a relationship involving
arbitrary values $p^{:L^{A}}$, $q^{:L^{B}}$, and $f^{:A\times B\rightarrow C}$.
However, the type signature of \lstinline!ap! is $L^{A\rightarrow B}\rightarrow L^{A}\rightarrow L^{B}$,
and so we expect the laws of \lstinline!ap! to have arbitrary arguments
of types $L^{A\rightarrow B}$ and $L^{B}$. It is not clear how to
choose the types in the laws of \lstinline!map2! in order to derive
a complete set of laws for \lstinline!ap!.

To resolve this issue, we turn to category theory for guidance. The
type signature of \lstinline!ap! looks like a \textsf{``}lifting\textsf{''} from
values of type $L^{A\rightarrow B}$ to functions of type $L^{A}\rightarrow L^{B}$.
Category theory suggests to describe this lifting as part of a suitable
categorical functor.\index{functor!in category theory}\index{category theory!functor}
A categorical functor can exist only between two categories. So, we
need to show that values of type $L^{A\rightarrow B}$ (\textsf{``}wrapped
functions\textsf{''}) can play the role of morphisms in a suitably defined
category. To define that category, we need to produce objects, morphisms,
the identity morphism, and the composition operation, and prove their
laws. Finally, we will need to prove that \lstinline!ap! satisfies
the identity and composition laws appropriate for a (categorical)
functor.

This section will follow these considerations in order to derive and
verify the laws of \lstinline!ap!.

The first step is to define two categories between which we will establish
a categorical functor. The objects of both categories are ordinary
types ($A$, $B$, etc.). We would like the morphisms of the first
category to be values of type $L^{A\rightarrow B}$, and the morphisms
of the second category to be functions of type $L^{A}\rightarrow L^{B}$.
The second category was called \textsf{``}$L$-lifted\textsf{''} in Section~\ref{subsec:Motivation-for-using-category-theory}.
Functions of type $L^{A}\rightarrow L^{B}$ already satisfy the required
properties of morphisms: the identity morphism is the function $\text{id}^{:L^{A}\rightarrow L^{A}}$,
and the ordinary function composition is associative and respects
the identity. To establish that values of type $L^{A\rightarrow B}$
also satisfy the properties of morphisms, we need to define a composition
operation for those values and prove its properties.

It turns out that there are two ways of defining the composition operation,
as shown in Statement~\ref{subsec:Statement-ap-category-laws} (a)
and (b) below. The difference between the two definitions disappears
if $L$ is a \emph{commutative} applicative functor. We will use the
definition (b); the choice is dictated by compatibility with the \lstinline!ap!
operation, as we will demonstrate in Statement~\ref{subsec:Statement-ap-functor-laws}.

\subsubsection{Statement \label{subsec:Statement-ap-category-laws}\ref{subsec:Statement-ap-category-laws}}

Given a functor $L$ with lawful \lstinline!zip! and \lstinline!pure!
methods, we define an \index{applicative composition (odot)@applicative composition ($\odot$)}\textbf{$L$-applicative
composition} operation (denoted by $\odot$). For any $p:L^{A\rightarrow B}$
and $q:L^{B\rightarrow C}$, the value $p\odot q$ will be of type
$L^{A\rightarrow C}$. The operation $\odot$ may be defined in two
different ways:
\begin{align*}
{\color{greenunder}\text{definition (a) of }\odot:}\quad & p^{:L^{A\rightarrow B}}\odot q^{:L^{B\rightarrow C}}\triangleq(p\times q)\triangleright\text{zip}\triangleright(f^{:A\rightarrow B}\times g^{:B\rightarrow C}\rightarrow f\bef g)^{\uparrow L}\quad,\\
{\color{greenunder}\text{definition (b) of }\odot:}\quad & p^{:L^{A\rightarrow B}}\odot q^{:L^{B\rightarrow C}}\triangleq(q\times p)\triangleright\text{zip}\triangleright(g^{:B\rightarrow C}\times f^{:A\rightarrow B}\rightarrow f\bef g)^{\uparrow L}\quad.
\end{align*}
Moreover, the special \textsf{``}wrapped identity\textsf{''} value, \lstinline!wid!,
of type $L^{A\rightarrow A}$, is defined by $\text{wid}_{L}^{A}\triangleq\text{pu}_{L}(\text{id}^{A})$.
The following properties then hold for each of the two definitions:\index{identity laws!of applicative composition}\index{associativity law!of applicative composition}
\begin{align*}
{\color{greenunder}\text{left identity law of }\odot:}\quad & \text{wid}_{L}^{A}\odot p^{:L^{A\rightarrow B}}=p\quad,\\
{\color{greenunder}\text{right identity law of }\odot:}\quad & p^{:L^{A\rightarrow B}}\odot\text{wid}_{L}^{B}=p\quad,\\
{\color{greenunder}\text{associativity law of }\odot:}\quad & (p^{:L^{A\rightarrow B}}\odot q^{:L^{B\rightarrow C}})\odot r^{:L^{C\rightarrow D}}=p\odot(q\odot r)\quad.
\end{align*}


\subparagraph{Proof}

We will prove the laws separately for the definitions (a) and (b).

\textbf{(a)} To verify the left identity law, substitute the definition
(a) of $\odot$:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{wid}_{L}\odot p=\text{pu}_{L}(\text{id})\odot p=(\text{pu}_{L}(\text{id})\times p)\triangleright\text{zip}\bef(f\times g\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{left identity law of }\text{zip}:}\quad & =p\triangleright(s^{:A\rightarrow B}\rightarrow\text{id}\times s)^{\uparrow L}\bef(f\times g\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{function composition under }^{\uparrow L}:}\quad & =p\triangleright(\gunderline{s\rightarrow\text{id}\bef s})^{\uparrow L}=p\triangleright\text{id}^{\uparrow L}=p\quad.
\end{align*}

To verify the right identity law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & p\odot\text{wid}_{L}=p\odot\text{pu}_{L}(\text{id})=(p\times\text{pu}_{L}(\text{id}))\triangleright\text{zip}\bef(f\times g\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{right identity law of }\text{zip}:}\quad & =p\triangleright(s^{:A\rightarrow B}\rightarrow s\times\text{id})^{\uparrow L}\bef(f\times g\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{function composition under }^{\uparrow L}:}\quad & =p\triangleright(\gunderline{s\rightarrow s\bef\text{id}})^{\uparrow L}=p\triangleright\text{id}^{\uparrow L}=p\quad.
\end{align*}

To verify the composition law, begin rewriting its left-hand side,
trying to bring the \lstinline!zip! functions to the left of the
expression:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (p\odot q)\odot r=\big((p\times q)\triangleright\text{zip}\bef(f\times g\rightarrow f\bef g)^{\uparrow L}\big)\odot r\\
 & =\big(\big(\text{zip}\left(p\times q\right)\triangleright\gunderline{(f\times g\rightarrow f\bef g)^{\uparrow L}}\big)\times r\big)\triangleright\gunderline{\text{zip}}\bef(k\times h\rightarrow k\bef h)^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =(\text{zip}\left(p\times q\right)\times r)\triangleright\text{zip}\bef\gunderline{((f\times g)\times h\rightarrow(f\bef g)\times h)^{\uparrow L}\bef(k\times h\rightarrow k\bef h)^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\left(\text{zip}\left(p\times q\right)\times r\right)\triangleright((f\times g)\times h\rightarrow f\bef g\bef h)^{\uparrow L}\quad.
\end{align*}
The right-hand side is rewritten similarly:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & p\odot(q\odot r)=p\odot\big(\text{zip}\left(q\times r\right)\triangleright(f\times g\rightarrow f\bef g)^{\uparrow L}\big)\\
 & =\big(p\times\big(\text{zip}\left(q\times r\right)\triangleright\gunderline{(g\times h\rightarrow g\bef h)^{\uparrow L}}\big)\big)\triangleright\gunderline{\text{zip}}\bef(f\times k\rightarrow f\bef k)^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =(p\times\text{zip}\left(q\times r\right))\triangleright\text{zip}\bef\gunderline{(f\times(g\times h)\rightarrow f\times(g\bef h))^{\uparrow L}\bef(f\times k\rightarrow f\bef k)^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\left(p\times\text{zip}\left(q\times r\right)\right)\triangleright(f\times(g\times h)\rightarrow f\bef g\bef h)^{\uparrow L}\quad.
\end{align*}
It is clear that we need to use the associativity law of \lstinline!zip!.
To be able to use that law, we express the functions $(f\times g)\times h\rightarrow f\bef g\bef h$
and $f\times(g\times h)\rightarrow f\bef g\bef h$ through the conversion
functions $\varepsilon_{1,23}$ and $\varepsilon_{12,3}$ (defined
in Section~\ref{subsec:Deriving-the-laws-of-zip}):
\begin{align*}
 & \big((f\times g)\times h\rightarrow f\bef g\bef h\big)=\varepsilon_{12,3}\bef(f\times g\times h\rightarrow f\bef g\bef h)\quad,\\
 & \big(f\times(g\times h)\rightarrow f\bef g\bef h\big)=\varepsilon_{1,23}\bef(f\times g\times h\rightarrow f\bef g\bef h)\quad.
\end{align*}
Using these equations, we show that the two sides of the associativity
law of $\odot$ are equal:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \gunderline{\text{zip}\left(\text{zip}\left(p\times q\right)\times r\right)\triangleright\varepsilon_{12,3}}\bef(f\times g\times h\rightarrow f\bef g\bef h)\\
{\color{greenunder}\text{right-hand side}:}\quad & =\gunderline{\text{zip}\left(p\times\text{zip}\left(q\times r\right)\right)\triangleright\varepsilon_{1,23}}\bef(f\times g\times h\rightarrow f\bef g\bef h)\quad.
\end{align*}
The underlined expressions are equal due to the associativity law~(\ref{eq:zip-associativity-law-with-epsilons})
of \lstinline!zip!.

\textbf{(b)} To verify the left identity law, substitute the definition
(b) of $\odot$:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{wid}_{L}\odot p=\text{pu}_{L}(\text{id})\odot p=(p\times\text{pu}_{L}(\text{id}))\triangleright\text{zip}\bef(g\times f\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{right identity law of }\text{zip}:}\quad & =p\triangleright(s^{:A\rightarrow B}\rightarrow s\times\text{id})^{\uparrow L}\bef(g\times f\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{function composition under }^{\uparrow L}:}\quad & =p\triangleright(\gunderline{s\rightarrow\text{id}\bef s})^{\uparrow L}=p\triangleright\text{id}^{\uparrow L}=p\quad.
\end{align*}

To verify the right identity law:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & p\odot\text{wid}_{L}=p\odot\text{pu}_{L}(\text{id})=(\text{pu}_{L}(\text{id})\times p)\triangleright\text{zip}\bef(g\times f\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{left identity law of }\text{zip}:}\quad & =p\triangleright(s^{:A\rightarrow B}\rightarrow\text{id}\times s)^{\uparrow L}\bef(g\times f\rightarrow f\bef g)^{\uparrow L}\\
{\color{greenunder}\text{function composition under }^{\uparrow L}:}\quad & =p\triangleright(\gunderline{s\rightarrow s\bef\text{id}})^{\uparrow L}=p\triangleright\text{id}^{\uparrow L}=p\quad.
\end{align*}

To verify the composition law, begin rewriting its left-hand side:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & (p\odot q)\odot r=\big((q\times p)\triangleright\text{zip}\bef(g\times f\rightarrow f\bef g)^{\uparrow L}\big)\odot r\\
 & =\big(r\times\big(\text{zip}\left(q\times p\right)\triangleright\gunderline{(g\times f\rightarrow f\bef g)^{\uparrow L}}\big)\big)\triangleright\gunderline{\text{zip}}\bef(h\times k\rightarrow k\bef h)^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =(r\times\text{zip}\left(q\times p\right))\triangleright\text{zip}\bef\gunderline{(h\times(g\times f)\rightarrow h\times(f\bef g))^{\uparrow L}\bef(h\times k\rightarrow k\bef h)^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\left(r\times\text{zip}\left(q\times p\right)\right)\triangleright(h\times(g\times f)\rightarrow f\bef g\bef h)^{\uparrow L}\quad.
\end{align*}
The right-hand side is rewritten similarly:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & p\odot(q\odot r)=p\odot\big(\text{zip}\left(q\times r\right)\triangleright(g\times f\rightarrow f\bef g)^{\uparrow L}\big)\\
 & =\big(\big(\text{zip}\left(q\times r\right)\triangleright\gunderline{(h\times g\rightarrow g\bef h)^{\uparrow L}}\big)\times p\big)\triangleright\gunderline{\text{zip}}\bef(k\times f\rightarrow f\bef k)^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =(\text{zip}\left(q\times r\right)\times p)\triangleright\text{zip}\bef\gunderline{((h\times g)\times f\rightarrow(g\bef h)\times f)^{\uparrow L}\bef(k\times f\rightarrow f\bef k)^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\left(\text{zip}\left(q\times r\right)\times p\right)\triangleright((h\times g)\times f\rightarrow f\bef g\bef h)^{\uparrow L}\quad.
\end{align*}
To apply the associativity law of \lstinline!zip!, we use the conversion
functions $\varepsilon_{1,23}$ and $\varepsilon_{12,3}$ and write:
\begin{align*}
 & \big(h\times(g\times f)\rightarrow f\bef g\bef h\big)=\varepsilon_{1,23}\bef(h\times g\times f\rightarrow f\bef g\bef h)\quad,\\
 & \big((h\times g)\times f\rightarrow f\bef g\bef h\big)=\varepsilon_{12,3}\bef(h\times g\times f\rightarrow f\bef g\bef h)\quad.
\end{align*}
Using these equations, we show that the two sides of the associativity
law of $\odot$ are equal:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \gunderline{\text{zip}\left(r\times\text{zip}\left(q\times p\right)\right)\triangleright\varepsilon_{1,23}}\bef(h\times g\times f\rightarrow f\bef g\bef h)^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & =\gunderline{\text{zip}\left(\text{zip}\left(q\times r\right)\times p\right)\triangleright\varepsilon_{12,3}}\bef(h\times g\times f\rightarrow f\bef g\bef h)\quad.
\end{align*}
The underlined expressions are equal due to the associativity law
of \lstinline!zip!. $\square$

The laws of the composition operation ($\odot$) show that values
of type $L^{A\rightarrow B}$ are indeed morphisms. Let us call \textsf{``}$L$-applicative\textsf{''}
the category with those morphisms. We may now write the laws of a
(categorical) functor between the $L$-applicative and the $L$-lifted
categories. Such a functor consists of two mappings: a mapping between
objects: each type $A$ is mapped to the type $L^{A}$; and a mapping
between morphisms: each $L$-applicative morphism $f:L^{A\rightarrow B}$
is mapped to the morphism $\text{ap}\left(f\right):L^{A}\rightarrow L^{B}$
in the $L$-lifted category. In other words, we expect the function
\lstinline!ap! to play the role of the morphism mapping of that functor.
This mapping must obey the laws of identity and composition.

The identity law says that the function \lstinline!ap! must map the
identity morphism of the $L$-applicative category into the identity
morphism of the $L$-lifted category:\index{identity laws!of ap@of \texttt{ap}}
\begin{align}
{\color{greenunder}\text{left identity law of }\text{ap}:}\quad & \text{ap}\,(\text{pu}_{L}(\text{id}^{:A\rightarrow A}))=\text{id}^{:L^{A}\rightarrow L^{A}}\quad.\label{eq:identity-law-of-ap}
\end{align}

The composition law says that the composition $p\odot q$ of any two
$L$-applicative morphisms $p^{:L^{A\rightarrow B}}$ and $q^{:L^{B\rightarrow C}}$
must be mapped by \lstinline!ap! into the composition $\text{ap}\left(p\right)\bef\text{ap}\left(q\right)$:
\begin{align}
{\color{greenunder}\text{composition law of }\text{ap}:}\quad & \text{ap}\big(p^{:L^{A\rightarrow B}}\odot q^{:L^{B\rightarrow C}}\big)=\text{ap}\left(p\right)\bef\text{ap}\left(q\right)\quad.\label{eq:composition-law-of-ap}
\end{align}
In this law, the operation $\odot$ needs to be defined as in Statement~\ref{subsec:Statement-ap-category-laws}(b).
Expressing that operation via \lstinline!ap!, we obtain the following
formulation of the composition law of\index{composition law!of ap@of \texttt{ap}}
\lstinline!ap!:
\[
\text{ap}\big(\text{ap}\big(q^{:L^{B\rightarrow C}}\triangleright(g^{:B\rightarrow C}\rightarrow f^{:A\rightarrow B}\rightarrow f\bef g)^{\uparrow L}\big)(p^{:L^{A\rightarrow B}})\big)=\text{ap}\left(p\right)\bef\text{ap}\left(q\right)\quad.
\]
Instead of proving this complicated law, we will prove the functor
laws of \lstinline!ap!:

\subsubsection{Statement \label{subsec:Statement-ap-functor-laws}\ref{subsec:Statement-ap-functor-laws}}

Given a functor $L$ with lawful \lstinline!zip! and \lstinline!pure!
methods, we define the \lstinline!ap! method as in Statement~\ref{subsec:Statement-zip-ap-equivalence}:
\[
\text{ap}\,(r^{:L^{A\rightarrow B}})(p^{:L^{A}})\triangleq\text{zip}\,(r\times p)\triangleright\text{eval}^{\uparrow L}=(r\times p)\triangleright\text{zip}\bef(f^{:A\rightarrow B}\times a^{:A}\rightarrow f(a))^{\uparrow L}\quad.
\]
The operation $\odot$ is defined by Statement~\ref{subsec:Statement-ap-category-laws}(b).
Then the \lstinline!ap! method satisfies Eqs.~(\ref{eq:identity-law-of-ap})\textendash (\ref{eq:composition-law-of-ap}). 

\subparagraph{Proof}

To verify the identity law~(\ref{eq:identity-law-of-ap}), apply
both sides to some $p^{:L^{A}}$:
\begin{align*}
{\color{greenunder}\text{expect to equal }p:}\quad & \text{ap}\,(\text{pu}_{L}(\text{id}))(p^{:L^{A}})=(\gunderline{\text{pu}_{L}(\text{id})}\times p)\triangleright\gunderline{\text{zip}}\bef\text{eval}^{\uparrow L}(f^{:A\rightarrow A}\times a^{:A}\rightarrow f(a))^{\uparrow L}\\
{\color{greenunder}\text{left identity law of }\text{zip}:}\quad & =p\triangleright\gunderline{(a^{:A}\rightarrow\text{id}\times a)^{\uparrow L}\bef(f^{:A\rightarrow A}\times a^{:A}\rightarrow f(a))^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =p\triangleright(a^{:A}\rightarrow\text{id}\,(a))^{\uparrow L}=p\triangleright(a\rightarrow a)^{\uparrow L}=p\triangleright\text{id}=p\quad.
\end{align*}

To verify the composition law~(\ref{eq:composition-law-of-ap}),
apply both sides to an arbitrary value $r^{:L^{A}}$:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & r\triangleright\text{ap}\big(p^{:L^{A\rightarrow B}}\odot q^{:L^{B\rightarrow C}}\big)=\text{ap}\,(p\odot q)(r)=((p\odot q)\times r)\triangleright\text{zip}\bef\text{eval}^{\uparrow L}\\
{\color{greenunder}\text{definition (b) of }\odot:}\quad & =\big(\big(\text{zip}\left(q\times p\right)\triangleright\gunderline{(h\times g\rightarrow g\bef h)^{\uparrow L}}\big)\times r\big)\triangleright\gunderline{\text{zip}}\bef\text{eval}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =\big(\text{zip}\left(q\times p\right)\times r\big)\triangleright\text{zip}\triangleright\gunderline{((h\times g)\times a\rightarrow(g\bef h)\times a)^{\uparrow L}\bef\text{eval}^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\big(\text{zip}\left(q\times p\right)\times r\big)\triangleright((h\times g)\times a\rightarrow a\triangleright g\bef h)^{\uparrow L}\quad.
\end{align*}
Now rewrite the right-hand side of Eq.~(\ref{eq:composition-law-of-ap})
applied to $r$:
\begin{align*}
{\color{greenunder}\text{right-hand side}:}\quad & r\triangleright\text{ap}\,(p)\bef\text{ap}\,(q)=\text{ap}\,(q)\big(\text{ap}\,(p)(r)\big)=\text{zip}\big(q\times\text{ap}\,(p)(r)\big)\triangleright\text{eval}^{\uparrow L}\\
 & =\gunderline{\text{zip}}\big(q\times\big(\text{zip}\,(p\times r)\triangleright\gunderline{\text{eval}^{\uparrow L}}\big)\big)\triangleright\text{eval}^{\uparrow L}\\
{\color{greenunder}\text{naturality law of }\text{zip}:}\quad & =\text{zip}\big(q\times\text{zip}\left(p\times r\right)\big)\triangleright\gunderline{(h\times(g\times a)\rightarrow h\times g(a))^{\uparrow L}\bef\text{eval}^{\uparrow L}}\\
{\color{greenunder}\text{composition under }^{\uparrow L}:}\quad & =\text{zip}\big(q\times\text{zip}\left(p\times r\right)\big)\triangleright\big(h\times(g\times a)\rightarrow h(g(a))\big)^{\uparrow L}\quad.
\end{align*}
The remaining difference is reduced to the associativity law of \lstinline!zip!:
\begin{align*}
{\color{greenunder}\text{left-hand side}:}\quad & \gunderline{\text{zip}\big(\text{zip}\left(q\times p\right)\times r\big)\triangleright\varepsilon_{12,3}}\bef(h\times g\times a\rightarrow a\triangleright g\triangleright h)^{\uparrow L}\\
{\color{greenunder}\text{right-hand side}:}\quad & =\gunderline{\text{zip}\big(q\times\text{zip}\left(p\times r\right)\big)\triangleright\varepsilon_{1,23}}\bef\big(h\times g\times a\rightarrow a\triangleright g\triangleright h\big)^{\uparrow L}\quad.
\end{align*}
The underlined expressions are equal due to the associativity law~(\ref{eq:zip-associativity-law-with-epsilons})
of \lstinline!zip!. $\square$

It is important to use the definition (b) of the operation $\odot$
from Statement~\ref{subsec:Statement-ap-category-laws}. The definition~(a)
does create a valid category but describes a reversed order of effects
and cannot be used to write the law of \lstinline!ap! in the form
of a (categorical) functor composition law. So, we will use the definition~(b)
for the category we call \textsf{``}$L$-applicative\textsf{''}. Statements~\ref{subsec:Statement-ap-category-laws}\textendash \ref{subsec:Statement-ap-functor-laws}
show that the laws of \lstinline!ap!, viewed as the laws of the $L$-applicative
category together with the laws of a functor from the $L$-applicative
to the $L$-lifted category, are a consequence of the laws of \lstinline!zip!.

In this way, we have used the guidance of category theory to formulate
the laws of \lstinline!ap!.

The proofs of Statements~\ref{subsec:Statement-ap-category-laws}\textendash \ref{subsec:Statement-ap-functor-laws}
use the left identity law of \lstinline!zip! but not the right identity
law. That law is equivalent to an additional law of \lstinline!ap!
(see Exercise~\ref{subsec:Exercise-additional-law-of-ap}).

\subsection{The pattern of \textquotedblleft functorial\textquotedblright{} typeclasses
and category theory\label{subsec:The-pattern-of-functorial-typeclasses}}

In the previous chapters, we have derived several equivalent formulations
of the laws of various typeclasses (such as functor, contrafunctor,
filterable, monad, applicative). We found that some of these formulations
are easier to use when verifying the laws by hand. However, in every
case we have found a certain formulation of the typeclass laws in
terms of the laws of a \textsf{``}lifting\textsf{''} (namely, the identity and composition
laws). This formulation is important because it makes contact with
category theory, which provides assurance that the laws are chosen
reasonably and correctly. Let us now summarize what we have learned
about the laws of the typeclasses.

In many cases considered so far, we were able to find a certain typeclass
method whose type signature looks like a \textsf{``}lifting\textsf{''} between functions
of one sort and functions of another sort. For instance, the \lstinline!Filterable!
typeclass has the \lstinline!liftOpt! method (Section~\ref{subsec:Motivation-and-laws-for-liftopt-and-equivalence})
with type signature:
\[
\text{liftOpt}:\left(A\rightarrow\bbnum 1+B\right)\rightarrow(F^{A}\rightarrow F^{B})\quad.
\]
Here, we wrote the optional parentheses around $(F^{A}\rightarrow F^{B})$
to emphasize that \lstinline!liftOpt! maps from one sort of functions
to another. Category theory generalizes functions (with type $A\rightarrow B$)
to \textsf{``}morphisms\textsf{''}, which in most cases are functions with a modified
type signature (such as $A\rightarrow\bbnum 1+B$, or $F^{A}\rightarrow F^{B}$,
or something else). Table~\ref{tab:functorial-typeclasses} lists
the type signatures of the \textsf{``}lifting\textsf{''} functions and the corresponding
morphism types for some typeclasses. We see that in each case the
\textsf{``}lifting\textsf{''} method maps functions of a selected morphism type to
functions of type $F^{A}\rightarrow F^{B}$.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
\textbf{\footnotesize{}Typeclass} & \textbf{\footnotesize{}Morphism type} & \textbf{\footnotesize{}Type signature of the \textsf{``}lifting\textsf{''}}\tabularnewline
\hline 
\hline 
{\footnotesize{}functor} & {\footnotesize{}$A\rightarrow B$} & {\footnotesize{}$\text{fmap}:\left(A\rightarrow B\right)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}filterable} & {\footnotesize{}$A\rightarrow\bbnum 1+B$} & {\footnotesize{}$\text{liftOpt}:\left(A\rightarrow\bbnum 1+B\right)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}monad} & {\footnotesize{}$A\rightarrow F^{B}$} & {\footnotesize{}$\text{flm}:(A\rightarrow F^{B})\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}applicative} & {\footnotesize{}$F^{A\rightarrow B}$} & {\footnotesize{}$\text{ap}:F^{A\rightarrow B}\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}contrafunctor} & {\footnotesize{}$B\rightarrow A$} & {\footnotesize{}$\text{cmap}:\left(B\rightarrow A\right)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}profunctor} & {\footnotesize{}$\left(A\rightarrow B\right)\times\left(B\rightarrow A\right)$} & {\footnotesize{}$\text{xmap}:\left(A\rightarrow B\right)\times\left(B\rightarrow A\right)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}contrafilterable} & {\footnotesize{}$B\rightarrow\bbnum 1+A$} & {\footnotesize{}$\text{liftOpt}:\left(B\rightarrow\bbnum 1+A\right)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
{\footnotesize{}comonad} & {\footnotesize{}$F^{A}\rightarrow B$} & {\footnotesize{}$\text{coflm}:(F^{A}\rightarrow B)\rightarrow(F^{A}\rightarrow F^{B})$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Some typeclasses that follow the functorial pattern.}
\label{tab:functorial-typeclasses}
\end{table}

We\index{typeclass!functorial pattern} call \textbf{functorial} all
typeclasses that follow this pattern. The word \textsf{``}functorial\textsf{''} will
remind us that there exists a certain (categorical) functor at the
core of the typeclass. Having defined that functor, we can derive
the methods and the laws of the typeclass via the following steps:
\begin{enumerate}
\item Define each category\textsf{'}s composition operation and identity morphism.
\item Verify that the category laws hold.
\item Write the type signature of the \textsf{``}lifting\textsf{''} method corresponding
to the functor.
\item Impose the naturality laws and the functor laws on the \textsf{``}lifting\textsf{''}
method.
\item Derive other typeclass methods (and their laws) from the \textsf{``}lifting\textsf{''}
method and its laws.
\end{enumerate}
Let us go through this pattern for the applicative functor typeclass
studied in this chapter. Following the functorial pattern, we say
that a covariant type constructor $F$ is applicative if there exists
a (categorical) functor between the $F$-applicative and $F$-lifted
categories. The morphisms of the $F$-applicative category are values
of type $F^{A\rightarrow B}$; the morphisms of the $F$-lifted category
are values of type $F^{A}\rightarrow F^{B}$. This is all we need
to start with. All other properties of applicative functors are then
derived in a systematic way. With this approach, we do not need to
memorize the complicated type signatures and laws of the methods \lstinline!map2!
and \lstinline!ap!.

The first step is to define the $F$-applicative category\textsf{'}s morphisms.
We define identity morphisms (\lstinline!wid[A]! of type $F^{A\rightarrow A}$)
and the composition operation ($\odot$) that composes $F^{A\rightarrow B}$
with $F^{B\rightarrow C}$ to obtain $F^{A\rightarrow C}$. The second
step is to verify that the category laws hold with those definitions;
this is done in Statement~\ref{subsec:Statement-ap-category-laws}.
The $F$-lifted category with morphisms $F^{A}\rightarrow F^{B}$
is shared by all functorial typeclasses we have seen, and we already
checked that it satisfies the category laws (Section~\ref{subsec:Motivation-for-using-category-theory}).
The third step is to write the type signature of the \textsf{``}lifting\textsf{''}
function of the functor:
\[
\text{ap}:F^{A\rightarrow B}\rightarrow F^{A}\rightarrow F^{B}\quad.
\]
The fourth step is to require that \lstinline!ap! obey the naturality
and the functor laws. The last step is to derive other methods that
are equivalent to \lstinline!ap! but more convenient to use. We have
shown that the \lstinline!map2! and \lstinline!zip! methods with
suitable laws are equivalent to \lstinline!ap! (see Section~\ref{subsec:Equivalence-of-map2-zip-ap}
and Statement~\ref{subsec:Statement-ap-functor-laws}). 

In this way, we derive the applicative typeclass by following the
functorial typeclass pattern. 

Many typeclasses can be derived from this pattern, but some cannot.
For example, applicative contrafunctors (Section~\ref{sec:Applicative-contrafunctors-and-profunctors})
and traversable functors (Chapter~\ref{chap:9-Traversable-functors-and})
are not functorial typeclasses. Their laws must be motivated by other
considerations.

\subsection{Exercises\index{exercises}}

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-3}\ref{subsec:Exercise-applicative-II-3}}

Implement an applicative instance for $F^{A}=\bbnum 1+\text{Int}\times A+A\times A\times A$.

\subsubsection{Exercise \label{subsec:Exercise-function-type-construction-not-applicative}\ref{subsec:Exercise-function-type-construction-not-applicative}}

Show that the following functors $F^{\bullet}$ \emph{cannot} be applicative: 

\textbf{(a)} $F^{A}\triangleq(A\rightarrow P)\rightarrow Q\quad.$

\textbf{(b)} $F^{A}\triangleq\left(A\rightarrow P\right)\rightarrow\bbnum 1+A\quad.$

Here $P$ and $Q$ are some arbitrary, unknown, but fixed monoidal
types.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-4-2}\ref{subsec:Exercise-applicative-II-4-2}}

Show that the ternary tree functor from Exercise~\ref{subsec:Exercise-applicative-I-1-1}
is a lawful commutative applicative functor. Use applicative functor
constructions to avoid long proofs.

\subsubsection{Exercise \label{subsec:Exercise-applicative-I-1-1-1}\ref{subsec:Exercise-applicative-I-1-1-1}}

Implement a \textsf{``}padding\textsf{''} \lstinline!zip! method for lists \lstinline!List[A]!
such that the shorter list is padded with the \emph{last} value until
its length is equal to that of the longer list. Zipping with an empty
list returns again an empty list. A sample test:
\begin{lstlisting}
def paddingZip[A, B](left: List[A], right: List[B]): List[(A, B)] = ???

scala> paddingZip(List(1, 2), List("a", "b", "c", "d"))
res0: List[(Int, String)] = List((1, "a"), (2, "b"), (2, "c"), (2, "d"))

scala> paddingZip(List(), List(1, 2, 3))
res1: List[(Unit, Int)] = List()
\end{lstlisting}
Prove that the laws of \lstinline!zip! hold for \lstinline!paddingZip!
with a suitable \lstinline!wu!.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-4}\ref{subsec:Exercise-applicative-II-4}}

Show that $F^{A}\triangleq G^{A}+H^{G^{A}}$ is a lawful applicative
functor if $G$ and $H$ are. Show that $F$ is commutative if $G$
and $H$ are. Use applicative functor constructions to avoid long
proofs.

\subsubsection{Exercise \label{subsec:Exercise-zip-pure-pure}\ref{subsec:Exercise-zip-pure-pure}}

For an arbitrary applicative functor $F$ with a lawful \lstinline!zip!
method, prove this law:
\[
\text{zip}_{F}\big(\text{pu}_{F}(a^{:A})\times\text{pu}_{F}(b^{:B})\big)=\text{pu}_{F}(a\times b)\quad.
\]


\subsubsection{Exercise \label{subsec:Exercise-simplify-law-omit-lifted-function}\ref{subsec:Exercise-simplify-law-omit-lifted-function}}

It is given that two functions $u:A\rightarrow F^{B}$ and $v:A\rightarrow F^{B}$
(where $F$ is some functor) satisfy the law $u\bef f^{\uparrow F}=v\bef f^{\uparrow F}$
with an \emph{arbitrary} function $f:B\rightarrow C$. Show that the
function $f$ can be omitted from the law: an equivalent law is simply
$u=v$.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II}\ref{subsec:Exercise-applicative-II}}

Show that $\text{pu}_{L}(f)\odot\text{pu}_{L}(g)=\text{pu}_{L}\left(f\bef g\right)$
for an applicative functor $L$. The operation $\odot$ is defined
in Statement~\ref{subsec:Statement-ap-category-laws}(b).

\subsubsection{Exercise \label{subsec:Exercise-additional-law-of-ap}\ref{subsec:Exercise-additional-law-of-ap}}

Show that if \lstinline!ap! is defined via a lawful \lstinline!zip!
then \lstinline!ap! satisfies the following law:\index{identity laws!of ap@of \texttt{ap}}
\begin{align*}
{\color{greenunder}\text{right identity law of }\text{ap}_{L}:}\quad & \text{ap}_{L}\,(r^{:L^{A\rightarrow B}})(\text{pu}_{L}(a^{:A}))=r\triangleright(f^{:A\rightarrow B}\rightarrow f(a))^{\uparrow L}\quad.
\end{align*}


\subsubsection{Exercise \label{subsec:Exercise-applicative-of-monoid-is-monoid}\ref{subsec:Exercise-applicative-of-monoid-is-monoid}}

Show that $P^{S}$ is a monoid if $S$ is a fixed monoidal type and
$P^{\bullet}$ is any applicative functor, contrafunctor, or profunctor.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-1}\ref{subsec:Exercise-applicative-II-1}}

Prove the following statements (which complement Statement~\ref{subsec:Statement-applicative-composition}):

\textbf{(a)} If $F^{\bullet}$ is an applicative functor and $G^{\bullet}$
is an applicative contrafunctor then the contrafunctor $L^{A}\triangleq G^{F^{A}}$
is applicative.

\textbf{(b)} If $F^{\bullet}$ and $G^{\bullet}$ are both applicative
contrafunctors then $L^{A}\triangleq F^{G^{A}}$ is an applicative
\emph{functor}.

\textbf{(c)} In both parts \textbf{(a)} and \textbf{(b)}, if $F^{\bullet}$
and $G^{\bullet}$ are commutative then $L^{\bullet}$ is also commutative.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-4-1}\ref{subsec:Exercise-applicative-II-4-1}}

Show that $F^{A}\triangleq H^{A}\times G^{A}$ is applicative and
co-pointed if $G$ and $H$ are applicative functors and $H$ is co-pointed.
Show that the compatibility law~(\ref{eq:compatibility-law-of-extract-and-zip})
holds for $\text{zip}_{F}$ and $\text{ex}_{F}$ if it holds for $\text{zip}_{H}$
and $\text{ex}_{H}$.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-5}\ref{subsec:Exercise-applicative-II-5}}

Explicitly prove the laws in the construction of Statement~\ref{subsec:Statement-applicative-contrafunctor-product}.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-7}\ref{subsec:Exercise-applicative-II-7}}

Show that the recursive functor $F^{A}\triangleq\bbnum 1+G^{A\times F^{A}}$
is applicative if $G^{A}$ is applicative and $\text{wu}_{F}$ is
defined recursively as $\text{wu}_{F}\triangleq\bbnum 0+\text{pu}_{G}\left(1\times\text{wu}_{F}\right)$.
Use applicative functor constructions.

\subsubsection{Exercise \label{subsec:Exercise-applicative-profunctor-composition}\ref{subsec:Exercise-applicative-profunctor-composition}}

Prove Statement~\ref{subsec:Statement-applicative-profunctor-composition}.

\subsubsection{Exercise \label{subsec:Exercise-profunctor-example}\ref{subsec:Exercise-profunctor-example}}

The type constructor $Q^{\bullet}$ is defined by: 
\[
Q^{A}\triangleq\left(A\rightarrow\text{Int}\right)\times A\times\left(A\rightarrow A\right)\quad.
\]
Show that $Q^{A}$ is neither covariant nor contravariant in $A$,
and express $Q^{A}$ via a profunctor. Is $Q^{A}$ applicative?

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-11}\ref{subsec:Exercise-applicative-II-11}}

\textbf{(a)} For any given profunctor $P^{A}$, implement a function
of type $A\times P^{B}\rightarrow P^{A\times B}$. 

\textbf{(b)} Show that, for some profunctors $P^{A}$, one \emph{cannot}
implement a fully parametric function of type $A\times P^{B}\rightarrow P^{A}$.

\subsubsection{Exercise \label{subsec:Exercise-applicative-II-10}\ref{subsec:Exercise-applicative-II-10}}

Implement profunctor and applicative instances for $P^{A}\triangleq A+Z\times G^{A}$
where $G^{A}$ is a given applicative profunctor and $Z$ is a monoid.

\subsubsection{Exercise \label{subsec:Exercise-profunctor-pure-not-equivalent-1}\ref{subsec:Exercise-profunctor-pure-not-equivalent-1}}

For the profunctor $P^{A}\triangleq A\rightarrow A$:

\textbf{(a)} Show that $P^{A}$ is pointed: there exist a value \lstinline!wu!
of type $P^{\bbnum 1}$ and a method \lstinline!pure! of type $A\rightarrow P^{A}$. 

\textbf{(b)} Show that the type of fully parametric functions $\text{pu}_{P}:A\rightarrow P^{A}$
is \emph{not} equivalent to the type $P^{\bbnum 1}$.

\begin{comment}
this is chapter eight devoted to applicative hunters and pro functors
the first part will be practical examples main motivation for applicati
factors comes from considering kinetic computations or computations
in the factor block as I color in case when effects are independent
and competitive or in case when these effects could be executed in
parallel while the result is still correct here is an example consider
a portion of a functor block or a for yield block in Scala that looks
like this there are three future values and X will be waiting until
with this future value is ready imagine that these are some long-running
computations and after these three lines we can use XYZ and further
computations any further computations will be waiting for these three
futures now if we write code like this then these three futures will
be created sequentially that is first this future will be created
and scheduled on some thread and then when it\textsf{'}s done you get the value
X and then this second future will be created and scheduled even though
it doesn't use the value X but the monadic block or the for yield
block or the Thunder block whatever you want to call it I call it
a factor block the Thunder block is such that every generator line
locks everything else until it\textsf{'}s done so in the future we'll be created
one at a time and so obviously this is not optimal if you translate
this into flat map code and map that\textsf{'}s the code so you have a first
future but to which you append this flat map so you schedule it is
further computation only when the first computation is ready the second
future will be then started and this will be waiting until the second
future is ready and then the third future will be started and this
will be ready will be waiting until the third future is ready clearly
this is not optimal would like to parallelize these things and we
have seen in a previous tutorial that a very easy way of paralyzing
such computations is to create the futures before starting the factor
block but this is actually a specific feature of scholar where futures
already start computing when you create them there is no separate
method to start computing them which is actually a design flaw and
we would like to express more carefully that certain computations
can be done in parallel because they're independent whereas other
computations really have to wait one for another now in this example
like I said C 1 C 2 and C 3 are just some fixed computations that
don't depend on the values computed previously if we do have this
dependency and of course there\textsf{'}s no way except wait until each previous
computation is done but in case they're they're independent we would
like to be able to compute them in parallel and we would like to express
that in a better way you know code rather than just a hacky way where
you create these futures separately and then put into some variables
and then you write this code because that will work for future will
not work for some other factor another use case where moon ads or
monadic effects are not exactly what we want is a case when we perform
computations that may give errors now we have seen in the previous
chapter that using a moon ad such as either moon ad option will not
we could stop at the first error very easily but sometimes we don't
want to stop at the first error we want to maybe accumulate all errors
as much as possible and give the user more information so {[}Music{]}
monads cannot do that in general they cannot accumulate all errors
because if one monadic step fails then the next one won't be executed
so if as I said effects are independent then we would like to have
a different way of computing than the moon at computation shown here
so monads are inconvenient for expressing independent effects also
they're inconvenient for expressing commutative effects because typically
when you change the order of generator lines like these two you change
the results even though these generator lines seem to be independent
of each other they aren't the computation will first iterate with
X going over this list and for each X Y it will be going through that
list in that order first X will be iterated over in this list then
Y over that list now order is fixed and you can do nothing to change
it so for example if you interchange these two lines you would get
a different list as a result the first iteration would be interchange
with the second one and the list will have a different value now for
lists the order of the order of elements is important perhaps for
other monads usually this is also the case that you cannot just interchange
lines in a factor block and expect the result not to change however
there are some cases where these computations logically speaking are
independent and what we would like to do is express this very clearly
this is how it is done we would like to have a function that computes
something like this but assuming that the effects are commutative
how would that function work well look at what it does we have here
two containers and we compute some function from values that are stored
in these two containers and put all the results in a new container
so if we just express this computation as a function that function
would have the following type signature it will take two containers
let\textsf{'}s call them FA and FB so let\textsf{'}s say F is list and aliened b are
going to be the types of the elements in these two containers in this
example both integers but in general we might have two different types
a and B so this function takes these two containers it also takes
a function f which is a function from a pair a B to some type C as
a result we get a container of type C now clearly this kind of function
let\textsf{'}s call this map to because it\textsf{'}s like map except you have two containers
and two arguments of a function that you use now of course this computation
has this type signature so if you have a monad for F and then you
can define this function using this code very easily X left arrow
F a YF arrow left arrow FB yield F of x1 so that is the code that
implements map to in terms of flat map and map now the key observation
here is that we would like to not use the flat map because flat map
will force this ordering of effects and it will prevent us from having
commutative effects whereas this function we could define differently
we could define it separately from flat map in a different way such
that this function is symmetric in a and B for example we could do
that and once we are able to do that we can use that function by itself
without writing furniture blocks may be using a different syntax but
basically just using this function to prepare this data in this way
so to process several containers in a symmetric way that is to endure
more independent containers have independent effects and a function
that maps their values to some new type and the result is a new container
so that is the key idea behind applicative functors applicative factors
are factors F for which this function is available somehow also the
function P or the same function as in monads with the type signature
a going to f of a must be available but that function just like in
the case of Moonen is much less important for practical coding than
this function so applicative functor is a factor that has these but
first of all it does not have to be a monad and even if it is a monad
in some way the definition of map 2 does not have to come from the
definition of flat map like this so the reason is that we want a different
logic we don't want the calculation going like this with flat maps
we actually want to avoid using flat map so that\textsf{'}s why we would define
map 2 separately not via flat map and use that function map to directly
on some containers in this way for example we would be able to define
a function of three futures that takes also a function of three numbers
and compute the future of the result now that would be map 3 as it
were so let\textsf{'}s consider this as a generalization so as I said for a
monad we can define these functions through a flat map and let\textsf{'}s just
use that as a guide we're not going to define through flatmap really
but we should be able to get a lot of intuition by just looking at
the functions that we get from flat map because the types are going
to be the same and so a lot of intuition comes from looking at the
type of a function so if we just have one container then it\textsf{'}s a ordinary
map which is equivalent to this code in Scala I remind you that the
left arrow or the generator arrow is translated into either flat map
or map it\textsf{'}s if there are several generator arrows it\textsf{'}s the first until
the last one so the last but one all of them are flat maps and the
last one is map so the last left arrow before yield is just translated
into a map so that\textsf{'}s a map now this would be first a flat map than
an ordinary map but we want to replace that with this kind of syntax
perhaps where we have map - it takes a tuple of containers it also
takes a function f with two arguments and it returns a container here
is a similar thing with three containers where we have map three so
the function f needs to have three arguments and the result is again
a single container so how do we generalize this let\textsf{'}s write down the
type signatures so map the usual map let\textsf{'}s call it map one just to
be systematic then it\textsf{'}s like this this is the standard type signature
for the map function map to has this type signature takes two containers
as we already saw on the previous slide map three takes three containers
takes a function with three arguments instead of two and it again
returns a new container so clearly this is how we can generalize map
one map to map three and so on the type signatures are clear now applicative
factors have all of these functions map in in fact we will see in
a later tutorial that once you have map one and map two it is sufficient
in order to be able to define all the others you don't need to have
separate definitions for all the other maps in you can define map
in taking a list of these a list of these it\textsf{'}s easy once you have
met one and map - we will not dwell on it in this tutorial that\textsf{'}s
the subject of the next tutorial because it requires us to consider
the laws that must be satisfied by these map end functions and then
it will be much easier to understand why all these functions are interrelated
to each other but at the same time it will require us to go much deeper
into the relationships between these functions the equations and the
laws so that\textsf{'}s going to be the subject of the next tutorial part 2
of chapter 8 in this part one I'm just going to talk about practical
considerations practical examples of how you use this map in concentrating
mostly on map to map n will be used in a very similar way so what
are examples where we want to use such things let\textsf{'}s look at the example
code so the first example is the easier type so imagine we're doing
some computations when results could be given as values of type a
or as you could have a failure an error message that is represented
by a string an example of such a computation would be the safe divide
function where I divide but I check whether it is not 0 and if it\textsf{'}s
0 then I don't divide I'll give an error now this is just an example
of this kind of approach where your computations have the type up
of double instead of double and this is a type constructor that encapsulate
some effect some error accumulation or some other effect so in this
case this constructor represents errors usually as we have seen before
so let\textsf{'}s implement map 2 in a way that would help us understand how
we can collect all errors so map 2 needs to have this type signature
it takes up of a up of B it also takes a function from a B to Z and
it returns up of Z so it needs to have type parameters a B and Z how
do we compute this OP of Z well let\textsf{'}s find out we have up of a and
up of B now op is an either so we can imagine it let\textsf{'}s match right
away on both and the now that there\textsf{'}s a case when we have two errors
and in this case it will be interesting for us to accumulate these
error messages so let\textsf{'}s not throw away information here and let\textsf{'}s
do this so we can catenate the strings in more general situation this
type instead of string it could be some other type which could be
a monoid and then we could combine two values of a mono it into a
larger value in case we have one error and one result you can't really
do much except return that error and in case we have two results we
can actually perform a computation with this requesting which is this
function f and return a right so in these two cases there isn't enough
data to call F because we only have one value but not the other so
we cannot call F and the only thing we can return is an error so this
is how we implement map two now notice that this map 2 does more than
a monadic factor block would would do because the magnetic factor
block would stop after the first error this one doesn't stop not to
the first air it takes also look looks at the second computation and
if it also is an error then it accumulates the errors into into this
value this is something a monadic factor block cannot do cannot accumulate
errors because once it finds the first there it stops and returns
that error so here\textsf{'}s an example how we could compute for example here
is map - I'm just going to apply this function to the values you find
like this as an example now in a real computation this doesn't seem
to be very convenient to always have to write computations like this
Scala can give you a lot of convenience by defining domain-specific
language with syntax and operators so that you don't have to write
all that stuff but under the hood it must do this stuff so this is
the way it works under the hood for any kind of library that would
do this more conveniently so you could call this function map two-on-two
computations now notice they are both dividing by 0 so these two computations
will give you two error messages and now the function that you pass
is a subtraction and that function by itself doesn't give any errors
but it never gets called because you actually have errors in both
of these two computations so the tests asserts we actually have two
error messages telling you what you're dividing by zero now if you
instead wrote this kind of thing which has the same type signature
now you would have just one error message the first one would have
stopped the functor block from continuing so this is how it works
now we can define map three let\textsf{'}s take a look is a very simple type
can't be so bad now however we now have three computations each could
be an error or a result so there are eight possible cases we're not
going to write them down that would be really not great so how do
we do that well let\textsf{'}s think about how we could use map to instead
of writing this function map 3 from scratch map to only works by applying
it to two containers ok let\textsf{'}s do that let\textsf{'}s apply it to a and B and
well the function f requires three arguments we can't apply it let\textsf{'}s
not apply it let\textsf{'}s just apply it to some function that doesn't do
anything to the arguments just accumulate them in a tuple which is
almost an identity function just to remind you that in the Scala syntax
this is not actually a tuple this is anonymous function that takes
two arguments and if you wanted a tuple you'd have to write like this
which is the to pull of X while I'm going to the tuple of X Y this
is the actual identity function on tuples but this is not what we
defined so it\textsf{'}s almost an identity function up to some syntax we could
have defined the syntax so that it is really an identity so we don't
we just combine these two what\textsf{'}s the result the result is this so
it\textsf{'}s an oak of a tuple now we can use again map 2 to combine this
up of a tuple with C and then the map 2 will have this type of argument
now we can apply F so this kind of thing is how you could use me up
to to define map 3 now clearly this can be generalized a little awkward
to generalize but it can be generalized the awkwardness comes from
here it'll be hard to generalize this case expression obviously if
you have map n you need to apply map 2 then again map 2 again map
2 when you do that every time I have a more more 2 pulls in your case
expression so that\textsf{'}s a little awkward can be done with some more work
but not going to look at this right now yeah easier ways of defining
all these map and functions here is an example of how it works a map
3 working on three computations of this type accumulates the two errors
that you see the third computation does not give an error so that\textsf{'}s
fine now a use case for this kind of type is where you want to validate
value so imagine if has some case class with a few values you want
to you want to validate them and each validation is separate from
other validations just like each of these computations is separate
so only when all of the three parts of a tuple or a case class pass
validation you want to create actually a value of the case class otherwise
you want to fail but when you want when you fail you want to gather
all the errors this is how you would do that you would say you do
a map three of these three validated computations safe and divided
and then you would apply that map three on that to a function that
takes three values and maps and makes C case class value out of them
which is C don't apply no see don't apply is a function that takes
three arguments and returns case class I've made out of them so this
C dot apply is defined automatically by Scala very convenient so in
this case in this example obviously there aren't any divisions by
zero and so we get the result second so we have gone through these
two examples the third example is when we prefer a future our computations
concurrently and here\textsf{'}s how we would do that we would define lab 2
for futures now since these arguments are eagerly evaluated in there
there are already given when you call up to these futures already
were started on some threads and so we can just write the free on
block like we did before these futures are not really sequential because
they already have started so they run most likely on separate parallel
threads already when we are starting to evaluate this function so
that\textsf{'}s why it\textsf{'}s ok then you find map 2 through a flat map for futures
now that is a test when we do a map - like this on these two features
and then there\textsf{'}s another addition so it\textsf{'}s 1 + 2 3 + 4 3 that will
be 10 and map n can be defined like this so there\textsf{'}s a list of futures
and you returned the future of alistel doesn't really map in there
must be still a function so let\textsf{'}s call it something else in the standard
library it\textsf{'}s called future that sequence sequence is a kind of confusing
name it refers to changing of the order of these two type constructors
first you had a list of futures and then you have the future of the
list very important and useful function in the standard library that
essentially implements most of map in you still need to map this list
to some other value so the real map and we'll take a list of futures
they also take a function from a list of A to B so we need a type
of Z and we will return a future of B and the reason yeah and then
we would just do that in order to get it working so that was the example
of the futures another example where applicative factors could be
useful is when you have a reader mu net worth or functions with some
standard arguments and you are getting tired of passing these arguments
over and over to all kinds of functions so as we have seen in the
previous chapter the reader moment does that kind of thing and it
turns out that the reader monad whose effect is this constant value
D that you could always read which is kind of an standard argument
of all computations that you are doing now this e is a constant it
is an immutable given value and so and that\textsf{'}s the only effect of this
moment so this monad always has independent and commutative effects
so in this monad you don't have to worry about the order of effects
it is already independent and in other words we can define map to
buy a flat map there\textsf{'}s no penalty for doing so here\textsf{'}s an example imagine
we have some application that needs to use a logger now a logger we
will just do a quick and dirty logger which has a side effect which
takes whatever value of an any type and prints it in some way and
he returns unit now this is a very dirty way of doing login but it\textsf{'}s
quick it\textsf{'}s dirty in the sense that it\textsf{'}s hard to see that you have
logged anything or not because this function doesn't return any useful
values but let\textsf{'}s continue with that for now a functional logger of
this clean is the writer monad it tells you explicitly that there
is an extra value being created with each computation which is perhaps
a log message and it tells you also that you can bind the log messages
together using a Monod so that\textsf{'}s a clean way of logging but let\textsf{'}s
just for the sake of this example consider this logger so we have
an empty logger and maybe some non empty loggers and now suppose that
every computation we want is going to be logged so every computation
is going to use the slogger and call this print function many times
for whatever reason and so every computation needs a logger as an
argument because that logger will provide the printing functionality
and if you do that also it becomes much easier to test such code then
you can pass an empty lager or you can pass a test only debugging
lager or anything like that but then of course all your computations
become more cumbersome because now you have functions that have these
in this argument so here\textsf{'}s for example a computation that adds two
integers it returns a logged integer it is a function that takes a
logger it does this computation who logs it and returns the result
so it\textsf{'}s a typical kind of code that you would use now suppose I wanted
to combine these computations so I'm loading one plus two I'm logging
10 plus 20 the results are X\&Y and I have X plus y now changing the
order of computations gives the same result except for the side effect
printed of course that is not going to be commutative first it\textsf{'}s going
to print this and then it\textsf{'}s going to print that but the side effect
is invisible in the types of these expressions and so the expressions
are going to be equal after interchanging the order as this test shows
and this is another illustration of the bad nature of side effects
you don't see them you have no assurance that the side effects have
been performed in the order you want so for this example it will be
okay so now let\textsf{'}s define map to just define it via flat map like this
or we can define it by hand like this which is kind of obvious because
these this is a function from logger to a this is a function from
blogger to B and this is a function from logger to Z so we obviously
just call these two functions with a and B provided by those two functions
so echo F on this that\textsf{'}s the only way to implement map to now this
way is exactly the same actually we can verify this symbolically but
the code is exactly the same because maverick is this for the reader
munna flat map is that there\textsf{'}s no freedom here the types fix the code
uniquely and then we can calculate what is mapped to of a B which
is going to be this is this is the translation of the for yield construction
we can do that by first looking at the map substituting the definition
of the map right here and applying the function to the argument so
then you get this function and finally we need to put that function
into flat map substitute the definition of flat map which is this
and we get this code so by just symbolically transforming the code
I step-by-step arrive at the code of the other function so in this
way you could use map to map N and so on and have your standard arguments
passed to functions but so I do that with the reader Mona if you can
just do the for yield construction well what if you can't yeah there
is no Timon on somewhere in case your type is more complicated than
this you might be in a situation where you don't have a monad but
still you need to pass standard arguments and you can do that with
map in another example is the list which is a monad and we have seen
that every minute is already an applicative because you can always
define map to buy a flat map but there might be a different definition
map to in case of list there are various definitions we'll just use
a standard one right now and show how we can transpose a matrix so
let\textsf{'}s define map - first of all on map - is a very simple thing for
this is the zip so first we can zip see what we need is a function
from list a and list B - a list of pairs a B no look you just may
up with F and that\textsf{'}s what we write here so we do an a sub B which
gives us a list of pairs a B and we can map that now the zip function
has a specific implementation we could change that especially when
a and B do not have the same length then there are lots of choices
about what to do do you want to cut short so you want to fill with
some default values or something like that but what let\textsf{'}s not discuss
it right now we will see in in the second part of this tutorial how
to define map 2 in various ways in case there are several definitions
so for now we just use the standards library in Scala to have the
zip function so that\textsf{'}s clearly what map - does so you can see that
map 2 has a close connection to the zip function for lists that takes
two lists and returns a list of pairs and then you just map over that
list with a function f so in up to on a pair of these two lists with
a plus function will give you pairwise sums so how do we transpose
a matrix now the matrix represented here is a list of lists needs
to be transposed so in fact we need to understand how we represent
a {[}Music{]} matrix as list of lists so let me have a an example
so this matrix is a list of three lists and the transpose matrix is
a list of two lists but the first has these three elements together
and the second list has these three elements together so to transpose
them what we need is to take the heads of each list and put these
heads into a list of their own and then we can use transpose on the
tails of each list in the same way so we are we're going to have a
recursive function obviously so how do we do that so suppose that
this list of lists has heads and tails so what does it mean heads
so this is the heads now actually {[}Music{]} what you want is to
append this to this and to this you want to have a list that goes
like that so clearly the head of that list is going to be the head
of lips and let\textsf{'}s transpose the tails so tails are these so sorry
tails are these if we transpose them then we will have a list of these
two and then a list of these two now what we need to do is we need
to append this to that list and this to that list now this append
looks like a component wise operation on this list and on the list
of those so that\textsf{'}s where we use map to we use map to on heads which
is this and on transposed tails which is it transposed this to append
the first elements to the rest which means that we will append this
to that will append this to that and so on so it remains to transpose
the tails now it\textsf{'}s very easy to transpose the tails you just call
transpose on the tails right here everything else in this code is
bookkeeping that is designed carefully to avoid problems where you
have empty lists now that code is kind of cumbersome and error-prone
you do a map with a function that returns empty lists and here you
just return an empty list it took me a few tries to get it right this
code in this code but the other thing is more important so this is
how we use map tool in order to concatenate component-wise this with
this at the same time this with this and so on so this is the kind
of typical computation that the clickity factors do they do component
wise computations component wise computations are independent of each
other\textsf{'}s results so this is independent of this and that\textsf{'}s where you
use applicative factors and some tests to show that this is correct
so after these examples in principle there are a few other examples
where you use applicative factors and before going to those examples
that are a little more compact complicated I'd like to talk about
applicative contractors an applicative crew factors now the reason
I'm talking about this is that actually they are sometimes quite useful
you can have an applicative instance or a function such as map 2 or
zip for a tight constructor that is not a factor we have seen in a
previous tutorial that type constructors could fail to be factors
in a number of ways one of them is when they are contractors that
has contravariant type constructors but there\textsf{'}s another way where
they can be neither factors nor contra factors and that happens when
your type parameter is both in a covariant and in a contravariant
position in the type constructor sorry this is an example so if you
have a as your main type prime Z let\textsf{'}s say is a constant type it is
your main type parameter then this a is to the right of the function
arrow and this is to the left of the function area so this a and this
a are in a covariant position and this is in a contravariant position
because this type constructor contains both covariant and contravariant
position for a it cannot be a functor and it cannot be a control factor
for in terms of a but nevertheless it has still properties that are
quite nice it is called the true factor when you can see all type
parameters either in a covariant or in a contravariant position I
will talk about Pro factors later in more detail but for now just
keep in mind that it\textsf{'}s very easy to see which position is coherent
and which is controlling and just look at the function errors in your
type and everything to the left of a function arrow becomes contravariant
if there is an arrow inside that then it can again become covariant
so we have seen examples of this so here\textsf{'}s how Pro factors can become
useful this is an example where we can define a semigroup typeclass
instance for a type that has parts such as a tuple or a case class
where each part also has a semigroup typeclass instance and let\textsf{'}s
see how that works just for brevity I will define semigroup here like
this it\textsf{'}s a very simple tight class it just has one method and this
method basically is equivalent to this data type a product of two
S\textsf{'}s going to 1s so clearly this data type considered as a data type
is not a factor and not a control funky in the type parameter S now
we usually don't consider typeclass traits as containers as data types
but we can we could and what will happen if we do is we will notice
that usually there would not be factors and not become true factors
because they have too many things in both covariant and control positions
so these two are contravariant position this is a covariant position
but it is a pro functor because it has nothing but things in covariant
and contravariant positions in that case it\textsf{'}s going to be a pro factor
in fact all exponential polynomial types are going to be Pro factors
there\textsf{'}s no way you could fail to be be a profounder as long as your
stables and exponential polynomial types now the interesting thing
is that we could define a zip function for this type constructor so
let\textsf{'}s forget front at the moment that we're going to be using this
type constructor just to define a few implicit values for the typeclass
and let\textsf{'}s just consider that as a type constructor and try to see
what zip function would be for that type constructor well zip function
would have this signature we'll take one semi group another semi group
and we'll return a semi group of a pair how do we define that wallets
pretty straightforward you need to define a combined function that
combines a B and a B into a B or you just combine this a with this
a into this one and you combine this B with this B into this one using
the combine operation from the semigroups P\&Q and that\textsf{'}s the code
and now we can use that to define semi grouped typeclass instance
for a pair it\textsf{'}s just same I just I'm just going to use their syntax
and to test that this works i define semigroup instances for integer
and double in some way and i have a pair of int double and now I am
able to use the syntax so after this implicit definition I am able
to use this index so this is a zip function is very closely related
to map to we did not actually use map tool right now but once you
have zip then you get your container of pairs and all you need to
do is to map your function of two arguments over this container to
get the map tool so zip and map two are very closely related once
you implement one you can implement the other the second example I'd
like to give is that let\textsf{'}s consider this cofactor and let\textsf{'}s just implement
an app to for it now map to for a profounder has a different type
signature than map to for a factor and also it\textsf{'}s called imap2 in the
cat\textsf{'}s library and in any case to indicate that this is no this is
not a functor map to they call it invariant so prou factors are called
invariant which is quite confusing to me because invariant has so
many other meanings in different contexts such as how you make a transformation
but something doesn't change then you say something is invariant with
respect to the transformation but none of that is happening here nothing
is being transformed such that it remains invariant in geometry for
example the length of line segment is invariant under rotations the
the area of a triangle is invariant under rotations so that\textsf{'}s the
kind of context that I'm familiar with and calling this invariant
would lead me to to ask what is the transformation that you're applying
such that this does not change and that makes no sense at all in this
context or for instance in the computer science there is something
called loop invariant which is an expression that remains constant
throughout each iteration between each iterations of the loop there
is no loop here and nothing remains constant so again that meaning
of the word invariant does not apply so that\textsf{'}s confusing to me so
I don't want to talk about invariant factors I want to say Pro factors
it\textsf{'}s shorter anyway so let\textsf{'}s look at how we can implement an analogue
of map to function for this type constructor now this type constructor
as I said already it\textsf{'}s not a functor cannot be possibly in a factor
because it has the type parameter a in a contravariant position and
also in the covariant position so it cannot be a contra factor either
but it is nevertheless what I call zero ball you could define zip
for it and you can define imap2 for it let us see how do we define
i map to Fred so here I define this type constructor F now in order
to do an app - we use the function from a B to C and we applied that
to a container with a and container with D so here we have a container
with le here we have a container with B and here we have a function
from A to B to C but it turns out that\textsf{'}s insufficient you also need
a function from C back to a B only then it makes sense to define mapping
functions for a profounder and the reason is that when you want to
define a mapping function and you transform the type parameter say
a into some other type frame and say C but when you have your type
in a covariant position then you just compose with the function that
transforms when you have your type in a contravariant position you
have to have a function that goes backwards from C to a side and then
you would take a C you will use that function to get an A and when
you put that a as an argument in a contravariant position so for this
reason the profanity requires a function from C back to a B now if
this were purely a control factor then we would just use this function
from C to a B and we will be done I remind you that contra factors
are just very similar to functions except the map function takes the
opposite direction of transformation in types but here it is neither
a functor nor a contra factor and actually we need both functions
F and G F goes from a B to C and G goes from C back to an a B and
then it turns out we can do what we need how do we do that well so
let\textsf{'}s write the code being guided by types and by the intuition of
what we need to do so we need to produce an F of C and Z so that\textsf{'}s
going to be a function from Z C to C C so we need to return a tuple
of C see how do we do that well it\textsf{'}s obvious that we need to use these
functions F and G somehow we have a see if we use G on that we get
a pair of any bit let\textsf{'}s do that so we get some new a and new be my
back transforming C so now these new and new B need to be substituted
into the contravariant positions which are these positions so we take
F AZ and we substitute Z and a now Z there\textsf{'}s only one Z and that\textsf{'}s
not going to change if those are not transforming that type parameter
Z at all so Z stays the same but here we substitute new and new B
into the contravariant position of the F the result is going to be
pairs of a a and B B so let\textsf{'}s call them new a nu B B so now we can
apply F to this data to get a new value of C we need to have two different
values of C and it makes sense that we would use the first two and
the second to like that to substitute that into F so that\textsf{'}s how we
would do that let me try to improvise and define a zip function for
this type constructor so what would be a zip function so will be F
is e type fpz of type F of Z and the result will be F of pair a B
and Z Z is not going to change we're just by the transform and what
do I need I need type parameters a B and Z so how am I going to do
that well obviously F is this on e to have a function that takes Z
and a pair of a B in Scala I cannot just have a function that takes
that as arguments I need to do a case expression so that Ida structure
these arguments as it\textsf{'}s called or a match from them so now I need
to produce a pair of actually a B a B I need to return a pair of tuples
like that what I have is a function from za to a a and from Z B to
B so clearly I need to use those functions if I apply faz to Z and
a here I get an a a and it makes sense that I will put these days
here a and a and I'll do the same with B so let me write it down new
a a is fa z of new baby is f DZ of Z B so now I've got my Paris baby
and a and now I can return this which is going to be basically this
I'm going to return this Pinner and I'm going to return that pin now
if you compare this and the code appears to be quite similar except
that I don't have an F and I don't have a G and that\textsf{'}s quite typical
of these pairs of functions one of them is equal to the other when
we put identities instead of types instead of some arguments and the
other is obtained using some kind of F map so we've seen this pattern
before and that\textsf{'}s what it will be but that will be in the next part
of this tutorial so in this way we find that this kind of type constructor
has a zip like function which is exactly the same type signature is
a typical zip we'll begin with this if you ignore the extra type parameter
and will be just a typical zip F of a F of B going to F apparently
just like lists instead of f however this F is not a functor is a
much more difficult object perhaps to work with so that was that was
the example I was interested in showing how you can define zip and
imap2 for those profounder now and call non disjunctive and the reason
is these type expression do not contain any disjunctions and actually
if they do contain these junctions and it\textsf{'}s not always possible to
define zip so I call them zip herbal so not all of them are zip about
so it\textsf{'}s just a lot of them are but not all but when they do not contain
any disjunctions and they're always zip alone so with this understood
let us now consider an interesting practical case where we can use
this knowledge and actually make a code simpler in this case is the
so called fusion for fold and the idea is that fold is a computation
that iterates typically over a container and if you need to iterate
several times and then do a computation on the results that\textsf{'}s kind
of wasteful that\textsf{'}s better to iterate once and accumulate more intermediate
results but when you write code for this B it becomes cumbersome if
you have to do it every time you have to write a different complicated
fold function so the idea here is that we can actually automatically
merge several of these fold functions into one so that everything
is computed in one traversal so there are several libraries so one
library is called Scala fooled which is this one where this is implemented
where you can combine different folds of as an example I will show
you if you want to compute the average of a list then you'll have
to traverse the list twice once to define its length and another one
another time to make a summation and that\textsf{'}s wasteful and so you can
merge these folds into one so you can define a fold separately and
then apply merged merge several folds into one and apply that one
fold to a list traversing just once so let\textsf{'}s see how that works so
fold is an operation that takes several parameters so let\textsf{'}s remind
ourselves for instance a list the standard would be our running example
of data so hold left for instance what does it require requires an
initial value of some type B and an operation that takes the previous
accumulated value a new element of the list which is going to be of
type double and returns a new accumulated value so a fold essentially
takes these two values apply it when you apply that to the list so
let\textsf{'}s just have a type that encapsulates these two arguments over
fold left so this is going to be a very simple type just going to
be a tuple of these two values of this just like that so Z is the
type of values in your collection and R is a type of the result so
in order to call the full left you need to provide this data and the
list of course so let\textsf{'}s put this data into a type constructor of its
own so here I call it fold 0 so it\textsf{'}s kind of a 0 version of version
zero of this implementation I put it in Turkey\textsf{'}s class so the case
class is a product and I just put names on it for convenience and
very simple syntax extension will make it possible for us to apply
this fold value to a collection that is foldable now the foldable
typeclass we haven't looked at yet but basically just it\textsf{'}s a it\textsf{'}s
some items it\textsf{'}s a collection it has fold left basically all polynomial
type constructors are foldable and no others so it\textsf{'}s I prefer to think
of foldable as just a property of polynomial type constructors now
it\textsf{'}s interesting that we can define an instance of what Catalan recalls
in variant semigroup oh and what I just called in a previous code
snippet as if Abel pro factor neither of these names are particularly
nice as I said in variant just gives me all kinds of wrong associations
and semi Drupal is a difficult thing to do a difficult thing to understand
because it is not a semi group so semi group all suggests that it
is a semi group but it isn't so zip abou proof factor is the same
as invariant semi global I'm not sure what terminology is better neither
is standard neither terminologies widely used right so let\textsf{'}s look
at this obviously we have our in the contravariant position and also
we have our in a covariant position so again this is going to be a
pro functor not a functor no matter we can do a zip on it and I just
use my career Howard library to implement the IMAP and the product
methods now the product is the same as zip Justin cats library uses
the name product instead of zip IMAP is this Pro functor property
where you you can map fold 0 of Z a two fold 0 of Z B if you have
functions from A to B and from B to a so this is a typical thing that
the proof factor requires you cannot map a pro factor of a into profounder
of B unless you have functions that map in both directions because
you would use this function to substitute in the covariant positions
and you would use this function to substitute in the contravariant
positions and since you have both positions for your type parameter
you need both functions but luckily enough this type is sufficiently
straightforward so that my Harvard library can automatically implement
these methods and then I can implement zip as a syntax by just using
this product because zip is exactly the same type signature as this
product in the Katz library and now how will we use that so let\textsf{'}s
define some actual folding operations for numeric data for instance
the length of a list or sum of elements so the length is going to
be a fold with initial value 0 and updater that just adds 1 to the
accumulated value ignoring the value that is in the collection and
here i'm i've used the numeric typeclass which comes from the spirit
library let me see what was so ridiculous oh yeah here\textsf{'}s the spire
mass library so I'm using the spire math numeric because I found that
standard scholars numeric they are very hard to use inspire has a
good library it has lots of interesting types and I could recommend
using that so for numeric n we have operations such as plus minus
and divided and multiplying so on so for those it makes sense to do
a sum by folding with this update function that accumulates the Sun
starting from 0 now we can combine these two now these two must be
diffs rather than vowels because they have type constraints that cost
constraints so typeclass constraint is really a implicit argument
of a function so it must be a function is not not a value and it has
a type parameter so it cannot be a value anyway can be a valence color
it has a type parameter and/or it if it has an argument type typeclass
constraint but it doesn't really matter you can just do it like that
now if we apply the zip operation and we get a fold that accumulates
a pair of two numbers so the chemo is the length and the sum separately
so we can apply this fold to a list we get this pair and then we can
divide the sum by the length and get the average of the sequence so
in this way we already realize what we wanted we have combined folds
and this is a single traversal because this is a single at full left
operation now this is however inconvenient because we need to do this
combining and then we have a tuple and we have to take parts of this
tuple we would like to incorporate this final computation somehow
already into the fold so when we apply this then all of this is done
automatically and also we don't want to worry about this so much this
we want two things to be automatic how do we do that well so the idea
is that this final computation that takes the accumulated value does
something to it and gives you a different result perhaps of a different
type like here the type of the accumulated value was tuple and in
and the type of the result was just in we would like to put this final
computation also into the data structure that is the fold so that
when we apply the fold to a list all this is done automatically well
easy to do we define this fold one which is the same as before it
has the initial value it has an update here and then it has this final
transform which takes the accumulated value and returns some result
value and the types our za and our so we now we have three type travelers
in this data structure but so what we can have as many as we need
we find a syntax I do this fold l1 now unfortunately the syntax cannot
clash with other define syntax I need to do this full l1 I cannot
just do fold left I would clash with Scala standard from left and
in order to apply this a new comprehensive fold so to speak first
we need to apply it ordinary fold to the initial value and the updater
and the result of that needs to be transformed using the transform
so that is how we apply the phone there is still just one traversal
because there is just one call to actual fold this is the fold left
the foldable class has this hold help function so there\textsf{'}s only one
traversal and we'd like to keep it that way so there won't be any
even if we combine mini folds together it will just be one traversal
so how do we combine the fold while we do the zip and I just told
it to implement nice now the interesting thing that happened after
we added this transform a element to the case class is that now the
type parameter R is only in the covariant position so in the covariant
position the a of course it is a contravariant position here but R
is only in the covariant in the covariant position which means that
with respect to the parameter R this type constructor is a functor
so we use fixed values of other type parameters and only vary the
type parameter R with respect to that it is a functor so we just implement
automatically have a functor instance and now how did that happen
why is it a functor well strictly speaking formally speaking it\textsf{'}s
because the typewriter are only occurs in a covariant position here
but actually the implementation of map for this factor is very easy
you want to transform our to some T just modify this element compose
this function with the function from R to T and you get a function
from a to T and that\textsf{'}s it so basically the map operation is the same
as and then applied to the transform element of the tuple or part
of the case class so that\textsf{'}s why I did not write code here I just said
implement it works it is completely fixed by types now defining the
length and the some operations as folds now we have these type parameters
which I wrote down because you have to not on the right-hand side
but you need to write them on the left and so it\textsf{'}s a function definition
so now you see same thing except I put identity here because it\textsf{'}s
a transforms as identity I need to put it in let\textsf{'}s combine the sum
and the length like this we we do a sum zip line so that\textsf{'}s going to
be accumulator of this type and then we apply this function which
will divide the sum by the length and get the average so the test
works now actually this is still quite cumbersome to write this kind
of thing we would why can't we write just this we can we just define
syntax instead of writing this syntax that would allow us to write
that so here\textsf{'}s the syntax as an syntax extension we define all these
operations an arbitrary binary operation basically does this just
X\textsf{'}s if y exactly like what we did here and then a function and this
is the binary operation so in this way we can easily define all the
binary operations that we want and here\textsf{'}s the code now very easy to
write the double type parameters unfortunately are required or you
could put the type parameters here that would be less intuitive so
this is the way that we can combine folds in a single traversal note
that the same structure is required for scans of scan is like fold
except all the intermediate accumulated values are still kept in in
the sequence or in a container and so that\textsf{'}s how you would apply a
fold operation that we defined as a skin it is very easy because scan
left has exactly the same types of arguments and these two arguments
are what the fold structure gives and then you just need to map with
transform now unfortunately this is going to be twice the traversal
that we had because we do a scan left that\textsf{'}s going to create one sequence
and then we do a map is going to be a second traversal there might
be a way of avoiding it but that\textsf{'}s less important that\textsf{'}s certainly
just to traversals we can combine as many folds as we want in a complicated
way they're still going to be just two traversals so if we can somehow
refactor this to have just one traversal maybe by refactoring the
scan lift itself that would solve that problem that\textsf{'}s not the focus
of this tutorial so here\textsf{'}s an example so if we use the average and
do the scan instead of fold and you have all the intermediate averages
accumulated as you iterate over the list so finally I would like to
point out the difference between applicative and one addict factors
so we have seen that fold could be seen as a not function as a proof
factor and yet it has a zipper ball property and so you can merge
these folds into one now this merging is similar to component wise
information so for example average of sequins is a division of Sun
of a sequence and length with sequins and some and links are completely
independent of each other so that\textsf{'}s a component wise operation in
a sense and so that\textsf{'}s a click ative by our intuition now monadic operation
on the other hand would be such that we depend on the previous results
in order to compute the next iteration of the fold and so for example
we could compute a running average that depends on running average
that we just computed previously so we can combine folds together
but each new iteration would depend on the previous accumulated result
so that would be a monadic fold so let\textsf{'}s see how that works we are
going to continue to use this type constructor because it\textsf{'}s a functor
in harm and as we know a mu naught cannot be not a function could
not be contravariant or pro factor it must be a function so if we
want a monad so that there is a useful kind of flat map then {[}Music{]}
you need to use this we cannot use fold 0 that was not not fall to
fold 0 all right but actually it is more difficult than that because
when you combine two different folds the type of the value that you
accumulate must change it must be a tuple of values that you previously
accumulated let\textsf{'}s look at the type signature of the combined so you
see you combine I skipped over this but if you look carefully the
first fold takes values of type Z from your sequence accumulates values
of type a and returns results of Thailand a result of Type R the second
fold is a different accumulated type and so the final combined fold
needs to accumulate a pair of a B and it returns a pair of our team
and so this is not quite the same as the type signature of a usual
zip because not only this type parameter is modified but also this
one but this is necessary so it is a slight generalization of a zipper
ball or or zip or type type signature but this small generalization
is important and necessary and similarly for the flat map if we want
to define flat map it would have to change the type of the accumulated
value a so here\textsf{'}s what it would be if you want to do a flat map for
fold then it will take three it will take two two arguments one and
be some initial foldable then you take the result of that fold and
the function will compute some other fold using that result and then
you want some how to combine these two into a fold that returns T
but this fold certainly needs to accumulate a as well otherwise you
won't be able to get ours and are necessary for this one to work so
the resulting fold must accumulate here maybe there\textsf{'}s no way around
that so this is going to be again slightly generalized type signature
for flatmap where usually you would not see this these type constructors
would be absent and you would just have f of r r2 f of t returning
f of t but now you have to return this kind of thing and that is as
I just explained unavoidable so how do we do that well we basically
very carefully look at what is going to be computed when we apply
this combined fold so first of all what is going to be the initial
value of this type while the initial value needs to be of this type
or clearly we need to use the initial value of the first fold and
where will we get the initial value of the second type the only way
to get it if is when we apply F to something but to what we need some
R well the only way to get R is when you transform with the first
fold from some a and there\textsf{'}s only one a here which is this initial
value so f of this is the new fold that has initial value B so let\textsf{'}s
take that and put that into the tuple let\textsf{'}s now look at the update
how would they update work now the update needs to be of this type
clearly the only way for us to get a new value of a is to use the
update from the first fold so let\textsf{'}s call it new a now the only way
to get anything of type B is to go through this function f so we need
to apply F to something to what now clearly to some R and the only
way to get an R is to transform at the first fold and the only reasonably
correct thing to transform is in new a well could transform the old
a but then and this is not right because we want the second fall to
depend on the result computed by the first but this is debatable we
could have a situation when this could be a 1 here it just seems to
me that this is better to use an update in value and in a specific
application you want to see you this is so so this is a new fold so
so this is a new value of this type now what we need is a new B so
we need to update obviously this B we need to update with new fold
we get a new B so now this is the new alien new B that we accumulate
so that\textsf{'}s how the the updater works there\textsf{'}s just one place here where
we have some some some ambiguity some choice everything else is pretty
much fixed the new transform is going to be from a beta T and that\textsf{'}s
again pretty easy to understand that we have to get a new fold somehow
and the only way to get it to do this and then we transform with b1
and we get a tea and so let me return a fold that has this in it this
update and this transform so let\textsf{'}s see how this fold works let\textsf{'}s have
the running average of the running average so one way of doing this
would be to do double traversal which is actually quadruple traversal
since each scan is the two traversals but let\textsf{'}s ignore this for now
so it\textsf{'}s a double traversal so first average gives you this and then
when you again do running average of this another scan with the same
fold then you get this now let\textsf{'}s combine the two averages together
in the melodic way so how would that work we do the average one do
a flat map so X is this running average after transformation so it\textsf{'}s
a running average and for each running average return a fold that
accumulates that running average does not actually accumulate values
of Z from the initial sequence it accumulates this running average
for each fold and then we divide that by length so see this is how
we would write that and now a single scan gives us exactly the same
results so in this way we have combined two folds into one in a monadic
way we can use the for yield syntax the functor block which is easier
to read and that would look like this so X is the running average
from that accumulator is the result of this accumulation which depends
on X which accumulates the values of exit actually ignores the Z it
accumulates the values of X n is the running average or running value
of this fold so these variables are to be visualized as running values
of the fold and finally we divide this accumulator by very visual
and we have again the same result so I would like to emphasize the
difference between negative fold combination and magnetic fold combination
is that applicative folds cannot depend on each other\textsf{'}s intermediate
results but monadic fold combinations can that is the main intuition
behind understanding the difference between monads and applicative
factors the packet of factors Express computations that are independent
of each other there are kind of component wise computations but monadic
combinations describe computations that are possibly dependent on
each other\textsf{'}s running values so the previous value can influence what
you get in the next map now there is a library called origami that
gives you a monadic fold but they are actually not magnetic they're
moon advanced if you take a look at that you would see it says monadic
fold but actually they are not the same as what I just described they
must operate on a monad and their update has this type which means
that your updater includes a monad as a as a result and every iteration
of your fold must flatmap over this monad so that\textsf{'}s why I would call
this a mu not valued fold so the result of the fold is a monad value
and so every time you accumulate your result you updated you actually
have to evaluate a flat map on that moment so that\textsf{'}s a very different
thing which is useful in different ways it is not the same as the
Magnetic composition of food so take a look at these libraries in
more detail if you're interested and the final worked example that
I'd like to explain is the difference between applicative parsers
and one addict parsers so again this is really a difference between
how we compose these parsers together just like with folds how you
compose phones together you can compose them in a negative way so
when they don't depend on each other but you must compose them in
a melodic way when they do so the same thing happens with parsers
as I will show now parsing is a very big topic with a lot of different
algorithms and complicated grammars that you can parse in different
ways and I'm going to explain very basic things that you can do easily
while trying to do this on your own you will certainly run into trouble
but that\textsf{'}s because parsing is hard easy languages can be parsed easily
so that\textsf{'}s why I take examples of very easy languages for parsing so
my first language for parsing looks like this it\textsf{'}s either a number
so this is end of file or end of string either either a number or
it\textsf{'}s this HTML tag that I invented with a number inside and a closed
tag or or it\textsf{'}s several of these tags and there\textsf{'}s always one number
inside and the tags must be balanced there must be only one number
and the idea is that you take a square root as many times as you put
these tags so you evaluate this to a number by taking square root
as many times out of this number as you have the tags opened and the
tags must be balanced so here\textsf{'}s an error for example not closed not
opened open closed but then there\textsf{'}s junk at the end which is also
not allowed so those must be errors those must be signaled as errors
so here I just created a type for errors which is just a list of strings
and these are the typical errors that I wanted to detect tags are
closed now I'm not closed not or not opened or there\textsf{'}s no number and
so on so how do I even approach this situation well it\textsf{'}s a very simple
idea is that a parser is a function that takes a string and it tries
to get something out of that string and if it succeeds it gives you
a value that it computes and it also returns the rest of the string
that did not consume so it consumes some part of it and returns the
rest of the string here\textsf{'}s a type that could be used for very simple
situations like this where you take a string and you return a tuple
which is either of error and some result type and also unconsumed
remain remaining portion of the string now if you failed to parse
then you return error of some kind and if you did not fail to first
on your return the value that you found computed in some way and so
this is the first idea is that you will define the values of this
type and you will combine them and the language parser will be a combination
of these smaller simpler parsers so here are the simpler parsers that
I found necessary for this language the first the simplest parser
is that it\textsf{'}s the end of file so the sparser succeeds only when the
string is empty now that returns unit so it\textsf{'}s a parser with unit type
parameter it returns unit otherwise it returns on here so I'm going
to use that when I require that there should be nothing else anymore
in the string and the second parser I found you the necessary is the
one that actually gives an error if there is no content in the string
so it kind of says there must be content in the string otherwise it\textsf{'}s
not right then I have a parser for a number so I have a regular expression
I'm just using very basic tools it\textsf{'}s certainly not the best way to
parse a large amount of data quickly but I'm interested in the principles
of how this works soap our servant will take a string it will match
the regular expression on the string so I'm using Scala regular expression
standard library which operates on string as as if it\textsf{'}s a case match
and each variable here the pattern variable will be equal to the group
that is matched so after this if this is matched and I have this number
and I have the rest of the string and I return this integer and the
rest of the string and if I did not match then I say there\textsf{'}s no number
and I return the entire string I didn't consume anything so in the
same way I define other persons so for example open tag if it\textsf{'}s this
then I return that otherwise I'll return errors close tag all right
so these are my test strings now how do I combine parsers this is
the most important question so the first Combinator is the zip like
Combinator\textsf{'}s applicative come to enter so let\textsf{'}s call it like that
and the idea is that I will combine two parsers let\textsf{'}s say parser a
and parser B may combine them by letting first parser a parse something
then also letting parser be parsed remainder and then I gather their
errors together in an applicative fashion so if they gave a result
great I'll return a tuple if they gave errors and I collect all these
errors and if there are two errors from both of them then I collect
all the errors just like we did initially in the example with either
where we could define the applicative instance or map to by collecting
errors rather than stopping at the first error so that\textsf{'}s the implicative
Combinator or zip of the two processors however note that the second
parser depends on the first because the second parser runs on the
rest that is remaining after the first parser has run so there is
a dependence of the second parser on the first just that this dependence
is not in the type a it it depends on the other things on the context
and the string that is passed around invisibly invisibly that is to
the types a and B the type a doesn't know that there is a string being
passed around so from the point of view of types a and B these are
independent but actually they are not independent so the parser B
is run after parser a has has run however it is applicative nevertheless
because parser a failing does not make parser be necessarily fail
parser B could succeed when parser a fails and then WordPress really
could also fail and then both errors would be collected so that\textsf{'}s
the difference there\textsf{'}s also a monadic like Combinator that uses flat
map where we first do parser a when we discard its result and then
we do parser B so if the parser a fails then nothing will be done
so this will not collect errors from person B another important Combinator
that has nothing to do with monads or applicatives is this alternative
come to meter which is that parser a will be run and if it succeeds
parser B will be round but if bursary does not succeed sorry if if
parser a fails parser B will be run if parser a succeeds parser B
will not be run and so parser B is actually here passed by lazy evaluation
for reasons that I will explain shortly so how do we do that so first
we run parser a we get the result and the rest of the string if the
result is not empty then we just return that if it is an error well
that error is ignored and we try parser B so that\textsf{'}s the alternative
and when we define map and flatmap because parsers are functors let\textsf{'}s
check that is a factor there\textsf{'}s a parser type parser type is type parameter
a is here to the right of the function error so yes it is a factor
now let\textsf{'}s define the language the language is defined like this so
first of all it must be not empty so and this is the melodic Combinator
so everything will fail if this fails so if the string is empty then
everything fails right away nothing else is tried if it\textsf{'}s not empty
and we try to parse the number if so we mount it to double if it fails
then we try open tag and then again we use a monadic Combinator because
if that fails that we should fail that\textsf{'}s either a number or an open
tag and it should not even try every anything else so it what happens
later is again we use language your exits a recursive call open tag
can contain any other thing in the language it can again contain openText
so we do a recursive call and then close that now this is an applicative
Combinator so if this fails we can still check that this fails or
not and the result is then mapped into a square root because we have
here if this succeeded then we had a tag which is a square root tag
so we have to compute a square root so in this way we define just
the language of these tags and then the final parser is this recursive
language parts are followed by end-of-file and this is again an applicative
combination so we can detect junk even if this failed junk at the
end of file will be detected and then we map to the first value returned
by this so then it\textsf{'}s a parser of double finally we define the function
parse language it takes a string and runs this language parser on
the string and takes its value which is going to be either of error
or double and here are the tests it works for example parse language
of this is 123 first language of this is 11 square root of this first
language of that is 10 this is square root of square root these are
not closed so in their junk at end and here we actually find several
errors there\textsf{'}s not opened not closed and here is not opened not closed
so this for example we find that the tag is not open the tag is not
closed and there is a junk at the end so obviously we are able to
find all those errors at once so as a second example of a language
consider this a number surrounded by tags but now the tags are arbitrary
their tag names are arbitrary they just have to be balanced so there
it must be B and B C and C now this is a different language where
the parser depends on the result of the previous parse now here the
first row did not depend on the result because this is parsed independently
of whether it is inside a spirit or not this by itself is parsed and
gives you hundred regardless of where it is it does not depend on
that but in the second language that I consider that is not true because
this needs to be followed by C which so if they said if this were
C that\textsf{'}s an error so you need to know that you have first be here
in order to parse this correctly so parsing this depends depends on
parsing this and that\textsf{'}s why we need to use a monadic Combinator so
I again define a few different small parsers and combine them into
larger parser so I find two parsers that parse any tag and return
the tag name as the result and then a parser that takes a specified
tag that should be closed as an argument so now this is a function
that returns a parser and I'm going to use a flat map so that it is
very similar parsers before I have an end the file is error than I
person number and then if not a number then it\textsf{'}s any tag but the tag
that I parts it needs to be closed right here so in this way I depend
on the results of the previous bars so this is when an addict combination
is is required and here are the tests and just to highlight the difference
so we do find several errors here as well because we use applicative
Combinator here and here as well but now we find fewer errors at once
because for example here remember this incorrect input that was tested
here here we got that it is not opened not closed and junkit and here
we only see that it\textsf{'}s not opened and jumpa then we do not see that
this is not closed because the closing tag unethically depends on
the result of the opening tag which failed and so nothing was tried
so that the parser a little parser that was here never got called
and so it never had a chance to fail and to tell us that the tag was
not closed so unfortunately there is no other way to parse this language
in this in this simple minded approach because we have to use this
flat map we have to depend on the result of this in order to check
that this has the same name so this is a difference between magnetic
and duplicative functors applicative factors describe computations
that so to speak they they occur component by component independently
so there are several parts and each part is processed independently
and the results are all accumulated so map to visa is a good example
so you have several parts that are all processed independently component
by component and the results are put into another container or as
monadic combination means that each next step here depends on the
result of the previous step and that is sometimes necessary however
the parsers is an interesting example because actually as I just showed
parsers do depend on previous results because the parser that is next
gets the remaining string and so it depends on how much of the previous
how much of the input string was consumed by the previous parsers
and so for this reason even if we only have applicative parser Combinator\textsf{'}s
you still cannot say they are fully independent and so for instance
you wouldn't be able to parallelize the person but in many cases applicative
combinations can be parallelized as we showed in the example with
the futures so here are some exercises on this material so here you
should implement map to or IMAP to is appropriate for these type constructors
some of them are not funky and as I showed for the semigroup but you
should do the same for the Montoya to you find a typeclass instance
for the pair then you should define a monoid instance for the type
FS now this is s is another type parameter is a fixed mono ed type
and so it\textsf{'}s just applying a type constructor to a fixed type should
show that this is a monoid this was similar to what was done in the
previous chapter for monads but now for applicative so sure you don't
need F to be a moon and for this and some exercises for parser and
folding Combinator\textsf{'}s this concludes part 1 

all right so this is about goals and the main question has well I
will talk about rules and output about parsers platform is so how
would we compute standard deviation of this list of data well we can
compute it in simple way and beautiful ends so this is always the
length of this list don't give the average refused average of squares
and you standard deviation well this is a simple way of doing it three
men were statistic widths it to give the way of doing this which is
to introduce the creation factor which is n divided by n minus one
due to sample variance all right this is it now the problem with this
competition is that which reverse is list three times every time you
do a length sum or the map traverse this list so actually here matures
it four times because map traverses and then assembles of traverses
so the way to avoid traversing many times is to further wait these
operations as some kind of full bridges and so let\textsf{'}s look at the type
signature of fold Oh blood for example what is the type signature
of net it takes a value B IV and it takes a function that updates
the accumulated value and so it goes over your list starting with
the initial value v the initial value of the accumulator and then
for each value age from the list it holds this function to update
the accumulated value and finally you have the accumulated value that
you output so that\textsf{'}s the type signature of fold so you can do this
with a fold start with for example the computer sound sort of doing
data to the sum you say you fold left so but the Sun let\textsf{'}s put this
into your blog okay so how do we compute them in for example you fold
with initial value 0 and the function that takes helpfully tells me
what it takes so accumulator and bullets would X going to well I'm
going to add 1 to B that went to be the length so I can I can express
those things very easily some is this sum squared is this right RZ
inz right now however we still if we combine those were still ready
to have multiple universes so they are yet but we want to pursue is
that we want to have some way of combining these folds automatically
and the single universal so that we don't have to revolve include
of course just do more work and make a single fold which will do all
of this will accumulate a complicated round will have to collect a
lot of data you cannot just do it in simple traversal like this have
to accumulate the length separately we have to accumulate the sum
separately we have to avoid the average squares sum of squares separately
and the end we'll have to perform this computation so that\textsf{'}s kind
of difficult we'll have to accumulate a triple at least and then at
the end we'll have to do this so we could do that by him but instead
we want to write some code that will automatically combine folds like
these I wanted basically you'd be able to combine these things together
automatic so that it automatically decides what needs to be accumulated
and we don't want to see all of this so the first attempt to do that
would be just very straightforward so let\textsf{'}s look at what we want to
combine so we want to combine bowls so once at forward but one of
them that we render them by the idea is that this data data that you
have to pass before life that is what you want to come you want to
encapsulate this data in a new data type and combine those somehow
and then at the end he will pass the combined data to the faultless
be wonderful left in the end and that\textsf{'}s how we will accomplish we
want combined the day time that both left that\textsf{'}s the idea what is
the data that the form of days the data consists of two pieces or
two parts the first part is a value of type B the second part is dysfunctional
others therefore define a type it has these two values and call it
foap 0 it has a type parameter B and so it has initial value IV and
update function of type the easy one is who I call it Z just so I
need to type parameters actually right so this is the signature of
boatlift f of E and I have some type of their sequence element both
of them need to be type parameters now because I'm trying to generalize
it alright let\textsf{'}s let\textsf{'}s call this a instantly white hole it'll be okay
so this is the data that fold needs to perform its operation now we
can define a syntax to apply our to apply the full duration today
and so we want to want to be able to define values of this type and
fold with them and then we also want to combine them so let\textsf{'}s do one
first first our syntax to perform old love using and that\textsf{'}s obviously
going to be a sin tax extension right it\textsf{'}s going to be implicit class
hold you syntax some type parameter which is going to take a sequence
of a let\textsf{'}s say right let\textsf{'}s that\textsf{'}s miss good for a listing same time
take a list of a and define a function which is going to be all 0
only taking actually messy here take an age old hold 0 of Z of a Z
and the result is going to be a B right so that\textsf{'}s going to be s hold
left of fall 0 needs all all 0 and a done so now we have this syntax
and now let\textsf{'}s define these parents for example playing plan 0 links
in some type actually with but what type would be we fold 0 of some
Z and say double that\textsf{'}s not not very generic now that do need is the
odds be too generic it can be done with the generic and now length
of its list or double right it\textsf{'}s going to be 4 0 of 0 and simulator
bar or interview waiting plus one right so that\textsf{'}s how you find a phone
now we can apply data don't hold 0 of things that\textsf{'}s should be all
the thin I'm going to run this right now for the main errors okay
Sergei what you've done is are you taking this whole operation when
you were originally running as you look at the other nine you stored
that operation itself with case class they executed bigger right so
I started a time that the full apparition needs and my idea is that
I want to be able to combine that big so that they their presents
the folder hasn't yet been done right it\textsf{'}s waiting to be applied and
here\textsf{'}s I still want around this test ok right so so this is how I
will later apply these fools now for mine so what does it mean to
combine I want to have a full 0 of za I don't have a full zero of
B I want to get full 0 see what of a big maybe pair baby right so
if I can do this then I can combine arbitrary folds into a big one
automatically but I can just apply that that would be a single traversal
so let\textsf{'}s see how we can do this so they come that combine function
has a type signature that\textsf{'}s very similar to see lists so I call it
let me call it zip zip zero I have folds your outer for one later
I'm permits I hopefully everybody will show that because that\textsf{'}s also
quite interesting so what is M 0 0 is going to take folds here on
the a40 of CV but to give me from 0 of saying I need a parameters
C B right how do you combine that kind of thing well let\textsf{'}s look at
the type Oh so we haven't - for basically we have a slot for this
basically named to having two parts listen this so we have this for
a and we have this for me we have a we have this function and this
function for me right we need a B and if we seem to be right so we
just want to return new in it and new update and so this is going
to be new in it type a B and you update of this type and if we can
do this we're done we have combined how to combine this well obviously
so much we can do to combine this a and this B alright so that\textsf{'}s all
0 in it or 1 right Elsie\textsf{'}s latest is done now how do we do this well
each return a function that takes this and returns that bullets start
writing the function is see check the types now we need to return
a tuple of Av by using those we can get an aid we get an if you have
an N Z plus F 0 of AZ every one of these sorry F 1 of easy right what
is oh oh yeah f 0 update ok so in this way we have returned the correct
ID and we have used F 0 and F 1 somehow by combining them which makes
sense now is that really what we want well yes because how would this
updater work it will take the previous pair of a B it will update
the first one using the first operator and now then the second one
using the second nominator and it reuses the same C which makes sense
is the same sequence verb folding over so this code in cups awaits
the idea that we're folding two things at once but doing only one
traversal but let\textsf{'}s now define another fold so we define length so
far let\textsf{'}s define some H we know how to do let\textsf{'}s define some squared
which is going to be like this let\textsf{'}s use this zip zero to combine
what will happen some and length which is going to be super serum
of some zero length here and look at the type the type is correct
now it\textsf{'}s going to fold and produce a repair Dallas apply using zip
all right so I'm going to apply holes you on this sum and length and
that\textsf{'}s going to give me a tuple write us all is something length zero
and then I say sum divided by length five point five because that\textsf{'}s
the average of this yes okay so this is how we succeeded already we've
already combined two folds and there\textsf{'}s only one chairs of course this
is very ugly have to do all this and this two plane and all this stuff
and then we have still two separate calculation after this is inconvenient
but we would like the code to be something like this you know data
does fold and then some divided my ladies wouldn't that be nice we
could have code like this and then we could have here a larger computation
some example you know some squared minus some time some something
like this no why can't we do it like this we'll be great we'll be
very declarative you can't do it so the way to do it is to understand
first what he want this once you understand what she wants design
your declarative language then it\textsf{'}s always what you need to do we
already know how to combine both so all these need to be false you
just need to define the creations that combine them and at the same
time perform the final computation after the forward so far our folds
always will give you some tools he wants to combine the tuples in
the final computation into single value once we realize that we want
this the natural thing to do is to put that final computation into
the full data structure let\textsf{'}s define fold one so I'm going to define
fold one by adding final computation of type a to me let\textsf{'}s call it
hard to be more visual so a is accumulating our is result so we're
going to have a structure fold one that encapsulate s-{}- both the
folding and the final computation after falling so now if we combine
these folds then this a could be a complicated tuple type but this
result computation will perform all this extraction out of the two
whole automatically and here\textsf{'}s single value that you want so in this
case we will be able to implement things like this and that what will
be remaining is just indexed we find syntax for this which is reasonably
easy so this is the main idea of how to combine false good thing else
is now just implementation we need to define again sin tax reform
for left we need to define the same things define the zip and need
to esta let\textsf{'}s work let\textsf{'}s just that anything so for one syntax or have
an old one right now old one still does the same things as anything
has updated but then has an extra type track which is going to be
any part right for the result and the result is going to be its own
either side round here and the result is going to be old one the result
of this and the this is of type our okay so everything else follows
pretty much so double-double int chickens well actually I can do this
or I can just say it\textsf{'}s an instant I accumulate and then I have okay
so let\textsf{'}s check that this works actually the things won't compile them
know yet but right now onion is except one which under to define and
okay so let\textsf{'}s do what\textsf{'}s story I remarkable I held upon a needs if
one here he\textsf{'}s someone length warm and is it ones to be old he paid
are one let\textsf{'}s call it a 1 R 1 with a two or two it\textsf{'}s going to be a
1 into R 1 R 2 now the zip still needs to needs to do this needs to
take a to talk and automatically create for me the accumulated type
which is going to be a two-fold will not use to kill plate and the
result type which is good to use it to turn right now let\textsf{'}s implement
the zip it\textsf{'}s a little more complicated it\textsf{'}s basically the same code
as we have the previous tip except we're going to have a transform
a deal right so it\textsf{'}s exactly the same game same code will work except
I need to do a new result and so new result of Type R 1 or 2 is going
to be an f0 result sorry what doesn't he result not the right type
and it\textsf{'}s going to be in one it\textsf{'}s you going to our 140 so it\textsf{'}s going
to be kiss anyone to going to have zero result one diamond one is
not to that you remain right done now just for your reference all
of this I have I have implemented all of this automatically another
another file which I'm not showing I have implemented this is zip
function on Z automatically using a quick outline room keys of marinus
basically implements functions based on type signature and the type
signature of this function is sufficiently respective so that only
wild reasonable implementation can implement is this Margaret I have
very Howard library anyway let me just show you where this this one
so I could just implement it it implemented hold this automatically
this code is in some sense were late it\textsf{'}s boilerplate in the sense
that the types we paid what I have to do there\textsf{'}s not much choice alright
let\textsf{'}s check that this works I'm just going to do some some squares
I need to have more type parameters here or one need add identity
are not transformational one all right done of one some live one and
see this works due to the test name this man in yes alright so it
works but now we can do much better we can actually define average
is it for I'm going to skip the types actually these types are not
necessary if I do a sip one of someone length one and then I what
I want is I want to add a transformation to this alright so how do
I add a transformation now I I can the type of this is this so I could
have a transformation I will take this result and divide it made the
final result let\textsf{'}s have a function of as a transformation so let\textsf{'}s
do syntax extension we have a full one syntax yes where they have
one x what doesn't matter what that means okay so I take a fold old
one so I need these type parameters and I returns something that has
a committed which is going to be and then some function are going
to see is name for one see 80 which is going to be full one don't
open result falls for one result and then okay so that\textsf{'}s going to
be my crew so I'm just I'm going to keep the fold as it is except
I'm going to change its result by a new function f so now I can do
and then some length points with some dividers what is wrong ooh a
hole in it oh because I already defined some but there\textsf{'}s a type of
this this is a two-fold why is that a tooth oh oh yeah because it\textsf{'}s
I can't I have to do in case that\textsf{'}s right okay it\textsf{'}s done so now I
can do this I can just compute this image it is still a bit verbose
right if you have to do all this stuff so in order to make it less
if it was is what you can do let me just show you the code on the
length of time here\textsf{'}s what I don't do I can I can do it there is magic
extension on the phone it\textsf{'}s exactly the same hole except now it\textsf{'}s
permit rise on numeric result and if the result is numeric I can do
arithmetic I can do plus minus and the result is that I can have I
don't have food like this where I directly fold all the one using
this as my and the Nationals and the result is so the syntax is now
almost like we wanted except I have to put the double in here because
in this code it\textsf{'}s completely genetic it\textsf{'}s a generic over numeric numeric
item in whereas in my example quote here I have double everywhere
explicitly so I don't need any type parameters but if you want to
have your code generic then that\textsf{'}s what you need you define your syntax
which is so all this code is going to be in the library easily and
your your user calls upon me like this so this is how you combine
folds into well you can also do you can also use can left with the
same data because scan is like a fold except you keep all the intermediate
accumulated values and so you can you can do scan left or the fold
data and in this way you can just use these fold things as the control
your scam in traditions in so for example you can do an running average
running average is this so I'm just doing a scan with this average
one with a defined can I get this so now if I combine several folds
into one I have put them into the argument of this scan one and there
will be one traversable technically speaking there to traversals with
mmm but I think it really and it there\textsf{'}s only one logically speaking
there\textsf{'}s only one fold with your appliance folders come together and
it encompasses the entire computation so I could express all kinds
of things like standard deviation like that I can also define pure
for fall so for example if you look at my plan here not sure if I
don't have time to go to do this this computation has Falls but I
might need to have constants as well for example a standard deviation
here requires this constant one so I might need to have a constant
as full fold that always returns one so I can define that but me we
do that here in a chamber or one so I headed a transformation defined
and constant old so this is what we call this pure one it\textsf{'}s going
to be some X and it\textsf{'}s going to give old one are see our heart so this
fold one is going to be starting from X 1 to have function that just
gives me X and it\textsf{'}s going to have a function that returns an X whatever
it\textsf{'}s it\textsf{'}s whatever I want I just want to return X I don't care about
any of a connection so if I have a phone like this then I can say
for example here of one or pure of two and I won't have a full that
always returns that constant and I can combine that with other fools
way that I combine for it is in the zero and so then I could amend
the computations isn't one let me show you okay any questions at this
point so we have basically achieved this syntax yeah basically achieved
our goal we can compute like this but I would like to show you now
is that actually this fold is a moment in lavender arm and so you
can implement in in a sense it\textsf{'}s frankly speaking it\textsf{'}s kind of a more
not I you can implement flat map and the only problem with flat mount
is that flat map could change the type both there\textsf{'}s some other folks
and so the accumulator type needs to change now you might have to
accumulate more than one value so a flat map they can in the usual
way does not allow you to change other type parameters it changes
one type parameter but not others it would have a flat map in the
usual way to take a full of art and function from R to full of T and
you're returning full of T however it won't work because you need
to accumulate more Nathan so you're forced to have a different type
parameter and there\textsf{'}s an inner zone so this is not really implement
in a sense and it is a generalized kind of that but the flat map is
a flat map so it\textsf{'}s usable in exactly the same way so after I implemented
this loudmouth which again it\textsf{'}s implemented kind of automatically
just try to try to see what what needs to be done and now first hit
accumulate the first fold and then you transform that when you take
the army apply to function you take the second fold accumulate that
is it\textsf{'}s kind of automatic so the result of this of assure you this
code very interesting-looking I can now write fold combining code
in this syntax now these are different combinators them zip because
they can depend on each other so first I take the average X is the
running average and then I can define a full that uses that X to accumulate
that running average in another accumulated and then I also have the
length which is a previous fall and then I divide this by them so
this gives me a running average over a running average I have a first
Pole and the second fall depends on the first event on the results
from the first I remind you that average or way to compute the result
divides already sung by the ladies this is already complicated for
I'm combining it with other tools so using this syntax it\textsf{'}s much more
visual what I do the result is a fold and if I apply this fold to
a list I get this which is you see it\textsf{'}s growing much slower because
it\textsf{'}s a running average of the running average so in this way I have
combined folds phonetically which means the second fold depends on
the results from the person using zip I combine forms in a way that
don't that they don't depend on each other they all running in parallel
from my signal when I combine them using flat map I can't make them
run so to speak sequentially later for and now this can be an arbitrary
updater but it depends upon the results of the previous fold an arbitrary
way this is this X is not the accumulated value inside this form is
the result is a running result after transformation it\textsf{'}s very powerful
way of combining computations and the result of this is still just
full it\textsf{'}s not yet running it\textsf{'}s just a fall I don't care about its
type here it\textsf{'}s different from zip so saving is duplicative convolution
and for a flat map is the melodic information so this illustrates
the difference between applicative and all not a complicated combination
means that the structure of the computation doesn't depend on previous
results and phonetic combination could depend and this could be a
different form each time depending on this X so that\textsf{'}s a much more
powerful way but yet and yet I have a single traversal so you see
this food inside it there is just a single big updater of motion that
is going to be substituted into the scan left it\textsf{'}s going to be just
a single Traverse with that the beta version and all of that is automatic
all right now I'd like to leave time for questions anything here needs
to be clarified let\textsf{'}s see the code for a cold one again yes for one
is this so it is a initial value of the accumulator it\textsf{'}s the updater
function and the final transform from a college transfer from a to
R so if you look at this pipe constructing the type parameter R is
in a covariant position but the type parameter a is not so it\textsf{'}s not
that functor if you spective are so sorry with respective a so it
isn't funky with respect to R so we could make it it will not only
with respect to honor so unless we add the transform here we could
lose him like we did with full zero not possibly do flatmap is a the
typewriter a occurs in a contravariant position here and the coherent
position here so zip can be implemented nevertheless zip does not
require being so it can be implemented many places but so see I'm
using automatic implementation for words all that I'm using also this
syntax or factor instance this is not not very important for this
you like if you want to know about this asking basically this has
a functor instances of one of instance but only the monarch with this
I change so the time traveller in the middle used to change otherwise
this given to us by Allah so there are libraries but implement these
foods to rest on unfortunately none of them were published in a at
the same time they're very small there\textsf{'}s not much code rather more
more than I showed here but much much more than this there are some
convenience is there are some predefined falls from the Marek data
basically it\textsf{'}s not a lot more functionality on this there are maybe
some more conveniences there are some libraries in differences I don't
think it is in the standard of living of any kind yet not sure something
in it so there are libraries I would say well maybe there aren't so
many use cases where you want to combine these fold but I think it\textsf{'}s
instructive to look at this to get an idea what you can do the basic
idea here is that we wanted to combine communications wanted to do
that you first define a data type that represents your computation
but does not get performance so your computation is fully defined
and specified but not yet performed when you define combinators from
that so you can combine your computation you can combine them as negatives
whereas once you have a pure flatmap have map yep zip so you can combine
your computations the variety of ways and then at the empty run once
you have combined all this your code is done at the end you run the
computation so you can organize your program as these computations
that are really data structures that are combined in flexible ways
but you run it at the end so that\textsf{'}s that\textsf{'}s the main idea and that\textsf{'}s
what enables is a musician all right anybody if remote having a question
alright so that was mine folks I didn't get to talk about our things
but that\textsf{'}s that\textsf{'}s quite seen in in spirit you present Mercer as a
data structure and combine them and you run my question is how you
combine them and the order to use and that you look for civility of
zipping them together or flat map and you know it you know how to
do with those once you find those we thought we have a pure and they
were flat nothing in the map I was here and once again we find that
which is combined in a very flexible way combine all of the divisions
we have a commune specific language of sorts and then you run it all
right well we're out of time thank you very much 
\end{comment}

\begin{comment}
this is part 2 of chapter 8 continuing applicative functors and pro
functors in part 1 we looked at practical examples of applicative
factors and pro founders and their use in part 2 we concentrate on
the theoretical properties of these factors to begin recall that applicative
factors have the map to operation but also they have map 3 not 4 and
so on do we need to define them separately for all n map in which
would be unfeasible perhaps or can we have it in some other way and
the answer to that question leads us to an operation called app before
we look at the properties of this operation let\textsf{'}s try to define map
in on a specific function either of string a so consider this type
constructor which is a factor in a and let\textsf{'}s try to define map in
and use it let\textsf{'}s see if we can do something better than just doing
up in here is map 2 it takes top of a up of B it also takes a function
from a B to some Z so a B and Z or arbitrary types parametrized here
and we return up of Z so how do we do that well for an either that\textsf{'}s
straightforward computation we match if there are two left we use
the monoi against a some string we were just concatenate the two strings
and otherwise if one of them is left one is right then the left remains
because that\textsf{'}s an error of some kind so we propagate the error and
only if we have both in the right and then we can apply the function
if so then we return the right of f of X 1 X 2 so it\textsf{'}s obvious how
to generalize this function to n arguments instead of 2 but the code
would have been very complicated to write and we'd have a lot of cases
so one solution would be to use a list of arguments and record over
it in some way and the second solution is to use curried arguments
which is going to lead us to the app method so let\textsf{'}s look at the first
solution so this map in one takes a list of up aim and returns an
OP of list a because can't without we don't have a function that\textsf{'}s
just instead of a function of n arguments let\textsf{'}s just put all these
arguments into a list so that will be sufficient we can always add
a function after that so we can see how that works if there\textsf{'}s an empty
list we go into an empty list and for a list having a head and tail
we do map to his head and then the rest we use map and one recursively
and the function that maps to uses is just appending X which is the
head to the list T which is in the tail after we already did my own
one so we have an optimist a so T is placed and X is an a so that\textsf{'}s
working but it\textsf{'}s not very great it\textsf{'}s still quite clunky to use because
then we would have to have a function that takes a list of arguments
so using this in practice would be quite inconvenient so let\textsf{'}s try
to see how we can do it better and the starting point is to define
a current version of map to lab two takes two arguments like this
and the function f takes two arguments like this now if we instead
use the curried version then a function f would have a type signature
like this so we would instead of taking a tuple of KB we will take
a and return the function that takes B and returns Z so that\textsf{'}s equivalent
but it allows us to do an interesting thing namely we have now a function
from A to B to Z and instead of taking a pair of a B and returning
of Z we also carry that so we take up a and return a function that
takes open B and return of C so that\textsf{'}s clearly just a rewriting of
map to using a different APA a different type which is equivalent
to the previous type so there is nothing new really except for this
current and a function type instead of unhurried now f map does not
have this type signature but here\textsf{'}s the definition now let\textsf{'}s also
introduce this syntax which will be more convenient remember we in
the short notation we often use this kind of notation when a function
is f mapped over a container so then we can just define that with
this index it\textsf{'}s just to write this instead of that so now once we
do that let\textsf{'}s apply F map to F of this type so why does F may not
give us this why do we need F map to that\textsf{'}s the question well F map
doesn't map this to this what does it do f map would take this which
is of the form a to something and after F mapping we would have op
of a to op of that something so the type will be like this OP of a
going to op of B to Z and that\textsf{'}s not what we wanted to have over here
we wanted above a going to op of be going to off of the so that\textsf{'}s
why F map doesn't work this way and we need F map - so what is missing
forgive me to be able to work this way so what is missing in a map
that F map - does here well clearly what is missing is to be able
to transform this into this so this is the transformation that f map
doesn't have but I've mapped two hands somehow AK map - already does
this let\textsf{'}s denote this transformation by app so this is how we usually
define F so this is the transformation that if added to F map will
give us F map - well we can define this transformation for either
just in the same way most more or less it\textsf{'}s a slightly less code to
write but basically the same code just there there\textsf{'}s one fewer argument
because the function f isn't here see the function if would be in
the F map so by considering app we have simplified our life F map
to has two concerns it takes this F it needs to map it to be and also
sorry I'm looking at that map I've map - it has two concerns it needs
to do this thing with two arguments and also it needs to apply F whereas
map only has one concern it only disentangled the two arguments somehow
and f is handled by map so let\textsf{'}s define F map - through app and F
map to see how works now for convenience that\textsf{'}s defined in fix syntax
for help which will be this it\textsf{'}s just so that we can write we can
write fa b star fa instead of up if i be okay so that\textsf{'}s just syntax
it doesn't change what this function does now with the syntax which
is defined here we can now define f map to via app and death map so
how do we do that well we need to return this function so we take
open up any take open B we need to return an OP see so let\textsf{'}s first
apply F map to up a so f has this type signature applying F map of
F to pop a gives us up of B to Z because F maps a to a function of
time bitters so now suppose we have this X so we could define it like
this now we can use app on that X and transform that X into this so
app of X of app V would be of type opposite so this will be out of
this of OB now that\textsf{'}s kind of clunky but if we write it in this in
fix syntax we write this instead of that and we write this instead
of that so in the Scala syntax these two operators will associate
to the left because the only way to associate to the right is to use
that colon as the first character of the special syntax of the of
the operator so therefore we can simply write this instead of F map
of f of opa of app and so on so the syntax is that first we compute
this so it\textsf{'}s associates to the left so it\textsf{'}s as if we had parentheses
around this around F F map OPA app OPB so and this syntax is very
similar to just as if we had have been able to directly apply a function
f notice this function has this type as if we could apply it directly
to OPA and OPB although we can't because F needs an argument of type
a and this is an Okie of it so it\textsf{'}s a functor of a so we can't directly
apply F to OPA this would be incorrect in terms of types so with these
special separators we can do it now so here\textsf{'}s how we can define F
map 3 OPA will be be obviously going to this what\textsf{'}s the final map
for like this so you see with this weird-lookin syntax we can define
f map 3 of map 4 and so on in a very easy way and we can actually
use that directly we don't need to define f map for and call it because
this code is so concise already so let\textsf{'}s see how we can just use directly
these operations now I just remind you that these operations are f
map and app they're just written in an infix syntax there is nothing
new about them or just F map and app so having these operations let\textsf{'}s
have a little test and do the safe divide so we divide by a number
but if that number is 0 we give an error message so if if it\textsf{'}s not
0 we divide so imagine we have a function of this type double to double
to double because now we need curried functions so now we can just
write code like this F safe divides 2 1 safe divide for 2 and another
test is we want to create a validated case class so what we need to
do is to have a current class constructor now the ordinary classical
structure would be uncared it will take a pair of arguments what we
need is 2 so this will be C to apply so this is the ordinary class
constructor with two arguments and dot curried is a standard method
on a function in Scala so this is in the standard library and the
result is a function of type double - double - C - which is a constructor
of the type C - but it\textsf{'}s clarity so now since it\textsf{'}s carrot we can just
use like that and we can use it like that so in this syntax we don't
need to call map - for example directly we just write if these are
directly applicable this is 2 divided by 1 this is 4 divided by 2
we could just implement safe divide as a syntax as well if we wanted
to and then we would have code that\textsf{'}s maybe easier to read in any
case now we can see that using the method app and in the syntax so
it\textsf{'}s basically app which is defined as this kind of function it\textsf{'}s
it becomes easier to write code with the clickety factors so this
app seems to be an important method that is simpler than F map - or
map - it allows us to define nap 3 in that 4 and so on and actually
it allows us to write code quicker without explicitly calling those
earth map three if not four and so on so basically what we have found
is that F map - can be defined through app if we have app using this
code and we just use F map F and then we apply up to the result that
was our code essentially defined first we do f9f of this and then
we apply up to the result to do this so that\textsf{'}s what we just found
now can we also define app through F map - yes and the way to do it
is to set a the type parameter a here to be the function B to Z we
are allowed to do that because these parameters are arbitrary so we
can just have a special case where a is equal to that and then we
here we would have a type like this be to Z going to be to Z so we
have an identity function of this type always and we can just apply
F map to to that identity function and the result would be a function
of type FA - FB 2 FZ but a is B - Z so the result will be of type
like this and so that\textsf{'}s just F map to applied to the identity function
of this type now because we have defined the type parameter a through
B in Z we have one fewer type parameter in app only two type parameters
in app or as is in F nap - we had three type parameters so it is in
this way that F map - is more complicated than the app so actually
F map to an app are computational equivalent we can define F map to
through app we can define up through a snap - and this diagram illustrates
how that works the type diagram starting from FA go by F naught F
F lowercase F has this type so if mapping it over F a gives you f
of B to Z now you app that you get this function of B to F Z or you
can directly F map to from here to here and that should be the same
and so clearly app is equal to something like F map - of identity
if this is identity so if we take F to be identity then this arrow
is just identity here so these are the same and therefore then app
is the same as f map - over identity so this diagram shows you at
once the two equations the expression expressing of f map to through
up and the expressing of app through F negative and similarly we can
define F map three of my four and so on and here\textsf{'}s a diagram for ff3
it is slightly more complicated because we need to first map to this
then we need to do app with these type parameters and then we need
to do app on this argument we need to take keep this argument constant
and app on this argument with another cz so in order to do that we
need to f map in other words we need to lift up from its ordinary
type which which is this into this type which is kind of a reader
monad with this as the environment so that as a reader functor F map
yeah so so this diagram shows how f map 3 is really defined but in
the code we don't need to worry about this because it\textsf{'}s automatic
because we already carry this argument so we don't need to explicitly
F map when we use the in fix syntax which we have just seen in the
code I would like to call your attention to this pattern that we have
seen before that we have some kind of equivalence between two functions
or two methods and the equivalence works by taking F map and and composing
it with one of those methods and usually one of those methods is a
natural transformation and another is a kind of lifting so we have
seen this pattern several times before where we were able to get two
functions that are computationally equivalent but one is simpler than
the other because there\textsf{'}s this F map but one of these functions already
does and the other doesn't so this function becomes simpler it has
fewer type parameters and fewer arguments so let\textsf{'}s recall the zip
operation that we have seen before and let\textsf{'}s see how that operation
is related to map to to find that and also let\textsf{'}s think about these
two types that are equivalent and their equivalents is given by the
curry and unclear methods in the scholars can standard library as
we have seen so we can take this F NAB tool that we had in the previous
slide the sect map to and we can unclear it and if we incur it will
have a type signature like this which is a tuple and here is also
a tuple so now let\textsf{'}s do the same trick we did in the previous slide
when we substituted an identity function into this F map to in order
to obtain a simpler natural transformation so in this case the identity
function will be of this type rather than what we had before we had
before was this type but we carry no sorry we uncaring now so instead
of a function type will have a tuple type like this so let\textsf{'}s let\textsf{'}s
do that and the result will be we have F map to with this now we take
C in other words equal to the tuple 8 or in B so we set the type C
to be the tuple a B and then we have a function of this type which
is FA of be going to F it'll be because C is tuple a B now this function
is called zip because it\textsf{'}s a very similar type signature to the standard
function zip different sequences where you take two lists you zip
them and you obtain a list of pairs so here F is the list type constructor
so then zip function can be seen as taking two lists of maybe two
different types not necessarily but maybe and returning a list of
pairs so let\textsf{'}s check that again zip and F map to our computational
equivalent so we define zip like this can we define F map to value
zip yes all we need to do is we first need to do zip that will give
us this and then we need to apply F F to F which will take F a B into
F C so this is the type diagram that illustrates this so we do is
if we get that and we apply enough map F and we get F C or we can
go directly from here to here using F map 2 or F which is the uncut
version so this diagram at once expresses the equivalence of the two
equations here because you can take this to be identity this function
and when we do that these two become identical because F map of identity
is identity by the factor law therefore these two are identical and
so zip is equal to F map to all that F of the identity so that is
this equation and otherwise you get the second equation so we can
think now that applicative functors that were initially defined as
something that has mapped to could be equivalently defined as something
that has zip of course work or something that has out there was a
roster equivalent and certainly laws should apply and hold but we
will look at Louis very shortly we could call factors a pebble if
this zip function exists for it and that would be actually weaker
than an applicative as we will see but there doesn't seem to be like
a good name that everybody uses for functors that just have the zip
method and nothing else there are typeclasses for that in different
libraries are called differently for example in the cats library it\textsf{'}s
called semi Drupal in this ecology library it\textsf{'}s called apply or something
like that notice also the same pattern which is a natural transformation
is computationally equivalent to lifting and the second one\textsf{'}s certainly
needs to be demonstrated rigorously we have not done this in the slide
here also have not done this we have indicated that they are defined
through each other but that\textsf{'}s insufficient to show that there are
computationally equivalent you have to show that this is actually
they're isomorphic in other words if you take app define F map to
from it and you take that off map tool and define a new app from it
then you have to show that that new app is the same as the app you
started from similarly here you would need to show that if you take
a zip say and you define as map tool through it and then you use that
F map to to define a new zip then that new zip will be exactly the
same as the own zip that you started with and you have to also show
that if you start from F nap to and go in the other direction F map
to define zip you find new f-type to ensure it\textsf{'}s equal to the old
F nectar now these proofs are very straight forward and we have seen
one of these proofs in a previous chapter in detail so I'm not going
to go through them again especially since it\textsf{'}s exactly the same pattern
where two functions are equivalent and they differ by applying I compose
a composition with F map F so this is a pattern that we have seen
time and time again and so it\textsf{'}s sufficient right now to recognize
it and understand that the proof is exactly equivalent analogous to
the proof we have seen in chapter 7 I believe where this was written
out in full is this equivalence proof finally we can also ask are
the operations app and zip equivalent they seem to be two sides of
the applicative coin indeed they are in order to figure that out let\textsf{'}s
remember that we started out by setting a to B to C function so let\textsf{'}s
do the same with zip we will get this transformation this is what
zip does now can we get an FC out of this and if we get if we do then
we will get an app because app is basically this going to this going
to FC so we could just carry this to get the correct type signature
for app if we could only convert this into an FC now obviously we
can convert this to FC because this is a function from B to C and
we have a B so you can clearly just apply this function to this B
and get a C let\textsf{'}s call that function eval which is this function and
then f map of eval will be a function of this type it is trivial to
define that function then we just do F map of it and we get this so
then we transform from here to here and we just uncurl we get an app
ID write it with two piece just so that it\textsf{'}s different name from app
but this is just the uncured version of app so the the uncured version
of app is equal to zip composed with F map of evil so we see young
exactly the same pattern that app is a zip with some F map although
it this is not an arbitrary function it\textsf{'}s a specific function but
it\textsf{'}s a very similar pattern and so it suggests computational curveballs
so let\textsf{'}s try to define in the other direction how to define zip through
app well that\textsf{'}s done like that so app and its functions that operate
on two different types instead of zip let\textsf{'}s prepare for this we would
need a function that makes appear out of two elements so let\textsf{'}s call
this function here and f map of parallels just denote f map like this
for brevity which I have already done in in previous chapters seems
to be a good notation and then clearly we get this kind of situation
if we apply this F map of pair to some F a of type F of a and we get
this and that is something that now resembles what app likes to take
as a as an argument so then let\textsf{'}s see zip of two different values
of a and F be you'll be equal to app applied to a pair so app has
this type signature we need to give it a product F b2c is this where
C is a to B and this is FB so then applying that app we get F C and
C is a to be so that\textsf{'}s exactly the right type namely F of a times
B so C is 8 times B knotted with sorry C is a times B and so we have
expressed zip through F and we have expressed up through Z and here
is the type diagram we need to take care about what types were using
so that\textsf{'}s why I have written about all these types in foam here the
type parameters for app for example you see app is defined like this
but now we need to use as its first type parameter we need to use
actually B to C so {[}Music{]} actually you know that is a mistake
sorry this needs to be deleted a because it\textsf{'}s not a B to C B this
is AB BC because this is how I defined it so this is just a PC I will
correct this slide so we start with this type we do a zip on it we
get this and then with your F map of eval and then we get F C or we
start with this type we do an app on it which is going to give us
F C directly so I need to delete these two symbols and then and also
these two symbols and then this diagram will be correct and these
type others are correct so clearly we can also do this with carried
arguments we just need to define current versions of zip which which
is like this and then we have exactly the same relationship between
up and fzp so I just call this F zip where it\textsf{'}s great and then f z
PQ is that and it\textsf{'}s exactly the same except we don't need to do the
two plane of arguments that we just need to use the arguments one
by one and now we see that this function takes this argument and this
function also takes that argument and so we can carry that away we
can omit the argument Q and we can write it like this which is nice
because it allows you to reason quicker about what is equal to what
you would have to write fewer symbols you see we can also omit the
argument P because this is now app of pair of P which is firstly apply
pair and a new clean up in my notation let\textsf{'}s function composition
in Y notation goes left to right and here are the explicit type parameters
that you need for this to work having looked at all this we still
don't know what the laws are now we have mapped to we have app and
we have zip let\textsf{'}s now derive the laws for these methods the motivation
for laws comes from our initial idea web map tool is basically a replacement
for a monadic block or a functor block with independent effects and
in other words map two with two arguments and a third argument which
is a function replaces the functor block of this comment so we started
with this we expect their fourth that whatever laws of the Monad hold
for this kind of construction should also hold for map 2 therefore
we just will take moon at louis which we considered in in the previous
chapter and write them replacing the functor block with map to where
where we can so the first two laws to consider are naturality laws
they come from manipulating data in one of the containers so for example
we can manipulate first in this container and that should be the same
as if we acted with the function f on X like this so if we rewrite
that in terms of map tool when we get on the left we get this map
to of this and this and of G on the right we have map to of this and
this and a modified G which takes X\&Y and returns this and similarly
if we apply map on cons to instead of count one we will get the right
natural T law which is like this very similar except acting on Y instead
of acting in X here also the monads have identity laws and associativity
laws so let\textsf{'}s look at those the associative law is that we can inline
the generators in a four yield block or a functor block that is the
right hand side and that should be the same as when we in one when
we write all of these in a single flat for yield block and we can
inline in two different ways and that should be the same so that that\textsf{'}s
the associativity law for the Monad now if we really write that so
the idea is that we have three containers and we have a function of
three arguments let\textsf{'}s say and then we first go over the first container
and then Y Z go over the second container now we need to do this rewriting
because we we need to formulate everything in terms of for yield blocks
with just two lines because that\textsf{'}s map to and for yields blocks blocks
that don't depend on containers don't depend on each other on previous
owners so this is a map - and then this is also a map - so we have
a map - of count 1 and map - of Constituent 3 and then the function
that just takes two arguments puts them into the tuple that\textsf{'}s the
Scala syntax for that and then the function that takes X \& Y Z and
returns G of X Y Z so we need two unto pole like this as the result
of map tool will be a tuple on the right we first apply the for yield
block to container 1 and 2 and so this becomes mapped to of point
one con 2 with the two-point function and then we do map 2 of this
and country with this function which now has two n tuple X Y first
so therefore this is the associativity law for map - let\textsf{'}s consider
the identity laws identity laws expressed in the relationship between
map 2 and the P were united so it appears that applicative factors
also need the pyramid it\textsf{'}s very useful to need to demand that the
pure method exists and satisfy the identity laws turns out that this
constraints so negative factors in an interesting way a useful way
without identity laws they're just too many ways in which you can
define not to satisfying just associativity law so the identity laws
are that if you do a pure on the right hand side that\textsf{'}s the same as
if you just had X equal to a because that\textsf{'}s an empty context or empty
effect so then we have the left and the right identity laws because
the pure or the empty context can precede a generator or it can fall
it can follow a generator service on both of these cases we have this
equivalence and so these two laws are written like that so for example
here is a map to of pure a and the container and then the function
G and on the right is just a simple container map with this function
that substitutes a instead of X and similarly the right identity law
so now let\textsf{'}s derive the laws for zip the reason we want to do it right
away is that these laws are quite complicated so these are very complicated
combinations are all kinds of things that are complicated so basically
map two of conto on map two of County two point three that is simple
so you just change the order in which you apply map to but everything
else is just some kind of bookkeeping that we should be able to get
rid of and simplify somehow so for example we don't want to talk about
arbitrary function G in the law we'd like to simply find that so that
law doesn't contain arbitrary functions so we have seen before that
this kind of simplification can be obtained if you instead of lifting
as you consider natural transformations and so that\textsf{'}s computationally
equivalent but the laws are much simpler we have seen that before
that\textsf{'}s for example the laws for flatten in the Minard are much simpler
than the law for flat map a lot of point but it\textsf{'}s the same thing will
happen here and it\textsf{'}s reasonable to expect that this will happen so
let\textsf{'}s do that and derive the laws for zip so what are the natural
T laws now naturality laws written like this we here needed to deal
with this kind of function where we modify the function by taking
some arguments and so on now it\textsf{'}s not very nice to reason about such
functions so I would like to introduce a short notation for that the
source of difficulty or the source of lack of elegance is that we
need to write these arguments and then all we do is we put these arguments
right back into the function G except one of them gets modified so
whatever we need to modify both so that\textsf{'}s something we would like
to be able to write more concisely and so that\textsf{'}s why I introduced
this notation so first I need use this function product notation which
is a function on a product defined like this so this is just syntax
so to speak and this function is trivial and doesn't let me do very
much but using this notation now we can do a lot of interesting things
for example we can now rewrite the laws from mapped like this F map
tool is just map to where the first argument is the function with
this type and the second argument is a tuple is like as a pair just
swap the arguments as compared to my job now this is the short notation
for first container dot map F you see that then it\textsf{'}s a map to with
this tuple so we read that as a product like this and on the right
hand side it\textsf{'}s a lab two of that tuple unmodified but the function
G is modified in that the first argument gets acted upon by function
f and this is expressed like this in the short notation so we have
a function composition on tuple so this is a function that acts on
the tuple a B and this function applies F to a and identity to be
so that\textsf{'}s a function that takes a tuple a B and returns a tuple F
of a B and then we apply G to that therefore we get G of F of a B
that\textsf{'}s precisely what we have here and so therefore this notation
can be used somewhat shorter then the code and easier to reason about
so these are the two laws that we have so far for natural T this is
the associativity law where I just have this notation G 1 2 3 G 1
2 3 to indicate that this is the function G with three arguments but
this second and third arguments are in a tuple and here the first
and second arguments are in the tuple so I have just rewritten associativity
law in a short notation it\textsf{'}s slightly shorter than this but it\textsf{'}s still
quite ugly and finally the two identity laws which I rewrite like
this pure acting a and then tuple with Q and then this function and
so on that\textsf{'}s exactly the law here and then I write Y goes to jr. a
1 as B goes to G of a B so these are the laws for map 2 so let\textsf{'}s Express
F map to through zip we can express it certain like that written out
with all the arguments in folds like this F map to acting on G and
also acting on this tuple is zip followed by lifted G or F map of
G acting on this tuple and so since this tuple is the same we can
omit that argument in the cred equation where the both sides of this
equation are now functions acting on a tuple q1 q2 so this is how
we can express F map through zip and we can substitute this F map
into the laws and then we will obtain laws for zip to simplify things
we can combine the two natural G laws into one where we use two functions
F and G or F 1 and F 2 acting on Q so that\textsf{'}s how we can rewrite these
laws and naturality then follows four zip because F map G is zip followed
by lifted G so we can just write it here and the right-hand side of
the natural law is zip of this followed by lift G so now you see we
have this followed by left G equal to this followed by lift G \& G
is an arbitrary function so clearly we can just substitute G to be
identity and get rid of it so that\textsf{'}s therefore the cat\textsf{'}s reality naturality
well for zip if this law holds we can add an arbitrary function like
this and we restore this law so therefore the laterality low for zip
is equivalent to the natural go for f- so what does this naturality
law say it says we can first transform both arguments of the tuple
using some functions and then we can zip them or we can first zip
and then we can transform both arguments of the tuple using some functions
that\textsf{'}s a typical form of the natural reality law that says you can
f map something before your natural transformation or you can f map
it after natural transformation the results are the same let\textsf{'}s look
at the associativity law it\textsf{'}s more complex but if we do this substitution
so we take that law as written here and we just substitute F map G
as zip followed by lifted G everywhere so the result is that we have
G of zip and so on equals that so the arbitrary function G has a different
set of arguments on the left and on the right now you see as G 1 2
3 was actually this kind of function where we first I'm to pull 1
and 2 and then we have 3 or we have firstly on tuple 2 and 3 and then
we have 1 so this is what I am indicating in this very informal notation
I don't want to write a lot of parenthesis and so on I just want to
indicate what we want so this is just shorthand for kind of function
and that kind of function but if we substitute zip followed by G here
so now we'll have zip followed by G which means that we actually need
to substitute the full term zip followed by G on the queue so zip
on that followed by G that\textsf{'}s how every written it here now these identity
sorry these tuple transformations all come from this isomorphism which
is trivial that\textsf{'}s just we can undo pull this or we can untap on that
they're equivalent so let\textsf{'}s not have to let\textsf{'}s let\textsf{'}s not write all
of this explicitly every time we are on to playing and so on let\textsf{'}s
just do it when needed whenever we have a tuple of a tuple like this
we just unto pull and rearrange as required and this operation i want
to denote by a special symbol so this is the symbol which is kind
of equivalent it\textsf{'}s not precisely equal but it\textsf{'}s equivalent up to this
a trivial isomorphism so in other words very simple isomorphism that
doesn't require a lot of work and so if we get rid of that then the
G is an arbitrary function so let\textsf{'}s substitute identity instead of
G and the result is this law so we first do a zip on q1 and the zip
of q2 q3 or you first do a zip on q2 q1 q2 and then zip of that and
q3 and these should be equivalent up to rearranging the tuples like
this because the types are going to be different so let\textsf{'}s look at
the type diagram to see what types they are this type diagram is quite
large and so let\textsf{'}s look at look at it in this way so it starts from
these three if a FB FC which I have put into the diagram in this order
just arbitrarily it doesn't really matter now the first thing we do
is we make tuples say of FA and FB that that is indicated by these
two arrows so this is just to make a tuple out of these two also we
make a tuple of B and C here then we apply zip to fafb we get F a
B we apply the epitome of bit of C we get F BC now we can make a tuple
from this and this we get this tuple we make a tuple from this and
this we get this tuple now we apply zip again to this tuple so this
gives us F of that and this after applying zip gives us F of that
now actually they are equivalent to some F of this because of this
isomorphism so this corresponds to that and this corresponds to that
so this is the associativity law for zip its identity laws have complicated
form in particular because we have this arbitrary value a and actually
we can simplify them if we replace this pure emitted by an equivalent
method that doesn't have an arbitrary value a which is an interesting
trick actually so let me explain this simplification in some more
detail because this is a kind of a complicated law we want to simplify
it so here\textsf{'}s how we simplify it let\textsf{'}s consider pure of the unit just
apply pure to the value of type unit there\textsf{'}s only one now you have
that unit let\textsf{'}s apply we get a value of type f of a unit so I called
it w u because it\textsf{'}s a wrapped unit it\textsf{'}s a unit that\textsf{'}s wrapped in the
type constructor F in some way so with empty effect so it\textsf{'}s an empty
value unit wrapped in an empty effect so that\textsf{'}s why I called it wrapped
unit now it\textsf{'}s a pure of one so in order to restore the pure function
all we need to do is to map this one into a inside the container F
so that is what we need to do so in order to express pure through
wrapped unit so these are equivalent you can see that if we use wrapped
unit instead of here then things are actually certified and this is
how we do it so let\textsf{'}s substitute first instead of pure a we substitute
this now this we rewrite using this notation because argument B is
unchanged and instead of one we have argument a so we will call G
on this so you see so this actually is equal to that no way we can
pull out out of zip we can pull out this function using left natural
reality and then we get zip of W u cross Q where this now is outside
the zip it needs to act on the first argument and so this is how I
express that this function needs to act on the first argument of a
tuple second argument of a tuple is unchanged so now it looks like
we have a lot of this bookkeeping where we just put units and so on
so let\textsf{'}s denote the these things temporarily just so that we can reason
about that a little shorter so fine is this function and beta a is
this function so this is a trivial kind of code that just adds unit
and this code just takes a two of unit and B and returns a tuple of
a and B given a fixed value a so just substitutes a instead of unit
and the B is unchanged so this identity function is acting on B and
this function is acting on unit and this product this function product
gives me a function of this type so now this function is a composition
of this kind because we we first take a B we apply feet fight to it
which gives us 1 B so 1 times B when we apply beta a to it which gives
us a times B and when we apply G to it so that\textsf{'}s exactly what\textsf{'}s happening
here if we use that the advantage is that these are function compositions
we can reason very easily about function compositions so if we substitute
into that naturality law which we just had in this form then it becomes
G so this is beta le this is actually beta in my definitions of G
beta a zip now clearly data a can be composed with G and then lift
it because that\textsf{'}s just a functor law that we can lift after composition
and then the right hand side of the natural T of the identity law
like this which is what we simplified here as this composition therefore
the naturality law sorry the identity law becomes this equals that
but here we can also put Phi inside so we can lift Phi first and that
would act on the Q first so now we have an equality that has a common
prefix of some functions we can commit it and the law becomes much
simpler it\textsf{'}s like this so now Phi is this isomorphism between B and
the tuple of unit and B again this is a kind of a trivial isomorphism
but we can apply whenever needed we don't want to keep writing file
all over the place if I lifted to F is the neither morphism between
these two again what\textsf{'}s imagine it is applied whenever necessary we
will express that using this symbol but these are not really equal
but they're equivalent up to applying these isomer films isomorphisms
whenever necessary so the left identity law therefore can be rewritten
like this and the right identity law similarly like this so then let\textsf{'}s
actually simplify the notation some more {[}Music{]} instead of zip
of PQ let\textsf{'}s write this now this is just a symbol I invented it doesn't
matter what it is but it\textsf{'}s kind of a zip zipper symbol if we write
it like this then associativity and identity law look like that now
these laws are basically laws of a monoid up two different types that
I did not write so this is of type FA the size of type FB because
of type FC and assumed transformations that are isomorphisms here
and here natural reality law is written like this so you can lift
F act it on q1 lift f2 acted on q2 it\textsf{'}s the same as if you lifted
this and acting it on the Zipp now the wrapped unit has no laws at
all it\textsf{'}s just a fixed value of type F 1 F of a unit the natural reality
law for pure will follow automatically from the definition of pure
through the wrapped unit this is again the pattern we have seen where
one covalent method is simpler than another and it is it is equivalent
nevertheless and the other method is expressed through the first method
using some F map so that\textsf{'}s very similar tangent and so actually we
see that the laws of applicative have become extremely simple and
suggested there similar to the loss of a monoid there is a social
activity and to identity laws now this syntax of code is just zip
right so this is P zip Q in Scala you can write zip in fix notation
already so you can just say q1 zip keep to zip q3 and you shouldn't
you don't have to worry about parentheses that\textsf{'}s the essence of the
associativity law and identity laws mean that you can have enough
something that you zip with and it doesn't change that then you're
zipping with up to and isomorphism of course it\textsf{'}s they're not actually
equal they're isomorphic they're equal when you apply those isomorphic
transformations in the right places so obviously this is much simpler
than the laws when formulated in terms of map to I already discussed
in a previous tutorial that naturality usually follows from parameter
ECT in code that has type parameters so when we want to check laws
for specific factors then we don't usually need to check naturality
it will be obvious if our code is fully parametric and generic and
all type parameters then it will natural tea will be automatic associativity
of course needs to be checked and identity laws need to be checked
so actually it\textsf{'}s interesting that we have not seen third natural to
move from up to we have seen to natural reality laws and we have derived
naturality laws per zip but actually when we define map to three zip
there\textsf{'}s an there\textsf{'}s one more law one more natural T law to we'll see
shortly and now it became obvious that we are we were missing a law
so our consideration when we derived the laws they gave us two laws
but they didn't give us all all the laws we could have been more clever
right here but we weren't however once we understood how to formulate
the laws in the best way then obviously the right thing to do is to
start with these laws and then derive the laws from map to from those
this also gives us assurance that we haven't missed any laws this
is a very common construction I'm annoyed and generally this idea
of having a social tivity laws and identity laws is a very is a very
common pattern that happens time and again once we see that we have
assurance that the laws are complete this is the complete set of laws
for an applicative functor if we formulate the laws in terms of zip
zip and wrapped unit so we have now great assurance that we are on
the right path we have found the correct laws for duplicative factors
now factors that are zip able as I said before they did not have peer
necessarily they do not have wrapped unit they only have the associativity
and naturality laws but negative factors must have both this and these
laws so as I started from monads well clearly if a functor is a monad
then it will satisfy all these laws - if we define map - through the
mullet construction then we will automatically satisfy all aplicativo
since since those laws were motivated from monadic clause but there
are some applicative founders that cannot be monads and so actually
all monadic factors are negative but not vice versa and this is actually
a mistake it\textsf{'}s a strict superset so there is strictly more applicative
functors than Muniz and as another way in which negative factors are
a superset is that bucket of fun term employment asian may disagree
with the moon and implementation of the function map - in other words
we may want to define lab - and zip and so on in a different way for
a functor that already has a monad implementation but for some purposes
we might want to define it in a different way and we have already
seen that in the first part of this tutorial when a monad would for
example for the either factor and one odd implementation would take
only the first error in a computation and we want to gather all errors
so we define an applicative factor differently so that it collects
all errors so that definition is disagrees with the definition of
map - that would follow from the walnut so strictly speaking we should
rename this factor some and into some other name just to distinguish
it so that we don't get confused because we might want to write applicative
code in the magnetic for yield block and then they would accidentally
have the wrong implementation of map 2 so strictly speaking we should
avoid defining map 2 at the same time as a flat map so that they disagree
but in practice this does not happen very often but it but this is
another way in which applicative factors are a strict superset so
sorry about this mistake it\textsf{'}s a superset so what is the third natural
T law it\textsf{'}s a law where we transform the result of map two we have
not done this we have not thought of doing this transforming the result
of map two should be equivalent to doing map tool like this and transforming
the result of G so when we write it in terms of map two we get this
curve once and if we write it in a short notation then map 2 of G
followed by some lifted function must be mapped to of G on which we
act with that function or if we rewrite the same thing by substituting
explicitly the arguments P and Q which are here count one and count
two then we get this now this law follows automatically if we define
map to through Z because that\textsf{'}s a definition map 2 of G is this but
then clearly if we apply some function f to the result then it\textsf{'}s the
same as if we lifted a composition of G in F because this is the functor
composition law so this is a very obvious property then that is not
reality with respect to transformation of the result of the map so
we could have noticed that we're missing a naturality law firm map
- if we looked at map - type signature we see it has three type parameters
and usually there is a natural T law for each type parameter because
nationality law means we aren't we aren't changing the results of
any transformations applied to a certain type and for each type parameter
we can transform that value of that type separately from transforming
values of all other time parameters and so zip has two natural G laws
because it has two type parameters but my up two has three type parameters
that should have three naturality laws so that\textsf{'}s the third natural
table now I would like to go a little deeper in analyzing what the
properties of app turned out to be so that\textsf{'}s a very interesting direction
because it will show us more deeply what are the properties of app
and what are the laws of app now the laws of app are not so easy to
derive and we will have to prepare ourselves for that so app is a
function of this type now we can consider it as a kind of lifting
so we have this function and we lift it into this but actually this
is not a function this is a type constructor whose type parameter
is a function type so this is not really a lifting of a function into
some other functor but we can think about it like this we can think
well this is just like a function it could be except it\textsf{'}s a little
twisted so it\textsf{'}s kind of a lifting from a twisted function type into
the function of a type constructor to type constructor type now a
lifting should have identity and composition laws just like a lifting
of a factor the factor would have type signature a lifting from F
map going like lips a B to F a FB so that is a classic lifting where
we get our intuition about lifting now lifting in the functor case
has identity and composition laws so just two laws which means that
if we lift identity we get identity and if we lift composition of
two functions we get a composition of two functions that\textsf{'}s very reasonable
can we find the same laws for this lifting can we find identity and
composition laws for it now if we could then we would first of all
need a value of this type F a to a which would represent the identity
for for this lifting and then it would we would demand the function
takes that identity value and returns an identity transformation on
a PHA so how do we get a value of this type well let\textsf{'}s call it by
this symbol identity with this dot in a circle well we have a pure
method on an applicative factor so we can easily create an identity
transformation like this and put it into a pillar and that would be
a functor type value with empty effect because we're pure we were
using pure and identity transformation so that looks like a good candidate
for the kind of value that should not transform anything we will see
that this is indeed the case now the second question is how do we
check the composition lon well unusual functor lifting is clear what
the composition of two functions a to B B to C they compose only and
get a function e to C similarly here you can pose a fail to FB FB
to FC and you get FA to FC so here we have this type however so how
do we compose this and this so that the result is this well it is
not easy to do that necessarily but let\textsf{'}s try to use map to to implement
that composition can we maybe do that because we almost have it we
that we need with map to or we could use zip maybe zip is actually
even easier to visualize here if we zip this this we get an F with
a product a B and BC we can just compose those things in the product
and F map so that\textsf{'}s what the f map 2 gives us in one go with it takes
two functions P and Q now let\textsf{'}s denote by this symbol this kind of
twisted composition where we can post this with this and yield that
so we will define it now using this code which is f map of G H F map
to of G H where the function takes a to B and B to C and returns just
a composition of these two P compose key P and thank you now it seems
that we have defined a reasonable candidate for identity and the reasonable
candidate for composition let\textsf{'}s check the laws what are the laws well
it turns out there are precisely the laws of identity and composition
namely these the composition of this identity with something does
not change that something and composition is associative also there
are two naturality laws which basically say that f map or lifting
plays well with this composition if you can lift first and then you
compose or you can first compose and then lift this and act on the
result and so so you can you can check that these types make sense
the first three laws or actually the identity and associativity laws
of a category where the morphism type is this twisted function type
the identity is this and the composition is that I remind you that
the category laws for identity and composition laws you need to have
just these laws associative 'ti of composition and identity laws the
second the last two laws are natural reality was there connecting
this twisted category composition with the ordinary F map which is
the ordinary lifting that the F must already have because F must already
be a factor so these are the five laws that are very interesting because
they show that app is actually kind of a lifting is it satisfies functor
laws as if we had a functor between these two categories so in this
way we have justified calling up and lifting it\textsf{'}s it\textsf{'}s it\textsf{'}s like a
F map function except the category has a twisted type this is another
way in which we have assurance that our set of laws for the applicative
functor is reasonable it is not too large not too small because this
is just a category law in some suitably defined category in other
words in a category of functions with this type morphisms of the category
have this type I remind you what I call categories just morphisms
where this arrow is just a general symbol and you have to define what
it what it is in any particular category and then you need identity
laws and Composition laws which are these so having defined that let\textsf{'}s
check that this is actually true I haven't actually derived that these
laws must hold they are follow they follow from the map to laws so
I I will now go through this derivation for example let\textsf{'}s start with
the identity was let\textsf{'}s consider this we substitute the definition
of the product dog sorry of the composition which was here and also
we need to look at slide 7 because that defines laws for the F net
so if we substitute that we get F map to of this which is list definition
here but we are working on pure of identity function so according
to the law on on slide 7 this is equal to that so we have F map to
of something and pure so that\textsf{'}s supposed to be equal to this so let\textsf{'}s
just copy this over and we get that but now you see identity function
followed by B is just B so now we have be going to be which is an
identity function lift that and still an identity function so that\textsf{'}s
equal to H so that\textsf{'}s why they are tentative or holds similarly we
derive the right identity law associated with T law we also need to
substitute then according to the third naturality law now this I'm
just using a very short notation where this is a function that takes
two arguments and returns the composition of these two arguments as
functions that\textsf{'}s what I want to use here this is this function this
I just denote this by that it\textsf{'}s a single now we use the third naturality
law and move the function this function out of f map to so then F
map to with this function H K is equal to this function lifted of
have not evolved identity HK so that\textsf{'}s the third naturality law and
once we do that we can do we can just see that whenever we have the
category composition it\textsf{'}s just basically f map of this composition
function and so then every time you have this you have a composition
so then you basically have F map two of these and this is mapped to
that and that is mapped to that but this is basically the same as
a formulation of a socially divisive o4f map to where we had to use
these functions that rearrange the tuples now this function is the
same this is the G in the F map to law here this is the G and these
are precisely the right hand sides and the left hand sides of this
law this is that and that precisely that long and so because F map
2 has that associativity law this must be equal to that because so
we find that these are equivalent so so associative et offereth map
to a group is equivalent to relativity law for this the naturality
laws can be derived by writing out again what is the morality law
for map tool that we have now three natural tables come up to so for
example you do this you act with something on G we write a definition
of the category composition there is no name for this category by
the way I don't think there is but I just think about it as the category
allows me to think of applicative factors as having a lifting sometimes
it\textsf{'}s just a category where this is the composition so this becomes
my left hand side and then I can transform it so just put this outside
put this function outside so I get that put it in here and I have
GH then I I put that is the is the mother this F map to of this alright
I have another naturality right where I act with some function on
the G so that\textsf{'}s the third natural T so I can pull this out of F map
to which is this part I pull it out with and the result is that I
have just XY going to X and then Y which is this so then I pull this
out and this is a definition of G dot H so then I have my natural
to look for yo dot H and similarly the other natural tool so now the
laws for app let\textsf{'}s write them out so app has this type and is defined
like this identity law is that app of identity is identity right so
the laws for app are just the laws of of lift and we like that lifted
identity must become identity and lifted composition must become composition
so these are the laws for app identity law let\textsf{'}s derive it well we
can derive it in a pedestrian way and let\textsf{'}s do it first so app of
identity you applied to some Q so what is that so let\textsf{'}s write the
definition let\textsf{'}s f map of identity and then this is a definition of
this single applied to Q so let\textsf{'}s write it all out we have this F
map two of this functional identity of this type is actually this
function if you think about it but we uncured it so we we need to
incur it because we want to have map to which is uncured you see here
I have this F map subscript to just clear it and F map without substrate
is uncared applied to a product of F X is just this this is equivalent
to this identity and now I'm going to just simplify this by using
the identity law firm F map to which is going to give me this if I
substitute we identity law for F map to which is this and therefore
I have this code now identity of X is just X we have X to X lifted
still identity the result is Q now there\textsf{'}s an easier derivation which
I would like to use now because it\textsf{'}s interesting and simpler consider
these isomorphisms they are obviously they are not the same obviously
because this is a value and this is not yet evaluated you have to
give it an argument and so on but they're equivalent computationally
and then this app becomes basically after this equivalence if you
look at this app it basically becomes a categorical composition of
Q and P so Q has this type you cannot do categorical composition unless
both have function types but because of our P isomorphisms we can
replace a by 1 to a here and so we do that this must have been a double
arrow I will correct the slide so Q F 1 to B can be composed with
pfb to Z and you get something of type F of 1 to Z which is equivalent
to F Z so that\textsf{'}s F Z so that\textsf{'}s in this way we can equivalently rewrite
app as just this categorical composition and then everything becomes
just very easy because then app of identity on Q is Q composed with
categorical identity and that\textsf{'}s just Q by categorical laws so composition
law again they are easy we say AB PQ is just Q P and we just rewrite
that as q GH and then this is where rewritten as app of each of app
of G of Q which is f of h q g q GH now these are equivalent because
of associativity so in other words once we establish the category
laws app becomes a lawful lifting and all the other laws follow so
it\textsf{'}s sufficient to establish the category laws or it\textsf{'}s sufficient
to establish the zip laws and everything else follows so these are
the all the possible ways of looking at the laws for placated functors
so we can choose the one you like best and different ones have different
utility for instance the category laws are not directly so useful
for for coding and they're not so easy to check perhaps because you
need this complicated categorical composition and all these type parameters
I basically have three type parameters in this composition which is
harder to check than zip that has two type parameters but it it allows
us to look at those things in a very general way which we'll see later
so now I will go on to define various constructions that you can use
to make applicative functors out of other types we have seen in the
previous chapter a large number of constructions for monads since
all monads are applicative they all those one etic constructions also
hold but sometimes they hold for weaker conditions in other words
not as an magnetic construction that for example this was also magnetic
construction but it requires both of them to be Mona\textsf{'}s but now we
only require them to be applicatives negatives is a superset of monads
so this is a similar construction but it is a superset and so on so
we'll now go through these constructions and show that the laws of
the implicative hold for each of them assuming that the laws hold
for the parts after which you build 

the first construction is constant factor and identity factor both
of these factors are also Mullins and their applicative instance is
following from the moon unit instance nevertheless let\textsf{'}s look at the
code the constant factor is a factor that takes a type parameter a
and returns a unit type so we could define it as this type function
that takes the type parameter and returns always a unit type independently
of the type parameter so that\textsf{'}s why it\textsf{'}s a constant factor it\textsf{'}s like
a constant type function now for the constant factor there\textsf{'}s only
one value that data can be if the data is of type F of a namely the
unit value and so all methods of the factor including monadic methods
flat map pure map zip we can only return the unit value since all
methods return unit value all laws are trivially satisfied because
the laws say that one combination of methods should be equal to another
combination but all of these always return the unit value so they're
always equal so in this case it is trivial we don't need to check
any any laws here let\textsf{'}s consider the identity function it\textsf{'}s a type
function that takes a type parameter a and returns the same type a
so you can look at it as a type function that is the identity function
at type level let\textsf{'}s define this type constructor like this let\textsf{'}s define
the factor instance there\textsf{'}s a standard very simple code that just
applies the function f to the data there is nothing else we could
do and let\textsf{'}s look at the applicative instance now I define this typeclass
that I called Mujib which is just a typeclass that expresses the implicative
property of the function f by defining methods wrapped unit and zip
i made this typeclass require a typeclass constraint that the type
constructor f should be a factor i also defined a pure metal because
we can now using the factor instance for f we can define pure through
wrapped unit i also define some convenience methods such as getting
the zip evidence value of this type getting the wrapped unit value
for a given type constructor so I can just write W U of F at any point
it\textsf{'}s very quick to get the wrapped unit value I also defined a converter
to catch- instance now on the CAD Slimer II applicative instance requires
pure and app but when you reason about the properties of Lickety factors
and when you check the laws it\textsf{'}s much easier to reason about functions
zip and wrapped unit then about alkanes pure and so they're equivalent
as we have seen but it\textsf{'}s much easier to reason about applicative filters
in terms of zip and wrapped unit so that\textsf{'}s why I defined this as an
as an adapter but I don't actually use the cat lick ative I use my
top class with zip instead and I also define a syntax it allows me
to say if a zip FB which is shorter and easier to read which is equivalent
to this slightly more verbose so to define a typeclass instance for
zip it is required to have a functor type of instance and so that\textsf{'}s
why I will always define first a functor instance for a type constructor
and then a with zip instance so how do we define a with zip instance
well we just need to define the wrapped unit the whoo and the zip
in this case the wrapped unit is just unit there\textsf{'}s no structure that
wraps here anything so it\textsf{'}s just unit so we have nothing else to do
except write these functions like this and we could have written them
using the carry Howard library implementing them automatically because
this code is completely determined by the types there is nothing else
you can do for example in order to return a value of type a tuple
a B if you're given a value of a and the value of B there\textsf{'}s nothing
else you can do in this so you could just say well this is shorter
than implement it\textsf{'}s the same let\textsf{'}s check the laws now the associativity
law says that this combination let\textsf{'}s undo my changes to see what the
code is since we need to know zip of a and the result of the zip of
FB and FC is just that and the zip and the other order is this so
we need to show that these are equivalent but this is exactly the
definition of equivalence which is rearrangement of nested tuples
so there is nothing else we need to do here to prove and similarly
identity laws and say that zip of wrapped unit and FA must be equal
to FA but wrapped here it is just unit and zip is just a tuple and
so this is a tuple of unit and FA and that should be equivalent to
FA but this is actually the definition of our equivalence that we
have equivalence that could rearrange two poles when necessary and
also add or remove a unit in a tuple when necessary so that\textsf{'}s just
a definition of our records so the laws are satisfied by definition
without much kind of calculation here consider now the second construction
this is a product of two factors so if two functions are replicated
then the product like this is a factor that is also applicative now
the product construction is seen in pretty much every typeclass in
the functors product of two factors is a functor monads product of
two minutes there is a moment filter balls product of two filter rules
as a filterable same as for application it\textsf{'}s a it\textsf{'}s a product is always
going to be a construction for every of these typeclasses however
the sum or the disjunction is not a construction for applicative so
the disjunction of two applicative factors in general is not effective
and also it wasn't so Ramona we'll see examples shortly non-intuitive
factors of this shape so let\textsf{'}s look at the code for construction to
now we need to define the type constructor somehow now we can't just
say type F of a because we want G and H as parameters the way around
that is to use the syntax of the so called kind projector in Scala
which I've been using in this tutorial and this is the syntax that
represents the type constructor that is the product of G and H so
it takes that type of parameter and returns this type so this is just
a syntax for type function written like this the lambda is a key word
that the plug-in defines so first we need to implement the function
instance here it is G is a factor H is a factor then we define a factor
of this so we need to define just a founder instance for product which
is a standard code that you take component by component the first
component of a fail you map in the second component of a fail you
map and these maps this is in G and this is an age so component by
component you define it the same way we define the rap unit and the
zip we defined component by component so for example zip from map
things like this we can just take this and this together which will
be the first component of each of them when we take this and this
together to zip which will be the second component of each of them
let\textsf{'}s just like that it\textsf{'}s the first value and the second value so
the first component on the first value is zipped with the first component
of the second value and you get this which is the first component
of the result so component by component we define wrapped unit in
Z the wrapped unit is just a tuple of the two wrapped units for G
and H zip is a tuple of - zips from the image this is in zip regime
and this is a fridge the walls will hold separately in each part of
the Pinner because the computations proceeded in each part of the
pair independently so the first component of the result depends only
on first components of the data the first component of the result
only depends on the first component of the data and so on for the
second component so it is kind of easy to understand the laws will
all hold very easily let\textsf{'}s nevertheless take a look at how that can
be written down so let\textsf{'}s write a social tickity so a pair of gah a
so I assume G X of type G of a and H a is of type h ma so I so this
will be of type F of a when F is this this type constructor we don't
use the name F here but I used it in the slide so if we first zip
the first two together and then take the result and zip it with G
CHC so what does it give you give us first we zip like this the first
two and then we zip the result with the rest now if we apply the definition
we need to zip the first component with the first component so that
will give you this now I stop you writing parentheses here because
this is the zip in the G factor which is associative by assumption
we already assumed that G and H have associated so I don't need to
write parenthesis like this I don't have to write it like this it
doesn't matter if I write it like this or if I write it like this
result is guaranteed to be the same already by circulating in the
function G so also here in the frontal H I don't write the parentheses
and clearly these expressions don't depend on the order in which we
zip so let\textsf{'}s {[}Music{]} let\textsf{'}s zip together the second two and the
result won't be like this we have this pair and then we zip it with
this ugha so again we zip the first component with the second with
the first endpoint the second component for the second component and
the result is this so we get exactly the same result after using a
social activity for G and H so that proves the associative law more
rigorously only identity laws are proving similarly we take the wrapped
unit which is defined like this by our code and we zip it with some
arbitrary GEHA and the result is the zipping of wrapped unit in each
component so we assume that this is equivalent to just gah a in each
component because there are the entity laws as we assumed cold for
G and H already and so this shows equivalence but we need this is
equivalence - yes and similarly we can show the right identity law
the 3rd construction is a free point advantage I already talked about
this in the previous chapter actually this construction as well as
this one of three constructions which I will talk about in a later
tutorial now it\textsf{'}s just a name that I'm using to invite myself to do
this and for this tutorial I am just using this definition this is
a factor Construction defined like this I'm not going to use the fact
that it is a free construction because we haven't yet gone through
this in the tutorials so for the function defined like this assuming
that G is duplicative we need to show that F is also implicated so
let\textsf{'}s see how that works so this is this factor again we will use
the kind ejector in order to denote this type construction and so
syntax will be like this since the type function that takes the type
parameter a and return the disjunction of either a or G of a note
that in the syntax for the type function a is a type parameter that
is only defined in the argument of this type function so this is not
a type parameter here it is not a type parameter of this function
it is a type parameter only of the type expression here so that\textsf{'}s
that\textsf{'}s important to keep in mind so I could use a different letter
here in principle and it\textsf{'}s not a type parameter in in the value here
so the function the factor instance is standard we take a function
f from A to B and we apply it to a here or to a here depending on
where we are in the disjunction and if we are here that we need to
map it over the G function so this is a melting the G factor so we
certainly need to assume that G is a function for this to work this
is Stanley let\textsf{'}s define the zip {[}Music{]} instance so I have put
here a type constraint so that they're both a with zip and a factor
and this is just so that I could more easily use map G if I ever need
it but it seems I don't really need it so let me remove it from from
here how do i define the wrapped unit and the zip so that\textsf{'}s actually
an interesting question because wrapped unit is required to be a value
of type F of unit which is either unit or geo unit so we have two
possibilities to implement this method we could have the left of unit
which is this or we could have the right of G of unit which is the
wrapped unit of G which is this so we could have either this or that
we actually I don't know up front which one is correct we need to
find out so it turns out that only this for fit fulfills identity
laws this one doesn't we'll see why very shortly but at the beginning
I don't know so you know that there are two possibilities you should
have to explore both of them how do we define the zip method well
as if needs to take either a G of a either B G of B and return either
of this a B G of a B so clearly we have several possibilities here
we can have a left left we can have a left right and so on so let\textsf{'}s
match on all these possibilities so we have four cases if both are
on the left and we have a value of type any and the value of type
B what can we return what we need to return a value of this type so
we could return the left of a be like this or we also could return
the right of G of a B where we could easily lift the pair a B into
the function G by using the pure method of the top of the function
G and very easily just left so the question is should we do that we
return this or we return a right of G pure of this turns out that
we have to return this in other words the identity laws will not hold
we'll see why now let\textsf{'}s see what happens when we have a mixed turn
left and there right now in that case we have an a and we have a G
of B what can we return well we can't possibly return a pair of a
B because we don't have a B we have a GOP so we must return the right
part of the disjunction so we have to return right of G of nd now
clearly we have to combine somehow a with G and B and the way to get
G of a B is to use it on G because that\textsf{'}s the only thing we have that
seems to be the right way but then we need to lift a into the function
G and we have this and this is actually according to the identity
law in the function G which should hold this is equivalent to just
F mapping or mapping over G over G B of a function that takes B and
returns a pair in D so let\textsf{'}s just write it like this because it\textsf{'}s
easier to understand what\textsf{'}s happening so similarly if we have a right
on the Left we must return the right of this combination where we
lift the B into the function G and then zip together with G finally
if we have both on the right then we just zip them together and they
remain on the right so you see only when both of the data values are
in the left part of the disjunction only then we can return the left
in other cases we must return it so that\textsf{'}s how it must be now in my
intuition it will be already suspicious if we wanted to return right
in all four cases it means that somehow we are losing information
never returning a left and that\textsf{'}s going to lose information and certainly
that could be an obstacle to satisfying the laws we'll see that indeed
that is so let\textsf{'}s check associativity not verify so objectivity we
could have just written F a zip FB zip FC and substitute with this
code but that would be very cumbersome because there will be 8 cases
to consider it will be a lot of writing in the work let\textsf{'}s instead
try to visualize how this operation zip works in this function and
the way to verify associativity easier is to consider zip of three
values like this with DIF different parentheses first in this way
down in that way but try to define zip in such a way that it is manifestly
independent of the way that you put parentheses so try to reformulate
how the zip is defined so that we can somehow define this operation
directly for three elements rather than defining it only for two if
we are able to define a zip operation directly for three elements
like this in a way that is clearly independent of where the parentheses
are in that a B and C all enter in the same way in this computation
lady there isn't any precedence is not that a and B are first and
then you see somehow then it will be manifestly associative so let\textsf{'}s
see how that works in this case so consider this this situation now
we could have all of them on the left and then we would be in this
situation or return the left after the first pair and also after the
second zip or in the other word will be exactly the same so if all
of them are on the left then it\textsf{'}s just going to be the result is just
going to be to this triple with parentheses here or with parentheses
here but that\textsf{'}s equivalent according to our definition of equivalence
and so that\textsf{'}s associative so if they are all on the left then this
operation is associative the result is manifestly associative now
if even just one of them is on the right then we know that the full
result is always going to be on the right and we know that what what
will happen is that all the parts that are on the left and maybe for
example this is on the left but this is on the right and this is again
on the left all of those that are on the left are going to be lifted
into the front of G using the pure operation and so basically you
could imagine that first we lift all of these that are on the left
into the pure into the G factor and then we have all of them from
the right as if and then we just sit them together so the result would
be as if in the G factor of three values from the G factor and that\textsf{'}s
associative so therefore we have formulated a computation in a way
that is manifestly associative it\textsf{'}s independent of the order of parentheses
and so in this way we figure out that it\textsf{'}s associative it\textsf{'}s much faster
to write out this code let\textsf{'}s check the identity law so the wrapped
unit is on the Left it\textsf{'}s the unit when the left part of the disjunction
which I denote like this in my short notation consider the right identity
law for example so we have some fa which is this type we zip it with
this now if a face on the left then the result will be according to
our definition this which is going to be this left of a and the unit
and that\textsf{'}s equivalent to left away according to our convention because
we can always add or remove units from the tuple and so then the result
is this which is equivalent to that so that\textsf{'}s an identity identity
law if a face on the right you need to lift this into the right and
then zip with FA when we lift unit into G using pure the result is
the wrapped unit which is pure of unit that\textsf{'}s the definition of what
wrapped unities so in relation to the pyramid and so we have the zip
equal to this right of this with a wrapped unit of G now the identity
law for G tells us that this is equivalent to G a therefore the result
is equivalent to right of G which is exactly FA so in other words
zipping FA with this always give you gives you something equivalent
to FA and that\textsf{'}s the identity law and clearly in the same way we check
their left identity if we define the wrapped unit like this instead
then we can see that the identity law will be broken here\textsf{'}s why consider
the zip of a left value with this wrapped unit defined as the right
of wrap unit of G according to our code whenever something is on the
right the result is on the right so the result of this is going to
be right of something now whatever we compute in here we couldn't
possibly or produce a left of anything because it\textsf{'}s already on the
right so it cannot be equal to the left of a our equivalence rules
are that we can rearrange tuples and we can add or remove units unit
values from tuples our equivalence rules do not allow us to interchange
anything else or to change at all anything else so we could not change
left into right and so that\textsf{'}s not going to be covered and similarly
here if we define this to be right then zipping left with anything
could never give a left and so the identity law that would have no
chance and so in this way we see that this implementation is the only
one that respects the laws there are several implementations that
are of the right type signatures but only one implementation respects
the laws so that\textsf{'}s an interesting situation which happens for this
construction now the next construction is the Freeman and over a function
G so G here is an arbitrary contract and this is a recursive type
that\textsf{'}s defined language now we have seen in the previous chapter that
this is a monad and anyone odd is an applicative as well we can define
zip function for a monad like this and we can define pure will not
already has peer and we can define the wrapped unit as pure of one
pure unit so if we have a moon at construction then we already have
the Monad laws the implicative laws are consequences of the Monad
laws when the zip and the pure are defined like this through flat
map so we don't actually need to check any laws here for this construction
because we already checked the Monad laws and the applicable laws
or a consequence when when we defined implicative instances through
melodic instances and the same codes for this construction this construction
was also a monadic construction so we don't need to consider it separately
in this chapter notice that we did have to provide proofs for these
constructions because even though we had anodic constructions of the
same form those required for instance G and H both to be monads now
we don't require this we only require them to be applicatives similarly
here we only require this to be applicative so we have relaxed recall
some requirements as compared with the constructions in the monad
and so since applicatives are a strict superset of monads and we have
relaxed those requirements we have to provide a proof for these constructions
and does not assume units from the user just assume duplicative and
that\textsf{'}s what we have done we provided proofs of the implicit of laws
but only assumed that G and H satisfy applicative laws we did not
assume that G H permanent now in these two constructions there aren't
any constraints on G and H that we changed these are exactly the same
constructions with exactly the same conditions from G H was in the
Monod constructions and therefore we don't need to prove anything
you know they are exactly the same these constructions are stronger
than we need they prove that these are mu nuts and it follows that
these are also ticket ifs but that\textsf{'}s okay that they are stronger and
they prove more than we need in this chapter but that\textsf{'}s fine now the
takeaway is that these constructions also work for negatives because
they are monadic constructions now here are some constructions that
do not correspond to any mimetic constructions these are the last
three the construction six is the constant factor giving a monoid
value so it\textsf{'}s a type Z some type that we know is the 108 it is not
a unit actually we will see this shortly so the type constructor is
defined like this and we don't want to define it as a type F because
we want to keep this Z as a parameter so we again use the kind projector
and we write this kind of type constructor for the type function that
takes type parameter a and returns the type Z independent of a that\textsf{'}s
okay quite fine so as usual first we define the function instance
the functor instance is standard there\textsf{'}s just one remark that I'd
like to make here which is that this function f is not used in the
map function so the map function is supposed to take a value of type
F of a and return a value of type F of B but both F of a and F of
B are just the type Z they don't depend on a and B so we just take
this Z and we need to return a zero we return the same thing it can't
do much else reasonably and we can't use F because we don't have any
age to apply F to and also we don't have any other thing that would
apply to F or any function that would consume this type we don't have
any of them so there\textsf{'}s no way what we could use F in the code of map
so we're losing information but that\textsf{'}s okay so we're losing this F
that\textsf{'}s fine let\textsf{'}s look up the implicative instance the wrapped unit
is monoid empty value this seems to be reasonable what what else can
we return an iterator in a value of type Z but we don't have any data
to compute that so the only value we can return is the empty value
of the monoid and the zip operation is just a monoid combination mono
in operation on a fan of B and we could have defined it in the opposite
order and I will still be valid so this is somewhat this was an arbitrary
choice really so why do the laws hold for this it is because if you
look at the applicative laws formulated like this they look exactly
like monoid laws except that in the applicative laws all of these
are values with type parameters so this is some F of a is a summary
of B the system F of C however if F is the constant factor then all
of these are just the same type Z and then this becomes exactly 1
2 1 1 oide follows there aren't any more type transformations here
they're just this becomes equality and this B\textsf{'}s 3 laws become exactly
the monoid lost so therefore this operation instead of zip exactly
satisfies all the laws and if we interchange B and a it would be exactly
the same laws with being they interchanged where appropriate so this
would be exactly equivalent let us see why this type constructor cannot
be imminent so we could define the pure function by again returning
1 with empty there is nothing else we can return because we can't
reuse use a value of type a to compute a monoid value of type Z so
we could implement pure like this it could implement flatmap by again
returning this F a unmodified now we can't use F at all just like
we couldn't use it in the map so we are losing information in this
function we could also return here an empty value of the monoid or
we could return this value but in any case we would have to lose information
about the function f now the left identity law for a monad is this
which is a flat map with F which is applied to pure should be F but
flat map loses all information about F as we have just seen so this
can't possibly recover F whatever you compute here however you define
here it cannot possibly recover F from a function that ignores its
argument F and so this law could not possibly hold and it could not
hold however you define this you could define it in this way or you
could define it as Z dot empty but still you could not recover F and
this law would not hold so the only constant functor that is a monad
is this factor is the unit because then you're losing information
but there is no information to lose there is only the unit value there
is nothing else that you could possibly return and so you aren't actually
losing any information because there wasn't any information to lose
to begin with so for this reason this is a construction that does
not correspond to an another construction next construction also doesn't
it\textsf{'}s similar to this one except we're having here instead of the pain
we have demantoid Zee now G must be a positive just like here so why
is this construction not magnetic because even if you take for G the
same first factor for example this one the constant constant function
then we will have Z plus 1 which is a monoid but not a moaner as we
just have seen the only constant factor that is one note as a monad
is this the unit so this is going to be not a unit it\textsf{'}s going to be
Z plus unit and so it it\textsf{'}s not going to be Amana so it is impossible
to have a construction like this for a unit where even if we require
that G is a monad still this is not going to be imminent for all G
let'em units let\textsf{'}s see how this construction works for applicative
families so first we define the function instance which is standard
we just apply function f to a under G using the map or if we're in
the left we don't apply it it\textsf{'}s the Z value remains unchanged so that\textsf{'}s
the standard implementation of the functor instance so let\textsf{'}s now look
at the construction so again I have put here perhaps constraint that
I don't need now we need to define the wrapped unit which is of this
type and we have again just like in the other construction we have
two possibilities we could return the left of the empty mandroid value
like this or we could return the right of the wrapped unit of G so
it turns out in this case that the correct way is to return the right
and the left of not know where the empty breaks that the identity
laws will see one define the zip again we need to on the four cases
in the left we just use the monoidal pressure to add together two
values of demantoid now let\textsf{'}s look at what happens when we have the
cross term so a Z on FA is on the left as a Z but FD is on the right
now we have a value of type G of B and we have a value of times Z
we're supposed to return this I'm supposed to return either of Z and
gob can we return this no because we don't have an A we have a Z and
we return and we have G of B we can't possibly get an A from anywhere
so we can't possibly return G of a B therefore we must return left
and the left has only seen so you must return left of this of this
Z and we must ignore whatever was on the right similarly in the other
kids and only when both are on the right then we can zip them together
using the zip in the G factor so in this case it seems we don't have
much choice but how to implement zip we still have a choice about
implementing rap unit let\textsf{'}s check the laws and see how that works
so again we will just do a consideration that zips together with three
values and formulates a result in a way that is manifestly associated
so consider these values if at least some of these three are on the
left then according to our code result is going to be on the left
and we're going to ignore everything that\textsf{'}s on the right so therefore
we will obtain a result which is going to be a monoidal operation
home all of the Z\textsf{'}s that are on the left ignoring everything that\textsf{'}s
on the right we could have three things - Z\textsf{'}s or ones but that\textsf{'}s all
so that\textsf{'}s obviously associative because it doesn't depend on the order
of parentheses and because Z is amyloid and so this operation is associative
in the monoid now the other case is that we have all of these three
on the right in that case the result is the right of zipping of these
three which is associative because by assumption zip is associative
in the function G so we have reformulated as computation in a way
that is manifestly associative the wrapped unit we need to make a
short computation here so for example we have this zipping it with
that if this is on the left result is again the left which is exactly
the same and so that\textsf{'}s identity law if we have G a on the right and
we're dipping it with this then we're zipping this ga with the wrapped
unit which is equivalent to GA by assumption since the identity law
holds for G so we have again right of GA which is equivalent to writing
G in both cases they identity law holds and the same way we check
the left identity law now if we define the wrapped unit as a left
side of the disjunction when we consider zipping of left and right
and the result must be on the left because anything that has at least
one of them on the left it turns the left and therefore it cannot
possibly be equivalent to the right of GE because whatever you compute
is going to be left of something and it\textsf{'}s not going to be right of
G so that breaks the identity law so in this way we find that this
is the only correct definition for the wrapped in it finally look
at the construction 8 is functor composition when G and H are both
applicative factors when their composition is which is G of H of a
is again an applicative factor now this is not a monadic construction
because composition of two Moniz is not necessarily Amanat it may
be or may not be in some cases so it is not guaranteed whereas with
applicatives it is guaranteed I will see an example explicitly in
this tutorial where you have a composition of monads and the composition
is not alone odd but you couldn't get such an example for applicatives
let\textsf{'}s see how that works so we have two factors we define their composition
which is defined like this as a type function and then we define the
map which is standard we just map over H under G so we have a smashed-up
map now let\textsf{'}s define the zip so how do we define a zip we need to
define a transformation like this we need to define also the wrapped
unit so the only way to define the wrapped unit is to do first take
the wrapped unit of H and then lift it into G by using the pure from
G that\textsf{'}s the only way we can get a value of this type the definition
of zip is straightforward we first zip the two genes together using
the zip energy factor the result is a G of the product H a and H B
and then we map under the function f function G we map H a and H B
into H of a times B which is this function which uses the zip in the
edge let\textsf{'}s check the laws for this so I simplified and so let\textsf{'}s check
the associativity the definition is like this like I'm just going
to rewrite it a little shorter because it\textsf{'}s easier to reason about
when when notation is shorter once we have the zip we map that with
essentially zip in the age factor and when we have three values JH
h HB HC when we accept them like this then this is the result now
in order to simplify this use natural tea loafers if the naturality
law says that for this zip if one of these argument one of its arguments
has a map working on it we could put this map over here and work on
the first type inside the zip leaving the second type unchanged that\textsf{'}s
that\textsf{'}s a natural allottee law let\textsf{'}s remind ourselves of the naturality
law that is here if we have a zip that on which some function works
on the argument then we could pull this out we have a zip of those
things without anything acting on it but we can act on the result
of zip by the product of functions so that\textsf{'}s maybe it\textsf{'}s a little easier
to see here so zip yes so zip on which some functions have worked
is equivalent to zip followed by some map so we will do that and we
will pull this outside so how do we put it outside well the result
is that we we still have this back here but before that we have this
map of zip H which takes the first tool it\textsf{'}s a product of two functions
one the zip age and the other is identity so this isn't changed and
zip age is now acting on these two because it was acting on these
two so I'm just using the shorthand but me and it\textsf{'}s kind of it it\textsf{'}s
the same as map of case image being so this map of the page here it
was like this but now we need to apply this to the result of zip which
is going to be of type G of H of M be sr g of h a times H B and H
so then HC needs to be outside of this tuple so that is the result
now we can combine these two maps together using the function composition
law just take this we first compute this and we match on that with
this case and we have this result now obviously this is a associative
combination for zip age where we quickly could commit in parentheses
and so when we do the same computation for this expression we get
a similar function with parentheses in the other place and if we compare
these two we see that these are equivalent because the G factor satisfies
associativity and these are equivalent because that each function
satisfies associative ET and these are just equivalent by definition
and so we have found that the equivalence holds the identity law is
satisfied as well for the in the same way we do a zip with map we
use the identity law for zip in the g-factor first identity law says
that this is equal to gh a dot map of this function and then we just
add zip H on top now this becomes the zip H of wrapped unit H which
is equivalent to H a so this becomes H a to H it becomes identity
function so mapping with identity function does not change the result
so therefore zipping gh a with wrapped unit is equivalent to G eg
similarly we can check the right identity laws so these are the constructions
that I was able to find that build new applicative factors out of
old ones now I would like to give another example of an applicative
factor that disagrees with its own net so what what does it mean exactly
well this is typically a situation when you have a function that has
a moon an instance and also it has an applicative instance and sometimes
you would like the implicit of instance to do something different
than what what a moon adds definition would do we have seen this in
the first part of the tutorial where you would have either moment
which stops up first error but you can define an applicative on either
that accumulates all errors so this is an example of a positive that
disagrees with its moment it\textsf{'}s often the case that you want will not
to agree with applicative but sometimes we want them conditionally
and here\textsf{'}s an interesting example of type constructor where it is
easy to see that it\textsf{'}s really reasonable for them to disagree this
type constructor is basically a lazy list so I will explain what that
means if you delete this function from unit to that and you would
have a definition of the list it\textsf{'}s a recursive definition of the list
factor adding this function arrow means that elements of the list
are not yet computed necessarily you could compute them by calling
this function you can always call it it doesn't require any extra
date and just a unit value which you always have so you can always
call this function and get the next element and the tail of the list
on which you can again call this function get again next elements
and maybe some computations could be encoded in this way where maybe
it\textsf{'}s expensive to compute these elements and this is only done when
you want them so in this in this sense it\textsf{'}s a lazy list it does not
have all its elements already evaluated it\textsf{'}s waiting until you need
them when you need them you call this function and you get your next
element so let\textsf{'}s see how we define this is a list as application factor
this is the short type notation that we have just seen and since this
is a recursive type we cannot use the kind projecting we have to use
a class also cannot use a type alias we have to use a class can you
find it in Scala so let\textsf{'}s do that and I'm just using a very direct
encoding of this like so it\textsf{'}s either unit or a function from unit
to this now in Scala this syntax does not actually mean a function
with unit argument it\textsf{'}s a function with no arguments with an empty
list of arguments Scala has this syntax it doesn't really change anything
for us just a little less typing if we wanted to have a function with
unit argument it would be like this just more typing for no game in
particular let\textsf{'}s define some utility functions so that we can easily
work with data of this type first what\textsf{'}s doing empty list which is
just returning this first unit which is left of unit let\textsf{'}s define
a function that takes an ordinary list and creates this lazy list
it would be useful for testing the idea is that we would create F
with a right which would have a function that will return a tuple
of the first element and the rest the rest will be again F of right
and the gamma function will be the second element and so on and finally
would be an empty list so that\textsf{'}s easily done we match on the list
if it\textsf{'}s empty we return the empty which will you find here if it\textsf{'}s
not empty we match on head and tail will return a F with a right inside
and the function like this which will return a tuple of head and again
a value of type F of a which is the result of applying the same function
list to the tail of the list so that is easily tested now we can create
value like this and in order to get anything out of those lazy lists
we need to actually call the function so this fetches all those things
and calls the function and the result is that you have a tuple with
one element which was a and the rest which is again the same story
again you have to do this in order to get anything out of it so let\textsf{'}s
for convenience convert to ordinary list so that\textsf{'}s how we convert
straightforward function I would call and then call itself again so
these functions are not tail recursive just using it for tests right
now so these tests check that we need these lists work first you find
a factory instance let us straight forward because it\textsf{'}s just a function
so we need to create a new function where we replace arguments and
do a map recursively and let\textsf{'}s create with zip instance and applicative
instance and that\textsf{'}s where the interesting things happen so the first
question is what is the wrapped unit no we could in principle return
a left or we could return a right because they're the type has an
either so we could return the left or we could return the right we
return the left and it will be just an empty list so the wrapped unit
would be an empty list of type unit if we return a right then we could
return a function that has this a which is a unit and then again returns
the same wrapped unit that\textsf{'}s what we do in fact it\textsf{'}s a recursive definition
that in effect it is equivalent to a never-ending sequence of unit
values in the list because the list is lazy it doesn't actually evaluate
infinitely many elements but if you request the next element if you
evaluate this function and you would get again the same function that
would generate the rest of the list so the rest of the list is exactly
the same as the list itself and so in other words it\textsf{'}s a never-ending
sequence it\textsf{'}s not actually an infinite sequence in memory of course
it\textsf{'}s conceptually equivalent to the infinite sequence because you
can request next elements as many times as you want you will still
have always next elements that I found the same elements which is
empty so converting this to list would be a stack overflow or memory
over out of memory error fix the Stack Overflow first now zip operation
is interesting but it\textsf{'}s really trivial in a sense because if one of
them is empty will return empty and if we have two functions so GA
and GB are functions of this type and we again return a function which
just has the tuple of the two first elements and then zip in the rest
recursively so this is a typical implementation of a zip of two lists
except that we need to dance around with these function calls we have
these functions and we need to match in a different way and and so
on but other world other than that it\textsf{'}s the same and so zipping a
list of two elements with a list of three elements cuts this list
of three elements to length two it gives you this as a result so this
is a standard behavior of the zip method on lists but also this wrapped
unit acts as a unit if you zip it with some list the result is the
same as the initial list after the equivalence transformation so how
does it work well since the wrapped unit logically represents and
never-ending Western unterminated sequence in this list is finite
the zip function will cut the longer sequence because that\textsf{'}s the code
if one of them is is on the Left which is an empty list then the result
is an empty list so whenever get an empty list we cut and so that\textsf{'}s
going to be the result indeed zip defined in this way I can't do much
more than cut because if it didn't cut we would have to produce here
another element of type string unit but there\textsf{'}s no string to get to
right here logically you would have to cut certainly in some applications
this is not what you wanted but in many applications that\textsf{'}s what you
want you to cut the laundry list and you could conceivably you could
do other things but what if the first list is empty when do you didn't
there aren't any values for you to fill you have to cut and sew however
if you use the list monad the definition of pure is not the same it
is not an infinite sequence it is a sequence of length one so the
pure would be just this be in one element list and the standard zip
function zipping with this would certainly not do what you what you
want it would cut at length one now certainly if you define zip through
the munna instance and not in this way and you would have a very different
behavior then of course the laws will hold just zip defined from the
monad will take each element from the first list and each element
from the second list and put all those pairs as a result in waste
so for example zip of waste and one to waste of 1020 it would be a
list of 1/10 1:22 10 to 20 so certainly this is very far from the
standard function zip on lists it could be reasonable for some applications
but the standard zip function doesn't do this it does that and therefore
for the standard zip function the correct wrapped unit is this infinite
sequence P not this and so this is an example where the standard zip
function which suggests an applicative disagrees with the mu naught
which is also standard for the standard flat map and disagrees with
that and so that\textsf{'}s but but actually they're both useful in different
contexts so this is exactly the kind of example I was talking about
and the reason that this disagreement might be troublesome is that
if you defined for example code that has a for yield construction
when you might be able to simplify it using zip what if you notice
that you have this kind of construction and you say all that this
is obviously a map - this is a map - or this is a zip like this zip
and this would be a map - and then you will be tempted to refactor
this code by doing something like this but if the zip implementation
disagrees with the Munna instance then you would have changed the
functionality you cannot replace this code with this code unless the
applicative instance defines map to in the way that is exactly the
same as what would follow from flat Mac now if you are using a type
constructor for which the model instance and the implicative instance
disagree then you should never refactor code like this but you might
be tempted to or you might just do it without thinking it\textsf{'}s very confusing
but these things are not the same and so it is recommended therefore
to avoid these situations are not to define applicative instances
that disagree with model instances if you need that the easiest way
out is to rename then a type constructor to some other rename have
an alias or some different type would only have the implicative instance
but not a model instance and the first type would be a monad and you
would only use the first type when we when you use as a monad in the
second when you use as applicative so but in this way you avoid potential
for bugs that would come out of this kind of confusion so finally
let\textsf{'}s look at some examples of non applicative factors so these are
the examples now the first example is the disjunction of two reader
mode ads essentially so reader mode on of course is applicative also
it is this construction but their disjunction is in general not implicative
another example of not duplicative is this this is a functor in a
because a is here in a covariant position so this is to the left of
the function arrow so this entire group is contravariant in the controller
in position but inside it a is also in a contravariant position so
that cancels out and the result a is covariant here similarly here
is covariant and also a is here covariant to the right of the function
error so these are functors but they're not applicative let\textsf{'}s see
how we can verify that so i defined these type constructors i also
defined this G which is I've just reversed these two errors the result
is that it\textsf{'}s a contra factor it\textsf{'}s not a fuck function anymore but
it\textsf{'}s very similar in structure so now these are our three examples
F H and Kane I'm going to try to implement the zip function using
the curry Harvard library so I'm going to ask a Craig Howard library
to implement any code that has this type signature and this is the
Olav type method so I'm going to find all of these so that the result
is going to be a sequence of implementations and then I'm going to
look at those sequences so this is all happening at compile time and
so these tests run now the length of zip F is 1 which means that there
is one implementation of this function which is this type but when
we look at the code which is printed in a short notation we see that
the code is that as function returns always none so it always returns
an empty option a zip function that always return an empty option
is obviously going to violate laws as for instance identity laws well
it won't violate a social Timothy because always returns the same
thing but it\textsf{'}s going to violate identity laws that require it to preserve
information and since the identity laws say that zip of something
with the ident with the wrapped unit must not change the something
so it should not lose any information if zip losses information then
it\textsf{'}s certainly going to violate the identity walls and so the only
implementation we have of this type signature which is the zip type
signature from this funding is going to violate identity laws so there
is no good implementation and for these other factors there\textsf{'}s not
a single implementation at all of this type so those types could not
be instantiated could not be implemented by any code so there\textsf{'}s no
code that\textsf{'}s generic and all these parameters that implements this
type signature and also this type signature but for the G there are
actually two implementations G\textsf{'}s a country factor I just took this
and I reverse the arrows and it turns out there are two implementations
so that\textsf{'}s interesting and so let\textsf{'}s look at contra factors now all
a blicket of puncture laws that are formulated via unzip and wrapped
unit actually don't use the map function so we can formulate the same
laws for contra factors so we can say an applicative contra factor
is a contra function that has a zip and wrapped unit methods with
the same type signature and the same laws identity laws law and social
dignity law so that\textsf{'}s the definition so we will look at constructions
shortly but one thing we need to keep in mind as a contra funders
are different from functions and that they don't have map they have
contour map so for instance you cannot take a wrapped unit and map
it to get a pure out and because you don't have a map you have a contra
map and so it slightly it\textsf{'}s slightly different another thing is that
we always have a function like this just drop B and take the first
element out of a tuple if you control map with this function of CA
then you get C of a B because that goes and you're in the opposite
direction CA with this function goes from here to here and to see
a B which looks like what you want to implement the zip you take the
CA you just contra map it and you get c but that\textsf{'}s invalid as an implementation
of zip because it loses information about CB and we know that would
violate the left identity law if you put identity here you should
reproduce CB on the right hand side but you lost all information about
it so naturally T must hold but with contra mapping set of map so
that\textsf{'}s another difference now if you try to control map this with
a function from one to a you can't it\textsf{'}s the wrong direction you can
confirm map it with a function from a to 1 and you get C away for
any a so that but that\textsf{'}s kind of that\textsf{'}s different from pure but it
can be seen as an analog pure has a type signature a going to F a
this does not have a going to anything you don't well you could imagine
that you have it but you don't use that information so you don't use
any values of type a to create this already can't read them so that\textsf{'}s
yet another difference and also there is no contra app there is cerebral
mind there is no analog of app for control factors so these are the
these are the control factors and indeed we'll see this is one of
the constructions that the disjunction of to a positive control factor
is again a pretty check this is this is this construction which we
will look at shortly so all right so now we verified using the curry
Howard library the these factors are not applicable by trying to implement
the type signature of zip and finding in this way that for this factor
there is one implementation of this type signature but it loses information
and so it could not possibly satisfy laws and for these two types
we found that there aren't any implementations at all for the type
signature of zip now I should comment here that actually is quite
difficult to prove that there are no implementations of zip it\textsf{'}s actually
quite difficult to prove so I'm using the very hard library that performs
exhaustive search of possible implementations but certainly it\textsf{'}s not
really a proof I'm just running some code maybe it has bugs so it\textsf{'}s
hard to actually produce a good proof and I'm not going to do it it\textsf{'}s
it\textsf{'}s if you if you go back to the Carey Hubbard correspondence tutorial
you will see but this is equivalent to finding a proof in the constructive
logic or or showing that there is no proof so there are methods for
showing that there is no proof proof theory gives you tools for doing
this but it is difficult and cumbersome and so that\textsf{'}s why I'm using
the curry covered library in which I have a certain degree of confidence
where I can just ask it how many implementations and given type signature
hands so having finished with constructions we notice that some of
them contain one nodes so you know it\textsf{'}s seem to play an important
role they are very similar to applicative function in some interesting
ways so let\textsf{'}s actually ask what are more node types so what are the
types that I could use here in this construction the answer is surprisingly
in Scala any type ism or not all non parametrized exponential polynomial
types are mono it\textsf{'}s what does it mean we have mono at constructions
and in the previous chapter and I could have made this observation
already in the previous chapter that these three constructions give
new monoids out of previous ones and also that all the primitive types
integers floating points and so on sequences and strings unit all
of these types have at least one way in which they can be implemented
as monoids instances of the monomial typeclass for instance these
are all like numbers they can be added together and that\textsf{'}s a fundamental
operation these are all like sets where you can have an union of two
sets so here is a sequence and union is just concatenation and this
is a set Union and this is a map merging Union but basically they
are set like their MA nodes and strings can be concatenated because
they are grooving to sequence of integers and unit is a trivial annoyed
and case classes you can define using these constructions and function
types you can define using this construction so a function from say
float to a sequence of strings it\textsf{'}s just one of these constructions
applied to one of these types and so they're monoids so all exponential
polynomial types that is all types constructed from these three operations
starting from the primitive types are going to be always one with
at least in one way sometimes in more than one way so here\textsf{'}s an example
consider this type expression in Scala code this would be implemented
as a sealed trait with three case classes because there are three
parts of the disjunction this is a monoid I don't have to worry about
how to implement it because I have these constructions and once I
decompose this into some of these constructions starting from primitive
types I can just generate the monorail instance automatically or mechanically
notice that this does not have any type parameters if I have type
parameters then I don't know if that type is a monoid I don't know
how to implement and the moment instance for it maybe it will be able
know it when this type expression is actually used in my code but
I don't know that so I don't know how to implement and here\textsf{'}s an example
of an envelope type with type parameters so if this were a to a that
would be already a monoid it\textsf{'}s a function one weight with function
composition but here I cannot compose two functions of type A to B
and get a third function again of type A to B no way I don't know
how to combine A\textsf{'}s I don't know how to combine B\textsf{'}s there are unknown
types if I knew that they were more nodes but at least be if I knew
that B is I don't know that\textsf{'}s enough already I have this construction
but I don't know that I don't know how to combine so that\textsf{'}s that is
to say all types that don't have type parameters and our exponential
formula such as this one they're all Minuit\textsf{'}s so it is in this sense
that I say all non traumatised exponential trinomial types and paranoids
very interesting conclusion follows from it namely that constructions
one two six and seven give us a way of expressing all polynomial factors
with monoidal coefficients as applicative factors starting from one
nodes we can build applicative instance for any polynomial factor
so how to do that the short summary of this of this construction is
that we need to rewrite the polynomial in this form which we always
can do isomorphic Li we can transform the type into this form just
like in school algebra this form is called Horner\textsf{'}s scheme for polynomials
so a is the argument and these are coefficients in school algebra
that would be numbers constants and this is the variable in the polynomial
and so because we can write it like this these are just a sequence
of constructions that you apply and you get an applicative function
so let\textsf{'}s see how that works suppose you have a polynomial factor and
which you can write in a short type notation like this you have some
coefficient that is constant type could be complicated but it\textsf{'}s constantly
you know it could be like this but it does not have type parameters
it\textsf{'}s a constant type multiplied by some number of A\textsf{'}s or some power
of a but a fixed number and another monoidal coefficient so we assume
they're all one which is Z and Y because they are of this kind so
they do not have type parameters we can start with the highest power
of the polynomial like we do in school algebra and then some smaller
power and then finally we go down to power one and zero with some
constant types here so this is one way of writing a polynomial which
we can always do another way would be then to put parentheses in like
this so starting from the lowest power we take a out of parentheses
and then in fact factor it out the result is a polynomial of lower
power which we can write again factor out in the same way and finally
in the middle of it there will be some last a and Z Z is the coefficient
and the highest power of the point number so we can always transform
the polynomial into this into this shape into corner scheme and notice
here some steps could contain more than one a so for example it could
be like this because some coefficients could be zero conceptually
speaking or zero type as also something we could use here for generality
but we don't have to each of these steps corresponds to construction
7 which is this Z plus some G of a actually I think it was called
and construction 2 which is multiplying a and G of a where G is already
negative and a Z can be then used like this they can add Z and or
you can multiply by a and that\textsf{'}s still applicative these are the constructions
construction 2 and construction 7 so indeed we can find that it is
a picketing but how does the implicative instance actually operate
so what does the applicative method wrapped unit for example do or
zip what do they do two types like this how can we how can we visualize
those things so here\textsf{'}s how the wrapped unit for construction 7 is
this we have found and wrapped unit for construction - is this and
so the wrapped unit for the entire polynomial is going to be every
time you step through construction 7 you discard the Z so you you
go to the right like this in other words every time in construction
7 when you go you discard this then you keep a discard this you keep
any discard that keep any discard that keep a and finally you're here
so you have discarded everything but Z eh eh eh in other words everything
but the term of the highest power of the polynomial so therefore the
wrapped unit for FFA is of this form is the highest power term the
polynomial where you take instead of Z the unit value or empty value
of the monoi oil and instead of a you put the unit values so that
is going to be the wrapped unit let\textsf{'}s look at how zip is defined so
it would be good if we visualize for example zip of these two terms
then if we can do it then since zip is distributive one zip of this
and some other polynomial essentially is just one zip of this train
because a value of this type must be in one of the parts of the disjunction
so it\textsf{'}s one of these either this or maybe it\textsf{'}s this so it\textsf{'}s sufficient
to be able to compute zip for two monomials like this so let\textsf{'}s see
so construction seven says that the result of these is of type RA
a because construction seven says if one of them was on the Left we
discard what is on the right in other words if we are here for example
we discard this so if two polynomials are zipped together we discard
the one that has higher power so we discard this as discard the power
so we discard the coefficient at the higher power and the result is
the coefficient of the lower power that that remains and we need to
discard so we will go through these and we will discard as many of
these B\textsf{'}s as we need to in order to have a pair for each a so in other
words we'll discard B 3 and before here and we'll keep B 1 B 2 actually
it might be that we discard B 1 B 2 and we keep B 3 before that could
be an equivalent equivalent definition doesn't matter right now we
just want to understand the principle so the principle is first of
all we choose the polynomial of the lower of the two powers we discard
the monoidal value of the other one and we discard the extra values
that cannot be paired up with ours because we need to zip those together
and we return this so we treat them as lists we zip them together
as lists so we cut the longer list when we do that and we discard
a coefficient at the longer list also keep a coefficient and the shortened
list if the two lists are the same length we don't discard them as
we use the monoidal operation from the coefficients and we just zip
the lists as before so in this way we can visualize how constructions
2 and 7 as well as constructions 1 and 6 as far as I remember correctly
yes as we need a constant factor sometimes we need the entity factor
sometimes and we need these two constructions so in this way we have
defined how they work through constructions and we have visualized
how they actually work on specific terms of these types so that certainly
is plausible that you could take any polynomial factor and more or
less mechanically transform it into this way into this form check
that all the coefficients are monoids and then generate noise the
Romero instance for each of them and then generate the applicative
instance for effect here are examples of polynomial factors but are
applicative because they are point omean so the first example is interesting
because this type constructor cannot be defined as a walnut cannot
have a model instance you can define the methods pure and flatmap
with the right type signatures but they will not satisfy the laws
there would not be associative 'ti and identity laws for an unsatisfied
no matter how you try people several people have verified this by
explicit calculations but it is applicative so the implicit of instance
is very easy and just this construction of adding Z to an applicative
factor which is a product of two identity and factors so obviously
this is an applicative factor also it\textsf{'}s interesting to look at this
as a composition of two functions one option so one plus something
is option and then a factor which is the pair a a so that\textsf{'}s clearly
a polynomial factor so the composition of option option of prae is
not am honored but option by itself is a monad and tear a a by itself
is also a moment so this is an example where composition of two units
is not among that the composition in the opposite order will be imminent
the pair of two models one plus eight times one plus eight that is
a moment but not in not composition in this order and in this example
is just polynomial is just to visualize what I mean by polynomial
with monoidal coefficients it\textsf{'}s like a polynomial but the coefficients
must be unknown so any types that have no parameter a in them so like
the Z that must be a monument now this this is a one that this this
factor I believe is a Mona but it\textsf{'}s not obvious how you need to find
Mona constructions but for example we have here a writer Monette obviously
and then you you have a plus so you multiply identity with the writer
Monat so that is a product construction for bullets and then you add
a that\textsf{'}s a three-pointed construction and that\textsf{'}s also a moment so
through finding a sequence of constructions starting with a writer
mu naught which we know is a minute we can show that this is a moment
without explicitly having to prove the blows hold and notice that
our examples of non applicative factors were all non polynomial so
they all had function in them christmas they're not polynomial factors
indeed there aren't any examples of polynomial but not applicative
factors that have no type parameters and only monoidal coefficient
is very easy to drop in something like this and say all this is not
monoidal and if i have a coefficient like this then obviously I don't
have applicative factors so that\textsf{'}s certainly would be an example so
polynomial factor with non monoidal coefficients that\textsf{'}s very interesting
perhaps but still it\textsf{'}s a valid example so let\textsf{'}s continue and take
a look at Concha funky constructions I already described how Concha
front was are defined so in the next and final portion of part 2 of
chapter 8 I will talk about the applicative control under constructions
as well as Pro factors and applicative profounder constructions 

the first construction is the constant factor where the type Z must
be alone right now this is exactly the same as we just had here against
construction six and in fact construction six was examined and we
have found construction six like this with we have verified the walls
but we did not actually use the fact that the type constructor was
considered to be a factor so we could have defined exactly the same
code without using this typeclass instance of furniture and in fact
I have defined also typeclasses that I pass control Mujib which is
the same methods except it expects a contra factor instead of factor
but other than that it\textsf{'}s exactly the same type signatures so the result
is that we actually don't need to prove any more than we already have
the previous proof goes exactly the same word for word for a contra
function here as it went for the function because we never used the
fact that we consider this type constructor to be a factor of this
type constructor is of course somewhat trivial it does not depend
on type a but never but this exactly the same logic will apply to
many of our constructions that we just considered four factors for
example the product construction is exactly exactly the same kind
the product construction is defined here for a factor but we actually
don't use a factor instance as a constraint when we define the zip
and the wrapped unit and if we look at the truth or associativity
and identity laws we never use map in any of these proofs we actually
never assumed that G or H are factors we assume that for example equivalences
can be established between these these values but these grow answers
can be established for contra factors equally easily as four factors
so for example for a contra functor if you if you want to compact
if you want to convert for example C of a C into c of a b c like this
this is an equivalence that is required in proven social TV t for
certain functions on constructions what you need is a function that
goes the opposite way from here to here and of course this function
exists it\textsf{'}s a trivial reordering of the tuples or as for functor we
needed a function that went in the other way but also that function
was clearly available so there is not another problem going through
exactly the same proof and just substitute in this kind of equivalence
which is exactly similar whenever we need equivalence between list
of tuples and also the equivalence between a tuple with unit value
and a single value which is necessary for it you know the identity
was the non-trivial construction is this one because there is no analogue
of this construction for applicative functors or for Mullens product
matter as we have just seen in general the disjunction of to placated
factors is not applicable in all cases and this junction of two monads
is not abundant in all cases so let\textsf{'}s look at the implementation of
this construction we use the cats library for contravariant which
is their name for contra factor and we define this type constructors
in either of G\&H assuming that both G and H are contravariant and
this is also controlling it this is how we establish that we define
continuity so contra map should map A to B using a function from B
to a well this goes exactly like in the functor case except we use
Concha maps instead of apps so if we're on the Left we use left and
we map in the G country furniture where on the right we return right
and we map in the H control factor so now we can use applicative instance
and we will use both contravariant and contract lucrative typeclasses
we actually will need that here\textsf{'}s how it goes so the wrapped unit
first needs to be defined so that\textsf{'}s a value of this type now here
we could return the left with wrapped unit of G or we could return
a right with wrapped unit of H and actually this choice is arbitrary
it could return a left or we could return a right we could then define
the zip accordingly and laws would hold the reason this is so is because
this construction is completely symmetric there is no difference between
G and H and so if we are able to define things with this choice and
just by swapping G and H we will be able to define this construction
with the right choice so let\textsf{'}s make this choice arbitrarily so that
we're on the left with wrapped unit so now how do we define a zip
we need to transform this and this into this if we are on the Left
then clearly we just turn the left we have two left G of a and G of
B we can just zip them together using the zip from the G country factor
and if we're in the right if both of them are on the right we can
just zip in the H country factor so that\textsf{'}s clear now what do we do
if one of them is on the left and the other is on the right well we
need to return either left of G or right of age what do we do well
we have now seen two cases when we had a disjunction and we implemented
the zip function for this Junction in each of these two cases what
we had to do is that if the wrapped unit was on the left then the
mixed case needs to be on the right if the wrapped unit wood is on
the right and the mixed case needs to be on the left and that was
kind of the pattern we have seen in the previous two examples so let\textsf{'}s
follow that pattern and we'll see that this actually works so how
do we return the right of H a B now we have only an H of B and we
have also a G of a now we can't possibly combine G and H there\textsf{'}s no
method for that so we have to ignore G and we have to transform H
of B into H of a B but that\textsf{'}s possible H is a control factor so we
can confirm mark it with this function that transforms a B I call
that x and y here but it would be easier negative read the code I
called it a and B are because then the types will be more clear so
I always have this function that transforms a B into B it actually
ignores eight so I can kill that and that function transforming a
B into B contri Maps HB into H of a B and that\textsf{'}s what I need I need
to transform HB into H of a B and so that\textsf{'}s why this code has the
right type and I do the same thing in the other mixed case I return
a right of H a and I contour map it like this so I transform H a into
H of a B so having implemented it let\textsf{'}s check the laws how do we reformulate
it so if we consider this kind of combination if all of them are on
one side so both all three on the right or all three on the left then
we we can do immediately we can see what happens it will be either
on the right of h HB HC whole zip or on the left of GH g BG c all
zipped so that\textsf{'}s clearly associative because we are now doing zip
into in the factor H or an F and Q G and that\textsf{'}s associative by assumption
now if some of these are on the left and others are in the right when
according to our code all the left ones are ignored and all the right
ones are control mapped so that they have the right type and the Contra
mapping is just with the trivial substitution of tuples so clearly
all the right ones are going to be just converted to this using the
trivial konchem up and then zipped together and so the result would
be something like this it will be a contra map with this where what
say C was on the left so it was ignored and a and B were on the right
so they were not ignored and then we we need to zip so that\textsf{'}s associative
because the condition that we are ignoring all the ones on the left
that condition is independent of the order which is no matter what
we add parentheses we put in first around these two or around these
two the condition of dropping all of these that are on the left that\textsf{'}s
associative not independent of the order of parentheses and then we
will come to map it finally into this type unzip them all together
{[}Music{]} that\textsf{'}s associative as well for with the zip ages out of
place here now identity laws are actually checked using a cop as an
explicit computation because you cannot just argue about it a lot
of some symmetry consideration we have to actually compute and verify
that our intuition was right that we we had a left here therefore
we need a right here - let\textsf{'}s check so let\textsf{'}s take some arbitrary FA
and zip it with the wrapped unit which is this and the result is that
we need to match according to this code you can match like this not
only two pieces remain because we have a left here if they're both
on the left we do a zip now this one GB is actually this so that\textsf{'}s
going to be equivalent if it\textsf{'}s on the right then we're actually ignoring
this GB according to our code we need to ignore this and we are on
the right we do a contour map which is like this and this is just
a contour map that is the isomorphism or the equivalents that we allow
so that\textsf{'}s the equivalence expressed by this symbol as I'm using it
and that\textsf{'}s equivalent to right away J cuz contour map with this isomorphism
is precisely the equivalence and so then right away J here we have
right over J here so identity was cold so clearly this would not hold
if we didn't have a right in this in these two places so in this way
we verify this construction now this construction says that for any
factor and applicative contrapuntal G this function is applicative
as contractor let\textsf{'}s see how that works this is the type constructor
so as a type function it\textsf{'}s contravariant because obviously this is
a factor by by assumption H was the front and this is a control factor
so the concha functor is here in a covariant position and functor
is a in a contravariant position so the result is contravariant to
implement that contravariance is easy you return a function which
takes HB now you have to compute some GFP so how do you do that you
can get G of B if you first get G of a and then control map it with
this so how do you get G of a you just substitute into a fey some
HIV but how do you get H of a you take H of B and map it with F because
H is a factor so that\textsf{'}s what we do we map HB with F substitute that
into FA and then confirm map the result with that and this could be
generated automatically for us now let\textsf{'}s implement the zip now we
are using the function instance on H to do that but we're actually
not using a contravariant instance on G for this let\textsf{'}s delete this
we are not using concern up in this code we're using map on H so we
need the function constraint on H but we don't need contraband two-factor
constraint on G for this code the wrapped unit it\textsf{'}s a value of this
type so how do we generate a function of this type role we can take
some H of unit but there\textsf{'}s no way for us to use that H of unit to
make G of unit we already have G of unit anyway it\textsf{'}s the wrapped unit
so let\textsf{'}s ignore the argument and return that wrapped unit how do we
do this so that\textsf{'}s a bit of a complication we have an H of a - G of
a H of B 2 G of B and we need to return this function so let\textsf{'}s return
this function so we return a function that takes HJ b and returns
G a B so how do we get a GLB well clearly we need GA and GB for that
so we need H a and H B so that we can substitute those into F a and
every how do we get h + HP while we take H a B and project out side
B we drop the beat so that\textsf{'}s just a map with this function look drops
the be out of a tuple we could write this function more concisely
like this but anyway so that\textsf{'}s how we do that so now we get H a I
get H beam and we obtain G of a we obtain G of B and we zip them in
the country function G we're done what type is correct what seems
like it was the only possibility to implement these types let\textsf{'}s check
the laws so here it\textsf{'}s hard to reason about these these values in some
hand waving in fashion and and we formulate them explicitly it has
when you firstly applicative I'm sorry manifestly associative it\textsf{'}s
it\textsf{'}s hard let\textsf{'}s write down the code so let\textsf{'}s consider this expression
first so this expression has this type from this to this we write
down those things we just in line just in line hjb map 1 HK be mapped
to I in line them so let\textsf{'}s now zip this with FC so that would be more
complicated we have FA zip FB and then we have apply this function
because this is this 2h a BC map one zip FC h ABC map 2 so if we just
substitute the definition of zip again and we have this expression
now notice we have here H ABC map 1 map to map one map 1 map 1 map
2 and H ABC map to now our H ABC is actually of this type so it has
a nest tuple let\textsf{'}s simplify that let\textsf{'}s map this each ABC as the flat
tuple and convert it into our non flat tuple first so we convert it
and then we would have these expressions which are basically just
taking first element of this tuple which is this then again taking
the first element which is this so this is basically projecting ABC
onto a which is to be equal to that so that simplifies our expression
into into this and then we contra map the result in in this way so
that we know well that that will transform the nested to fall back
into non-listed since it\textsf{'}s a country map so here\textsf{'}s the code that results
from this operation we start from each ABC which is flattened and
then we I just substituted all of this country mapped at the end and
then I have simplified these things so I have F a of this zip FB of
this zip FC of this and so I first have to zipped together and then
a zip it with this FC now the last step is actually zip in the G contractor
because this is zip between values of F of Sun and F is a function
from H to G so this zip is associative and I'm just applying some
kind of a isomorphism which is equivalence don't care about that I
can always apply it whenever necessary and so the main result is that
I have here at this expression that is manifestly associative by assumption
because this is a zip in that country functor G and everything else
is perfectly symmetric so if a HIV seen that one as being HIV seen
up to FC hn be seen up straight so there\textsf{'}s no asymmetry and so I expect
when I start with the other order of parentheses to get exactly the
same thing with different parentheses and so then because of associativity
of G these two are equivalent and these two rows I can always add
or remove whenever necessary so that verifies the associativity law
and let\textsf{'}s look at identity the identity law is that this zip of some
arbitrary FA with this function that we define that always returns
wrapped unit of G ignoring its argument so let\textsf{'}s find out what that
does so the code is like this so we have some H a and H beam that
we define right here and then we do FH a zip F PHP now HB is actually
this but it\textsf{'}s the function FB that is acting on it is this function
that ignores HB so it means actually ignored and result is always
this so the result is going to be fa h a zip this now this is the
zip in the g control function which we assume satisfies the identity
law and therefore this is just mapped into G of a into G of a unit
which is the awesome orphism and so basically we start with function
FA and we have a function that takes H a B maps it into H a and applies
FA to that so that\textsf{'}s basically if you look at how its how its map
its mapped using this isomorphism which is going to be the isomorphism
between a common unit and a that\textsf{'}s our equivalence and so that\textsf{'}s basically
H a B is equivalent to H a and therefore and so FA of H a is the same
as FA of HIV well of the equivalents and so we take FA and we return
a function that takes H a and applies Ephrata HJ so we take a favorite
turn effect so that\textsf{'}s identity so therefore this returns a function
that\textsf{'}s equivalent to FA up to there is some morphisms that we have
such as this one and so the left identity the right identity holds
that we just found and left identity holds in the same way just H
beans to the AJ so so much for this construction now this construction
is a functor G at the contrapunto edge so if we look at the corresponding
construction for functors this one you see we are not using the funk
terminology you're using functor on the edge actually I think that
is a subtype this is a mistake so we use a map on on G G must be a
functor but we are not using map on H so we need to we can delete
this we don't need type constraint constraint that each is a factor
and when we check the laws we use map but always on G so we we take
out these maps but these are values of the function G so we never
use map on H and here also we use a map but only for G you never use
map on any values of H and because of this the proof that we gave
for this construction for functors where G was a functor and H also
a functor now G must remain a funkier because were using map on G
all over the place but it doesn't have to be a frontier it could be
a country functor so this construction is very similar to this one
and the proof goes through exactly the same because we don't actually
need the functor property of H we only need the lucrative property
but not the functor property so we are not using map or country map
on each hello they're only using zip and rapped in the proof will
go through exactly the same word for word so these are the constructions
that I was able to find for country factors now the interesting thing
here is that we have constructions that have constant factor we don't
have identity factor because it\textsf{'}s a con is not a country factor it\textsf{'}s
a factor but we have a constant culture factor we have product disjunction
or some and function or exponentiation so we have exponential polynomial
constructions all of them are here we also have composition but this
is not necessary if we have these three constructions we can come
we can build up an arbitrary exponential polynomial culture factor
with monoidal coefficients as long as we have these all the constant
types that occur must be money or it\textsf{'}s so this is what means to have
constant coefficients for exponential polynomial country factors and
then we have covered all possible country funders so essentially all
exponential trinomial country factors with manorial coefficients have
an applicative instance through these constructions so as long as
we found any exponential polynomial country factor with monoidal coefficients
we know it\textsf{'}s applicative there\textsf{'}s no there\textsf{'}s no question and no counter
examples of non duplicative such country factors so this is very interesting
so we did not have this four factors that all exponential point normal
are factors but that\textsf{'}s the example I have seen so I was trying to
do counter example so this is a counter example four factors this
is not applicable but if you reverse these two arrows you get a contra
function and it\textsf{'}s applicative and that we have seen that it it had
implementations and we know why it\textsf{'}s this construction so the conclusion
is that all the clickety of contra ventures are basically you can
write down are going to be exponential polynomial contra factors and
vice versa all the exponential polynomial contra factors are going
to be negative now let\textsf{'}s talk about true factors in the first part
of this chapter we have seen type constructors that are not functors
but applicative and these were option actually also not control factors
and there are two main examples that we have seen one was the typeclass
from annoyed and the other was the fold the type transfer fold or
rather the type just the data structures were fold we had full diffusion
and the data structures were fold the first variant of it was neither
a function or a contra factor and yet it was applicatives we could
have an negative Combinator for it so those are proof factors informally
speaking pro functors are type constructors that have the type parameter
and the type parameter occurs both in contravariant and covariant
positions because it occurs in both of these positions you cannot
have a map and you cannot have a contra map for this type parameter
here\textsf{'}s an here some examples typically this would be an example so
you have a function from this to this and the type parameter occurs
both in contravariant and covariant positions another example you
have a disjunction or some type sum of a and a function but so here
a is covariant and here is contravariant and so these are typical
Pro factors what would be an example of a non-pro factor no clearly
anything you can write using exponential polynomial operations there\textsf{'}s
going to be a pro factor because the type type parameter is going
to be either on the right or or on the left of the function in room
and the only way to have a non-pro factor is to have a non exponential
polynomial type constructor an example this would be a so-called generalised
generalized algebra data type which I find not a helpful name but
these are basically I would say these are type functions that are
partial partial type functions we have seen that concept in the chapter
on typeclasses these are just partial type functions they are not
defined on all types they're only defined on certain types in in some
specific ways and here\textsf{'}s an example in Scala so you have a trait which
is parameterize by type a but there are only two case classes that
that implement this trait and they have specific type parameters values
here int and unit and so it means you cannot instantiate a value of
type say F of double it\textsf{'}s impossible you can only instantiate F of
int and F of the unit and as a result you also cannot instantiate
F of a for arbitrary a and so there\textsf{'}s no hope for you to have napkins
from the app or anything like that because map and confirm happily
require you to be able to transform F of A to F of B for arbitrary
a and B and you can't even instantiate those F of a and F would be
for arbitrary amb and so no hope for that kind of property for these
types so these are partial type functions no hope for them to satisfy
good properties I'm trying to invent a good notation for these kind
of type functions I'm not sure this is a good notation but this basically
means a string but it is ascribed that this type F event and an int
but it\textsf{'}s considered of type F of unit just by by hand we just say
this is a for unit and this is evident maybe this notation is useful
but in any case at this point I have very little to say about these
partial type functions except that they are not true factors this
is an example of a non-pro factor now notice I have not yet actually
give it even a good definition of a profounder I've Illustrated what
I wanted to achieve these kind of things are pro functors and have
a in a certain position but this is not a good definition because
it depends on being able to write a type in a certain dessert away
and what if we don't know how to write it in a certain way maybe what
if this is actually a profounder just I don't know how to write it
correctly we need a better definition a rigorous definition there\textsf{'}s
a typo I'll fix it a rigorous definition or a profounder which is
that it is a type function with two type parameters such that it is
a contra filter in a and the factor in B so a is contravariant b is
covariant if such a type function exists that PA is defined like this
when we put a and to be the same type then that type function is a
pro factor so for each Pro functor there must be a way to split it
into a function of type function of two type parameters one of them
is strictly called contra variant and the other strictly covariant
and if you can do that then defining P like this then P is a contra
fun it is a pro factor that\textsf{'}s the definition of a pro factor so obviously
we can do this here we can say well this is going to be a and this
is going to be B in the Q so we define Q of a be like this so Q is
1 plus int times a going to be and here we put here be because this
is covariant and here a because this is contra here so that will become
the Q that corresponds to the P so basically the idea of this definition
is that a pro functor is a type function where really you can split
it into a function with two type parameters each purely covariant
or purely contravariant and that\textsf{'}s and then then obviously for the
contravariant you have a conscience where the covariant you have a
layup with the Loess separately holding for a and for me with a usual
laws and since you have that then you can apply something called X
map which is just a combination of map and contra map map in being
in control map in a accepted then after that you said the same type
triangle the result is that in order to map a profounder from one
type to another you need functions from A to B and from B to a going
in both directions if you have that and you use this in the map use
this in the Contra map for Q it\textsf{'}s a bit confusing here I use a and
B both for this and for this so I will fix this here I would use this
as x and y instead of a you'll be on the temp side x and y then PX
is and where P a is Q a h2o but this is going to be x and y so then
this end B plays very different role from this so because we assumed
that Q exists and so it has a map and a contra map then obviously
P will have an X map which is called in the scholars Eli rains the
cats lair is called I map not sure what is a good name but let\textsf{'}s go
let me call it X map so this will be defined this follows from a definition
and the laws identity and composition laws oops there is a mistake
must be G 2 followed by G 1 because their composition for the contravariant
art is the opposite order okay I'll fix that in the slide so this
is G 2 and G 1 the important idea is that once you have this cue it
means you already have map in B and country map in a and the ones
for those called separately and so it can be derived but these laws
hold because they're just positions of those functions and so therefore
these laws can be seen as consequence of this definition and it\textsf{'}s
not a new a new assumption or new requirement it\textsf{'}s a consequence of
the definition and so that\textsf{'}s why we believe it\textsf{'}s reasonable to impose
these laws so as I said all exponential polynomial type constructors
are pro founders because in all of them there will be type expressions
of this kind the type parameter must be either on the right or on
the left and so then you can easily relate the covariant and contravariant
by different type parameters and then we can define the Q that you
are required to have and then you have your pro functor defined and
you prove that easily just applicative proof functor is defined in
exactly the same way using zip and wrapped unit with the same boss
there is no corresponding app method but there is a pure method from
a to P because you can take wrapped unit which is p1 and you can X
map it with a function from a to one and the function from 1 to a
which you will always have since you have an a then you have a function
from 1 to a and the function from a to 1 you always have and so you
can X map your rapped unit from P 1 to P a given a so that\textsf{'}s the pyramid
but you cannot derive app or any analog event I believe so what are
the constructions all the previous constructions still work because
profounder is a superset of both a functor and contra factor is any
factor is a profounder any control hunter is a pro factor is a superset
of both of them so all of the constructions we had before also cold
as proof under constructions in the trivial sense however there are
more construction so the product construction again then there is
the monoid addition then there is the free pointed and a function
now a function works and all these constructions work in exactly the
same way as they work for the factor and this works in exactly the
same way as it works for contra factor you see here it\textsf{'}s a functor
here it\textsf{'}s not it\textsf{'}s not control factor it\textsf{'}s a factor it\textsf{'}s important
it doesn't work doesn't seem to work for control factors really confer
pro factors I tried but I couldn't make this construction work I could
implement the types of zip and the wrapped Union but I could not get
associativity law to hold for for this kind of construction so I don't
think this construction works but these constructions certainly work
in the composition also in order to find that they work you don't
need any more proofs actually you just look at your old proofs and
you find that you aren't using the property of G as functor or contra
factor or anything you don't use map on G when you do these things
you don't use contra map either in this construction we use pure but
you have pure for the pro functor in this construction we use only
zip and wrapped in it and nothing else with this construction you
only use zip and wrapped unit in this construction you only use a
functor for F but you don't use anything for Q except wrapped in it
and zip and similarly here and so all these constructions actually
go through with no further proofs necessary I was able to find out
that these constructions work by pretty much try on air I hope I found
all the important constructions and that exists improves so for example
here we don't use we use map on H but we don't use anything on G we
only do mark on each not never on G so I'm pretty sure that this construction
does not work and I'm pretty sure these constructions all work because
these proofs do not use properties of pro factor other than zip and
wrapped unit let me just check the sum of a and govt so this this
is construction that uses pure so let me just look at the proof of
that construction here this it does not use the functor instance on
G just uses the zip and rap unit there\textsf{'}s never any map there\textsf{'}s a pure
so we need a pure on G that\textsf{'}s true and it needs to satisfy identity
law that\textsf{'}s true other than that we do not use map on anywhere so we
we use the equivalences yes but those are available for pro factors
in the same way they are available for country factors we can for
example use x map and rearrange tuples in the forward direction in
the backward direction and then this will give us a rearrangement
of tuples in the type of the pro factor so yeah so to summarize these
are the constructions i believe exist for applicative functors contra
hunters and cro factors now i'd like to have a little comment on an
interesting property of applicative factors which is symmetry or commutativity
but before symmetry because it\textsf{'}s not really committed to beauty literally
speaking monoidal operation can be commutative or symmetric with respect
to the Pregnant\textsf{'}s it\textsf{'}s probably better to say cumulative it\textsf{'}s just
this property now if you want to apply the same property to zip so
you I would use this notation for the zip operation you cannot literally
say it FA of being must be equal to if BFA because there are different
types so you need to map the types by rearranging the types because
this is going to be of type F of a B and this is going to be of type
F of B a but if you implicitly say that this isomorphism is included
in the equivalence and you could write it like this so that is the
same symmetry property not not all applicative factors are symmetric
what does it mean that it is symmetric well it means that the effects
somehow are independent in such a way that the second effect is independent
of the first now in the implicative factor the second effect is independent
of the value returned by the first container of the value it is an
independent but it may not be independent of the effect and if it
is so then this symmetry will not hold but there are some examples
where it holds for example list is symmetric manifestly so because
you can just permit the list dominance a good example of non symmetric
applicative frontier is parsers we have looked at this briefly parsers
are not symmetric because when you do applicative composition of parsers
the first parser already might have consumed some part of the input
string and the second parser starts from the place left over by the
first parser well the first parser stopped and so the second parser
depends on this effect now the value returned by the first parser
does not indicate necessarily a position where it stopped the value
is the value it parsed out of the input and the second part is independent
of that value but it depends implicitly on where the first parser
stopped and so the parser combination using applicative composition
would not satisfy this requirement of symmetry also if you define
anything through the mu net most likely it\textsf{'}s not going to be symmetric
well for lists we know it is not symmetric we have seen that the order
is different in the result when you define zip through through the
moment but we know that for lists the mu not defined zip is not usually
what you want you want applicative defined zip which is incompatible
for lists all polynomial factors with monoidal coefficients that are
symmetric will be symmetric applicative entries because we have seen
how polynomial factors combine their values and then so the minute
they put commute these elements in a different order if you change
the order of these two so the moon at zip is not symmetric now the
polynomial one is symmetric because it basically combines elements
like this so if you put this first it doesn't matter it will still
take these two and combine with these two so similarly here so the
typical polynomial factor with symmetrical B with symmetric when one
of the coefficients like integer which is a symmetrical mono at boolean
of symmetrical node with strings and string concatenation is not as
commutative monoid so that would not be commutative and so the polynomial
factor with strength coefficients would not be a symmetric or neither
or commutative may be gathered aside but I've seen usage of the word
symmetric so I would say the commutative or symmetric negative funky
will be probably the same meaning now it\textsf{'}s interesting to say that
first of all most of our constructions preserve symmetry so if if
you think about how we defined all these constructions of the last
portion there was deep in this functor and it was symmetric in how
you provided arguments for this zip so if G is symmetric then this
is also symmetric and G is symmetric this is symmetric if G and H
are symmetrical commutative then this is also commutative and so on
so there are some constructions that would not be symmetric usually
coming from units but all these constructions are symmetric so if
you have symmetric coefficients and if you have symmetric applicative
factors and you combine them using any of these constructions that
we have seen and you get again symmetric wickety factors and control
factors and pro functions work exactly the same way and they have
the same commutativity property because well we have formulated the
cumulative 'ti with symmetry property using this which is a map but
if this were a contra factor that we would just use contra map here
and this function is an isomorphism so it is available in both directions
and so we can use X map as well because we have this in both directions
commutativity or symmetry makes it easier to prove associativity for
negative factors I have not used this in my proofs but mostly because
I have specific instructions I didn't want to assume symmetry but
if you had a commutative applicati factor that you could just rearrange
this into this by permuting and and permitting this with this so you
have that which is almost what you need to prove associative you just
need to swap FC and FA and so that\textsf{'}s perhaps much less work to do
that we just compute this and then you swap FA and I've seen it and
you demand that the result be the same up to a swapping of types and
that\textsf{'}s less work for symmetry for symmetric code then another wise
that\textsf{'}s otherwise you'd have to first compute this separately then
you compute this separately but in most cases it\textsf{'}s significantly easier
to prove I think it was proved that we can see so I would like to
finish this tutorial with an overview of standard filter classes using
category theory so strictly speaking this is just kind of a theoretical
perspective which doesn't add much practically to programming it\textsf{'}s
more or less just a justification as a an explanation of why these
factors exist why these specific properties exist and why we have
assurance that we have the right laws for them and here\textsf{'}s how it goes
so consider typeclasses such as functor filterable monad applicative
contra functor and so on each of these when we consider them depth
the laws for them we found a function with type signature that looked
like a lifting and by lifting I mean it was a function of higher order
that a function like this and produced another function and the function
on the left was of one type and the function on the right was of a
different type and on the right it was in the function f so always
this lifting took something and produced a function from FA to FB
so it took this produced a fatal FB took this was his flat map produced
a fatal be missus app it took this produced a fatal of me contra factor
took this produced a fatal FB this should be X map rather than Dyna
prevented own I will fix this in the slides it took something it gives
a fate of B and so on so this is what I mean by lifting it is this
kind of functions type signature that takes some kind of function
as argument and returns this kind of function FA to FB let\textsf{'}s look
carefully at the types of functions that are taken as arguments by
these liftings all of these our functions of a types contain a and
B their functions from something to something that contains a and
B in some way this is what we consider to be a category in the previous
lecture namely it\textsf{'}s a description of what is the type of a function
that we twist so we kind of twist a function type we don't just take
a to be like here we twist it a bit we add something on top of it
to be like or we can even reverse the area we don't have B to a instead
of A to B or we have a to F of B or we have a to one plus B or B to
1 plus a or somewhere so it\textsf{'}s kind of twisted it\textsf{'}s a twisted function
type and so all these liftings take a twisted function type and give
me a function type in the functor so functor transformation second
thing we notice is that when we looked at the laws that need to be
acquired for these typeclasses each of these strange twisted function
types had identity and composition laws there was always a way to
compose two of these and get another one of those so for example we
composed a to B and B to C and we got a to C we can post a to one
plus B beta 1 plus C and we got a to 1 plus C we can pose a to FB
b to FC we get a 2 FC we can pose F of A to B and F of B to C we get
F of A to C and so we compose functions of this type also we had an
identity function of this twisted type here it was the ordinary identity
this was a pure option that creates so 8 goes to 0 plus a so it creates
a sum away this was the pure function of the mullet this was the pure
of identity this is identity we did not consider those things in this
way but they have exactly the same structure there is an identity
function of this twisted type and there is a composition for functions
of twisted type so in our simplified definition of category that\textsf{'}s
what category is is a twisted type for functions such that we can
compose these twisted functions and we have a twisted identity so
here in applicative i noted it like this this is the twisted identity
and this is a twisted composition and so for example a function of
a value of type f a to b can be composed with other values of type
say F B to C and you get a value of type F a to B and F a to sir sorry
so that\textsf{'}s the general scheme of things it seems for each of these
examples we have this twisted function type a twisted composition
of these twisted function types and the twisted identity and the laws
of identity and composition hold in other words composition of identity
and something is equal to that something and composition is associative
these are the axioms of a category the category laws so to speak a
category is described by saying what is the type of the twisted function
that it has a twisted function is also called morphism so what is
the type of morphism that the category has this type should somehow
depend on a and B so there must be a and B in this type somewhere
other than that we don't know what that type is it could be many different
type expressions having a and B in it and it better be some kind of
function it could be like this as well it\textsf{'}s not clear exactly what
qualifies is fun is this a function we don't know one example of applicative
is a monoid with no dependence of type parameter that\textsf{'}s what the functions
prickly speaking but it doesn't matter a morphism is just a type such
that you have an identity element of this type and the composition
of elements of this type and the type is indexed by two type parameters
a and B what\textsf{'}s all it needs to be it\textsf{'}s just the morphism is a type
expression depending on two parameters and that\textsf{'}s it each category
has its own different definition of what that morphism is some categories
have names this category is the plain type category which is not twist
it\textsf{'}s just a function it was not Christian so this is the category
of ordinary functions this is a class Li category of the function
f this is the class the category of the font option so it\textsf{'}s a specific
example actually of this with the option is a specific F this is the
applicative category but that\textsf{'}s just what I call it there is no accepted
name for it it seems this is the opposite category category that all
functions with opposite type signature I don't know what this is called
this is the opposite to option class Lee so the generalization that
category theory cell gives us here is that all of these typeclasses
are seen in one kind of way you define them by specifying the type
of the morphism defines the special some special category that you're
interested in automatically you demand laws the category laws is also
you specified identity morphisms once you have specified this and
demanded that laws of category and also natural T there must be in
all of these naturality laws which relate like we have here in here
the naturality laws which relate this twisted composition and the
ordinary usual F map so that must be their natural T because we are
now we need to always relate this category and the category or working
with so but once you specify these was which are always the same always
the same laws so they're not different somehow in each in each case
there are the same laws just for a different category it\textsf{'}s a very
powerful generalization where you basically say everything is fixed
except for the type of the morphism and you can choose that type in
many different ways and you get different typeclasses automatically
with all the period clause automatically chosen for you remember the
filterable functor had four laws which were kind of ad hoc if you
look at them like that moon had also had laws that we kind of guessed
applicative we kind of guessed what these laws must be but if you
look at those from the category point of view they're not at all arbitrary
once you specified this type expression for the morphism type or the
twisted function type everything is fixed all the laws are fixed you
give you derive all the possible constructions for the factors that
satisfy these laws all the laws for example for zip and wrapped unit
are equivalent as we have shown to the category laws for this category
and the same thing we have shown in previous chapters about filter
balls and moments the category laws are equivalent to the laws obtained
previously in terms of different functions so because the category
laws are always the same it\textsf{'}s just that the types are different we
have assurance that we found the right laws that all these classes
are somehow correctly defined we didn't forget some law for filterable
also we didn't have too many laws we have exactly the laws we want
and that\textsf{'}s one thing the second thing we we obtained from the systematic
picture is we can try to generalize and find more typeclasses I mean
we have found these typeclasses are there any more have we forgotten
some interesting typeclasses how do we generalize well in this scheme
the only way to generalize is to change the type of morphism in the
category and let\textsf{'}s see we had a to be we had a to F of B we had F
of a to b but we didn't have F of a going to B we didn't have that
why not indeed excellent question and if we consider that as the type
of the categories morphism and demand that these functions twisted
functions or morphisms have a composition law and identity one we
obtained something called the comonad we have not yet considered comonad
and we did not find any other motivation to consider it but this is
a this is a motivation that kind of falls out of this consideration
so we have a to be it says a to FB with some specific f we could take
some other specific F and see what happens then maybe it\textsf{'}s not something
interesting and I've generalized filterable so from option to something
else maybe it\textsf{'}s useful maybe not monad applicative now doesn't seem
to be any other way of putting F on here we already exhausted everything
we have a to be we have F a to F of B F of a to b f a to B now let\textsf{'}s
reverse we have B to a we have B to one plus a what about B to F of
a what about F of B to a and what about F B to a I tried some of them
and they don't work so actually this scheme does not always work so
you can't choose an arbitrary type expression here and expect it to
work in what in what says it do I say that it does not work in the
sense that you cannot find a composition law for this category that
would at the same time fit with this scheme so because what you want
is not just some arbitrary category with some arbitrary composition
law but you also want to lift it then to your functor and that\textsf{'}s the
that\textsf{'}s what doesn't work so you basically find that there aren't any
factors that satisfy this property so another thing that is not great
about it is that some typeclasses don't seem to be covered by this
scheme such as for example contra applicative and pro functor replicative
i haven't been able to find a formulation of them using this scheme
so maybe there is a trick that I'm missing but right now and especially
since for example contract leakages don't have an app they don't so
you cannot write like this you know you could think contra black Egyptians
contra functor means this category and applicative means this category
contra placated means f of B to e no so you can't have this kind of
thing with F of B to a so the conclusion is we have a very interesting
and general scheme that shows that there are some typeclasses like
these that are in some sense natural there\textsf{'}s some sense they are all
part of one approach which is define a category lifts from that category
morphism to this morphism for some functor f derive the laws from
the category laws we must have here liftings laws our function was
they're fixed as well I didn't talk about this but the laws for this
are fixed because it\textsf{'}s of just a functor from one category to another
and so the laws are identity must go to identity composition must
go to composition so again no freedom in choosing the laws we have
fixed laws they're fixed laws here we derive then what are the properties
of the function f such that these laws hold and we formalize this
as a typeclass so in this way we can make a case that all these classes
all these typeclasses are in some sense standard they are all obtained
from the same method and their laws are fixed so there we have good
assurance that we we have found the right louis and the right typeclasses
and we can generalize so communal is one example that follows from
this with this choice of the morphism type some possibilities like
contra monad you know when you try to do moon ad and do B to F FA
for example it doesn't seem to work and some classes like contra functors
applicatives don't seem to be covered by risk but other than this
seems to be a very powerful generalization and an elegant way of conceptualizing
and justifying that the laws are correct and understanding why the
laws must be like that and how to analyze those factors and so for
each of these types we have thoroughly analyzed what factors have
the properties that they have that they must have and in the later
tutorial we might do the same for como nuts so this is what category
theory brings it\textsf{'}s kind of an conceptual generalization it is not
so much in terms of specific code that we couldn't write until we
saw this table but in a certain sense it shows a direction in which
we could go on and that\textsf{'}s so far what I found category theory gives
so to finish off this tutorial here are some exercises for you along
the lines of what we have been doing and some of these exercises require
proof and others don't indicate where so here you do need to prove
and here you don't here you don't and here you do good luck so this
concludes the tutorial in Chapter eight 
\end{comment}

